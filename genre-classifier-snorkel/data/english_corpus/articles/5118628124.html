<!-- <DOCUMENT>
	<FILE>
		5118628124.html
	</FILE>
	<URL>
		http://cslu.cse.ogi.edu/HLTsurvey/ch4node3.html#SECTION41
	</URL>
	<TITLE>
		4.1 Overview
	</TITLE>
	<GENRE>
		articles
	</GENRE>
	<PLAINTEXT>
 4.1 Overview Next: 4.2 Syntactic Generation Up: 4 Language Generation Previous: 4 Language Generation Chapter 4: Language Generation 4.1 Overview Eduard Hovy University of Southern California, Marina del Rey, California, USA The area of study called natural language generation (NLG) investigates how computer programs can be made to produce high-quality natural language text from computer-internal representations of information. Motivations for this study range from entirely theoretical (linguistic , psycholinguistic ) to entirely practical (for the production of output systems for computer programs). Useful overviews of the research are [ DHRS92 , PSM90 , Kem87 , BH92 , MS87 , MBG 81 ]. The stages of language generation for a given application, resulting in speech output, are shown in Figure 4.1 . Figure 4.1: The stages of language generation. This section discusses the following: the overall state of the art in generation, ignificant gaps of knowledge, and new developments and infrastructure . For more detail, it then turns to two major areas of generation theory and practice: single-sentence generation (also called realization or tactical generation ) and multisentence generation (also called text planning or strategic generation ). 4.1.1 State of the Art No field of study can be described adequately using a single perspective. In order to understand NLG it is helpful to consider independently the tasks of generation and the process of generation. Every generator addresses one or more tasks and embodies one (or sometimes two) types of process. One can identify three types of generator task : text planning, sentence planning, and surface realization. Text planners select from a knowledge pool what information to include in the output, and out of this create a text structure to ensure coherence. On a more local scale, sentence planners organize the content of each sentence, massaging and ordering its parts. Surface realizers convert sentence-sized chunks of representation into grammatically correct sentences. Generator processes can be classified into points on a range of sophistication and expressive power, starting with inflexible canned methods and ending with maximally flexible feature combination methods. For each point on this range, there may be various types of implemented algorithms. 18 The simplest approach, canned text systems , is used in the majority of software: the system simply prints a string of words without any change (error messages, warnings, letters, etc.). The approach can be used equally easily for single-sentence and for multi-sentence text generation . Trivial to create, the systems are very wasteful. Template systems , the next level of sophistication, are used as soon as a message must be produced several times with slight alterations. Form letters are a typical template application, in which a few open fields are filled in specified constrained ways. The template approach is used mainly for multisentence generation , particularly in applications whose texts are fairly regular in structure such as some business reports. The text planning components of the U.S. companies CoGenTex (Ithaca, NY) and Cognitive Systems Inc. (New Haven, CT) enjoy commercial use. On the research side, the early template-based generator ANA [ Kuk83 ] produced stock market reports from a news wire by filling appropriate values into a report template. More sophisticated, the multisentence component of TEXT [ McK85 ] could dynamically nest instances of four stereotypical paragraph templates called schemas to create paragraphs. TAILOR [ Par93a ] generalized TEXT by adding schemas and more sophisticated schema selection criteria. Phrase-based systems employ what can be seen as generalized templates, whether at the sentence level (in which case the phrases resemble phrase structure grammar rules) or at the discourse level (in which case they are often called text plans ). In such systems, a phrasal pattern is first selected to match the top level of the input (say, [SUBJECT VERB OBJECT]), and then each part of the pattern is expanded into a more specific phrasal pattern that matches some subportion of the input (say, [DETERMINER ADJECTIVES HEAD-NOUN MODIFIERS]), and so on; the cascading process stops when every phrasal pattern has been replaced by one or more words. Phrase-based systems can be powerful and robust, but are very hard to build beyond a certain size, because the phrasal interrelationships must be carefully specified to prevent inappropriate phrase expansions. The phrase-based approach has mostly been used for single-sentence generation (since linguists' grammars provide well-specified collections of phrase structure rules ). A sophisticated example is MUMBLE [ McD80 , MMA 87 ], built at the University of Massachusetts, Amherst. Over the past five years, however, phrase-based multisentence text structure generation (often called text planning ) has received considerable attention in the research community, with the development of the RST text structurer [ Hov88a ], the EES text planner [ Moo89 ], and several similar systems [ Dal90 , Caw89 , Sut93 ], in which each so-called text plan is a phrasal pattern that specifies the structure of some portion of the discourse, and each portion of the plan is successively refined by more specific plans until the single-clause level is reached. Given the lack of understanding of discourse structure and the paucity of the discourse plan libraries, however, such planning systems do not yet operate beyond the experimental level. Feature-based systems represent, in a sense, the limit point of the generalization of phrases. In feature-based systems, each possible minimal alternative of expression is represented by a single feature; for example, a sentence is either POSITIVE or NEGATIVE, it is a QUESTION or an IMPERATIVE or a STATEMENT, its tense is PRESENT or PAST and so on. Each sentence is specified by a unique set of features. Generation proceeds by the incremental collection of features appropriate for each portion of the input (either by the traversal of a feature selection network or by unification ), until the sentence is fully determined. Feature-based systems are among the most sophisticated generators built today. Their strength lies in the simplicity of their conception: any distinction in language is defined as a feature, analyzed, and added to the system. Their strength lies in the simplicity of their conception: any distinction in language can be added to the system as a feature. Their weakness lies in the difficulty of maintaining feature interrelationships and in the control of feature selection (the more features available, the more complex the input must be). No feature-based multisentence generators have been built to date. The most advanced single-sentence generators of this type include PENMAN [ Mat83 , MM85 ] and its descendant KPML [ BMTW91 ], the Systemic generators developed at USC/ISI and IPSI ; COMMUNAL [ Faw92 ] a Systemic generator developed at Wales; the Functional Unification Grammar framework (FUF) [ Elh92 ] from Columbia University; SUTRA [ VHHJW80 ] developed at the University of Hamburg; SEMTEX [ R86 ] developed at the University of Stuttgart; and POPEL [ Rei91 ] developed at the University of the Saarland. The two generators most widely distributed, studied, and used are PENMAN/KPML and FUF. None of these systems is in commercial use. 4.1.2 Significant Gaps and Limitations It is safe to say that at the present time one can fairly easily build a single-purpose generator for any specific application, or with some difficulty adapt an existing sentence generator to the application, with acceptable results. However, one cannot yet build a general-purpose sentence generator or a non-toy text planner. Several significant problems remain without sufficiently general solutions: lexical selection sentence planning discourse structure domain modeling generation choice criteria Lexical Selection : Lexical selection is one of the most difficult problems in generation. At its simplest, this question involves selecting the most appropriate single word for a given unit of input. However, as soon as the semantic model approaches a realistic size, and as soon as the lexicon is large enough to permit alternative locutions , the problem becomes very complex. In some situation, one might have to choose among the phrases John's car, John's sports car, his speedster, the automobile, the red vehicle, the red Mazda for referring to a certain car. The decision depends on what has already been said, what is referentially available from context, what is most salient, what stylistic effect the speaker wishes to produce, and so on. A considerable amount of work has been devoted to this question, and solutions to various aspects of the problem have been suggested (see for example [ Gol75 , ER92 , MRT93 ]). At this time no general methods exist to perform lexical selection . Most current generator systems simply finesse the problem by linking a single lexical item to each representation unit. What is required: Development of theories about and implementations of lexical selection algorithms, for reference to objects, event, states, etc., and tested with large lexica. Discourse Structure : One of the most exciting recent research developments in generation is the automated planning of paragraph structure. The state of the art in discourse research is described in chapter 6. So far no text planner exists that can reliably plan texts of several paragraphs in general. What is required: Theories of the structural nature of discourse, of the development of theme and focus in discourse, and of coherence and cohesion; libraries of discourse relations, communicative goals, and text plans; implemented representational paradigms for characterizing stereotypical texts such as reports and business letters; implemented text planners that are tested in realistic non-toy domains. Sentence Planning : Even assuming the text planning problem solved, a number of tasks remain before well-structured multisentence text can be generated. These tasks, required for planning the structure and content of each sentence, include: pronoun specification , theme signaling , focus signaling , content aggregation to remove unnecessary redundancies, the ordering of prepositional phrases , adjectives, etc. An elegant system that addressed some of these tasks is described in [ App85 ]. While to the nonspecialist these tasks may seem relatively unimportant, they can have a significant effect and make the difference between a well-written and a poor text. What is required: Theories of pronoun use, theme and focus selection and signaling, and content aggregation; implemented sentence planners with rules that perform these operations; testing in realistic domains. Domain Modeling : A significant shortcoming in generation research is the lack of large well-motivated application domain models, or even the absence of clear principles by which to build such models. A traditional problem with generators is that the inputs are frequently hand-crafted, or are built by some other system that uses representation elements from a fairly small hand-crafted domain model, making the generator's inputs already highly oriented toward the final language desired. It is very difficult to link a generation system to a knowledge base or database that was originally developed for some non-linguistic purpose. The mismatches between the representation schemes demonstrate the need for clearly articulated principles of linguistically appropriate domain modeling and representational adequacy (see also [ Met90 ]). The use of high-level language-oriented concept taxonomies such as the Penman Upper Model [ BMW90 ] to act as a bridge between the domain application's concept organization and that required for generation is becoming a popular (though partial) solution to this problem. What is required: Implemented large-size (over 10,000 concepts) domain models that are useful both for some non-linguistic application and for generation; criteria for evaluating the internal consistency of such models; theories on and practical experience in the linking of generators to such models; lexicons of commensurate size. Generation Choice Criteria : Probably the problem least addressed in generator systems today is the one that will take the longest to solve. This is the problem of guiding the generation process through its choices when multiple options exist to handle any given input. It is unfortunately the case that language, with its almost infinite flexibility, demands far more from the input to a generator than can be represented today. As long as generators remain fairly small in their expressive potential then this problem does not arise. However, when generators start having the power of saying the same thing in many ways, additional control must be exercised in order to ensure that appropriate text is produced. As shown in [ Hov88b ] and [ Jam87 ], different texts generated from the same input carry additional, non-semantic import; the stylistic variations serve to express significant interpersonal and situational meanings (text can be formal or informal, slanted or objective, colorful or dry, etc.). In order to ensure appropriate generation, the generator user has to specify not only the semantic content of the desired text, but also its pragmatic---interpersonal and situational---effects. Very little research has been performed on this question beyond a handful of small-scale pilot studies. What is required: Classifications of the types of reader characteristics and goals, the types of author goals, and the interpersonal and situational aspects that affect the form and content of language; theories of how these aspects affect the generation process; implemented rules and/or planning systems that guide generator systems' choices; criteria for evaluating appropriateness of generated text in specified communicative situations. 4.1.3 Future Directions Infrastructure Requirements: The overarching challenge for generation is scaling up to the ability to handle real-world, complex domains. However, given the history of relatively little funding support, hardly any infrastructure required for generation research exists today. The resources most needed to enable both high-quality research and large-scale generation include the following: Large well-structured lexicons of various languages. Without such lexicons, generator builders have to spend a great deal of redundant effort, collecting standard morphological and syntactic information to include in lexical items. As has been shown recently in the construction of the Penman English lexicon of 90,000+ items, it is possible to extract enough information from online dictionaries to create lexicons, or partial lexicons, automatically. Large well-structured knowledge bases . Paralleling the recent knowledge base construction efforts centered around WordNet [ Mil85 ] in the U.S., a large general-purpose knowledge base that acts as support for domain-specific application oriented knowledge bases would help to speed up and enhance generator porting and testing on new applications. An example is provided by the ontology construction program of the Pangloss machine translation effort [ HK93 ]. Large grammars of various languages. The general availability of such grammars would free generator builders from onerous and often repetitive linguistic work, though different theories of language naturally result in very different grammars. However, a repository of grammars built according to various theories and of various languages would constitute a valuable infrastructure resource. Libraries of text plans . As discussed above, one of the major stumbling blocks in the ongoing investigation of text planning is the availability of a library of tested text plans. Since no consensus exists on the best form and content of such plans, it is advisable to pursue several different construction efforts. Longer-term Research Projects: Naturally, the number and variety of promising long-term research projects is large. The following directions have all been addressed by various researchers for over a decade and represent important strands of ongoing investigation: stylistically appropriate generation psycholinguistically realistic generation reversible multilingual formalisms and algorithms continued development of grammars and generation methods generation of different genres/types of text Near- and Medium-term Applications with Payoff Potential: Taking into account the current state of the art and gaps in knowledge and capability, the following applications (presented in order of increasing difficulty) provide potential for near-term and medium-term payoff: Database Content Display : The description of database contents in natural language is not a new problem, and some such generators already exist for specific databases. The general solution still poses problems, however, since even for relatively simple applications it still includes unsolved issues in sentence planning and text planning. Expert System Explanation : This is a related problem, often however requiring more interactive ability, since the user's queries may not only elicit more information from a (static, and hence well-structured) database, but may cause the expert system to perform further reasoning as well, and hence require the dynamic explanation of system behavior, expert system rules, etc. This application also includes issues in text planning , sentence planning , and lexical choice . Speech Generation : Simplistic text-to-speech synthesis systems have been available commercially for a number of years, but naturalistic speech generation involves unsolved issues in discourse and interpersonal pragmatics (for example, the intonation contour of an utterance can express dislike, questioning, etc.). Only the most advanced speech synthesizers today compute syntactic form as well as intonation contour and pitch level . Limited Report and Letter Writing: As mentioned in the previous section, with increasingly general representations for text structure , generator systems will increasingly be able to produce standardized multiparagraph texts such as business letters or monthly reports. The problems faced here include text plan libraries, sentence planning, adequate lexicons, and robust sentence generators. Presentation Planning in Multimedia Human-Computer Interaction: By generalizing text plans, [ HA91 ] showed that it is possible also to control some forms of text formatting, and then argued that further generalization will permit the planning of certain aspects of multimedia presentations. Ongoing research in the WIP project at Saarbr&#252;cken [ WAGR91 ] and the COMET project at Columbia University [ FM90 ] have impressive demonstration systems for multimedia presentations involving planning and language generation . Automated Summarization : A somewhat longer-term functionality that would make good use of language generation and discourse knowledge is the automated production of summaries. Naturally, the major problem to be solved first is the identification of the most relevant information. During the past two decades, language generation technology has developed to the point where it offers general-purpose single-sentence generation capability and limited-purpose multisentence paragraph planning capability. The possibilities for growth and development of useful applications are numerous and exciting. Focusing new research on specific applications and on infrastructure construction will help turn the promise of current text generator systems and theories into reality. Next: 4.2 Syntactic Generation Up: 4 Language Generation Previous: 4 Language Generation 
	</PLAINTEXT>
	<CONTENT>
-->
<!DOCTYPE HTML PUBLIC "-//W3O//DTD W3 HTML 2.0//EN">
<!Converted with LaTeX2HTML 95 (Thu Jan 19 1995) by Nikos Drakos (nikos@cbl.leeds.ac.uk), CBLU, University of Leeds >
<HEAD>
<TITLE>4.1 Overview</TITLE>
</HEAD>
<BODY background="../images/marble.jpg">
<meta name="description" value="4.1 Overview">
<meta name="keywords" value="HLTsurvey">
<meta name="resource-type" value="document">
<meta name="distribution" value="global">
<P>
 <BR> <HR><A NAME=tex2html258 HREF="ch4node4.html"><IMG ALIGN=BOTTOM ALT="next" SRC="/latex2html_icons/next_motif.gif"></A>   <A NAME=tex2html256 HREF="ch4node2.html"><IMG ALIGN=BOTTOM ALT="up" SRC="/latex2html_icons/up_motif.gif"></A>   <A NAME=tex2html250 HREF="ch4node2.html"><IMG ALIGN=BOTTOM ALT="previous" SRC="/latex2html_icons/previous_motif.gif"></A>   <A NAME=tex2html260 HREF="ch4node1.html"><IMG ALIGN=BOTTOM ALT="contents" SRC="/latex2html_icons/contents_motif.gif"></A>       <A HREF="indextop.html"><IMG WIDTH=43 HEIGHT=24 ALIGN=BOTTOM ALT="index" SRC="http://www.cse.ogi.edu/icons//index_motif.gif"></A><BR>
<B> Next:</B> <A NAME=tex2html259 HREF="ch4node4.html">4.2 Syntactic Generation</A>
<B>Up:</B> <A NAME=tex2html257 HREF="ch4node2.html">4 Language Generation</A>
<B> Previous:</B> <A NAME=tex2html251 HREF="ch4node2.html">4 Language Generation</A>
<BR> <HR> <P>
<b>Chapter 4:  Language Generation</b>
<H1><A NAME=SECTION00210000000000000000>4.1 Overview</A></H1>
<P>
Eduard Hovy<BR> 
University of Southern California, Marina del Rey, California, USA
<A NAME=sechovy></A>
<P>
The area of study called natural language generation
(NLG)<A NAME=83></A>
investigates how computer programs can be made to produce high-quality
natural language text from computer-internal representations of
information.  Motivations for this study range from entirely
theoretical (linguistic<A NAME=84></A>, psycholinguistic<A NAME=85></A>) to entirely practical (for the production of output systems for computer
programs). Useful overviews of the research are
[<A HREF="ch4node6.html#DaleHovy92">DHRS92</A>,<A HREF="ch4node6.html#ParisSwartout90">PSM90</A>,<A HREF="ch4node6.html#Kempen87ka">Kem87</A>,<A HREF="ch4node6.html#BatemanHovy92">BH92</A>,<A HREF="ch4node6.html#McKeownSwartout87">MS87</A>,<A HREF="ch4node6.html#MannBates81">MBG<IMG  ALIGN=BOTTOM ALT="" SRC="ch4img11.gif">81</A>].  The stages
of language generation for a given application, resulting in speech output, are
shown in Figure <A HREF="ch4node3.html#fighovy">4.1</A>.
<P><A NAME=90></A><A NAME=fighovy></A><IMG  ALIGN=BOTTOM ALT="" SRC="ch4img4.gif">
<BR><STRONG>Figure 4.1:</STRONG> The stages of language generation.<BR>
<P>
<P>
This section discusses the following:
<UL><LI> the overall state of the art in generation,
<LI> ignificant gaps of knowledge, and
<LI> new developments and infrastructure<A NAME=94></A>.
</UL>
For more detail, it then turns to two major areas of generation theory<A NAME=96></A> and
practice: single-sentence generation<A NAME=97></A> (also called realization<A NAME=98></A> or tactical generation<A NAME=99></A>) and multisentence generation<A NAME=100></A> (also called text planning<A NAME=101></A> or strategic generation<A NAME=102></A>).  
<H2><A NAME=SECTION00211000000000000000>4.1.1 State of the Art</A></H2>
<P>
No field of study can be described adequately using a single perspective.<A NAME=104></A><A NAME=105></A>
In order to understand NLG it is helpful to consider independently the 
<em>tasks</em> of generation and the <em>process</em> of generation.  Every 
generator addresses one or more tasks and embodies one (or sometimes two) 
types of process.  One can identify three types of generator <em>task</em>: 
text planning, sentence planning, and surface realization.  Text planners 
select from a knowledge pool what information to include in the output, 
and out of this create a text structure to ensure coherence.  On a more 
local scale, sentence planners organize the content of each sentence, 
massaging and ordering its parts.  Surface realizers convert sentence-sized 
chunks of representation into grammatically correct sentences.  Generator 
<em>processes</em> can be classified into points on a range of sophistication 
and expressive power, starting with inflexible canned methods and ending 
with maximally flexible feature combination methods.  For each point on 
this range, there may be various types of implemented algorithms.
18 
The simplest approach, <b> canned text systems</b>, is used in the majority 
of software: the system simply prints a string of words without any change 
(error messages, warnings, letters, etc.).  The approach can be used equally 
easily for single-sentence and for multi-sentence text generation<A NAME=111></A>.  Trivial 
to create, the systems are very wasteful.  <b> Template systems</b>, the next 
level of sophistication, are used as soon as a message must be produced 
several times with slight alterations.  Form letters are a typical template 
application, in which a few open fields are filled in specified constrained 
ways.  The template approach is used mainly for multisentence generation<A NAME=113></A>,
particularly in applications whose texts are fairly regular in structure 
such as some business reports.  The text planning components of the U.S. 
companies CoGenTex (Ithaca, NY) and Cognitive Systems Inc. (New Haven, 
CT) enjoy commercial use.  On the research side, the early template-based 
generator ANA [<A HREF="ch4node6.html#Kukich83phd">Kuk83</A>] produced stock market reports from a news
wire by filling appropriate values into a report template.  More 
sophisticated, the multisentence component of TEXT [<A HREF="ch4node6.html#McKeown85">McK85</A>] could 
dynamically nest instances of four stereotypical paragraph templates called 
schemas to create paragraphs.  TAILOR [<A HREF="ch4node6.html#Paris93em">Par93a</A>] generalized TEXT by 
adding schemas and more sophisticated schema selection criteria.
<P>
<b> Phrase-based systems<A NAME=117></A></b> employ what can be seen as
generalized templates, whether at the sentence level<A NAME=118></A> (in
which case the phrases resemble phrase structure grammar<A NAME=119></A>
rules) or at the
discourse level<A NAME=120></A> (in which case they are often called text plans<A NAME=121></A>).  In such systems, a phrasal pattern<A NAME=122></A> is first selected to match the top
level of the input (say, [SUBJECT VERB OBJECT]), and then each
part of the pattern is expanded into a more specific phrasal pattern
that matches some subportion of the input (say, [DETERMINER ADJECTIVES HEAD-NOUN MODIFIERS]), and so on; the cascading process
stops when every phrasal pattern has been replaced by one or more
words.  Phrase-based systems can be powerful and robust, but are very
hard to build beyond a certain size, because the phrasal
interrelationships must be carefully specified to prevent
inappropriate phrase expansions.  The phrase-based approach has mostly
been used for single-sentence generation<A NAME=125></A> (since linguists' grammars
provide well-specified collections of phrase structure rules<A NAME=126></A>).  A
sophisticated example is MUMBLE
[<A HREF="ch4node6.html#McDonald80phd">McD80</A>,<A HREF="ch4node6.html#MeteerMcDonald87">MMA<IMG  ALIGN=BOTTOM ALT="" SRC="ch4img12.gif">87</A>], built at the University of Massachusetts,
Amherst.  Over the past five years, however, phrase-based multisentence text structure generation<A NAME=128></A> (often called text planning<A NAME=129></A>) has received considerable attention in the research community, with the development of the RST text structurer<A NAME=130></A> [<A HREF="ch4node6.html#Hovy88acl">Hov88a</A>], the EES text planner<A NAME=132></A>
[<A HREF="ch4node6.html#Moore89phd">Moo89</A>], and several similar systems
[<A HREF="ch4node6.html#Dale90">Dal90</A>,<A HREF="ch4node6.html#Cawsey89">Caw89</A>,<A HREF="ch4node6.html#Suthers93">Sut93</A>], in which each
so-called text plan is a <em>phrasal</em> pattern that specifies the
structure of some portion of the discourse, and each portion of the
plan is successively refined by more specific plans until the
single-clause level is reached.  Given the lack of understanding of
discourse structure and the paucity of the discourse plan libraries,
however, such planning systems do not yet operate beyond the
experimental level.
<P>
<b> Feature-based systems<A NAME=136></A></b> represent, in a sense, the limit
point of the generalization of phrases. In feature-based systems, each
possible minimal alternative of expression is represented by a single
feature; for example, a sentence is either POSITIVE or NEGATIVE, it is a QUESTION or an IMPERATIVE or a STATEMENT, its tense is PRESENT or PAST and so on.  Each
sentence is specified by a unique set of features.  Generation
proceeds by the incremental collection of features appropriate for
each portion of the input (either by the traversal of a
feature selection network<A NAME=144></A> or by unification<A NAME=145></A>), until the sentence is fully determined.  Feature-based systems are among the
most sophisticated generators built today.  Their strength lies in the
simplicity of their conception: any distinction in language is defined
as a feature, analyzed, and added to the system.  Their strength lies
in the simplicity of their conception: any 
distinction in language can be added to the system as a feature.  Their 
weakness lies in the difficulty of maintaining feature interrelationships 
and in the control of feature selection (the more features available, 
the more complex the input must be).  No feature-based multisentence
generators have
been built to date. The most advanced single-sentence generators of this type include PENMAN [<A HREF="ch4node6.html#Matthiessen83">Mat83</A>,<A HREF="ch4node6.html#MannMatthiessen85">MM85</A>] and its descendant 
  KPML<A NAME=147></A>
[<A HREF="ch4node6.html#BatemanMaier91">BMTW91</A>], the Systemic generators developed at 
  USC/ISI<A NAME=149></A> and IPSI<A NAME=150></A>;
COMMUNAL [<A HREF="ch4node6.html#Fawcett92">Faw92</A>] a
Systemic generator developed at Wales; the Functional Unification
Grammar framework (FUF)
[<A HREF="ch4node6.html#Elhadad92phd">Elh92</A>] from Columbia University; SUTRA [<A HREF="ch4node6.html#VonHahnHoppner80">VHHJW80</A>]
developed at the University of Hamburg; SEMTEX
[<A HREF="ch4node6.html#Roesner86phd">R86</A>] developed at the University of Stuttgart; and
POPEL [<A HREF="ch4node6.html#Reithinger91phd">Rei91</A>] developed at the University of the
Saarland.  The two generators most widely distributed, studied, and
used are PENMAN/KPML and FUF.  None of these systems is in
commercial use.
<P>
<H2><A NAME=SECTION00212000000000000000>4.1.2 Significant Gaps and Limitations</A></H2>
<P>
It is safe to say that at the present time one can fairly easily build
a single-purpose generator for any specific application, or with some
difficulty adapt an existing sentence generator to the application, with
acceptable results.  However, one cannot yet build a general-purpose
sentence generator or a non-toy text planner.  Several significant
problems remain without sufficiently general solutions:
<UL><LI> lexical selection<A NAME=158></A>
<LI> sentence planning<A NAME=159></A>
<LI> discourse structure<A NAME=160></A>
<LI> domain modeling<A NAME=161></A>
<LI> generation choice criteria<A NAME=162></A>
</UL>
<P>
<b> Lexical Selection<A NAME=164></A>:</b> Lexical selection is one of the most
difficult problems in generation.  At its simplest, this
question involves selecting the most appropriate single word for a
given unit of input.  However, as soon as the semantic model<A NAME=165></A>
approaches a realistic size, and as soon as the lexicon<A NAME=166></A> is
large enough to permit alternative locutions<A NAME=167></A>, the problem
becomes very complex.  In some situation, one might have to choose
among the phrases <em>John's car, John's sports car, his speedster,
the automobile, the red vehicle, the red Mazda</em> for referring to a
certain car.  The decision depends on what has already been said, what
is referentially available from context, what is most salient, what
stylistic effect the speaker wishes to produce, and so on.
A considerable amount of work has been devoted to this question, and 
solutions to various aspects of the problem have been suggested (see 
for example [<A HREF="ch4node6.html#Goldman75">Gol75</A>,<A HREF="ch4node6.html#ElhadadRobin92">ER92</A>,<A HREF="ch4node6.html#McKeownRobin93">MRT93</A>]).
At this
time no general methods exist to perform lexical selection<A NAME=170></A>.
Most current generator systems simply finesse the problem by linking a
single lexical item to each representation unit.  <em>What is
required:</em> Development of theories about and implementations of
lexical selection algorithms, for reference to objects, event, states,
etc., and tested with large lexica.
<P>
<b> Discourse Structure<A NAME=172></A>:</b> One of the most exciting recent
research developments in generation is the automated planning of
paragraph structure.  The state of the art in discourse research is
described in chapter 6. So far no text planner exists that can
reliably plan texts of several paragraphs in general.  <em>What is
required:</em> Theories of the structural nature of discourse, of the
development of theme and focus in discourse, and of coherence and
cohesion; libraries of discourse relations, communicative goals, and
text plans; implemented representational paradigms for characterizing
stereotypical texts such as reports and business letters; implemented
text planners that are tested in realistic non-toy domains.
<P>
<b> Sentence Planning<A NAME=174></A>:</b> Even assuming the text planning
problem solved, a number of tasks remain before well-structured
multisentence text can be generated.  These tasks, required for
planning the structure and content of each sentence, include:
pronoun specification<A NAME=175></A>, theme signaling<A NAME=176></A>, focus signaling<A NAME=177></A>, content aggregation<A NAME=178></A> to remove unnecessary redundancies, the ordering of prepositional phrases<A NAME=179></A>, adjectives, etc.  An elegant system that addressed some of these tasks
is described in [<A HREF="ch4node6.html#Appelt85">App85</A>].  While to the nonspecialist these
tasks may seem relatively unimportant, they can have a significant
effect and make the difference between a well-written and a poor text.
<em>What is required:</em> Theories of pronoun use, theme and focus
selection and signaling, and content aggregation; implemented
sentence planners with rules that perform these operations; testing in
realistic domains.
<P>
<b> Domain Modeling<A NAME=182></A>:</b> A significant shortcoming in
generation research is the lack of large well-motivated application
domain models, or even the absence of clear principles by which to
build such models.  A traditional problem with generators is that the
inputs are frequently hand-crafted, or are built by some other system
that uses representation elements from a fairly small hand-crafted
domain model, making the generator's inputs already highly oriented
toward the final language desired.  It is very difficult to link a
generation system to a knowledge base or database that was originally
developed for some non-linguistic purpose. The mismatches between the
representation schemes demonstrate the need for clearly articulated
principles of linguistically appropriate domain modeling and
representational adequacy (see also [<A HREF="ch4node6.html#Meteer90phd">Met90</A>]).  The use of
high-level language-oriented concept taxonomies such as the Penman Upper Model<A NAME=184></A> [<A HREF="ch4node6.html#BatemanMoore90">BMW90</A>] to act as a <em>bridge</em> between the
domain application's concept organization and that required for
generation is becoming a popular (though partial) solution to this
problem.  <em>What is required:</em> Implemented large-size (over 10,000
concepts) domain models that are useful both for some non-linguistic
application and for generation; criteria for evaluating the internal
consistency of such models; theories on and practical experience in
the linking of generators to such models; lexicons of commensurate
size.
<P>
<b> Generation Choice Criteria<A NAME=188></A>:</b> Probably the problem least
addressed in generator systems today is the one that will take the
longest to solve.  This is the problem of guiding the generation
process through its choices when multiple options exist to handle any
given input.  It is unfortunately the case that language, with its
almost infinite flexibility, demands far more from the input to a
generator than can be represented today.  As long as generators remain
fairly small in their expressive potential then this problem does not
arise.  However, when generators start having the power of saying
<em>the same thing</em> in many ways, additional control must be exercised
in order to ensure that appropriate text is produced.  As shown in
[<A HREF="ch4node6.html#Hovy88book">Hov88b</A>] and
[<A HREF="ch4node6.html#Jameson87">Jam87</A>], different texts generated from the same input carry
additional, non-semantic import; the stylistic variations serve to express
significant interpersonal and situational meanings (text can be formal or
informal, slanted or objective, colorful or dry, etc.).  In order to
ensure appropriate generation, the generator user has to specify not only
the semantic content of the desired text, but also its
pragmatic---interpersonal and
situational---effects.  Very little research has been
performed on this question beyond a handful of small-scale pilot studies.
<em>What is required:</em> Classifications of the types of reader
characteristics and goals, the types of author goals, and the interpersonal
and situational aspects that affect the form and content of language;
theories of how these aspects affect the generation process; implemented
rules and/or planning systems that guide generator systems' choices;
criteria for evaluating appropriateness of generated text in specified
communicative situations.
<P>
<H2><A NAME=SECTION00213000000000000000>4.1.3 Future Directions</A></H2>
<P>
<b> Infrastructure Requirements:</b> The overarching challenge for generation
is scaling up to the ability to handle real-world, complex domains.
However, given the history of relatively little funding support, hardly
any infrastructure required for generation research exists today.
<P>
The resources most needed to enable both high-quality research and large-scale
generation include the following:
<UL><LI> Large well-structured lexicons<A NAME=196></A> of various languages.  Without such
 lexicons, generator builders have to spend a great deal of redundant
 effort, collecting standard morphological and syntactic information
 to include in lexical items.  As has been shown recently in the
 construction of the Penman English lexicon of 90,000+ items, it is
 possible to extract enough information from online dictionaries to
 create lexicons, or partial lexicons, automatically.
<LI> Large well-structured knowledge bases<A NAME=197></A>.  Paralleling the recent
 knowledge base construction efforts centered around WordNet<A NAME=198></A>
 [<A HREF="ch4node6.html#Miller85">Mil85</A>] in the U.S., a large general-purpose knowledge
 base that acts as support for domain-specific application oriented
 knowledge bases would help to speed up and enhance generator porting
 and testing on new applications.  An example is provided by the
 ontology construction program of the Pangloss<A NAME=200></A>
 machine translation<A NAME=201></A> effort [<A HREF="ch4node6.html#HovyKnight93">HK93</A>].
<LI> Large grammars<A NAME=203></A> of various languages.
 The general availability of such grammars would free generator
 builders from onerous and often repetitive linguistic work, though
 different theories of language naturally result in very different
 grammars.  However, a repository of grammars built according to
 various theories and of various languages would constitute a valuable
 infrastructure resource.
<LI> Libraries of text plans<A NAME=204></A>.  As discussed above, one of the major
 stumbling blocks in the ongoing investigation of text planning is the
 availability of a library of tested text plans.  Since no consensus
 exists on the best form and content of such plans, it is advisable to
 pursue several different construction efforts.
</UL>
<P>
<b> Longer-term Research Projects:</b> Naturally, the number and variety of
promising long-term research projects is large.  The following directions
have all been addressed by various researchers for over a decade and
represent important strands of ongoing investigation:
<UL><LI> stylistically appropriate generation<A NAME=208></A>
<LI> psycholinguistically realistic generation<A NAME=209></A>
<LI> reversible multilingual formalisms and algorithms<A NAME=210></A>
<LI> continued development of grammars and generation methods<A NAME=211></A>
<LI> generation of different genres/types of text<A NAME=212></A>
</UL>
<P>
<b> Near- and Medium-term Applications with Payoff Potential:</b>  Taking into
account the current state of the art and gaps in knowledge and capability,
the following applications (presented in order of increasing difficulty)
provide potential for near-term and medium-term payoff:
<P>
<UL><LI> <b> Database Content Display<A NAME=216></A>:</b>
 The description of database contents in natural language is not a new
 problem, and some such generators already exist for specific
 databases.  The general solution still poses problems, however, since
 even for relatively simple applications it still includes unsolved
 issues in sentence planning and text planning.
<LI> <b> Expert System Explanation<A NAME=217></A>:</b>
 This is a related problem, often however requiring more interactive
 ability, since the user's queries may not only elicit more
 information from a (static, and hence well-structured) database, but
 may cause the expert system to perform further reasoning as well, and
 hence require the dynamic explanation of system behavior, expert
 system rules, etc.  This application also includes issues in
 text planning<A NAME=218></A>, sentence planning<A NAME=219></A>, and  lexical choice<A NAME=220></A>.
<LI> <b> Speech Generation<A NAME=221></A>:</b>
 Simplistic text-to-speech synthesis<A NAME=222></A> systems have been
 available commercially for a number of years, but naturalistic speech
 generation involves unsolved issues in discourse<A NAME=223></A> and
 interpersonal pragmatics<A NAME=224></A> (for example, the
 intonation contour<A NAME=225></A> of an utterance can express dislike,
 questioning, etc.).  Only the most advanced speech synthesizers today
 compute syntactic form<A NAME=226></A> as well as intonation  contour<A NAME=227></A> and pitch level<A NAME=228></A>.
<LI> <b> Limited Report and Letter Writing:</b> As mentioned in the previous
 section, with increasingly general representations for text structure<A NAME=230></A>,
 generator systems will increasingly be able to produce standardized
 multiparagraph texts such as business letters or monthly reports.  The
 problems faced here include text plan libraries, sentence planning, adequate
 lexicons, and robust sentence generators.
<LI> <b> Presentation Planning in Multimedia Human-Computer
Interaction:</b><A NAME=232></A>
 By generalizing text plans, [<A HREF="ch4node6.html#HovyArens91">HA91</A>] showed that it is
 possible also to control some forms of text formatting, and then
 argued that further generalization will permit the planning of
 certain aspects of multimedia presentations.  Ongoing research in the
 WIP project at Saarbr&#252;cken [<A HREF="ch4node6.html#WahlsterAndre91">WAGR91</A>] and the
 COMET project at Columbia University
 [<A HREF="ch4node6.html#FeinerMcKeown90">FM90</A>] have impressive demonstration systems for
 multimedia<A NAME=236></A> presentations involving planning and
 language generation<A NAME=237></A>.
<LI> <b> Automated Summarization<A NAME=238></A>:</b>
 A somewhat longer-term functionality that would make good use of
 language generation and discourse knowledge is the automated
 production of summaries.  Naturally, the major problem to be solved
 first is the identification of the most relevant information.
</UL>
<P>
During the past two decades, language generation<A NAME=240></A> technology
has developed to the point where it offers general-purpose
single-sentence generation<A NAME=241></A> capability and limited-purpose
multisentence paragraph planning<A NAME=242></A> capability.  The possibilities for
growth and development of useful applications are numerous and
exciting.  Focusing new research on specific applications and on
infrastructure construction will help turn the promise of current text
generator systems and theories into reality.
<P>
<BR> <HR><A NAME=tex2html258 HREF="ch4node4.html"><IMG ALIGN=BOTTOM ALT="next" SRC="/latex2html_icons/next_motif.gif"></A>   <A NAME=tex2html256 HREF="ch4node2.html"><IMG ALIGN=BOTTOM ALT="up" SRC="/latex2html_icons/up_motif.gif"></A>   <A NAME=tex2html250 HREF="ch4node2.html"><IMG ALIGN=BOTTOM ALT="previous" SRC="/latex2html_icons/previous_motif.gif"></A>   <A NAME=tex2html260 HREF="ch4node1.html"><IMG ALIGN=BOTTOM ALT="contents" SRC="/latex2html_icons/contents_motif.gif"></A>      <BR>
<B> Next:</B> <A NAME=tex2html259 HREF="ch4node4.html">4.2 Syntactic Generation</A>
<B>Up:</B> <A NAME=tex2html257 HREF="ch4node2.html">4 Language Generation</A>
<B> Previous:</B> <A NAME=tex2html251 HREF="ch4node2.html">4 Language Generation</A>
<BR> <HR> <P>
</BODY>

