<!-- <DOCUMENT>
	<FILE>
		7571528267.html
	</FILE>
	<URL>
		http://www.telecom.csuhayward.edu/~stat/Gibbs/GibbsSess4.htm
	</URL>
	<TITLE>
		Session 4_ Brief Intro. to Bayesian Inference
	</TITLE>
	<GENRE>
		articles
	</GENRE>
	<PLAINTEXT>
 Appendix B: Brief Intro. to Bayesian Inference CSU Hayward Statistics Department Session 4: A Brief Introduction To Bayesian Estimation Note: This session is intended to be a free-standing introduction to certain aspects of Bayesian inference. The material is of interest in its own right, but is also necessary background for Session 4. This session expands considerably upon the very brief Appendix B for Suess, Fraser, and Trumbo: &quot;Elementary Uses of the Gibbs Sampler: Applications to Medical Screening Tests,&quot; in STATS #27, Winter 2000. It contains more examples, more detailed explanations, some additional topics, and extensive exercises suitable for instructional use. For another introduction to Bayesian inference, see the article by Hal Stern: &quot;A Primer on the Bayesian Approach to Statistical Inference,&quot; in STATS #23, Fall 1998, pages 3-9. 3.1. Introduction Bayesian and frequentist statistical inference take fundamentally different viewpoints toward statistical decision making. The frequentist view of probability, and thus of statistical inference, is based on the idea of an experiment that can be repeated very many times. The Bayesian view of probability and of inference is based on personal assessments of probability and on data from a single performance of an experiment. In practical application, both ways of thinking have advantages and disadvantages, some of which we will explore here. Statistics is a young science. For example, interval estimation and hypothesis testing have become common in scientific research and business decision making only within the past 75 years, and then only gradually. On this time scale it seems strange to talk about &quot;traditional&quot; approaches. Nevertheless, frequentist viewpoints are currently much better established, particularly in scientific research, than Bayesian ones. Recently, the use of Bayesian methods has been increasing, partly because improvements in computation have made these methods easier to apply in practice and partly because the Bayesian approach seems to be able to get useful solutions in some applications where frequentist approaches cannot. The Gibbs sampler, examined more fully in Session 4, is one example of a broadly applicable, computationally intensive Bayesian method. We will see later on that, for the very simple examples considered here, Bayesian and frequentist methods give similar results. But that is not the main point. We hope you will gain some appreciation that Bayesian methods are sometimes the most natural useful ones in practice. For most people, the starkest contrast between frequentist and Bayesian approaches is that Bayesian inference provides the opportunity  even makes it a requirement  to take into account &quot;information&quot; that is available before any data is collected. That is where we begin. 3.2. Prior Distributions: Personal Opinions and Expert Knowledge The Bayesian approach to statistical inference treats population parameters as random variables (not as fixed, unknown constants). The distributions of these parameters are called prior distributions . Often both expert knowledge and mathematical convenience play a role in selecting a particular type of prior distribution. 3.2.1. Example 1: Election polling Suppose that Proposition A is on the ballot for an upcoming statewide election, and that a political consultant has been hired to help manage the campaign for its passage. The proportion P of prospective voters who currently favor Proposition A is the population parameter of interest here. Based on her knowledge of the politics of the state, the consultant's judgment is that the proposition is almost sure to pass, but not by a large margin. She believes that the most likely proportion of voters in favor is 55% and that the percentage is not likely to be below 51% or above 59%. It is reasonable to consider the beta distribution to model the expert's opinion of the proportion in favor because distributions in the beta family take values in the interval (0, 1) as do proportions. This family of distributions has density functions of the form f ( p ) = K 1 p a 1 (1  p ) b 1 , 0 &lt; p &lt; 1. where a , b &gt; 0, and where K 1 is a constant chosen so that f ( p ) integrates to 1 over (0, 1). A member of the beta family that corresponds roughly to the expert's opinion has a = 331 and b = 271. o This density curve has its mode at ( a  1) / ( a + b  2) = 0.55. o Numerical integration shows that P(0.51 &lt; P &lt; 0.59) = 0.95. Of course, many other distributional shapes share these two numerical properties, but we choose a member of the beta family because it makes the mathematics relatively easy and because we have no reason to believe its shape is inappropriate here. If the political consultant's judgments about the political situation are correct, then they may be helpful in managing the campaign. If she too often brings bad judgment to her clients, her reputation will suffer and she will be out of the political consulting business before long. Fortunately, as we will see, the details of her judgments become less important if we also have polling data to rely upon. 3.2.2. Example 2: Weighing an object A construction company buys steel beams with a nominal weight of 200 lb. Experience with a particular supplier of these beams has shown that their beams very seldom weigh less than 180 or more than 220 lb. In these circumstances it may be convenient and reasonable to suppose that the distribution of weights of beams from this supplier is normal with mean 200 and standard deviation 10. Usually, the exact weight of a beam is not especially important, but there are some situations in which it is crucial to know the weight of a beam more precisely. Then a particular beam is selected and weighed several times on a scale. The scale is known to give results that are normally distributed without bias, but with a standard deviation of 1 lb. Here it seems reasonable to use N(200, 10 2 ) as the prior distribution of the weight m of a beam. The hope is that the weighing process will help us to know its true weight more accurately. Theoretically, the frequentist statistician would ignore &quot;prior&quot; or background experience in doing statistical inference, basing statistical decisions only on the data collected when a beam is weighed. In real life it is not so simple. For example, the design of the weighing experiment will very likely take past experience into account in one way or another. For the Bayesian statistician the explicit codification of some kinds of background information into a prior distribution is a required first step. 3.2.3. Example 3: Counting mice An island in the middle of a river is one of the last known habitats of an endangered kind of mouse. The mice rove about the island in ways that are not fully understood and so are taken as random. Ecologists are interested in the average number of mice to be found in particular regions of the island. To do the counting in a region they set many traps there at night, using bait that is irresistible to mice at close range. In the morning they count and release the mice caught. It seems reasonable to suppose that almost all of the mice in the region the previous night were caught and that the number of them on any one night has a Poisson distribution. The purpose of the trapping is to estimate the mean l of this distribution. Even before the trapping is done the ecologists doing this study have some information about l . For example, even though the mice are quite shy, there have been occasional sightings of them in almost all regions of the island, so it seems likely that l &gt; 1. On the other hand, from what is known of the habits of the mice and the food supply in the regions, it seems unlikely that there would be as many as 25 of them in any one region at a given time. In these circumstances, it may be reasonable to use a gamma distribution with shape parameter a = 4 and scale parameter b = 3 as a prior distribution for values l of the random variable L . This gamma distribution has density function f ( x ) = K 1 x a 1 e  b x (for x &gt; 0), mean ab = 12, mode ( a  1) b = 9, variance ab 2 = 36, and standard deviation 6. A member of the gamma family may be a reasonable prior because l must be positive and members of the gamma family take only positive values. We will see later that choosing a gamma prior simplifies some important mathematical computations. These technicalities of the prior distribution aside, it is clear that the experience of the ecologists with the island and its endangered mice will influence the course of this investigation in many ways: dividing the island into meaningful regions; modeling the randomness of mouse movement as Poisson, the number of traps to use and where to place them, what to use for bait so as to attract mice from a region of interest but not from all over the island, and so on. The expression of some of their background knowledge as a prior distribution is perhaps a relatively small use of their expertise. But it is a necessary first step in Bayesian inference, and it is perhaps the only aspect of their expert opinion that will be explicitly tempered by the data that are collected. Problems 3.2.1. In Example 1, show that for appropriate values of a and b , the density function has a unique mode at ( a  1)/( a + b  2). [Hint: Differentiate the density function, set the derivative equal to 0, and note the values of a and b for which the solution of this equation yields an absolute maximum.] 3.2.2. In Example 3, verify the stated value of the mode, and say for what values of a and b the mode exists. 3.2.3. In Example 1, suppose that a = 2 and b = 1. Find the value of the constant K 1 that makes this beta density function integrate to 1. Find the mean and variance. Find the values a and b such that P( P &lt; a ) = P( P &gt; b ) = 0.025. Sketch the density function. Repeat for a = 1 and b = 1 3.2.4. In Example 3, suppose that a = 1 and b = 10. Find K 1 , E( L ), V( L ), and a and b such that P( L &lt; a ) = P( L &gt; b ) = 0.025. 3.2.5. For each of the three examples of this section sketch the given prior distribution (for the specific parameter values stated). Then use tables, Minitab, S-Plus, or other statistical software to find values a and b that cut off the upper and lower 2.5% of the stated prior distributions. Shade in the corresponding areas. An example of a typical Minitab instruction is as follows: MTB &gt; invcdf 0.975; SUBC&gt; beta 331 271. For Example 2, it is obvious that the true weight cannot be negative. Why is it safe to ignore the fact that the suggested normal prior distribution includes negative values? 3.2.6. Suggest reasonable prior distributions in the following cases: (a) In Example 1, suppose that a very successful professional political fundraiser from another state has been hired. He doesn't know what Proposition A is about and has no experience with the politics of this state. Before he sees some polling data he has absolutely no idea what percentage of the voters favor Proposition A. (b) In Example 2, the mean weight of the beams from the supplier is around 200 lb. and we suppose that about half of them weigh between 195 and 205 lb. (c) In Example 3, we seek a prior with P( L &lt; 0.5) » 0.025, P( L &gt; 75) » 0.025 and E( L ) » 20. (d) In Example 1, now suppose the fundraiser is unfamiliar with opinions in the state, does know what Proposition A is about, is sure that it will be extremely controversial with a very low probability that the percentage in favor is anywhere near 1/2, but he has no opinion whether the sentiment will be overwhelmingly for or overwhelmingly against. (Suggest general ranges of parameter values.) 3.3. Data and Posterior Distributions The second step in Bayesian inference is to collect data and to combine the information in the data with the expert opinion represented by the prior distribution. The result is a posterior distribution that can be used for inference. The computation of the posterior distribution uses Bayes' Theorem. You have probably seen Bayes' Theorem stated for an event E and a partition { B 1 , B 2 , ..., B k } of a sample space S . (The B i are mutually exclusive events whose union is S ): for j = 1, ..., k . Even in this most elementary setting, the quantities P( B j ) are called prior probabilities and P( B j | E ) are called posterior probabilities. A more general version of Bayes' Theorem for distributions involving data x and a parameter p is as follows: where the integral is taken over the region where the integrand is positive. The denominator of this fraction is a constant J . The conditional density f ( p | x ) is the posterior distribution of P given X , evaluated for the specific numerical values X = x and P = p . 3.3.1. Example 1 (election) continued. In our election example, suppose n = 1000 subjects are selected at random. Assuming the specific parameter value P = p , the number X of them in favor of Proposition A is a random variable with the binomial distribution f ( x | p ) = K 2 p x (1  p ) n  x , for x = 0, 1, 2, ..., n , and where K 2 = n ! /[ x ! ( n  x ) ! ] is the constant that makes the distribution sum to 1. Then the general version of Bayes' Theorem gives the posterior distribution f ( p | x ) = K 3 p x (1  p ) n  x p a 1 (1  p ) b 1 = K 3 p x + a 1 (1  p ) n  x + b 1 , where K 3 = K 1 K 2 / J . If x = 621 of the n = 1000 subjects interviewed favor Proposition A, it is then clear that the posterior has a beta distribution with parameters x + a = 952 and n  x + b = 650. According to this posterior distribution, P(0.570 &lt; P &lt; 0.618) = 0.95, so that a 95% posterior probability interval for the percent in favor is (57.0%, 61.8%). Note the following three aspects of this development: o The interval estimate for P is a straightforward probability statement. Unlike frequentist &quot;confidence intervals,&quot; it does not require the user to imagine a repeatable experiment. o The mathematical forms of the beta prior and the binomial data are similar, making it especially easy to find the posterior. In such cases we say that the beta is a &quot;conjugate prior&quot; to the binomial. Often in Bayesian distributional computations it is unnecessary to know the actual values of the constants involved. Then the proportionality symbol µ can be used to show relationships. For example, the first equation in this subsection could have been written as f ( x | p ) µ p x (1  p ) n  x , where the expression on the right is the kernel of the density function. The kernel is the part of a density function that displays necessary variables and parameters, but does not display the constant of integration. 3.3.2. Example 2 (weighing a beam) continued. Suppose that a particular beam is selected from among the beams available. The prior distribution is N(200, 10 2 ) and so f ( m ) µ exp [( m  m 0 ) 2 / t 2 ], where m 0 = 100 and t = 10. The data x = ( x 1 , ..., x 5 ) provide the likelihood function f ( x | m ) µ exp {  S [( x i  m ) 2 /2 s 2 ] } , where m determined by the prior distribution, the x i are five observed numbers, and s 2 = 1. Then, after some algebra (see Problem 3.3.6), the posterior can be written as f ( m | x ) µ f ( m ) f ( x | m ) µ exp [( m  m n ) 2 /2 V n ], which is the kernel of N( m n , V n ), where It is common to use the term precision to refer to the reciprocal of a variance. Using this terminology, we say that m n is a weighted average of the prior mean and the sample mean, where the precisions are the weights. Here the precision of each observation is much greater than the precision of the prior. In this case even our small sample of five observations is enough to diminish the impact of the prior on the posterior distribution. 3.3.3. Example 3 (counting mice) continued Suppose that a region of the island is selected where the gamma prior distribution with parameters a = 4 and b = 3 is reasonable. This prior has density f ( l ) µ l a - 1 e l/ b . Over a period of about a year traps are set out on n = 50 nights with total number of captures T = S x i = 256, for an average of 5.12 mice captured per night. This gives the likelihood Thus, the posterior distribution f ( l | x ) is gamma with parameters given by a n = T + a = 260 and 1/ b n = n + 1/ b = 50 + 1/3, so that b n = 0.0199. A 95% Bayesian probability interval for l based on this posterior distribution is (4.56, 5.82). Problems 3.3.1. This problem deals with point and interval estimates of the population proportion in the continuation of Example 1. (a) It might be reasonable to use the mode of the posterior distribution as a point estimate of the proportion in favor of Proposition A in the population. What is its numerical value? (b) Use a computer package to verify the endpoints of the Bayesian probability interval given in the Section 3.3.2. This is a probability-symmetric interval which excludes 2.5% of the probability in each tail of the posterior distribution. (c) Taking a frequentist point of view, use the normal approximation to the binomial distribution to find an approximate 95% confidence interval for the population proportion. What is the main difference between this interval and the one verified in part (a)? (d) What Bayesian point estimate (mode) and 95% probability interval would have resulted from using the uniform distribution on (0, 1) as the prior distribution? 3.3.2. This problem deals with point and interval estimates of m in Example 2. Suppose that the five measurements of the weight of the beam are: 198.14, 198.45, 196.59, 197.64, 198.12 (mean 197.79). (a) What is the point estimate of m . In this instance, does it matter whether we choose the mean, the median, or the mode of the posterior distribution as our point estimate? Explain. (b) Find a 95% Bayesian probability estimate of m . (c) Suppose we would be unwilling to use this beam for a particular purpose if we thought it weighed less than 197 lb. What are the chances of that? (d) Taking a frequentist point of view, use the five observations and the known variance of measurements produced by the scale to give a 95% confidence interval for the true weight of the beam. (e) What Bayesian point estimate and 95% probability interval would have resulted from using a normal distribution with mean 202 and standard deviation 5 as the prior? 3.3.3. This problem deals with point and interval estimates of l in Example 3 based on the data given in Sec. 3.3.3. (a) Find the mean, median, and mode of the posterior distribution. (b) Verify the 95% Bayesian probability interval given in Sec. 3.3.3. Strictly speaking, shorter 95% probability intervals can be found because of the skewness of the distribution. For such a shorter interval would slightly more or slightly less that 2.5% be omitted from the right tail? (c) Taking a frequentist point of view, find an approximate 95% confidence interval for n l and hence for l . Use the fact that T has a Poisson distribution that is well-approximated by an appropriate normal distribution. (d) Suppose that the prior distribution had been gamma with parameters a = 3 and b = 1. Give an interval that contains 95% of the probability under this prior distribution. Use the same data as above ( T = 256) and find the corresponding 95% Bayesian probability interval for l . 3.3.4. Derive the posterior distribution resulting from a beta prior and binomial data. Verify the specific parameter values given in Sec. 3.3.1. 3.3.5. Derive the posterior distribution resulting from a gamma prior and Poisson data. Verify the specific parameter values given in Section 3.3.3. (The gamma prior is conjugate to the Poisson distribution of the data.) 3.3.6. In this problem we invite you to derive the posterior distribution for Example 2 in Section 3.3.2. (a) Show that the expression f ( x | m ) µ exp {  S [( x i  m ) 2 /2 s 2 ] } for the likelihood function is correct and can be re-expressed as The likelihood function is the joint density function of the data viewed as a function of m (rather than of the x i ). By independence, the joint density function is proportional to P exp { [( x i  m ) 2 /2 s 2 ] } , which is clearly equal to the first expression for the likelihood function. To put the first expression in the form displayed just above, write expand the square and sum over i . On distributing the sum you should obtain three terms. One of the terms provides the desired result; another is 0, and the third is irrelevant because it does not contain the variable m . (A constant term in the exponential amounts to a constant factor in the density, which is not included in the kernel.) (b) Derive the expression for the kernel of the posterior given in Section 3.3.2. Multiply the kernels of the prior and the likelihood, and expand the squares in each. Put everything in the exponential over a common denominator. Then, remembering that m is the variable, collect terms in m 2 and m . Terms in the exponent that do not involve m are constant factors in the posterior density that may be adjusted at will to complete the square in order to obtain the desired kernel. 3.4. More About Prior Distributions One issue of concern in Bayesian inference is how strongly the particular selection of a prior distribution influences the results of the inference. Particularly if results are to be used by people who may question the expert's opinion, it is desirable to have enough data that the influence of the prior is slight. An uninformative prior (sometimes called a flat prior) is one that provides little or no information. Depending on the situation, uninformative priors may be quite disperse, may avoid only impossible or quite preposterous values of the parameter, or may not have modes. Uninformative priors sometimes give results similar to those obtained by traditional frequentist methods. Returning once again to our election example, the first row of the table below summarizes a prior that matched the expert's opinion, the posterior, and our inference. Now suppose that another expert also believes that Proposition A is more likely than not to pass, but his views about current public opinion are more vague. Perhaps his beta prior has parameters 56 and 46. This prior also has mode 55%, but here the prior has P(0.45 &lt; P &lt; 0.64) = 0.95. Without seeing any data, the second expert gives Proposition A a non-trivial chance of losing if the election were held today: P( P &lt; 0.5) = 0.16. After he sees the results of the poll with 621 out of 1000 in favor, his posterior beta distribution has parameters 677 and 425, so that his 95% posterior interval estimate is (59.5%, 65.2%), as recorded in the second row of the table. The reasonable choice for an uninformative prior in this situation is the uniform distribution (which is beta with parameters 1 and 1, and which has no mode); it gives the 95% posterior interval estimate (59.1%, 65.1%). In this example, the 1000 observations are enough data that the two expert priors and the uninformative prior all give somewhat similar results. (The agreement would not be so good with a small amount of data.) A non-Bayesian 95% confidence interval, based on the normal approximation, is (59.1%. 65.1%). Beta Prior Parameters Mode of Prior Distribution Prior 95% Prob. Interval Mode of Posterior Distribution Posterior 95% Prob. Interval a = 331, b = 271 55 (51.0%, 59.0%) 56.4% (57.0%, 61.8%) a = 45, b = 46 55 (45.0%, 64.0%) 61.5% (59.5%, 65.2%) a = 1, b = 1 No Mode (2.5%, 97.5%) 62.1% (59.1%, 65.1%) Frequentist (No Prior) (No Prior) ^p = 62.1% CI: (59.1%, 65.1%) Notice that, in view of the polling results, both experts were a little too pessimistic about the prospects for Proposition A. However, the first expert's &quot;sharper&quot; prior (first row of the table) is roughly equivalent in influence to surveying 600 people and so the results of the actual survey of 1000 people are not enough to override this expert's view that the true percentage in favor is unlikely to be above 60%. The second expert's somewhat &quot;flatter&quot; prior (second row) is roughly equivalent in influence to surveying 100 people and the actual survey results from 1000 people nearly override his opinion. The uninformative prior (uniform distribution in the third row) gives a mode and probability interval that agree with the frequentist point estimate and confidence interval. However, the philosophical bases of Bayesian and frequentist methods differ, as do the interpretations of the results. The Bayesian results are statements about a posterior distribution for the particular situation at hand, whereas the frequentist results must be interpreted in terms of many hypothetical repetitions of the sampling experiment. If the frequentist results depend on prior experience, it is by way of the design of the survey, whereas the Bayesian results can depend explicitly on personal probability assessments. Problems 3.4.1. The table of this section shows four rows with various point and interval estimates for the election example. Make a similar table for the beam weight example. Use the data of Problem 3.3.2: 198.14, 198.45, 196.59, 197.64, 198.12 (mean 197.79). In the first row show results for the prior N(200, 10 2 ), and in the second row results for the prior N(202, 5 2 ). There is no such thing as a truly uninformative normal prior, but select a normal prior for the third row that gives results numerically similar (to two decimal places) to frequentist results, which you should show in the fourth row. Summarize and comment upon the results shown in the table, making any extra rows that you think would add to your understanding of this example. 3.4.2. The table of this section shows four rows with various point and interval estimates for the election example. Make a similar table for the mouse counting example. Suppose, as in Section 3.3.3 that 256 mice altogether are trapped in 50 nights. In the first row show results for the gamma prior distribution with parameters a = 4 and b = 2, and in the second row results for the gamma prior with a = 2 and b = 5. There is no such thing as a truly uninformative gamma prior, but see if you can find a gamma prior for the third row that gives results numerically similar (to two decimal places) to frequentist results, which you should show in the fourth row. Summarize and comment upon the results shown in the table, making any extra rows that you think would add to your understanding of this example. Copyright © 2000 Bruce E. Trumbo. All rights reserved. Intended mainly for instructional use at California State University, Hayward. This is a draft; comments and corrections are welcome. To request permission for other uses please contact btrumbo@csuhayward.edu. 
	</PLAINTEXT>
	<CONTENT>
-->
<html xmlns:v="urn:schemas-microsoft-com:vml"
xmlns:o="urn:schemas-microsoft-com:office:office"
xmlns:w="urn:schemas-microsoft-com:office:word"
xmlns="http://www.w3.org/TR/REC-html40">

<head>
<meta http-equiv=Content-Type content="text/html; charset=windows-1252">
<meta name=ProgId content=Word.Document>
<meta name=Generator content="Microsoft Word 9">
<meta name=Originator content="Microsoft Word 9">
<link rel=File-List href="./GibbsSess4_files/filelist.xml">
<link rel=Edit-Time-Data href="./GibbsSess4_files/editdata.mso">
<!--[if !mso]>
<style>
v\:* {behavior:url(#default#VML);}
o\:* {behavior:url(#default#VML);}
w\:* {behavior:url(#default#VML);}
.shape {behavior:url(#default#VML);}
</style>
<![endif]-->
<title>Appendix B: Brief Intro. to Bayesian Inference</title>
<style>
<!--
 /* Style Definitions */
p.MsoNormal, li.MsoNormal, div.MsoNormal
	{mso-style-parent:"";
	margin:0in;
	margin-bottom:.0001pt;
	mso-pagination:widow-orphan;
	font-size:12.0pt;
	font-family:"Times New Roman";
	mso-fareast-font-family:"Times New Roman";}
a:link, span.MsoHyperlink
	{color:blue;
	text-decoration:underline;
	text-underline:single;}
a:visited, span.MsoHyperlinkFollowed
	{color:purple;
	text-decoration:underline;
	text-underline:single;}
p
	{margin-right:0in;
	mso-margin-top-alt:auto;
	mso-margin-bottom-alt:auto;
	margin-left:0in;
	mso-pagination:widow-orphan;
	font-size:12.0pt;
	font-family:"Times New Roman";
	mso-fareast-font-family:"Times New Roman";}
@page Section1
	{size:8.5in 11.0in;
	margin:1.0in 1.25in 1.0in 1.25in;
	mso-header-margin:.5in;
	mso-footer-margin:.5in;
	mso-paper-source:0;}
div.Section1
	{page:Section1;}
 /* List Definitions */
@list l0
	{mso-list-id:1415980679;
	mso-list-type:hybrid;
	mso-list-template-ids:-2147325792 624213822 1544576800 -1157990642 -961634580 1698833650 -1390388642 1036549606 1295175660 533856854;}
@list l0:level1
	{mso-level-number-format:bullet;
	mso-level-text:\F0B7;
	mso-level-tab-stop:.5in;
	mso-level-number-position:left;
	text-indent:-.25in;
	mso-ansi-font-size:10.0pt;
	font-family:Symbol;}
@list l0:level2
	{mso-level-number-format:bullet;
	mso-level-text:o;
	mso-level-tab-stop:1.0in;
	mso-level-number-position:left;
	text-indent:-.25in;
	mso-ansi-font-size:10.0pt;
	font-family:"Courier New";
	mso-bidi-font-family:"Times New Roman";}
@list l1
	{mso-list-id:1544637002;
	mso-list-type:hybrid;
	mso-list-template-ids:-2078347616 -66174040 -74182648 1129453308 319568502 1103541152 -2049814164 526919206 -412215156 901417666;}
@list l1:level1
	{mso-level-number-format:bullet;
	mso-level-text:\F0B7;
	mso-level-tab-stop:.5in;
	mso-level-number-position:left;
	text-indent:-.25in;
	mso-ansi-font-size:10.0pt;
	font-family:Symbol;}
@list l1:level2
	{mso-level-number-format:bullet;
	mso-level-text:o;
	mso-level-tab-stop:1.0in;
	mso-level-number-position:left;
	text-indent:-.25in;
	mso-ansi-font-size:10.0pt;
	font-family:"Courier New";
	mso-bidi-font-family:"Times New Roman";}
ol
	{margin-bottom:0in;}
ul
	{margin-bottom:0in;}
-->
</style>
<!--[if gte mso 9]><xml>
 <o:shapedefaults v:ext="edit" spidmax="1051"/>
</xml><![endif]--><!--[if gte mso 9]><xml>
 <o:shapelayout v:ext="edit">
  <o:idmap v:ext="edit" data="1"/>
 </o:shapelayout></xml><![endif]-->
<meta name=Template content="C:\PROGRAM FILES\MICROSOFT OFFICE\OFFICE\html.dot">
</head>

<body lang=EN-US link=blue vlink=purple style='tab-interval:.5in'>

<div class=Section1>

<table border=0 cellspacing=0 cellpadding=0 style='mso-cellspacing:0in;
 mso-padding-alt:0in 0in 0in 0in'>
 <tr>
  <td style='padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal align=center style='text-align:center'><b><span
  style='color:#AAAAAA'>CSU</span><o:p></o:p></b></p>
  </td>
  <td style='padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal align=center style='text-align:center'><b><span
  style='color:#880000'>Hayward</span><o:p></o:p></b></p>
  </td>
  <td style='padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal><![if !supportEmptyParas]>&nbsp;<![endif]><o:p></o:p></p>
  </td>
 </tr>
 <tr>
  <td style='padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal><![if !supportEmptyParas]>&nbsp;<![endif]><o:p></o:p></p>
  </td>
  <td style='padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal align=center style='text-align:center'><b><i>Statistics</i><o:p></o:p></b></p>
  </td>
  <td style='padding:.75pt .75pt .75pt .75pt'>
  <p class=MsoNormal align=center style='text-align:center'><b><i><span
  style='color:#AAAAAA'>Department</span></i><o:p></o:p></b></p>
  </td>
 </tr>
</table>


<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>


<h2 align=center style='text-align:center'><span style='font-family:Arial'>Session
4: A Brief Introduction<br>
To Bayesian Estimation<o:p></o:p></span></h2>


<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>


<p style='margin-left:1.0in'><span style='font-size:10.0pt;font-family:Arial'>Note:
This session is intended to be a free-standing introduction to certain aspects
of Bayesian inference. The material is of interest in its own right, but is
also necessary background for Session&nbsp;4. This session expands considerably
upon the very brief Appendix&nbsp;B for Suess, Fraser, and Trumbo:
&quot;Elementary Uses of the Gibbs Sampler: Applications to Medical Screening
Tests,&quot; in STATS&nbsp;#27, Winter&nbsp;2000. It contains more examples,
more detailed explanations, some additional topics, and extensive exercises
suitable for instructional use. For another introduction to Bayesian inference,
see the article by Hal Stern: &quot;A Primer on the Bayesian Approach to
Statistical Inference,&quot; in STATS&nbsp;#23, Fall&nbsp;1998, pages&nbsp;3-9.<o:p></o:p></span></p>


<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>


<h3><span style='font-family:Arial'>3.1. Introduction<o:p></o:p></span></h3>

<p style='margin-left:1.0in'>Bayesian and frequentist statistical inference
take fundamentally different viewpoints toward statistical decision making. The
frequentist view of probability, and thus of statistical inference, is based on
the idea of an experiment that can be repeated very many times. The Bayesian
view of probability and of inference is based on personal assessments of
probability and on data from a single performance of an experiment. In
practical application, both ways of thinking have advantages and disadvantages,
some of which we will explore here.</p>

<p style='margin-left:1.0in'>Statistics is a young science. For example,
interval estimation and hypothesis testing have become common in scientific
research and business decision making only within the past 75 years, and then
only gradually. On this time scale it seems strange to talk about
&quot;traditional&quot; approaches. Nevertheless, frequentist viewpoints are currently
much better established, particularly in scientific research, than Bayesian
ones. Recently, the use of Bayesian methods has been increasing, partly because
improvements in computation have made these methods easier to apply in practice
and partly because the Bayesian approach seems to be able to get useful
solutions in some applications where frequentist approaches cannot. The Gibbs
sampler, examined more fully in Session 4, is one example of a broadly
applicable, computationally intensive Bayesian method.</p>

<p style='margin-left:1.0in'>We will see later on that, for the very simple
examples considered here, Bayesian and frequentist methods give similar
results. But that is not the main point. We hope you will gain some
appreciation that Bayesian methods are sometimes the most natural useful ones
in practice.</p>

<p style='margin-left:1.0in'>For most people, the starkest contrast between
frequentist and Bayesian approaches is that Bayesian inference provides the
opportunity  even makes it a requirement  to take into account
&quot;information&quot; that is available before any data is collected. That is
where we begin.</p>

<h3><span style='font-family:Arial'>3.2. Prior Distributions: Personal Opinions
and Expert Knowledge<o:p></o:p></span></h3>

<p style='margin-left:1.0in'>The Bayesian approach to statistical inference
treats population parameters as random variables (not as fixed, unknown
constants). The distributions of these parameters are called <i>prior
distributions</i>. Often both expert knowledge and mathematical convenience
play a role in selecting a particular type of prior distribution.</p>

<p style='margin-left:.5in'><b><span style='font-family:Arial'>3.2.1. Example
1: Election polling<o:p></o:p></span></b></p>

<p style='margin-left:1.0in'>Suppose that Proposition A is on the ballot for an
upcoming statewide election, and that a political consultant has been hired to
help manage the campaign for its passage. The proportion <span
style='font-family:Symbol'>P</span> of prospective voters who currently favor
Proposition A is the population parameter of interest here. Based on her knowledge
of the politics of the state, the consultant's judgment is that the proposition
is almost sure to pass, but not by a large margin. She believes that the most
likely proportion of voters in favor is 55% and that the percentage is not
likely to be below 51% or above&nbsp;59%.</p>

<p style='margin-left:1.0in'>It is reasonable to consider the beta distribution
to model the expert's opinion of the proportion in favor because distributions
in the beta family take values in the interval (0,&nbsp;1) as do proportions.
This family of distributions has density functions of the form</p>

<p align=center style='margin-left:1.0in;text-align:center'><i>f</i>(<span
style='font-family:Symbol'>p</span>) = <i>K</i><sub>1</sub><span
style='font-family:Symbol'>p<sup>a</sup></span><sup>1</sup>(1  <span
style='font-family:Symbol'>p</span>)<sup><span style='font-family:Symbol'>b</span>1</sup>,
0 &lt; <span style='font-family:Symbol'>p</span> &lt; 1.</p>

<p style='margin-left:1.0in'>where <span style='font-family:Symbol'>a</span>,&nbsp;<span
style='font-family:Symbol'>b</span>&nbsp;&gt;&nbsp;0, and where <i>K</i><sub>1</sub>
is a constant chosen so that <i>f</i>(<span style='font-family:Symbol'>p</span>)
integrates to 1 over (0,&nbsp;1).</p>

<p style='margin-left:1.0in'>A member of the beta family that corresponds
roughly to the expert's opinion has <span style='font-family:Symbol'>a</span>&nbsp;=&nbsp;331
and <span style='font-family:Symbol'>b</span>&nbsp;=&nbsp;271. </p>

<p class=MsoNormal style='mso-margin-top-alt:auto;mso-margin-bottom-alt:auto;
margin-left:1.5in;text-indent:-.25in;mso-list:l1 level2 lfo2;tab-stops:list 1.0in'><![if !supportLists]><span
style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:"Courier New";
mso-bidi-font-family:"Times New Roman"'>o<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span><![endif]>This density curve has its mode at (<span
style='font-family:Symbol'>a</span>&nbsp;&nbsp;1)&nbsp;/&nbsp;(<span
style='font-family:Symbol'>a</span>&nbsp;+&nbsp;<span style='font-family:Symbol'>b</span>&nbsp;&nbsp;2)
=&nbsp;0.55. </p>

<p class=MsoNormal style='mso-margin-top-alt:auto;mso-margin-bottom-alt:auto;
margin-left:1.5in;text-indent:-.25in;mso-list:l1 level2 lfo2;tab-stops:list 1.0in'><![if !supportLists]><span
style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:"Courier New";
mso-bidi-font-family:"Times New Roman"'>o<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span><![endif]>Numerical integration shows that P(0.51&nbsp;&lt;&nbsp;<span
style='font-family:Symbol'>P</span>&nbsp;&lt;&nbsp;0.59) =&nbsp;0.95. </p>

<p style='margin-left:1.0in'>Of course, many other distributional shapes share
these two numerical properties, but we choose a member of the beta family
because it makes the mathematics relatively easy and because we have no reason
to believe its shape is inappropriate here.</p>

<p style='margin-left:1.0in'>If the political consultant's judgments about the
political situation are correct, then they may be helpful in managing the
campaign. If she too often brings bad judgment to her clients, her reputation
will suffer and she will be out of the political consulting business before
long. Fortunately, as we will see, the details of her judgments become less
important if we also have polling data to rely upon.</p>

<p><b><span style='font-family:Arial'>3.2.2. Example 2: Weighing an object<o:p></o:p></span></b></p>

<p style='margin-left:1.0in'>A construction company buys steel beams with a
nominal weight of 200&nbsp;lb. Experience with a particular supplier of these
beams has shown that their beams very seldom weigh less than 180 or more than
220&nbsp;lb. In these circumstances it may be convenient and reasonable to
suppose that the distribution of weights of beams from this supplier is normal
with mean&nbsp;200 and standard deviation&nbsp;10. </p>

<p style='margin-left:1.0in'>Usually, the exact weight of a beam is not
especially important, but there are some situations in which it is crucial to
know the weight of a beam more precisely. Then a particular beam is selected
and weighed several times on a scale. The scale is known to give results that
are normally distributed without bias, but with a standard deviation of
1&nbsp;lb. Here it seems reasonable to use N(200,&nbsp;10<sup>2</sup>) as the
prior distribution of the weight <span style='font-family:Symbol'>m</span> of a
beam. The hope is that the weighing process will help us to know its true
weight more accurately.</p>

<p style='margin-left:1.0in'>Theoretically, the frequentist statistician would
ignore &quot;prior&quot; or background experience in doing statistical
inference, basing statistical decisions only on the data collected when a beam
is weighed. In real life it is not so simple. For example, the design of the
weighing experiment will very likely take past experience into account in one
way or another. For the Bayesian statistician the explicit codification of some
kinds of background information into a prior distribution is a required first
step.</p>

<p style='margin-left:.5in'><b><span style='font-family:Arial'>3.2.3. Example
3: Counting mice<o:p></o:p></span></b></p>

<p style='margin-left:1.0in'>An island in the middle of a river is one of the
last known habitats of an endangered kind of mouse. The mice rove about the
island in ways that are not fully understood and so are taken as random.
Ecologists are interested in the average number of mice to be found in
particular regions of the island. To do the counting in a region they set many
traps there at night, using bait that is irresistible to mice at close range.
In the morning they count and release the mice caught. It seems reasonable to
suppose that almost all of the mice in the region the previous night were
caught and that the number of them on any one night has a Poisson distribution.
The purpose of the trapping is to estimate the mean <span style='font-family:
Symbol'>l</span> of this distribution.</p>

<p style='margin-left:1.0in'>Even before the trapping is done the ecologists
doing this study have some information about <span style='font-family:Symbol'>l</span>.
For example, even though the mice are quite shy, there have been occasional
sightings of them in almost all regions of the island, so it seems likely that <span
style='font-family:Symbol'>l</span> &gt; 1. On the other hand, from what is
known of the habits of the mice and the food supply in the regions, it seems
unlikely that there would be as many as 25 of them in any one region at a given
time. In these circumstances, it may be reasonable to use a gamma distribution
with shape parameter <span style='font-family:Symbol'>a</span>&nbsp;=&nbsp;4
and scale parameter <span style='font-family:Symbol'>b</span>&nbsp;=&nbsp;3 as
a prior distribution for values&nbsp;<span style='font-family:Symbol'>l</span>
of the random variable&nbsp;<span style='font-family:Symbol'>L</span>. This
gamma distribution has density function <i>f</i>(<i>x</i>)&nbsp;=&nbsp;<i>K</i><sub>1</sub>&nbsp;<i>x</i><sup><span
style='font-family:Symbol'>a</span>1</sup><i>e</i><sup></sup><sup><span
style='font-family:Symbol'>b</span><i>x</i></sup> (for <i>x</i>&nbsp;&gt;&nbsp;0),
mean <span style='font-family:Symbol'>ab</span>&nbsp;=&nbsp;12, mode (<span
style='font-family:Symbol'>a</span>&nbsp;&nbsp;1)<span style='font-family:
Symbol'>b</span>&nbsp;=&nbsp;9, variance <span style='font-family:Symbol'>ab</span><sup>2</sup>&nbsp;=&nbsp;36,
and standard deviation&nbsp;6. </p>

<p style='margin-left:1.0in'>A member of the gamma family may be a reasonable
prior because <span style='font-family:Symbol'>l</span> must be positive and
members of the gamma family take only positive values. We will see later that
choosing a gamma prior simplifies some important mathematical computations. </p>

<p style='margin-left:1.0in'>These technicalities of the prior distribution
aside, it is clear that the experience of the ecologists with the island and
its endangered mice will influence the course of this investigation in many
ways: dividing the island into meaningful regions; modeling the randomness of
mouse movement as Poisson, the number of traps to use and where to place them,
what to use for bait so as to attract mice from a region of interest but not
from all over the island, and so on. The expression of some of their background
knowledge as a prior distribution is perhaps a relatively small use of their
expertise. But it is a necessary first step in Bayesian inference, and it is
perhaps the only aspect of their expert opinion that will be explicitly
tempered by the data that are collected.</p>

<h3 style='margin-left:.5in'><i><span style='font-family:Arial'>Problems<o:p></o:p></span></i></h3>

<p style='margin-left:1.0in'>3.2.1. In Example 1, show that for appropriate
values of <span style='font-family:Symbol'>a</span> and <span style='font-family:
Symbol'>b</span>, the density function has a unique mode at (<span
style='font-family:Symbol'>a</span>&nbsp;&nbsp;1)/(<span style='font-family:
Symbol'>a</span>&nbsp;+&nbsp;<span style='font-family:Symbol'>b</span>&nbsp;&nbsp;2).
[Hint: Differentiate the density function, set the derivative equal to&nbsp;0,
and note the values of&nbsp;<span style='font-family:Symbol'>a</span> and&nbsp;<span
style='font-family:Symbol'>b</span> for which the solution of this equation
yields an absolute maximum.]</p>

<p style='margin-left:1.0in'>3.2.2. In Example&nbsp;3, verify the stated value
of the mode, and say for what values of&nbsp;<span style='font-family:Symbol'>a</span>
and&nbsp;<span style='font-family:Symbol'>b</span> the mode exists.</p>

<p style='margin-left:1.0in'>3.2.3. In Example&nbsp;1, suppose that <span
style='font-family:Symbol'>a</span>&nbsp;=&nbsp;2 and <span style='font-family:
Symbol'>b</span>&nbsp;=&nbsp;1. Find the value of the constant&nbsp;<i>K</i><sub>1</sub>
that makes this beta density function integrate to&nbsp;1. Find the mean and
variance. Find the values&nbsp;<i>a</i> and&nbsp;<i>b</i> such that P(<span
style='font-family:Symbol'>P</span>&nbsp;&lt;&nbsp;<i>a</i>)&nbsp;= P(<span
style='font-family:Symbol'>P</span>&nbsp;&gt;&nbsp;<i>b</i>)&nbsp;=&nbsp;0.025.
Sketch the density function. Repeat for <span style='font-family:Symbol'>a</span>&nbsp;=&nbsp;1
and <span style='font-family:Symbol'>b</span>&nbsp;=&nbsp;1</p>

<p style='margin-left:1.0in'>3.2.4. In Example 3, suppose that <i>a</i>&nbsp;=&nbsp;1
and <i>b</i>&nbsp;=&nbsp;10. Find&nbsp;<i>K</i><sub>1</sub>, E(<span
style='font-family:Symbol'>L</span>), V(<span style='font-family:Symbol'>L</span>),
and <i>a</i>&nbsp;and&nbsp;<i>b</i> such that P(<span style='font-family:Symbol'>L</span>&nbsp;&lt;&nbsp;<i>a</i>)&nbsp;=
P(<span style='font-family:Symbol'>L</span>&nbsp;&gt;&nbsp;<i>b</i>) = 0.025.</p>

<p style='margin-left:1.0in'>3.2.5. For each of the three examples of this
section sketch the given prior distribution (for the specific parameter values
stated). Then use tables, Minitab, S-Plus, or other statistical software to
find values <i>a</i> and <i>b</i> that cut off the upper and lower 2.5% of the
stated prior distributions. Shade in the corresponding areas. An example of a
typical Minitab instruction is as follows: </p>

<p style='margin-left:1.0in'><span style='font-size:10.0pt;font-family:"Courier New"'>MTB
&gt; invcdf 0.975;<br>
SUBC&gt; beta 331 271.<o:p></o:p></span></p>

<p style='margin-left:1.0in'>For Example&nbsp;2, it is obvious that the true
weight cannot be negative. Why is it safe to ignore the fact that the suggested
normal prior distribution includes negative values?</p>

<p style='margin-left:1.0in'>3.2.6. Suggest reasonable prior distributions in
the following cases:</p>

<p style='margin-left:1.0in'>(a) In Example 1, suppose that a very successful
professional political fundraiser from another state has been hired. He doesn't
know what Proposition&nbsp;A is about and has no experience with the politics
of this state. Before he sees some polling data he has absolutely no idea what
percentage of the voters favor Proposition&nbsp;A.</p>

<p style='margin-left:1.0in'>(b) In Example&nbsp;2, the mean weight of the
beams from the supplier is around 200&nbsp;lb. and we suppose that about half of
them weigh between 195 and&nbsp;205&nbsp;lb.</p>

<p style='margin-left:1.0in'>(c) In Example&nbsp;3, we seek a prior with P(<span
style='font-family:Symbol'>L</span>&nbsp;&lt;&nbsp;0.5)&nbsp;<span
style='font-family:Symbol'>»</span> &nbsp;0.025, P(<span style='font-family:
Symbol'>L</span>&nbsp;&gt;&nbsp;75)&nbsp;<span style='font-family:Symbol'>»</span>
&nbsp;0.025 and E(<span style='font-family:Symbol'>L</span>)&nbsp;<span
style='font-family:Symbol'>»</span> &nbsp;20.</p>

<p style='margin-left:1.0in'>(d) In Example&nbsp;1, now suppose the fundraiser
is unfamiliar with opinions in the state, does know what Proposition&nbsp;A is
about, is sure that it will be extremely controversial with a very low
probability that the percentage in favor is anywhere near&nbsp;1/2, but he has
no opinion whether the sentiment will be overwhelmingly for or overwhelmingly
against. (Suggest general ranges of parameter values.)</p>

<h3><span style='font-family:Arial'>3.3. Data and Posterior Distributions <o:p></o:p></span></h3>

<p style='margin-left:1.0in'>The second step in Bayesian inference is to
collect data and to combine the information in the data with the expert opinion
represented by the prior distribution. The result is a <i>posterior distribution</i>
that can be used for inference. The computation of the posterior distribution
uses <i>Bayes' Theorem.</i> You have probably seen Bayes' Theorem stated for an
event <i>E</i> and a partition {<i>B</i><sub>1</sub>,&nbsp;<i>B</i><sub>2</sub>,&nbsp;...,&nbsp;<i>B</i><sub>k</sub>}
of a sample space&nbsp;<i>S</i>. (The&nbsp;<i>B<sub>i</sub></i> are mutually
exclusive events whose union is&nbsp;<i>S</i>):</p>

<p align=center style='margin-left:1.0in;text-align:center'><img width=188
height=68 id="_x0000_i1028" src=S4PartitBayes.gif></p>

<p style='margin-left:1.0in'>for <i>j</i>&nbsp;=&nbsp;1, ..., <i>k</i>. Even in
this most elementary setting, the quantities P(<i>B<sub>j</sub></i>) are called
prior probabilities and P(<i>B<sub>j</sub></i>|<i>E</i>) are called posterior
probabilities. </p>

<p style='margin-left:1.0in'>A more general version of Bayes' Theorem for
distributions involving data&nbsp;<i>x</i> and a parameter&nbsp;<span
style='font-family:Symbol'>p</span> is as follows:</p>

<p align=center style='margin-left:1.0in;text-align:center'><img width=192
height=49 id="_x0000_i1029" src=S4GenlBayes.gif></p>

<p style='margin-left:1.0in'>where the integral is taken over the region where
the integrand is positive. The denominator of this fraction is a constant&nbsp;<i>J</i>.
The conditional density <i>f</i>(<span style='font-family:Symbol'>p</span>|<i>x</i>)
is the posterior distribution of&nbsp;<span style='font-family:Symbol'>P</span>
given&nbsp;<i>X</i>, evaluated for the specific numerical values <i>X</i>&nbsp;=&nbsp;<i>x</i>
and&nbsp;<span style='font-family:Symbol'>P</span>&nbsp;=&nbsp;<span
style='font-family:Symbol'>p</span>. </p>

<p style='margin-left:.5in'><b><span style='font-family:Arial'>3.3.1. Example 1
(election) continued.<o:p></o:p></span></b></p>

<p style='margin-left:1.0in'>In our election example, suppose <i>n</i>&nbsp;=&nbsp;1000
subjects are selected at random. Assuming the specific parameter value <span
style='font-family:Symbol'>P</span>&nbsp;=&nbsp;<span style='font-family:Symbol'>p</span>,
the number&nbsp;<i>X</i> of them in favor of Proposition&nbsp;A is a random
variable with the binomial distribution</p>

<p align=center style='margin-left:1.0in;text-align:center'><i>f</i>(<i>x</i>|<span
style='font-family:Symbol'>p</span>) = <i>K</i><sub>2</sub> <span
style='font-family:Symbol'>p</span><i><sup>x</sup></i>(1&nbsp;&nbsp;<span
style='font-family:Symbol'>p</span>)<i><sup>n</sup></i><sup><i>x</i></sup>,</p>

<p style='margin-left:1.0in'>for<i> x</i>&nbsp;=&nbsp;0,&nbsp;1,&nbsp;2, ..., <i>n</i>,
and where <i>K</i><sub>2</sub>&nbsp;=&nbsp;<i>n</i><b>!</b>/[<i>x</i><b>!</b>(<i>n</i><i>x</i>)<b>!</b>]
is the constant that makes the distribution sum to&nbsp;1. Then the general
version of Bayes' Theorem gives the posterior distribution</p>

<p align=center style='margin-left:1.0in;text-align:center'><i>f</i>(<span
style='font-family:Symbol'>p</span>|<i>x</i>) = <i>K</i><sub>3</sub><span
style='font-family:Symbol'>p</span><i><sup>x</sup></i>(1&nbsp;&nbsp;<span
style='font-family:Symbol'>p</span>)<i><sup>n</sup></i><sup><i>x</i></sup>&nbsp;<span
style='font-family:Symbol'>p<sup>a</sup></span><sup>1</sup>(1&nbsp;&nbsp;<span
style='font-family:Symbol'>p</span>)<sup><span style='font-family:Symbol'>b</span>1</sup>&nbsp;=&nbsp;<i>K</i><sub>3</sub><span
style='font-family:Symbol'>p</span><i><sup>x</sup></i><sup>+</sup><sup><span
style='font-family:Symbol'>a</span>1</sup>(1&nbsp;&nbsp;<span
style='font-family:Symbol'>p</span>)<i><sup>n</sup></i><sup><i>x</i>+</sup><sup><span
style='font-family:Symbol'>b</span>1</sup>,</p>

<p style='margin-left:1.0in'>where <i>K</i><sub>3</sub>&nbsp;=&nbsp;<i>K</i><sub>1</sub><i>K</i><sub>2</sub>/<i>J</i>.
If <i>x</i>&nbsp;=&nbsp;621 of the <i>n</i>&nbsp;=&nbsp;1000 subjects
interviewed favor Proposition&nbsp;A, it is then clear that the posterior has a
beta distribution with parameters <i>x</i>&nbsp;+&nbsp;<span style='font-family:
Symbol'>a</span>&nbsp;=&nbsp;952 and <i>n</i>&nbsp;&nbsp;<i>x</i>&nbsp;+&nbsp;<span
style='font-family:Symbol'>b</span>&nbsp;=&nbsp;650. According to this
posterior distribution, P(0.570&nbsp;&lt;&nbsp;<span style='font-family:Symbol'>P</span>&nbsp;&lt;&nbsp;0.618)
=&nbsp;0.95, so that a 95% <i>posterior probability interval</i> for the
percent in favor is (57.0%,&nbsp;61.8%). </p>

<p style='margin-left:1.0in'>Note the following three aspects of this
development: </p>

<p class=MsoNormal style='mso-margin-top-alt:auto;mso-margin-bottom-alt:auto;
margin-left:1.5in;text-indent:-.25in;mso-list:l0 level2 lfo4;tab-stops:list 1.0in'><![if !supportLists]><span
style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:"Courier New";
mso-bidi-font-family:"Times New Roman"'>o<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span><![endif]>The interval estimate for&nbsp;<span style='font-family:
Symbol'>P</span> is a straightforward probability statement. Unlike frequentist
&quot;confidence intervals,&quot; it does not require the user to imagine a
repeatable experiment. </p>

<p class=MsoNormal style='mso-margin-top-alt:auto;mso-margin-bottom-alt:auto;
margin-left:1.5in;text-indent:-.25in;mso-list:l0 level2 lfo4;tab-stops:list 1.0in'><![if !supportLists]><span
style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:"Courier New";
mso-bidi-font-family:"Times New Roman"'>o<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span></span><![endif]>The mathematical forms of the beta prior and the
binomial data are similar, making it especially easy to find the posterior. In
such cases we say that the beta is a &quot;conjugate prior&quot; to the
binomial. </p>

<p style='margin-left:1.0in'>Often in Bayesian distributional computations it
is unnecessary to know the actual values of the constants involved. Then the
proportionality symbol <span style='font-family:Symbol'>µ</span> can be used to
show relationships. For example, the first equation in this subsection could
have been written as <i>f</i>(<i>x</i>|<span style='font-family:Symbol'>p</span>)&nbsp;<span
style='font-family:Symbol'>µ</span>&nbsp;<span style='font-family:Symbol'>p</span><i><sup>x</sup></i>(1&nbsp;&nbsp;<span
style='font-family:Symbol'>p</span>)<i><sup>n</sup></i><sup><i>x</i></sup>,
where the expression on the right is the kernel of the density function. The <i>kernel</i>
is the part of a density function that displays necessary variables and
parameters, but does not display the constant of integration.</p>

<p style='margin-left:.5in'><b><span style='font-family:Arial'>3.3.2. Example 2
(weighing a beam) continued.<o:p></o:p></span></b></p>

<p style='margin-left:1.0in'>Suppose that a particular beam is selected from
among the beams available. The prior distribution is N(200,&nbsp;10<sup>2</sup>)
and so <i>f</i>(<span style='font-family:Symbol'>m</span>)&nbsp;<span
style='font-family:Symbol'>µ</span> &nbsp;exp&nbsp;[(<span style='font-family:
Symbol'>m</span>&nbsp;&nbsp;<span style='font-family:Symbol'>m</span><sub>0</sub>)<sup>2</sup>/<span
style='font-family:Symbol'>t<sup>2</sup></span>], where <span style='font-family:
Symbol'>m</span><sub>0</sub>&nbsp;=&nbsp;100 and <span style='font-family:Symbol'>t</span>&nbsp;=&nbsp;10.
The data <b>x</b>&nbsp;=&nbsp;(<i>x</i><sub>1</sub>,&nbsp;...,&nbsp;<i>x</i><sub>5</sub>)
provide the likelihood function </p>

<p align=center style='margin-left:1.0in;text-align:center'><i>f</i>(<b>x</b>|<span
style='font-family:Symbol'>m</span>) <span style='font-family:Symbol'>µ</span>
exp <span style='font-size:13.5pt'>{</span><span style='font-size:13.5pt;
font-family:Symbol'>S</span>[(<i>x<sub>i</sub></i>  <span style='font-family:
Symbol'>m</span>)<sup>2</sup>/2<span style='font-family:Symbol'>s</span><sup>2</sup>]<span
style='font-size:13.5pt'>}</span>,</p>

<p style='margin-left:1.0in'>where <span style='font-family:Symbol'>m</span>
determined by the prior distribution, the <i>x<sub>i</sub></i> are five
observed numbers, and <span style='font-family:Symbol'>s</span><sup>2</sup>&nbsp;=&nbsp;1.
Then, after some algebra (see Problem&nbsp;3.3.6), the posterior can be written
as</p>

<p align=center style='margin-left:1.0in;text-align:center'><i>f</i>(<span
style='font-family:Symbol'>m</span>|<b>x</b>) <span style='font-family:Symbol'>µ</span>
<i>f</i>(<span style='font-family:Symbol'>m</span>)<i>f</i>(<b>x</b>|<span
style='font-family:Symbol'>m</span>) <span style='font-family:Symbol'>µ</span>
exp [(<span style='font-family:Symbol'>m</span>  <span style='font-family:
Symbol'>m</span><i><sub>n</sub></i>)<sup>2</sup>/2<i>V<sub>n</sub></i>],</p>

<p style='margin-left:1.0in'>which is the kernel of N(<span style='font-family:
Symbol'>m</span><i><sub>n</sub></i>,&nbsp;<i>V<sub>n</sub></i>), where</p>

<p align=center style='margin-left:1.0in;text-align:center'><img width=288
height=94 id="_x0000_i1030" src=S4MnVarNormPost.gif></p>

<p style='margin-left:1.0in'>It is common to use the term <i>precision</i> to
refer to the reciprocal of a variance. Using this terminology, we say that <span
style='font-family:Symbol'>m</span><i><sub>n</sub></i> is a weighted average of
the prior mean and the sample mean, where the precisions are the weights.</p>

<p style='margin-left:1.0in'>Here the precision of each observation is much
greater than the precision of the prior. In this case even our small sample of
five observations is enough to diminish the impact of the prior on the
posterior distribution.</p>

<p style='margin-left:.5in'><b><span style='font-family:Arial'>3.3.3. Example 3
(counting mice) continued<o:p></o:p></span></b></p>

<p style='margin-left:1.0in'>Suppose that a region of the island is selected
where the gamma prior distribution with parameters <span style='font-family:
Symbol'>a</span>&nbsp;=&nbsp;4 and <span style='font-family:Symbol'>b</span>&nbsp;=&nbsp;3
is reasonable. This prior has density <i>f</i>(<span style='font-family:Symbol'>l</span>)&nbsp;<span
style='font-family:Symbol'>µ</span> &nbsp;<span style='font-family:Symbol'>l<sup>a</sup></span><sup>
- 1</sup><i>e</i><sup>l/</sup><sup><span style='font-family:Symbol'>b</span></sup>.
Over a period of about a year traps are set out on <i>n</i> = 50 nights with
total number of captures <i>T</i>&nbsp;=&nbsp;<span style='font-family:Symbol'>S</span>&nbsp;<i>x<sub>i</sub></i>&nbsp;=&nbsp;256,
for an average of 5.12 mice captured per night. This gives the <i>likelihood<o:p></o:p></i></p>

<p align=center style='margin-left:1.0in;text-align:center'><img width=225
height=30 id="_x0000_i1031" src=S4PoisPost.gif></p>

<p style='margin-left:1.0in'>Thus, the <i>posterior</i> distribution <i>f</i>(<span
style='font-family:Symbol'>l</span>|<b>x</b>) is gamma with parameters given by
<span style='font-family:Symbol'>a</span><i><sub>n</sub></i>&nbsp;= <i>T</i>&nbsp;+&nbsp;<span
style='font-family:Symbol'>a</span>&nbsp;=&nbsp;260 and 1/<span
style='font-family:Symbol'>b</span><i><sub>n</sub></i>&nbsp;=&nbsp;<i>n</i>&nbsp;+&nbsp;1/<span
style='font-family:Symbol'>b</span>&nbsp;= 50&nbsp;+&nbsp;1/3, so that <span
style='font-family:Symbol'>b</span><i><sub>n</sub></i>&nbsp;=&nbsp;0.0199. A 95%
Bayesian probability interval for <span style='font-family:Symbol'>l</span>
based on this posterior distribution is (4.56,&nbsp;5.82).</p>

<p style='margin-left:.5in'><b><i><span style='font-family:Arial'>Problems<o:p></o:p></span></i></b></p>

<p style='margin-left:1.0in'>3.3.1. This problem deals with point and interval
estimates of the population proportion in the continuation of Example&nbsp;1.</p>

<p style='margin-left:2.0in'>(a) It might be reasonable to use the mode of the
posterior distribution as a point estimate of the proportion in favor of
Proposition&nbsp;A in the population. What is its numerical value?</p>

<p style='margin-left:2.0in'>(b) Use a computer package to verify the endpoints
of the Bayesian probability interval given in the Section&nbsp;3.3.2. This is a
probability-symmetric interval which excludes 2.5% of the probability in each
tail of the posterior distribution. </p>

<p style='margin-left:2.0in'>(c) Taking a frequentist point of view, use the
normal approximation to the binomial distribution to find an approximate 95%
confidence interval for the population proportion. What is the main difference
between this interval and the one verified in part&nbsp;(a)?</p>

<p style='margin-left:2.0in'>(d) What Bayesian point estimate (mode) and 95%
probability interval would have resulted from using the uniform distribution on
(0,&nbsp;1) as the prior distribution?</p>

<p style='margin-left:1.0in'>3.3.2. This problem deals with point and interval
estimates of <span style='font-family:Symbol'>m</span> in Example&nbsp;2.
Suppose that the five measurements of the weight of the beam are: 198.14,
198.45, 196.59, 197.64, 198.12 (mean&nbsp;197.79).</p>

<p style='margin-left:2.0in'>(a) What is the point estimate of <span
style='font-family:Symbol'>m</span>. In this instance, does it matter whether
we choose the mean, the median, or the mode of the posterior distribution as
our point estimate? Explain.</p>

<p style='margin-left:2.0in'>(b) Find a 95% Bayesian probability estimate
of&nbsp;<span style='font-family:Symbol'>m</span>.</p>

<p style='margin-left:2.0in'>(c) Suppose we would be unwilling to use this beam
for a particular purpose if we thought it weighed less than 197&nbsp;lb. What
are the chances of that?</p>

<p style='margin-left:2.0in'>(d) Taking a frequentist point of view, use the
five observations and the known variance of measurements produced by the scale
to give a 95% confidence interval for the true weight of the beam.</p>

<p style='margin-left:2.0in'>(e) What Bayesian point estimate and 95%
probability interval would have resulted from using a normal distribution with
mean 202 and standard deviation 5 as the prior?</p>

<p style='margin-left:1.0in'>3.3.3. This problem deals with point and interval
estimates of <span style='font-family:Symbol'>l</span> in Example&nbsp;3 based
on the data given in Sec.&nbsp;3.3.3.</p>

<p style='margin-left:2.0in'>(a) Find the mean, median, and mode of the
posterior distribution.</p>

<p style='margin-left:2.0in'>(b) Verify the 95% Bayesian probability interval
given in Sec.&nbsp;3.3.3. Strictly speaking, shorter 95% probability intervals
can be found because of the skewness of the distribution. For such a shorter
interval would slightly more or slightly less that 2.5% be omitted from the
right tail?</p>

<p style='margin-left:2.0in'>(c) Taking a frequentist point of view, find an
approximate 95% confidence interval for&nbsp;<i>n</i><span style='font-family:
Symbol'>l</span> and hence for&nbsp;<span style='font-family:Symbol'>l</span>.
Use the fact that <i>T</i> has a Poisson distribution that is well-approximated
by an appropriate normal distribution.</p>

<p style='margin-left:2.0in'>(d) Suppose that the prior distribution had been
gamma with parameters <span style='font-family:Symbol'>a</span>&nbsp;=&nbsp;3
and <span style='font-family:Symbol'>b</span>&nbsp;=&nbsp;1. Give an interval that
contains 95% of the probability under this prior distribution. Use the same
data as above (<i>T</i>&nbsp;=&nbsp;256) and find the corresponding 95%
Bayesian probability interval for&nbsp;<span style='font-family:Symbol'>l</span>.</p>

<p style='margin-left:1.0in'>3.3.4. Derive the posterior distribution resulting
from a beta prior and binomial data. Verify the specific parameter values given
in Sec.&nbsp;3.3.1.</p>

<p style='margin-left:1.0in'>3.3.5. Derive the posterior distribution resulting
from a gamma prior and Poisson data. Verify the specific parameter values given
in Section&nbsp;3.3.3. (The gamma prior is conjugate to the Poisson
distribution of the data.)</p>

<p style='margin-left:1.0in'>3.3.6. In this problem we invite you to derive the
posterior distribution for Example&nbsp;2 in Section&nbsp;3.3.2.</p>

<p style='margin-left:2.0in'>(a) Show that the expression <i>f</i>(<b>x</b>|<span
style='font-family:Symbol'>m</span>)&nbsp;<span style='font-family:Symbol'>µ</span>
exp <span style='font-size:13.5pt'>{</span><span style='font-size:13.5pt;
font-family:Symbol'>S</span>[(<i>x<sub>i</sub></i>&nbsp;&nbsp;<span
style='font-family:Symbol'>m</span>)<sup>2</sup>/2<span style='font-family:
Symbol'>s</span><sup>2</sup>]<span style='font-size:13.5pt'>}</span> for the <i>likelihood
</i>function is correct and can be re-expressed as</p>

<p align=center style='margin-left:2.0in;text-align:center'><img width=225
height=49 id="_x0000_i1032" src=S4NormLike.gif></p>

<p style='margin-left:2.0in'>The likelihood function is the joint density
function of the data viewed as a function of&nbsp;<span style='font-family:
Symbol'>m</span> (rather than of the&nbsp;<i>x<sub>i</sub></i>). By independence,
the joint density function is proportional to <span style='font-size:13.5pt;
font-family:Symbol'>P</span>&nbsp;exp<span style='font-size:13.5pt'>{</span>[(<i>x<sub>i</sub></i>&nbsp;&nbsp;<span
style='font-family:Symbol'>m</span>)<sup>2</sup>/2<span style='font-family:
Symbol'>s</span><sup>2</sup>]<span style='font-size:13.5pt'>}</span>, which is
clearly equal to the first expression for the likelihood function. To put the
first expression in the form displayed just above, write </p>

<p align=center style='margin-left:2.0in;text-align:center'><img width=228
height=25 id="_x0000_i1033" src=S4NormLikeExp.gif></p>

<p style='margin-left:2.0in'>expand the square and sum over&nbsp;<i>i</i>. On
distributing the sum you should obtain three terms. One of the terms provides
the desired result; another is&nbsp;0, and the third is irrelevant because it
does not contain the variable&nbsp;<span style='font-family:Symbol'>m</span>.
(A constant term in the exponential amounts to a constant factor in the
density, which is not included in the kernel.)</p>

<p style='margin-left:2.0in'>(b) Derive the expression for the kernel of the <i>posterior</i>
given in Section&nbsp;3.3.2. Multiply the kernels of the prior and the
likelihood, and expand the squares in each. Put everything in the exponential
over a common denominator. Then, remembering that <span style='font-family:
Symbol'>m</span> is the variable, collect terms in <span style='font-family:
Symbol'>m</span><sup>2</sup> and&nbsp;<span style='font-family:Symbol'>m</span>.
Terms in the exponent that do not involve <span style='font-family:Symbol'>m</span>
are constant factors in the posterior density that may be adjusted at will to
complete the square in order to obtain the desired kernel.</p>

<h3><span style='font-family:Arial'>3.4. More About Prior Distributions<o:p></o:p></span></h3>

<p style='margin-left:1.0in'>One issue of concern in Bayesian inference is how
strongly the particular selection of a prior distribution influences the
results of the inference. Particularly if results are to be used by people who
may question the expert's opinion, it is desirable to have enough data that the
influence of the prior is slight.</p>

<p style='margin-left:1.0in'>An <i>uninformative prior</i> (sometimes called a
flat prior) is one that provides little or no information. Depending on the
situation, uninformative priors may be quite disperse, may avoid only
impossible or quite preposterous values of the parameter, or may not have
modes. Uninformative priors sometimes give results similar to those obtained by
traditional frequentist methods.</p>

<p style='margin-left:1.0in'>Returning once again to our election example, the
first row of the table below summarizes a prior that matched the expert's
opinion, the posterior, and our inference. Now suppose that another expert also
believes that Proposition&nbsp;A is more likely than not to pass, but his views
about current public opinion are more vague. Perhaps his beta prior has
parameters 56 and&nbsp;46. This prior also has mode&nbsp;55%, but here the
prior has P(0.45&nbsp;&lt;&nbsp;<span style='font-family:Symbol'>P</span>&nbsp;&lt;&nbsp;0.64)&nbsp;=&nbsp;0.95.
Without seeing any data, the second expert gives Proposition&nbsp;A a
non-trivial chance of losing if the election were held today: P(<span
style='font-family:Symbol'>P</span>&nbsp;&lt;&nbsp;0.5)&nbsp;=&nbsp;0.16. </p>

<p style='margin-left:1.0in'>After he sees the results of the poll with 621 out
of 1000 in favor, his posterior beta distribution has parameters 677 and 425,
so that his 95% posterior interval estimate is (59.5%,&nbsp;65.2%), as recorded
in the second row of the table. </p>

<p style='margin-left:1.0in'>The reasonable choice for an uninformative prior
in this situation is the uniform distribution (which is beta with parameters 1
and&nbsp;1, and which has no mode); it gives the 95% posterior interval
estimate (59.1%,&nbsp;65.1%).</p>

<p style='margin-left:1.0in'>In this example, the 1000 observations are enough
data that the two expert priors and the uninformative prior all give somewhat
similar results. (The agreement would not be so good with a small amount of
data.) A non-Bayesian 95%&nbsp;confidence interval, based on the normal
approximation, is (59.1%.&nbsp;65.1%).</p>

<div align=center>

<table border=1 cellspacing=1 cellpadding=0 width=700 style='width:525.0pt;
 mso-cellspacing:.7pt;mso-padding-alt:5.25pt 5.25pt 5.25pt 5.25pt'>
 <tr>
  <td width="21%" valign=top style='width:21.0%;background:#DDDDFF;padding:
  5.25pt 5.25pt 5.25pt 5.25pt'>
  <p align=center style='text-align:center'><b>Beta Prior<br>
  Parameters</b></p>
  </td>
  <td width="16%" valign=top style='width:16.0%;background:#DDDDFF;padding:
  5.25pt 5.25pt 5.25pt 5.25pt'>
  <p align=center style='text-align:center'><b>Mode of Prior<br>
  Distribution</b></p>
  </td>
  <td width="21%" valign=top style='width:21.0%;background:#DDDDFF;padding:
  5.25pt 5.25pt 5.25pt 5.25pt'>
  <p align=center style='text-align:center'><b>Prior 95%<br>
  Prob. Interval</b></p>
  </td>
  <td width="20%" valign=top style='width:20.0%;background:#DDDDFF;padding:
  5.25pt 5.25pt 5.25pt 5.25pt'>
  <p align=center style='text-align:center'><b>Mode of Posterior<br>
  Distribution</b></p>
  </td>
  <td width="21%" valign=top style='width:21.0%;background:#DDDDFF;padding:
  5.25pt 5.25pt 5.25pt 5.25pt'>
  <p align=center style='text-align:center'><b>Posterior 95% <br>
  Prob. Interval</b></p>
  </td>
 </tr>
 <tr>
  <td width="21%" valign=top style='width:21.0%;padding:5.25pt 5.25pt 5.25pt 5.25pt'>
  <p align=center style='text-align:center'><span style='font-family:Symbol'>a</span>&nbsp;=&nbsp;331,
  <span style='font-family:Symbol'>b</span>&nbsp;=&nbsp;271</p>
  </td>
  <td width="16%" valign=top style='width:16.0%;padding:5.25pt 5.25pt 5.25pt 5.25pt'>
  <p align=center style='text-align:center'>55</p>
  </td>
  <td width="21%" valign=top style='width:21.0%;padding:5.25pt 5.25pt 5.25pt 5.25pt'>
  <p align=center style='text-align:center'>(51.0%, 59.0%)</p>
  </td>
  <td width="20%" valign=top style='width:20.0%;padding:5.25pt 5.25pt 5.25pt 5.25pt'>
  <p align=center style='text-align:center'>56.4%</p>
  </td>
  <td width="21%" valign=top style='width:21.0%;padding:5.25pt 5.25pt 5.25pt 5.25pt'>
  <p align=center style='text-align:center'>(57.0%, 61.8%)</p>
  </td>
 </tr>
 <tr>
  <td width="21%" valign=top style='width:21.0%;padding:5.25pt 5.25pt 5.25pt 5.25pt'>
  <p align=center style='text-align:center'><span style='font-family:Symbol'>a</span>
  = 45, <span style='font-family:Symbol'>b</span> = 46</p>
  </td>
  <td width="16%" valign=top style='width:16.0%;padding:5.25pt 5.25pt 5.25pt 5.25pt'>
  <p align=center style='text-align:center'>55</p>
  </td>
  <td width="21%" valign=top style='width:21.0%;padding:5.25pt 5.25pt 5.25pt 5.25pt'>
  <p align=center style='text-align:center'>(45.0%, 64.0%)</p>
  </td>
  <td width="20%" valign=top style='width:20.0%;padding:5.25pt 5.25pt 5.25pt 5.25pt'>
  <p align=center style='text-align:center'>61.5%</p>
  </td>
  <td width="21%" valign=top style='width:21.0%;padding:5.25pt 5.25pt 5.25pt 5.25pt'>
  <p align=center style='text-align:center'>(59.5%, 65.2%)</p>
  </td>
 </tr>
 <tr>
  <td width="21%" valign=top style='width:21.0%;padding:5.25pt 5.25pt 5.25pt 5.25pt'>
  <p align=center style='text-align:center'><span style='font-family:Symbol'>a</span>
  = 1, <span style='font-family:Symbol'>b</span> = 1</p>
  </td>
  <td width="16%" valign=top style='width:16.0%;padding:5.25pt 5.25pt 5.25pt 5.25pt'>
  <p align=center style='text-align:center'>No Mode</p>
  </td>
  <td width="21%" valign=top style='width:21.0%;padding:5.25pt 5.25pt 5.25pt 5.25pt'>
  <p align=center style='text-align:center'>(2.5%, 97.5%)</p>
  </td>
  <td width="20%" valign=top style='width:20.0%;padding:5.25pt 5.25pt 5.25pt 5.25pt'>
  <p align=center style='text-align:center'>62.1%</p>
  </td>
  <td width="21%" valign=top style='width:21.0%;padding:5.25pt 5.25pt 5.25pt 5.25pt'>
  <p align=center style='text-align:center'>(59.1%, 65.1%)</p>
  </td>
 </tr>
 <tr>
  <td width="21%" valign=top style='width:21.0%;background:#EEEEEE;padding:
  5.25pt 5.25pt 5.25pt 5.25pt'>
  <p align=center style='text-align:center'><b>Frequentist</b></p>
  </td>
  <td width="16%" valign=top style='width:16.0%;background:#EEEEEE;padding:
  5.25pt 5.25pt 5.25pt 5.25pt'>
  <p align=center style='text-align:center'>(No Prior)</p>
  </td>
  <td width="21%" valign=top style='width:21.0%;background:#EEEEEE;padding:
  5.25pt 5.25pt 5.25pt 5.25pt'>
  <p align=center style='text-align:center'>(No Prior)</p>
  </td>
  <td width="20%" valign=top style='width:20.0%;background:#EEEEEE;padding:
  5.25pt 5.25pt 5.25pt 5.25pt'>
  <p align=center style='text-align:center'><i>^p</i> = 62.1%</p>
  </td>
  <td width="21%" valign=top style='width:21.0%;background:#EEEEEE;padding:
  5.25pt 5.25pt 5.25pt 5.25pt'>
  <p align=center style='text-align:center'>CI: (59.1%, 65.1%)</p>
  </td>
 </tr>
</table>

</div>

<p style='margin-left:1.0in'>Notice that, in view of the polling results, both
experts were a little too pessimistic about the prospects for Proposition A.
However, the first expert's &quot;sharper&quot; prior (first row of the table)
is roughly equivalent in influence to surveying 600 people and so the results
of the actual survey of 1000 people are not enough to override this expert's
view that the true percentage in favor is unlikely to be above 60%. The second
expert's somewhat &quot;flatter&quot; prior (second row) is roughly equivalent
in influence to surveying 100 people and the actual survey results from 1000
people nearly override his opinion. </p>

<p style='margin-left:1.0in'>The uninformative prior (uniform distribution in
the third row) gives a mode and probability interval that agree with the
frequentist point estimate and confidence interval. However, the philosophical
bases of Bayesian and frequentist methods differ, as do the interpretations of
the results. The Bayesian results are statements about a posterior distribution
for the particular situation at hand, whereas the frequentist results must be
interpreted in terms of many hypothetical repetitions of the sampling
experiment. If the frequentist results depend on prior experience, it is by way
of the design of the survey, whereas the Bayesian results can depend explicitly
on personal probability assessments. </p>

<p style='margin-left:.5in'><b><i><span style='font-family:Arial'>Problems<o:p></o:p></span></i></b></p>

<p style='margin-left:1.0in'>3.4.1. The table of this section shows four rows
with various point and interval estimates for the election example. Make a
similar table for the beam weight example. Use the data of Problem&nbsp;3.3.2: 198.14,
198.45, 196.59, 197.64, 198.12 (mean&nbsp;197.79). In the first row show
results for the prior N(200,&nbsp;10<sup>2</sup>), and in the second row
results for the prior N(202,&nbsp;5<sup>2</sup>). There is no such thing as a
truly uninformative normal prior, but select a normal prior for the third row
that gives results numerically similar (to two decimal places) to frequentist
results, which you should show in the fourth row. Summarize and comment upon
the results shown in the table, making any extra rows that you think would add
to your understanding of this example.</p>

<p style='margin-left:1.0in'>3.4.2. The table of this section shows four rows
with various point and interval estimates for the election example. Make a
similar table for the mouse counting example. Suppose, as in Section&nbsp;3.3.3
that 256 mice altogether are trapped in 50 nights. In the first row show
results for the gamma prior distribution with parameters <span
style='font-family:Symbol'>a</span>&nbsp;=&nbsp;4 and <span style='font-family:
Symbol'>b</span>&nbsp;=&nbsp;2, and in the second row results for the gamma
prior with <span style='font-family:Symbol'>a</span>&nbsp;=&nbsp;2 and <span
style='font-family:Symbol'>b</span>&nbsp;=&nbsp;5. There is no such thing as a
truly uninformative gamma prior, but see if you can find a gamma prior for the
third row that gives results numerically similar (to two decimal places) to
frequentist results, which you should show in the fourth row. Summarize and
comment upon the results shown in the table, making any extra rows that you
think would add to your understanding of this example.</p>

<p style='margin-left:1.0in'>&nbsp;</p>


<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>


<p><span style='font-size:10.0pt'>Copyright © 2000 Bruce E. Trumbo. All rights
reserved. Intended mainly for instructional use at California State University,
Hayward. This is a draft; comments and corrections are welcome. To request
permission for other uses please contact </span><a
href="mailto:btrumbo@csuhayward.edu"><span style='font-size:10.0pt'>btrumbo@csuhayward.edu.</span></a></p>


<div class=MsoNormal align=center style='text-align:center'>

<hr size=2 width="100%" align=center>

</div>

</div>

</body>

</html>

