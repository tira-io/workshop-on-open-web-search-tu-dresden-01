<!-- <DOCUMENT>
	<FILE>
		6678646273.html
	</FILE>
	<URL>
		http://www2.psy.uq.edu.au/~brainwav/ModelRepository/Memory/#TensorsExplained
	</URL>
	<TITLE>
		Tensors Explained
	</TITLE>
	<GENRE>
		articles
	</GENRE>
	<PLAINTEXT>
 Connectionist Models of Cognition: Long Term Memory and the Matrix Model Long Term Memory and the Matrix Model Simon Dennis, Janet Wiles and Rachael Gibson Special thanks to Jill White MEMORY! Always there, of course, but usually hidden. And then, sometimes, as a result of just the right kind of push, it could emerge suddenly, sharply defined, all in color, bright and moving and alive. Robots and Empire, Isaac Asimov Table of Contents Introduction Matching versus Retrieval Tasks: The Nature of the Output Episodic versus Semantic Tasks: The Role of Context Tensors Explained Vectors - Rank One Tensors Matrices - Rank Two Tensors Tensors of Rank 3 and Above The Matrix Model Memory Representations Accessing Memory Representations Matching versus Retrieval Tasks: Scalar or Vector Output Episodic versus Semantic Memory: Cuing with the Context Vector Recall in the Absence of Recognition --> Objective Checklist References Answers to Questions Introduction Long term memory has a profound effect on the way we live our everyday lives and is integral to many aspects of cognition. Whether we are recalling episodes of childhood abuse in the witness box, retrieving our favourite pumpkin scone recipe, or just trying to remember where we parked the car, memory plays a key role in allowing us to function, providing the texture of our experiences and defining our identities. Most of us have had the annoying experience of recognizing a face but not being able to remember to whom it belongs. We curse, make a reference to our age and reach for the nearest pop-psychology manual on how to improve our memories in ten easy steps. When you stop to think about it, however, the really astonishing thing is how often we don't forget. In a study by Shepard (1967) subjects were given a list of 580 arbitrary words to remember. On a forced choice test they scored at just under 88% correct. That's impressive. Furthermore, I could ask you what you were doing at 12:30pm one week ago and there is a good chance you would be correct, even if it isn't something you do all the time. Yet between then and now you have had thousands of experiences any one of which I could have queried. Somehow all of those experiences have left their mark, often without you even thinking about it. Long term memory is big. The other really astonishing thing about memory is how flexibly we can access it. Suppose I ask you to list all the situation comedies you have ever watched. Most people from television dependent cultures wont have too many difficulties coming up with a reasonably large collection. Yet how could you answer this question given the obscure cue "situation comedy"? A database system might store a set of records such as {(Giligan's Island, situation comedy), (Full House, situation comedy), (World News, current affairs), etc.} and then cycle through these one at a time retrieving those that have the situation comedy tag. But this would require that when you are watching television shows you are constantly tagging them as situation comedy, current affairs etc. If you had tagged the program as "the funny show with the skinny sailor in it" the search for situation comedy would come back with NO RECORDS FOUND . Human memory is much more robust. We often use what seem to be very obscure cues yet we are able to retrieve well. xenophobic ?". These questions require you to comment on the information stored in memory without necessarily retrieving the detail of any particular memory. --> Much of the research into human memory has been an exploration of just how flexible our memories are - of the different sorts of questions that we can use our memories to answer. In this chapter, we will consider two ways in which the questions that we ask of our long term memories can differ. The first of these refers to the nature of the output that a question requires - are we asking for a specific name, word or other item from memory (retrieval) or do we require a yes/no answer about whether we remember some fact or episode (matching). The second distinction refers to the role of context in the query - are we asking about what happened in a given episode or context (episodic) or is the query about the way that things tend to be in general (semantic). After considering these tasks, we will look in some detail at the Matrix Model of long term memory (Pike, 1984; Humphreys, Bain & Pike, 1989) which provides a theory of how these questions could be answered using the formalism of matrix algebra. Matching Versus Retrieval Tasks: The Nature of the Output Memory tasks differ with respect to the output that is required. The usual task that people have in mind when they think about human memory is one in which you are given one piece of information such as someone's face and you must retrieve another piece of information such as that person's name. These sorts of tasks seem to rely on a discrete or discontinuous form of information. The required output is an item. Two common examples of retrieval tasks are free association and cued recall with a list associate. Free Association provides a subject with a cue (e.g. "Type of animal") and requires them to respond with the first word that comes to mind (e.g. cat). Sometimes a prior study list is presented to the subject and it has been shown that words that occur in the prior study list are more likely to be produced even though the subject is not instructed to use the study list to make their decisions. Free association is a retrieval task because the required response is a word. Cued Recall with a List Associate requires the subject to study a list of pairs of words. At test, subjects are given a list of words and asked which word occurred with each of the test words during the study episode (e.g. "Which word occurred with boy in the study list?"). Again, cued recall with a list associate is a retrieval task because it requires the subject to respond with a word. In contrast some memory tasks known as matching tasks require what seems to be a more quantitative answer based on a continuous measure. The examples with which we will be concerned are familiarity rating and recognition. Familiarity rating refers to a task in which subjects rate (on a five point scale, for instance) how familiar a word is to them in general (e.g. "How familiar is the word house to you?"). Familiarity rating is a matching task because it seems to be based on a continuous form of information. Recognition requires a subject to study a list of words. At test, the subject is given a second list of words - some of which appeared in the first list and some of which did not. The subjects' task is distinguish the targets (words that were on the list) from distractors (words that weren't on the list). Either they are asked to make a yes/no decision or they provide their confidence that the word is old (which might be rated on a five point scale, for instance). Recognition is considered a matching task because it relies upon a continuous form of information. Episodic and Semantic Tasks: The Role of Context Another important dimension on which memory tasks can vary is whether they make reference to a study episode. Tasks that do specify the study episode are known as episodic tasks, whereas tasks that do not are known as semantic tasks (or generalized tasks). Tulving (1972) realized that the learning in a study episode is not continuous with the learning that occurs before study. In particular, he realized that when we ask a subject in a recognition task do they recognize a word we are not asking them whether they know the word at all (often all of the test words are known to the subject). What we are asking is if the word occurred in a given list (the study list). Similarly, in cued recall with a list associate, we are not asking what word generally goes with boy . We are asking what word went with boy in the study list. In contrast, familiarity rating and free association make no specific reference to a study episode and are semantic tasks. Table 1 categorizes the four memory tasks described above in terms of the nature of the output and the role of context. Table 1: Examples of Episodic/Semantic Tasks and Matching/Retrieval Tasks (adapted from Humphreys, Bain & Pike, 1989) Use of Context Cue Access Process Episodic Memory Semantic Memory Matching produces a rating value Recognition Familiarity Rating Retrieval produces a word Cued Recall Free Association Before looking at how the Matrix Model accounts for the differences between these tasks we need a grounding in tensors, the operations that can be performed on tensors and how tensors can be mapped to neural network architectures. If you are comfortable with these ideas you may skip the next section. Tensors Explained The Matrix Model of memory is built upon the mathematics of tensors. Tensors are convenient ways of collecting together numbers. For instance, a vector, which is also known as a rank one tensor, could be used to describe the age, gender, and salary of an employee. If Jeff is 50 years old, is male (where male = 0 and female = 1) and earns $ 56000 per annum then we could describe Jeff with the vector [50, 1, 56000] (see figure 1). Note that vectors (and tensors in general) are ordered. The vector [56000, 1, 50] would describe someone who was 56000 years old who made a total of $ 50 per annum! Figure 1: Examples of vectors. (a) a row vector describing Jeff, (b) a column vector, (c) a vector with N components. The rank one tensor described above has a dimension of three because it contains three components. There is no reason that vectors need be restricted to three dimensions, however. We could have added shoe size, for instance, to increase the dimension to four. Similarly, there is no reason that we need to restrict ourselves to a single row of numbers. A tensor with N rows and M columns is known as an NxM matrix and has a rank of two, indicating that the array of numbers extends in two directions (see figure 2). Figure 2: Examples of matrices. (a) a 2x2 matrix, (b) a 3x2 matrix, (c) an NxN matrix. The process of extending the number of directions in which the array extends can theoretically continue indefinitely, creating tensors of rank three, four, five etc. In the following sections, we will look at vectors, matrices and tensors of rank three (see figure 3) as they are critical to understanding the Matrix Model. Other models, such as the STAR model of analogical reasoning (Halford, Wiles, Humphreys and Wilson 1992), employ tensors of higher rank. Figure 3: A rank three tensor (NxNxN). Vectors - Rank One Tensors Tensors, and in particular vectors, can be represented in many different forms including: Cartesian form in which the components are enumerated explicitly. Figure 1 depicts vectors represented in Cartesian form. Geometric form in which the vector is plotted in N dimensional space. For instance, figure 4 shows the vector representing Jeff plotted in three dimensional space. Figure 4: The vector representing Jeff plotted in three dimensional space (Geometric form). Algebraic form in which a vector is represented as a bolded lower case letter (e.g. v ). Algebraic form is a particularly concise form of representation, which makes it easy to talk about the operations that can be performed on vectors such as addition (e.g. w = v + t ). Neural network form which diagrams a neural network architecture in which either a set of units or a set of weights contain the elements of the vector. For instance, a vector can be mapped to a two layer network (one input and one output layer) as depicted in Figure 5. The number of units in the input layer corresponds to the number of dimensions in the original vector, while the output layer contains only 1 unit. Each input unit is connected to each output unit. The input units represent one vector and the weights represent a second vector. Figure 5: The network corresponding to a vector memory. The output of this network is defined to be the dot product (or inner product) of the input and weight vectors. A Dot Product is calculated by multiplying together the values which are in the same position within the two vectors, and then adding the results of these multiplications together to get a scalar (see Figure 6a). In the case of the neural network, this involves multiplying each input unit activation by the corresponding weight value and then adding. The dot product of two vectors represents the level of similarity between them and can be extended to higher rank tensors (see figure 6b) Figure 6: The Dot Product. The dot product is expressed algebraically as a dot, that is, the dot product of the vectors v and w is written v . w . Learning occurs in this network by adding the input vectors. Vector addition superimposes vectors of the same dimension. It is calculated by adding together the elements in a particular position in each vector (see Figure 7a). In this way, multiple memories can be stored within the same vector. [Note: the network actually employs Hebbian learning (see Neural Networks by Example: Chapter three). However, when the output unit is fixed at one Hebbian learning is identical to vector addition.] Figure 7: (a) Vector Addition, (b) Matrix Addition. Again vector addition can be extended to tensors of arbitrary rank (see figure 7b). Vector addition is expressed algebraically as a plus sign (+). So if we wanted to talk about the dot product of v with the addition of w and x we would write v .( w + x ). Another useful property to keep in mind is that the dot product distributes over addition. That is: v .( w + x ) = v . w + v . x In the following exercises, you will build a vector network that learns to discriminate between stored items and new items (see figure 8). Figure 8: A vector memory network. Follow the instructions below to create the network and then work through the exercises. Load BrainWave Select 'New Hebbian Network' Set up the vector memory network: Create two input units and one output unit. Connect both input units to the output unit. Set up VALUE objects for the units and weights. Create the data sets: Create an Input Set containing the two input units Create a Test Set containing the two input units Create an Output Set containing the output unit The items in this exercise are FROG [0.95, 0.32], TOAD [0.49, 0.87], KOALA [0.32, -0.95]. Add the FROG pattern to the input set. Add a pattern containing a 1 to the output set. Add all three items, FROG, TOAD and KOALA, to the test set. Exercise 1: Train the network for one epoch and record the weights in the TRAIN FROG row of the following table. How have the weights changed? Weight 1 Weight 2 TRAIN FROG TRAIN FROG & KOALA TRAIN FROG & TOAD Exercise 2: Test each of the items, FROG, TOAD and KOALA, and record the match values (the activation of the output unit) in the second table. Explain the match values. TEST FROG TEST TOAD TEST KOALA TRAIN FROG TRAIN FROG & KOALA TRAIN FROG & TOAD Exercise 3: Train the network for one more epoch and test again. What happens to the match values after a second training trial? Why? Exercise 4: Add KOALA to the input set and an output value of 1 to the output set. Zero the weights (using the ACTIONS menu) and retrain the network on the updated input set. Test the network as before, recording the values in the table in the TRAIN FROG & KOALA row. Exercise 5: Delete KOALA from the input set and add TOAD. Zero the weights, retrain and test as above, recording the values in the TRAIN FROG & TOAD row. You should have six weight values and nine match values for each training trial. Create a graph of the match values after the first training trial: plot three lines, one for each test item, against the three training conditions. Explain the shape of each line on the graph. Exercise 6: For each of the three training conditions (FROG alone, FROG & KOALA, FROG & TOAD): Draw the geometric (graphical) representation of the weights, Provide the algebraic representation of the weights. Matrices - Rank Two Tensors The vector memory, discussed above, was capable of storing items so that at a later time it could be determined if they had appeared. A matrix memory allows two items to be associated - so that given one we can retrieve the other. Algebraically, a matrix is usually represented as a bolded upper case letter (e.g. M ). Associations are formed using the outer product operation. A outer product between two vectors is calculated by multiplying each element in one vector by each element in the other vector (see Figure 8). If the first vector has dimension d 1 and the second vector dimension d 2 , the outer product matrix has dimension d 1 xd 2 . For instance, a three dimensional vector multiplied by a two dimensional vector has dimension 3x2. Figure 8: The outer product. The outer product operation is expressed algebraically by placing the vectors to be multiplied next to each other. So the outer product of v and w is written as v w . These association matrices are then added into the memory matrix (as in the vector memory case) - so that all associations are stored as a single composite. A matrix memory maps to a two layer network (one input and one output layer) as depicted in Figure 9. The number of input units corresponds to the number of rows in the original matrix, while the number of output units corresponds to the number of columns. Each input unit is connected to each output unit. Figure 9: The network representation of a matrix. In the following exercise you will use a matrix memory network to store and recall pairs of items. Exercise 7: Load the simulator, BrainWave. From the NETWORKS menu - select Matrix Model (1). What rank tensor does this network implement? What are its dimensions? Exercise 8: The items in this exercise are: Cues: FROG [0.5, -0.5, 0.5, -0.5] KOALA [0.5, 0.5, -0.5, -0.5] SNAIL [-0.5, 0.5, 0.5, -0.5] TOAD [0.5, 0.4, 0.6, 0.45] Targets: FLIES [0.7, 0.5, 0.5] LEAVES [0.7, -0.5, -0.5] LETTUCE [0, -0.7, 0.7] The input set contains the items FROG, KOALA and SNAIL, paired with items in the output set FLIES, LEAVES and LETTUCE, respectively. Another input item, TOAD [0.5, 0.4, 0.6, 0.45], can be used to test the network on unfamiliar input. Calculate the similarity value (i.e. dot product) of the items FROG, KOALA, SNAIL and TOAD with themselves, and each other, and record the values in the table below: FROG KOALA SNAIL TOAD FROG KOALA SNAIL TOAD Exercise 9: Train the network for one epoch. Test each of the items FROG, KOALA, SNAIL and TOAD. What output is produced in each case? (Give the output pattern and also describe the output patterns in terms of their similarity to FLIES, LEAVES and LETTUCE). FROG KOALA SNAIL TOAD Exercise 10: Give the algebraic equation that describes the matrix memory formed from the three pairs of associates: M = Exercise 11: Give the equations that describe each of the retrievals in exercise 9. Use the similarity measures from the table above to simplify each equation to a weighted sum of the target patterns. FROG KOALA SNAIL TOAD Tensors of Rank Three and Above The final sort of tensor we need to demonstrate the matrix model is the rank three tensor. The rank three tensor allows a three way association to be represented. For instance, we could store the information that John loves Mary - [Loves John Mary] - or that Apple appeared with Pencil in List 1 [List 1, Apple, Pencil]. A tensor of rank three maps to a three layer network (one input layer with two sets of units, one output layer, and one layer of hidden units) as depicted in Figure 10. The number of units in the input sets and the output set correspond to the dimensionality of the tensor. The number of hidden units corresponds to the number of units in one input set times the number of units in the other input set. Each hidden unit has a connection from one input unit from each input set, with a hidden unit existing for each possible combination. These hidden units are SigmaPi units, the value of which is set to the multiplication of the two input units to which it is connected. To implement a rank three tensor, the weights in the first layer are frozen at one. Consequently, a hidden unit's activation will equal the multiplication of the activations of the input units to which it is connected. Each hidden unit is then connected to each output unit. Figure 10: The network representation of a rank three tensor. In these exercises, you will use both rank two and three tensor networks to store and recall triples of items. Exercise 12: Load the simulator, BrainWave. From the NETWORKS menu - Matrix Model (2). What rank tensor does this network implement? Exercise 13: The items in this exercise are: Cues: FROG [0.5, -0.5, 0.5, -0.5] KOALA [0.5, 0.5, -0.5, -0.5] SNAIL [-0.5, 0.5, 0.5, -0.5] TOAD [0.5, 0.4, 0.6, 0.45] Relations: EATS [0.5, -0.5, -0.5, 0.5] LIVES-IN [0.5, 0.5, -0.5, -0.5] Targets: FLIES [0.7, 0.5, 0.5] LEAVES [0.7, -0.5, -0.5] LETTUCE [0, -0.7, 0.7] POND [0.89, 0.43, -0.22] TREE [-0.22, 0.76, 0.62] SHELL [0.43, -0.5, 0.76] Notice that the vectors for the cues are the same as those used above. Also notice that EATS and LIVES-IN are orthogonal to each other - that is they have a dot product of zero. Calculate the similarity (dot product) table for the targets. FLIES LEAVES LETTUCE POND TREE SHELL FLIES LEAVES LETTUCE POND TREE SHELL Exercise 14: The cue+relation input set contains the items FROG-EATS, KOALA-EATS, SNAIL-EATS, FROG-LIVES_IN, KOALA-LIVES_IN and SNAIL-LIVES_IN, paired with items in the output set FLIES, LEAVES, LETTUCE, POND, TREE, and SHELL, respectively. Two other input items, TOAD-EATS and TOAD-LIVES_IN, can be used to test the network's response to unfamiliar input. Train the network for one epoch. Test each of the items FROG-EATS, KOALA-EATS, SNAIL-EATS, FROG-LIVES_IN, KOALA-LIVES_IN, SNAIL-LIVES_IN, TOAD-EATS and TOAD-LIVES_IN. What output is produced in each case? (Give the output pattern and also describe the output patterns in terms of their similarity to FLIES, LEAVES, LETTUCE, POND, TREE and SHELL) FROG-EATS KOALA-EATS SNAIL-EATS FROG-LIVES_IN KOALA-LIVES_IN SNAIL-LIVES_IN TOAD-EATS TOAD-LIVES_IN Exercise 15: How does the performance of this network compare with the performance of the network in Exercise 8? Why is it not as good? Exercise 16: Give the algebraic equation that describes the matrix memory formed from the three pairs of associates: M = Exercise 17: Give the equations that describe each of the retrievals from exercise 14. Use the similarity measures from the table above to simplify each equation to a weighted sum of the target patterns. FROG-EATS KOALA-EATS SNAIL-EATS FROG-LIVES_IN KOALA-LIVES_IN SNAIL-LIVES_IN TOAD-EATS TOAD-LIVES_IN Exercise 18: From the NETWORKS menu - select Matrix Model (3). What rank tensor does this network implement? Exercise 19: The inputs and outputs for this network are the same as for the previous one, but the connections and hidden SigmaPi units perform different calculations on the inputs to try and achieve the correct outputs. Train the network for one epoch. Test each of the items FROG-EATS, KOALA-EATS, SNAIL-EATS, FROG-LIVES_IN, KOALA-LIVES_IN, SNAIL-LIVES_IN, TOAD-EATS and TOAD-LIVES_IN. What output is produced in each case? (Give the output pattern and also describe the output patterns in terms of their similarity to FLIES, LEAVES, LETTUCE, POND, TREE and SHELL). FROG-EATS KOALA-EATS SNAIL-EATS FROG-LIVES_IN KOALA-LIVES_IN SNAIL-LIVES_IN TOAD-EATS TOAD-LIVES_IN Exercise 20: Which of the two networks performs the memory task better? Why? Exercise 21: Give the algebraic equation that describes the matrix memory formed from the three pairs of associates: M = Exercise 22: Give the equations that describe each of the cued recall tests from question 19. Use the similarity measures from the table above to simplify each equation to a weighted sum of the target patterns. FROG-EATS KOALA-EATS SNAIL-EATS FROG-LIVES_IN KOALA-LIVES_IN SNAIL-LIVES_IN TOAD-EATS TOAD-LIVES_IN In this section, we have been looking at the way in which tensors of rank one, two and three can be used to store information. In the next section, we will examine the Matrix Model, which uses precisely this mechanism to explain the nature of human memory. The Matrix Model The Matrix Model of Memory was developed by Humphreys, Bain and Pike (1989) and Pike (1984) to provide a coherent theoretical account of a range of different memory tasks, including episodic tasks, such as recognition and recall, and semantic tasks, such as familiarity rating and indirect production tasks. It is a distributed associative model in which items are modelled and stored as vectors of feature weights or elements, just as was the case in the previous section. Elements within each vector contribute conjointly to the representation of items. Thus memory representations are not located at specific points within a memory network, or within specific memory systems. Instead they are conceptualised as unique patterns of activation over a common set of elements. Typically, these patterns are thought to be sparse representations meaning that only a few of the elements are active. Memory Representations The memory representations in the Matrix Model include items, contexts or, combinations of items and contexts (associations). Items - Items can be any sort of stimuli including words, pictures, melodies etc. For the most part, however, the experiments to which the model has been applied use words. Each item is modelled as a vector of feature weights. Feature weights are used to specify the degree to which certain features form part of an item. There are two possible levels of vector representation for items. These are: modality specific peripheral representations (e.g., graphemic or phonemic representations of words) modality independent central representations (e.g., semantic representations of words). Item vectors are distinguished by subscripts (e.g. a i ). A distractor vector is indicated by a d . Contexts - To distinguish between episodic and non-episodic tasks the Matrix Model assumes the episode or context in which items are studied is also represented by a vector of feature weights. In episodic tasks this context vector must be reinstated so that it may be used as a cue to the memory system. The context vector is represented by an x . Associations - While individual items and contexts are represented as single vectors ( a , b , x ), associations between items and contexts are represented by matrices derived from the matrix product of these vectors. The resulting matrix product represents the association (or binding) between either items, or between items and context. The memory of the matrix Model is formed by adding these associations together. The model posits a number of different kinds of associations including: Two-way associations between a single item ( a ) and a context ( x , e.g. bacon x breakfast this morning) are represented as a context-to-item association ( x a ), where x = n element column vector a = n element row vector Associations between a list of items ( a 1 , a 2 ,..., a k ) and a context ( x ) are represented by multiplying each of the item vectors by the context vector and summing the resulting matrices. This sum represents the memory of the study list ( E ). E = x a 1 + x a 2 + ... + x a k Three-way associations between a list of word pairs ( a 1 b 1 , a 2 b 2 , ... a k b k ) and context ( x , e.g., bacon x dog x breakfast this morning) are represented by the rank three tensor ( x a j b j ), where x = n element column vector a j = n element row vector b j = n element orthogonal vector The resulting associations can be summed to form the memory for the list ( E ). E = x a j b j Note: the type of vector (i.e., row, column, orthogonal) can also be inferred from the order of the vector symbols, where: 1st vector = column vector; 2nd vector = row vector; and 3rd vector = orthogonal. Pre-existing memories ( S ) are added to list memories ( E ) because test performance can be influenced by both list memories and pre-existing memories. M = x a j b j + S Accessing Memory Representations Having constructed the memory matrix, we can now see how the Matrix Model goes about accessing this representation at test for a number of different tasks. All retrieval in the Matrix Model is direct . The memory matrix is presented with cues and access occurs in parallel. There is no sequential search process. Presenting the model with a cue involves taking the inner product (or dot product) of the cue vector with the memory matrix. One of the strengths of the Matrix Model is in the number of a ways in which information from the model can be accessed. In the introductory section, two dimensions on which tasks can differ were outlined. The first was the matching/retrieval dimension. Matching tasks are those based on a continuous form of information that typically require either a yes/no answer or a rating response (e.g. recognition). Retrieval tasks, by contrast, require a specific item to be returned (e.g. cued recall). This distinction is captured in the Matrix Model by the nature of the tensor that results once all cues have been applied. If the resultant tensor is a scalar we are dealing with a matching process. This scalar can be compared against criteria to determine a yes/no or rating value. If the resultant tensor is a vector then we have a retrieval process. The vector can be compared against all item vectors with the item being the output of the process. The next sections, goes through the mathematics of recognition and cued recall with a list associate demonstrating how matching and retrieval tasks are accomplished within the model. The second task dimension discussed in the introduction focussed on the the episodic/semantic dimension. Episodic tasks refer to a specific context, whereas in semantic (generalized) tasks information is integrated over a large number of experiences. The Matrix Model captures this distinction. In episodic tasks, a reinstated context vector is used as a cue. In semantic (generalized) tasks, a vector which is equally similar to all contexts is used so as to average over all experiences with the cue items (typically, this is a vector with all components set to 1/n where n is the dimension of the vector). The section entitled "Episodic versus Semantic Memory: Cuing with the Context Vector" describes an experiment designed to demonstrated the importance of the distinction and leads you through the process of modelling this experiment using the BrainWave simulator. Matching Versus Retrieval Tasks: Scalar or Vector Output In this section, a matching task, namely recognition, and a retrieval task, namely cued recall with a list associate, are compared within the Matrix Model framework. Recognition Recognition involves a matching process, where the overall similarity between the test cues ( x and a i ) and memory ( M ) is calculated. Because this is an episodic task, the test cues involve both word cues and a context cue. This episodic matching process is accomplished by combining the test cues into an associative matrix ( x a i ) and determining a dot product between: the cue matrix ( x a i ), and the memory matrix ( M = x a j + S ). [Note: Because the dot product operation is associative, the results are identical regardless of whether you form a combined x a i matrix and then take a dot product or take the dot product of each of the cues with the memory matrix progressively.] Studied Test Word (a i ) xa i . M = xa i . ( xa j + S ) = x a i . x a j + x a i . S = (x . x) (a i . a j ) + x a i . S = (x . x) (a i . a i ) + (x . x) (a i . a j ) + xa i . S Inserting the expected matching value: E[ x a i . M ] = c s + (k - 1) c m + g where c = similarity between the study and test context (assumed to large) s = similarity between the same word encoded at study and test (assumed to be large) m = similarity between different words at study and test (assumed to be small) g = contribution of pre-existing memories Non studied Test Word (d) x d . M = x d . ( xa j + S) = xd . xa j + xd . S = (x . x) (d . a j ) + xd . S where E[ x d . M ] = c m k + g Note that the matching operations in the above equations can be collapsed down into several components, including : a match between the test cue and the pre-experimental memories (i.e., x a i . S or x d . S ), and a match between the test cue and the experimental memories (i.e., x a i . x a j or x d . x a j ) The match between the test cue and the experimental memories can further be collapsed down into : a match between the context on study and test occasions ( x . x = c), and a match between the study and test items ( a i . a i = s and a i . a j = m) or ( d . a j = m) Thus the final dot product derived from these equations, represents the match of the contexts on the study and test occasions (c), weighted by the match of the items on the study and test occasions (s and m). Consequently, memories that are conjointly defined by context and test cues will be weighted more heavily than items not studied in that context. This mechanism enables the model to avoid interference (large weights) from other items studied in the same context and also from previous contexts in which items have appeared. Cued Recall with a List Associate Cued recall with a list associate involves a subject studying a list of pairs. At test they are given an item and are required to produce the word with which it was paired at study. This is an important task because it can be used to demonstrate that three way association are necessary to model human memory. Simple associations two-way associations between items are insufficient (Humphreys, Bain & Pike 1989). For this reason, cued recall with a list associate is modelled using rank three tensors that associate word pairs ( a 1 b 1 , a 2 b 2 ,... a k b k ) and context ( x ). The tensor is formed by taking the outer product of the context vector x and the two item vectors, a j and b j . M = x a j b j + S Subjects are then asked to recall list targets ( b i ) at test, using list associates ( a i ) and context ( x ) as cues. The retrieval cues ( x and a j ) are combined to form an associative matrix cue ( x a i ). Retrieval then involves the pre-multiplication of the rank three tensor ( M ) by the retrieval cue ( x a i ). x a i . M = x a i . x a j b j + S = [( x a i )( x a j )] b j + x a i . S = [( x . x ) ( a i . a j )] b j + x a i . S = ( x . x ) ( a i . a i ) b i + ( x . x ) ( a i . a j ) b j + x a i . S Inserting the expected values: E[ x a i . M ] = c s b i + c m b j + x a i . S The end product (matrix product) of this process will comprise a target vector of feature weights. This featural information can be used to produce a word or item response. The target vector is weighted by: the similarity of the context on the study and test occasions ( x . x = c), and the similarity of the list cue on the study and test occasions ( a i . a i = s) and ( a i . a j = m) Note that the weights for the same associate (s) will be greater than the weights for different associates (m) making the resulting vector look more like the correct associate (on average) than any other item. Noise will also be generated by the pre-existing memories. The assumption is that, in general, the similarity of the pre-existing contexts and the current context will be small leading to low levels of interference. Of course, if a recent context also included the cue word then much more interference will be generated because the context vectors will be more similar. Recall in the Absence of Recognition The Task and Phenomenon The relationship between recall and recognition has been central to the study of episodic memory for several decades. Most people have the intuition that recalling a specific item is more difficult than simply recognizing that you have seen an item, and often this is the case (I know I've seen that person before, but I can't remember their name). However, under some circumstances recall can be better than recognition (Tulving & Thompson 1973; Watkins & Tulving 1975). The discovery of recognition failure of recallable words has had an important impact on the development of memory theory. In particular, models in which recognition is a simple subprocess of recall (the generate - recognize models) underwent substantial modification as a consequence. Before looking more closely at the implications for memory modelling, however, we will describe the sort of experiment that has demonstrated recall without recognition as exemplified by Watkins and Tulving (1975, experiment one). The basic methodology of the Watkins and Tulving experiment is presented in table 1. Table 1: Basic methodology: Schematized Sequence of Procedures. Reproduced from Watkins and Tulving (1975). Step Procedure Example 1a List 1 Presented badge - BUTTON 1b Cued recall of List 1 badge - button 2a List presented preach - RANT 2b Cued recall of List 2 preach - rant 3 List 3 presented glue - CHAIR 4a Free-association stimuli presented table 4b Free-association responses made table 5a Recognition test sheets presented DESK TOP CHAIR 5b Recognized items circled DESK | TOP | CHAIR 5c Recognition confidence of circled items attempted DESK | TOP | 1 CHAIR 5d Recall of list cues of circled items attempted TOP - can't recall 6 Cued recall of List 3 glue - chair Steps one and two involve cued recall tests designed to influence how subjects encode lists of word pairs. Word pairs such as glue - CHAIR are presented and subjects are asked to learn them for a later test. The first word ( glue ) is designated the cue and the second word (CHAIR) is designated the target. Typically, subjects must encode the two words interactively for cued recall performance to be good. These first two steps are practise trials aimed at making sure their encodings are strong. The third step is the critical study list. It is similar to the first two study lists. This time, however, instead of an immediate cued recall test, subjects were asked to free associate to a set of words that did not appear on the study list (step four). These words were chosen so that it was likely that subjects would respond with one of the target words. The fifth step was the recognition test. Subjects were given groups of three words and asked to chose the word which appeared as a target in the study list (step three). Then they had to rate their confidence and try to recall the cue word that appeared with that target. Finally, in step six, subjects were given a cued recall test for the study list. Table 1: The Proportions of Targets Recognized and Recalled (Watkins & Tulving 1975) Recognized Not Recognized Recalled .25 .24 Not Recalled .14 .36 Table 1 shows the proportion of items recalled and recognized. The key point is that half of the items that were recalled were not recognized. This occurs despite the fact that the recognition test comes first, so that forgetting should affect recall more than recognition. --> In the last two sections we have seen how, in a mathematical sense, the Matrix Model distinguishes between matching and retrieval tasks. In the next section, we will examine the episodic/semantic distinction by using the Matrix Model to simulate data generated by Bain & Humphreys (1989). Episodic versus Semantic Memory: Cuing with the Context Vector Bain & Humphreys (1989, pg. 229) report an experiment which clearly demonstrates the difference between episodic and semantic matching tasks by reinstating the context during some, but not all, of the test conditions. Subjects were given a set of words and asked to produce a synonym for each. One week later the same subjects were given a passage containing unhighlighted target words, and asked to read the text and then answer questions on it. Half of the target words were common to both training stages. In addition to the test items already mentioned (synonym, passage, or both), words which appeared in neither training stage were also included as test items. Each set of test items contained equal numbers of high and low frequency words. The subjects were grouped into three test conditions. Group A was asked to give a general familiarity rating for the words (a generalized matching condition). Group B was asked to recognise which words had been in the synonym generation task (an episodic matching condition). Group C was asked to recognise which words had been in the passage reading task (also an episodic matching condition). The mean recognition and familiarity ratings are displayed in Figure 11. Figure 11: Mean ratings for three tasks as a function of presentation list(s) and word frequency. (a) Familiarity Rating Task (b) Recognition of Synonym Task words and (c) Recognition of Passage Task words. Note that in generalized familiarity task ratings depended only on the frequency of the word. For the episodic tasks, however, the lists in which the subjects were exposed to the word are critical. As Figure 11 shows, subjects performing the episodic matching tasks were affected by the training context indicated in the task instructions, while subjects performing the general matching task were not influenced by the prior training conditions. Furthermore, the subjects did not have trouble reinstating the synonym context as opposed to the passage context, and vice versa. These results suggest that subjects are able to distinguish episodic and semantic (or generalized) memory tasks quite well. One explanation is that the episodic and semantic memory systems are located in two different compartments in the brain. In the generalized familiarity task, subjects access the semantic store, in the episodic recognition task subjects access the episodic store. This may well be the case, however, Humphreys, Bain and Pike (1989) showed using the Matrix model that it need not be. The episodic/semantic distinction can be captured in a single coherent memory system by assuming differences in the types of cues supplied. In the following exercises, the Matrix Model will be used to demonstrate how the difference between generalized familiarity and episodic recognition can be captured. To simplify the modelling process we assume a design similar to that employed by Bain and Humphreys (1989), but in which only one study list is presented. What we are looking for is a difference in the pattern of results for target and distractor words when asking for generalized familiarity versus episodic recognition. The key distinction, from the model's point of view, is in the nature of the context cue. In episodic recognition it will be assumed that the context cue is the same as that at study. In contrast, when modelling generalized familiarity the context cue will be a a vector in which all components are 0.1. This context vector will be similar to all of the pre-experimental contexts and the study context to approximately the same degree and will therefore produce an output which is approximately the mean of all exposures - not just the study list exposures. Exercise 23: Load the simulator, BrainWave. From the NETWORKS menu - select Familiarity vs Recognition. This network contains three sets of units - the input units, which will contain the context vectors, the output units, which will contain the items to which a context is associated and the match units, which contain the item to be tested. Weights are connected between the input units and the output units. What rank tensor does this network implement? Above the units is a global value called "Dot Product". This global value indicates the dot product of the output units and the match units and is updated when you click on cycle. It is this value which will indicate the strength of a match in both the episodic recognition and generalized familiarity conditions. In addition, there are three collections of pattern sets. The pre-experimental sets contain the input/output pairs representing the subjects experience before entering the experiment. Each context is different indicating that subjects pre-experimental experience with words arises from many different contexts. Each context vector has just three units active and these units are active to different degrees. The same is true for the output patterns which represent the words. However, some of the word patterns are repeated representing the difference between high and low frequency words. The high frequency words are repeated three times while the low frequency words appear just once. Note that real words occur much more often. We have decreased the numbers here to facilitate modelling. It is important to consider, however, what effect increasing the numbers of presentations would have. A later exercise will be directed towards this question. In the pre-experimental output set (as well as the match and experimental output sets), the words are followed by a tag such as hft or lfd. The hf or lf stands for high frequency and low frequency respectively, and the t or d stands for target or distractor. This tag just allows you to easily remember the type of each word without having to cycle through the relevant pattern sets. Exercise 24: Click through the pre-experimental output set. How many presentations are there? How many unique words are there? The experimental set represents a subject's experience during the study list. At study, words are all presented the same number of times and in the experimental output set each word appears just once. In all cases the study context is the same. Note that only target words appear in the experimental list. Exercise 25: Click through the experimental output set. How many words are there? The final collection of pattern sets are those that will be used for testing the network. The input set contains the Study Context pattern and the Generalized Context pattern. When testing episodic recognition the Study Context pattern should be selected, when testing generalized familiarity the Generalized Context pattern should be selected. The output set contains no patterns because these sets will only be used for cycling, not for learning. The match set contains a copy of each of the words - both the targets and the distractors. Exercise 26: Click through the match set. How many words are there? Now we are ready to train and test the system. Train the network for one epoch with the Pre-experimental input and output sets and then for one epoch with the Experimental input and output sets (If you are learning for a second time remember to reset the weights - Actions Menu - so that current learning doesn't accumulate with the prior learning). To test whether the network is familiar with a word in the study context, or is familiar with a word generally (it can be both): select the test output set; select the word from the match set; select either the Study Context or the General Context from the test input set and cycle once. Exercise 27: Simulate the generalized familiarity task and fill in the the dot product values in Table 1 below. Table 1: Generalized Familiarity Task: Dot Product Values High Frequency Target Low Frequency Target High Frequency Distractor Low Frequency Distractor child avery horse crept phone elope space flank woman adage eight broth light dally sound envoy visit graft april aural green banjo leave debit river fidel table guise MEANS Exercise 28: Simulate the episodic recognition task and fill in the the dot product values in Table 2 below. Table 2: Episodic Recognition Task: Dot Product Values High Frequency Target Low Frequency Target High Frequency Distractor Low Frequency Distractor child avery horse crept phone elope space flank woman adage eight broth light dally sound envoy visit graft april aural green banjo leave debit river fidel table guise MEANS Exercise 29: Produce graphs similar to those in figure 11 for the mean values of the dot products. That is, plot the mean dot product values for targets and distractors for both low and high frequency words in the generalized familiarity condition on one graph, and the mean dot product values for targets and distractors for both low and high frequency words in the episodic recognition condition on another graph. Are the generalized familiarity graphs flatter than the episodic recognition graphs? Why? Exercise 30: In the generalized familiarity graph the model's results tend not to be as flat as the subject's data. Why might this be the case, and does it represent a refutation of the model? (Hint: consider the nature of pre-experimental experience). Objective Checklist In this chapter, we have been looking at the Matrix Model of long term memory. The following is a check list of skills and knowledge which you should obtain while working on this chapter. Go through the list and tick off those things you are confident you can do. For any item outstanding, you should refer back to the appropriate section or consult your tutor. understand the distributed representation of items and associations calculate the vector memory values when two patterns are superimposed, in terms of: network weights, Cartesian co-ordinates, vector addition. explain the difference between matching and retrieval tasks and model this difference in the Matrix Model explain the difference between episodic and semantic tasks and model this difference in the Matrix Model References Bain, J.D., & Humphreys, M.S. (1989). Instructional reinstatement of context: The forgotten prerequisite. In K. McConkey and A. Bennett (Eds.), Proceedings of the XXIV International Congress of Psychology, Vol. 3. Elsevier, North-Holland. Halford, G. S., Wiles, J., Humphreys, M. S., & Wilson, W. H. (1992). Parallel distributed processing approaches to creative reasoning: Tensor models of memory and analogy. unpublished manuscript. Humphreys, M.S., Bain, J.D., & Burt, J.S. (1989). Episodically unique and generalized memories: Applications to human and animal amnesics. In S. Lewandowsky, J.C. Dunn & K. Kirsner (Eds.) Implicit Memory: Theoretical Issues. (pp. 139-158). Erlbaum Associates: Hillsdale, N.J. Humphreys, M.S., Bain, J.D., & Pike, R. (1989). Different ways to cue a coherent memory system: A theory for episodic, semantic and procedural tasks. Psychological Review, 96, 208-233. Pike, R. (1984). A comparison of convolution and matrix distributed memory systems. Psychological Review, 91, 281-294. Wiles, J., & Humphreys, M.S. (1993). Using artificial neural networks to model implicit and explicit memory. In P.Graf & M. Masson (Eds.) Implicit Memory: New Directions in Cognition, Development, and Neuropsychology. (pp. 141-166). Erlbaum: Hillsdale, New Jersey. 
	</PLAINTEXT>
	<CONTENT>
-->
<HTML>
<HEAD>
<TITLE>Connectionist Models of Cognition: Long Term Memory and the Matrix Model</TITLE>
</HEAD>
<BODY BGCOLOR=#FFFFFF>
<H1 align=center>Long Term Memory and the Matrix Model</H1>

<center>
Simon Dennis, Janet Wiles and Rachael Gibson<BR>
Special thanks to Jill White
</center><P>

<center>
<table width=80%>
<tr><td><I>MEMORY! Always there, of course, but usually hidden. And then,
sometimes, as a result of just the right kind of push, it could emerge
suddenly, sharply defined, all in color, bright and moving and alive.<P>

<tr><td align=right>Robots and Empire, Isaac Asimov </I><P>
</table>
</center>

<H2>Table of Contents</H2>
<UL>
<LI><A HREF="#Introduction">Introduction</A>
<UL>
<LI><A HREF="#MatchingVersusRetrievalTasksTheNatureOfTheOutput">Matching versus Retrieval Tasks: The Nature of the Output</A>
<LI><A HREF="#EpisodicVersusSemanticTasksTheRoleOfContext">Episodic versus Semantic Tasks: The Role of Context</A>
</UL>
<LI><A HREF="#TensorsExplained">Tensors Explained</A>
<UL>
<LI><A HREF="#Vectors">Vectors - Rank One Tensors</A>
<LI><A HREF="#Matrices">Matrices - Rank Two Tensors</A>
<LI><A HREF="#Tensors">Tensors of Rank 3 and Above</A>
</UL>
<LI><A HREF="#TheMatrixModel">The Matrix Model</A>
<UL>
<LI><A HREF="#Memory Representations">Memory Representations</A>
<LI><A HREF="#Accessing Memory Representations">Accessing Memory Representations</A>
<LI><A HREF="#MatchingVersusRetrievalTasksScalarOrVectorOutput">Matching versus Retrieval Tasks: Scalar or Vector Output</A>
<LI><A HREF="#EpisodicVersusSemanticMemoryCuingWithTheContextVector">
Episodic versus Semantic Memory: Cuing with the Context Vector</A>
</UL>
<!--
<LI><A HREF="#RecallInTheAbsenceOfRecognition">
Recall in the Absence of Recognition</A>
-->
<LI><A HREF="#ObjectiveChecklist">Objective Checklist</A>
<LI><A HREF="#References">References</A>
<LI><A HREF="Answers.html">Answers to Questions</A>
</UL>

<A NAME="Introduction"><H2>Introduction</H2></A>

Long term memory has a profound effect on the way we live our everyday
lives and is integral to many aspects of cognition. Whether we are
recalling episodes of childhood abuse in the witness box, retrieving
our favourite pumpkin scone recipe, or just trying to remember where we
parked the car, memory plays a key role in allowing us to function,
providing the texture of our experiences and defining our
identities.<P>

Most of us have had the annoying experience of recognizing a face but
not being able to remember to whom it belongs.  We curse, make a
reference to our age and reach for the nearest pop-psychology manual on
how to improve our memories in ten easy steps.  When you stop to think
about it, however, the really astonishing thing is how often we don't
forget. In a study by Shepard (1967) subjects were given a list of 580
arbitrary words to remember. On a forced choice test they scored at
just under 88% correct. That's impressive. Furthermore, I could ask you
what you were doing at 12:30pm one week ago and there is a good chance
you would be correct, even if it isn't something you do all the time.
Yet between then and now you have had thousands of experiences any one
of which I could have queried. Somehow all of those experiences have
left their mark, often without you even thinking about it. Long term
memory is big.<P>

The other really astonishing thing about memory is how flexibly we can
access it. Suppose I ask you to list all the situation comedies you
have ever watched. Most people from television dependent cultures wont
have too many difficulties coming up with a reasonably large
collection. Yet how could you answer this question given the obscure
cue "situation comedy"? A database system might store a set of records
such as {(Giligan's Island, situation comedy), (Full House, situation
comedy), (World News, current affairs), etc.} and then cycle through
these one at a time retrieving those that have the situation comedy
tag. But this would require that when you are watching television shows
you are constantly tagging them as situation comedy, current affairs
etc. If you had tagged the program as "the funny show with the skinny
sailor in it" the search for situation comedy would come back with
<B>NO RECORDS FOUND</B>. Human memory is much more robust.  We often
use what seem to be very obscure cues yet we are able to retrieve well.<P>

<!-- Human memory is also flexible in terms of the type of information we
can to retrieve. The examples that we have discussed so far have all
required retrieval of the information presented at the study
opportunity.  Alternatively, we could ask questions like "Did you see
your boyfriend yesterday?", "How often do you visit your parents?" or
"Are you familiar with the term <I>xenophobic</I>?". These questions
require you to comment on the information stored in memory without
necessarily retrieving the detail of any particular memory.<P>
-->

Much of the research into human memory has been an exploration of just
how flexible our memories are - of the different sorts of questions
that we can use our memories to answer. In this chapter, we will
consider two ways in which the questions that we ask of our long term
memories can differ. The first of these refers to the <B>nature of the
output</B> that a question requires - are we asking for a specific
name, word or other item from memory (retrieval) or do we require a
yes/no answer about whether we remember some fact or episode
(matching). The second distinction refers to the <B>role of context</B> in the
query - are we asking about what happened in a given episode or context
(episodic) or is the query about the way that things tend to be in
general (semantic). After considering these tasks, we will look in some
detail at the Matrix Model of long term memory (Pike, 1984; Humphreys,
Bain & Pike, 1989) which provides a theory of how these questions could
be answered using the formalism of matrix algebra.<P>

<A NAME="MatchingVersusRetrievalTasksTheNatureOfTheOutput">
<H3>Matching Versus Retrieval Tasks: The Nature of the Output</H3></A>

Memory tasks differ with respect to the output that is required. The
usual task that people have in mind when they think about human memory
is one in which you are given one piece of information such as
someone's face and you must retrieve another piece of information such
as that person's name. These sorts of tasks seem to rely on a discrete
or discontinuous form of information. The required output is an item.
Two common examples of <B>retrieval tasks</B> are free association and
cued recall with a list associate.<P>

<B>Free Association</B> provides a subject with a cue (e.g. "Type of
animal") and requires them to respond with the first word that comes to
mind (e.g.  cat). Sometimes a prior study list is presented to the
subject and it has been shown that words that occur in the prior study
list are more likely to be produced even though the subject is not
instructed to use the study list to make their decisions. Free association
is a retrieval task because the required response is a word.<P>

<B>Cued Recall with a List Associate</B> requires the subject to study
a list of pairs of words. At test, subjects are given a list of words
and asked which word occurred with each of the test words during the
study episode (e.g. "Which word occurred with <I>boy</I> in the study
list?"). Again, cued recall with a list associate is a retrieval task
because it requires the subject to respond with a word.<P>

In contrast some memory tasks known as <B>matching tasks</B> require
what seems to be a more quantitative answer based on a continuous
measure. The examples with which we will be concerned are familiarity
rating and recognition.<P>

<B>Familiarity rating</B> refers to a task in which subjects rate (on a
five point scale, for instance) how familiar a word is to them in
general (e.g. "How familiar is the word <I>house</I> to you?").  
Familiarity rating is a matching task because it seems to be based on a
continuous form of information.<P>

<B>Recognition</B> requires a subject to study a list of words. At test,
the subject is given a second list of words - some of which appeared in
the first list and some of which did not. The subjects' task is
distinguish the targets (words that were on the list) from distractors
(words that weren't on the list). Either they are asked to make a
yes/no decision or they provide their confidence that the word is old
(which might be rated on a five point scale, for instance). Recognition
is considered a matching task because it relies upon a continuous form
of information.<P>

<A NAME="EpisodicVersusSemanticTasksTheRoleOfContext">
<H3>Episodic and Semantic Tasks: The Role of Context</H3></A>

Another important dimension on which memory tasks can vary is whether
they make reference to a study episode. Tasks that do specify the study
episode are known as episodic tasks, whereas tasks that do not are known
as semantic tasks (or generalized tasks). Tulving (1972) realized that the
learning in a study episode is not continuous with the learning that
occurs before study. In particular, he realized that when we ask a
subject in a recognition task do they recognize a word we are not
asking them whether they know the word at all (often all of the test
words are known to the subject). What we are asking is if the word
occurred in a given list (the study list). Similarly, in cued recall
with a list associate, we are not asking what word generally goes with
<I>boy</I>.  We are asking what word went with <I>boy</I> in the study
list. In contrast, familiarity rating and free association make no
specific reference to a study episode and are semantic tasks.  Table 1
categorizes the four memory tasks described above in
terms of the nature of the output and the role of context.<P>

Table 1: Examples of Episodic/Semantic Tasks and Matching/Retrieval
Tasks (adapted from Humphreys, Bain & Pike, 1989)<P>

<center>
<table border>
<tr><td><td align=center colspan=2>Use of Context Cue
<tr><td align=center>Access Process<td align=center>Episodic Memory<td align=center>Semantic Memory
<tr><td align=center>Matching<br><I>produces a rating value</I><td align=center>Recognition<td align=center>Familiarity Rating
<tr><td align=center>Retrieval<BR><I>produces a word</I><td align=center>Cued Recall<td align=center>Free Association
</table>
</center><P>

Before looking at how the Matrix Model accounts for the differences between
these tasks we need a grounding in tensors, the operations that can be
performed on tensors and how tensors can be mapped to neural network
architectures. If you are comfortable with these ideas you may skip
the next section.

<A NAME="TensorsExplained">
<h2>Tensors Explained</h2></A>

The Matrix Model of memory is built upon the mathematics of tensors.
Tensors are convenient ways of collecting together numbers. For instance,
a vector, which is also known as a rank one tensor, could be used to
describe the age, gender, and salary of an employee. If Jeff is 50
years old, is male (where male = 0 and female = 1) and earns $56000 per
annum then we could describe Jeff with the vector [50, 1, 56000] (see
figure 1). Note that vectors (and tensors in general) are ordered. The
vector [56000, 1, 50] would describe someone who was 56000 years old
who made a total of $50 per annum!<P>
<br><br><br><br>
<center>
<img src="ten1.gif">
</center><P>
Figure 1: Examples of vectors. (a) a row vector describing Jeff, (b) a
column vector, (c) a vector with N components.<P>

The rank one tensor described above has a dimension of three because it
contains three components. There is no reason that vectors need be
restricted to three dimensions, however. We could have added shoe size,
for instance, to increase the dimension to four. Similarly, there is no
reason that we need to restrict ourselves to a single row of numbers.
A tensor with N rows and M columns is known as an NxM matrix and has a
rank of two, indicating that the array of numbers extends in two
directions (see figure 2). <P>

<center>
<img src="ten2.gif">
</center><P>
Figure 2: Examples of matrices. (a) a 2x2 matrix, (b) a 3x2 matrix, (c) 
an NxN matrix.<P>

The process of extending the number of directions in which the array
extends can theoretically continue indefinitely, creating tensors of
rank three, four, five etc.  In the following sections, we will look at
vectors, matrices and tensors of rank three (see figure 3) as they are
critical to understanding the Matrix Model.  Other models, such as the
STAR model of analogical reasoning (Halford, Wiles, Humphreys and
Wilson 1992), employ tensors of higher rank.<P>

<br><br><br><br>
<br><br><br><br>
<center>
<img src="ten3.gif">
</center><P>
Figure 3: A rank three tensor (NxNxN).<P>



<A NAME="Vectors"><H3>Vectors - Rank One Tensors</H3></A>

Tensors, and in particular vectors, can be represented in 
many different forms including:<P>
<OL>
<LI><B>Cartesian</B> form in which the components are enumerated explicitly.
Figure 1 depicts vectors represented in Cartesian form.
<LI><b>Geometric</B> form in which the vector is plotted in N dimensional
space. 
For instance, figure 4 shows the vector representing Jeff plotted in three 
dimensional space. <P>
<center>
<img src="geometric.gif">
</center>
Figure 4: The vector representing Jeff plotted in three dimensional space (Geometric form).<P>
<LI><b>Algebraic</B> form in which a vector is represented as a
bolded lower case letter (e.g. <b>v</B>). Algebraic form is
a particularly concise form of representation, which makes it
easy to talk about the operations that can be performed on vectors
such as addition (e.g. <b>w</B> = <b>v</b> + <b>t</b>).
<li><b>Neural network</b> form which diagrams a neural network architecture
in which either a set of units or a set of weights contain the 
elements of the vector.
For instance, a vector can be mapped to a two layer network (one input
and one output layer) as depicted in Figure 5.  The number of units in
the input layer corresponds to the number of dimensions in the original
vector, while the output layer contains only 1 unit.  Each input unit
is connected to each output unit. The input units represent one vector
and the weights represent a second vector. <P>

<center>
<img src="net1.gif">
</center><P>
Figure 5: The network corresponding to a vector memory.<P>

The output of this network is defined to be the dot product (or inner
product) of the input and weight vectors. A <b>Dot Product</b> is
calculated by multiplying together the values which are in the same
position within the two vectors, and then adding the results of these
multiplications together to get a scalar (see Figure 6a). In the case
of the neural network, this involves multiplying each input unit
activation by the corresponding weight value and then adding. The dot
product of two vectors represents the level of similarity between them
and can be extended to higher rank tensors (see figure 6b)<p>

<center>
<img src="ten5.gif">
</center><P>
Figure 6: The Dot Product.<P>

The dot product is expressed algebraically as a dot, that is, the dot
product of the vectors <b>v</B> and <b>w</b> is written
<b>v</B>.<b>w</b>.<P>

Learning occurs in this network by adding the input vectors.  <b>Vector
addition</b> superimposes vectors of the same dimension.  It is
calculated by adding together the elements in a particular position in
each vector (see Figure 7a). In this way, multiple memories can be
stored within the same vector.  [Note: the network actually employs
Hebbian learning (see Neural Networks by Example: Chapter three).
However, when the output unit is fixed at one Hebbian learning is
identical to vector addition.]

<center>
<img src="ten4.gif">
</center><P>
Figure 7: (a) Vector Addition, (b) Matrix Addition.<P>

Again vector addition can be extended to tensors of arbitrary rank (see
figure 7b). Vector addition is expressed algebraically as a plus sign
(+). So if we wanted to talk about the dot product of <b>v</b> with the
addition of <b>w</b> and <b>x</B> we would write <b>v</b>.(<b>w</b> +
<b>x</b>). Another useful property to keep in mind is that the dot product
<b>distributes</b> over addition. That is:<P>
<b>v</b>.(<b>w</b> + <b>x</b>) = <b>v</b>.<b>w</b> + <b>v</b>.<b>x</b><P>

</OL>

In the following exercises, you will build a vector network that learns
to discriminate between stored items and new items (see figure 8). <P>

<center>
<img src="MatrixEx1.gif">
</center><P>
Figure 8: A vector memory network.<P>

Follow the
instructions below to create the network and then work through the
exercises.<P>

<ol>
<LI>Load BrainWave
<LI>Select 'New Hebbian Network'
<LI>Set up the vector memory network:
<ul>
<LI>Create two input units and one output unit.
<LI>Connect both input units to the output unit.
<LI>Set up VALUE objects for the units and weights.
</ul>
<LI>Create the data sets:
<ul>
<LI>Create an Input Set containing the two input units
<LI>Create a Test Set containing the two input units
<LI>Create an Output Set containing the output unit
<LI>The items in this exercise are FROG  [0.95, 0.32], TOAD [0.49, 0.87],
KOALA [0.32, -0.95]. Add the FROG pattern to the input set.
<LI>Add a pattern containing a 1 to the output set.
<LI>Add all three items, FROG, TOAD and KOALA, to the test set.
</ul>
</ul>

<I>Exercise 1:  Train the network for one epoch and record the weights
in the TRAIN FROG row of the following table. How have the weights
changed? <P> </I>
<center>
<table border>
<tr><td><td>Weight 1<td>Weight 2
<tr><td>TRAIN FROG<td>&nbsp;<td>&nbsp;
<tr><td>TRAIN FROG & KOALA<td>&nbsp;<td>&nbsp;
<tr><td>TRAIN FROG & TOAD<td>&nbsp;<td>&nbsp;
</table>
</center><P>

<I>Exercise 2:  Test each of the items, FROG, TOAD and KOALA, and
record the match values (the activation of the output unit) in the
second table. Explain the match values.</I><P> 
<center>
<table border>
<tr><td><td>TEST FROG<td>TEST TOAD<td>TEST KOALA
<tr><td>TRAIN FROG<td>&nbsp;<td>&nbsp;<td>&nbsp;
<tr><td>TRAIN FROG & KOALA<td>&nbsp;<td>&nbsp;<td>&nbsp;
<tr><td>TRAIN FROG & TOAD<td>&nbsp;<td>&nbsp;<td>&nbsp;
</table>
</center><P>

<I>Exercise 3: Train the network for one more epoch and test again.
What happens to the match values after a second training trial? Why?
</I><P>

<I>Exercise 4: Add KOALA to the input set and an output value of 1 to
the output set.  Zero the weights (using the ACTIONS menu) and retrain
the network on the updated input set.  Test the network as before,
recording the values in the table in the TRAIN FROG & KOALA
row.</I><P>

<I>Exercise 5: Delete KOALA from the input set and add TOAD. Zero the
weights, retrain and test as above, recording the values in the TRAIN
FROG & TOAD row. You should have six weight values and nine match
values for each training trial.  Create a graph of the match values
after the first training trial: plot three lines, one for each test
item, against the three training conditions.  Explain the shape of each
line on the graph.  </I><P>

<I>Exercise 6: For each of the three training conditions (FROG alone, FROG 
& KOALA, FROG & TOAD): <P>

<OL>
<LI>Draw the geometric (graphical) representation of the weights,
<LI>Provide the algebraic representation of the weights.
</I>
</OL>
<P>

<br>
<A NAME="Matrices"><H3>Matrices - Rank Two Tensors</H3></A>

The vector memory, discussed above, was capable of storing items so
that at a later time it could be determined if they had appeared. A
matrix memory allows two items to be associated - so that given one we
can retrieve the other. Algebraically, a matrix is usually represented
as a bolded upper case letter (e.g. <B>M</B>).<P>

Associations are formed using the outer product operation. A <b>outer
product</b> between two vectors is calculated by multiplying each
element in one vector by each element in the other vector (see Figure
8). If the first vector has dimension d<sub>1</sub> and the second
vector dimension d<sub>2</sub>, the outer product matrix has dimension
d<sub>1</sub>xd<sub>2</sub>. For instance, a three dimensional vector
multiplied by a two dimensional vector has dimension 3x2.<P>

<center>
<img src="ten6.gif">
</center><P>
Figure 8: The outer product.<P>

The outer product operation is expressed algebraically by placing the vectors
to be multiplied next to each other. So the outer product of <b>v</B>
and <b>w</b> is written as <b>v</B> <b>w</b>.<P>

These association matrices are then added into the memory matrix (as in
the vector memory case) - so that all associations are stored as a
single composite.  A matrix memory maps to a two layer network (one
input and one output layer) as depicted in Figure 9.  The number of
input units corresponds to the number of rows in the original  matrix,
while the number of output units corresponds to the number of columns.
Each input unit is connected to each output unit.<P>

<center>
<img src="net2.gif">
</center><P>
Figure 9: The network representation of a matrix.<P>

In the following exercise you will use a matrix memory network to store
and recall pairs of items.</b> <P>

<I>Exercise 7: Load the simulator, BrainWave. From the NETWORKS menu -
select Matrix Model (1). What rank tensor does this network implement? What
are its dimensions?</i><p>

<I>
Exercise 8: The items in this exercise are:
<ul>
Cues:
<ul>
FROG    [0.5, -0.5, 0.5, -0.5]
<br>
KOALA   [0.5, 0.5, -0.5, -0.5]
<br>
SNAIL   [-0.5, 0.5, 0.5, -0.5]
<br>
TOAD    [0.5, 0.4, 0.6, 0.45]
<br>
</ul>
Targets:
<ul>
FLIES   [0.7, 0.5, 0.5]
<br>
LEAVES  [0.7, -0.5, -0.5]
<br>
LETTUCE [0, -0.7, 0.7]
</ul>
</ul>

The input set contains the items FROG, KOALA and SNAIL, paired with
items in the output set FLIES, LEAVES and LETTUCE, respectively.
Another input item, TOAD [0.5, 0.4, 0.6, 0.45], can be used to test the
network on unfamiliar input.
Calculate the similarity value (i.e. dot product) of the  items FROG, KOALA,
SNAIL and TOAD with themselves, and each other,
and record the values in the table below:<P></I>

<center>
<table border>
<tr><td><td width=70>FROG<td width=70>KOALA<td width=70>SNAIL<td width=70>TOAD
<tr><td>FROG<td>&nbsp;<td>&nbsp;<td>&nbsp;<td>&nbsp;
<tr><td>KOALA<td>&nbsp;<td>&nbsp;<td>&nbsp;<td>&nbsp;
<tr><td>SNAIL<td>&nbsp;<td>&nbsp;<td>&nbsp;<td>&nbsp;
<tr><td>TOAD<td>&nbsp;<td>&nbsp;<td>&nbsp;<td>&nbsp;
</table>
</center><P>

<I>Exercise 9: Train the network for one epoch.  Test each of the items
FROG, KOALA, SNAIL and TOAD.  What output is produced in each case?
(Give the output pattern and also describe the output patterns in terms
of their similarity to FLIES, LEAVES and LETTUCE).<P>

<center>
<table border>
<tr><td>FROG<td width=70>&nbsp;<td width=70>&nbsp;<td width=70>&nbsp;
<tr><td>KOALA<td>&nbsp;<td>&nbsp;<td>&nbsp;
<tr><td>SNAIL<td>&nbsp;<td>&nbsp;<td>&nbsp;
<tr><td>TOAD<td>&nbsp;<td>&nbsp;<td>&nbsp;
</table>
</center>
</i><P>

<I>Exercise 10: Give the algebraic equation that describes the matrix memory
formed from the three pairs of associates: 
<br>
<ul>
</i><B>M = </B>
</ul>

<I>Exercise 11: Give the equations that describe each of the 
retrievals in exercise 9.  Use the similarity measures from the table
above to simplify each equation to a weighted sum of the target patterns.
<ul>
FROG
<br>
KOALA
<br>
SNAIL
<br>
TOAD
<br>
</ul>


</i><P>
<br><br><br>
<A NAME="Tensors"><H3>Tensors of Rank Three and Above</H3></A>

The final sort of tensor we need to demonstrate the matrix model is the
rank three tensor. The rank three tensor allows a three way association
to be represented. For instance, we could store the information that
John loves Mary - [Loves John Mary] - or that Apple appeared with
Pencil in List 1 [List 1, Apple, Pencil].<P>

A tensor of rank three maps to a three layer network (one input layer
with two sets of units, one output layer, and one layer of hidden
units) as depicted in Figure 10.  The number of units in the input sets
and the output set correspond to the dimensionality of the tensor.  The
number of hidden units corresponds to the number of units in one input
set times the number of units in the other input set.  Each hidden unit
has a connection from one input unit from each input set, with a hidden
unit existing for each possible combination.  These hidden units are SigmaPi
units, the value of which is set to the multiplication of the two input
units to which it is connected. To implement a rank three tensor, the
weights in the first layer are frozen at one. Consequently, a hidden
unit's activation will equal the multiplication of the activations of
the input units to which it is connected. Each hidden unit is then
connected to each output unit.<P>


<center>
<img src="net3.gif">
</center><P>
Figure 10: The network representation of a rank three tensor.<P>

In these exercises, you will use both rank two and three tensor
networks to store and recall triples of items.<P>

<P>
<I>Exercise 12: Load the simulator, BrainWave.
From the NETWORKS menu - Matrix Model (2).
What rank tensor does this network implement?</i><p>
<br><br><Br><br><br><br>
<I>Exercise 13: The items in this exercise are:<p>
Cues:
<ul>
FROG     [0.5, -0.5, 0.5, -0.5]
<br>
KOALA    [0.5, 0.5, -0.5, -0.5]
<br>
SNAIL    [-0.5, 0.5, 0.5, -0.5]
<br>
TOAD     [0.5, 0.4, 0.6, 0.45]
<br>
</ul>
Relations:
<ul>
EATS     [0.5, -0.5, -0.5, 0.5]
<br>
LIVES-IN [0.5, 0.5, -0.5, -0.5]
</ul>
Targets:
<ul>
FLIES    [0.7, 0.5, 0.5]
<br>
LEAVES   [0.7, -0.5, -0.5]
<br>
LETTUCE  [0, -0.7, 0.7]
<br>
POND     [0.89, 0.43, -0.22]
<br>
TREE     [-0.22, 0.76, 0.62]
<br>
SHELL    [0.43, -0.5, 0.76]
</ul>

Notice that the vectors for the cues are the same as those used above.
Also notice that EATS and LIVES-IN are orthogonal to each other - that is they
have a dot product of zero.<P>

Calculate the similarity (dot product) table for the targets.<P>
</i>

<center>
<table border>
<tr><td><td width=70>FLIES<TD width=70>LEAVES<td width=70>LETTUCE<td width=70>POND<td width=70>TREE<td width=70>SHELL
<tr><td>FLIES<td>&nbsp;<td>&nbsp;<td>&nbsp;<td>&nbsp;<td>&nbsp;<td>&nbsp;
<tr><td>LEAVES<td>&nbsp;<td>&nbsp;<td>&nbsp;<td>&nbsp;<td>&nbsp;<td>&nbsp;
<tr><td>LETTUCE<td>&nbsp;<td>&nbsp;<td>&nbsp;<td>&nbsp;<td>&nbsp;<td>&nbsp;
<tr><td>POND<td>&nbsp;<td>&nbsp;<td>&nbsp;<td>&nbsp;<td>&nbsp;<td>&nbsp;
<tr><td>TREE<td>&nbsp;<td>&nbsp;<td>&nbsp;<td>&nbsp;<td>&nbsp;<td>&nbsp;
<tr><td>SHELL<td>&nbsp;<td>&nbsp;<td>&nbsp;<td>&nbsp;<td>&nbsp;<td>&nbsp;
</table>
</center><P>

<I>Exercise 14: The cue+relation input set contains the items
FROG-EATS, KOALA-EATS, SNAIL-EATS, FROG-LIVES_IN, KOALA-LIVES_IN and
SNAIL-LIVES_IN, paired with items in the output set FLIES, LEAVES,
LETTUCE, POND, TREE, and SHELL, respectively.  Two other input items,
TOAD-EATS and TOAD-LIVES_IN, can be used to test the network's response
to unfamiliar input.<p>

Train the network for one epoch.  Test each of the items FROG-EATS,
KOALA-EATS, SNAIL-EATS, FROG-LIVES_IN, KOALA-LIVES_IN, SNAIL-LIVES_IN,
TOAD-EATS and TOAD-LIVES_IN.  What output is produced in each case?
(Give the output pattern and also describe the output patterns in terms
of their similarity to FLIES, LEAVES, LETTUCE, POND, TREE and SHELL)

<ul>
FROG-EATS
<br>
<br>
KOALA-EATS
<br>
<br>
SNAIL-EATS
<br>
<br>
FROG-LIVES_IN
<br>
<br>
KOALA-LIVES_IN
<br>
<br>
SNAIL-LIVES_IN
<br>
<br>
TOAD-EATS
<br>
<br>
TOAD-LIVES_IN
<br>
<br>
<br>
</ul><P>
</i>

<i>Exercise 15: How does the performance of this network compare with
the performance of the network in Exercise 8?  Why is it not as
good?</i><p>

<I>Exercise 16: Give the algebraic equation that describes the matrix memory
formed from the three pairs of associates: 
<br>
<ul>
<B>M = </B>
</ul></I><P>

<I>Exercise 17: Give the equations that describe each of the retrievals
from exercise 14.  Use the similarity measures from the table
above to simplify each equation to a weighted sum of the target patterns.

<ul>
FROG-EATS
<br>
<br>
KOALA-EATS
<br>
<br>
SNAIL-EATS
<br>
<br>
FROG-LIVES_IN
<br>
<br>
KOALA-LIVES_IN
<br>
<br>
SNAIL-LIVES_IN
<br>
<br>
TOAD-EATS
<br>
<br>
TOAD-LIVES_IN
<br>
<br>
<br>
</ul>

</i>


<I>Exercise 18: From the NETWORKS menu - select Matrix Model (3).
What rank tensor does this network implement?</i><p>
<br>
<br>


<I>Exercise 19: The inputs and outputs for this network are the same as
for the previous one, but the connections and hidden SigmaPi units
perform different calculations on the inputs to try and achieve the
correct outputs.  Train the network for one epoch.  Test each of the
items FROG-EATS, KOALA-EATS, SNAIL-EATS, FROG-LIVES_IN, KOALA-LIVES_IN,
SNAIL-LIVES_IN, TOAD-EATS and TOAD-LIVES_IN.<P>

What output is produced in each case? (Give the output pattern and also
describe the output patterns in terms of their similarity to FLIES,
LEAVES, LETTUCE, POND, TREE and SHELL).<P>

<ul>
FROG-EATS
<br>
<br>
KOALA-EATS
<br>
<br>
SNAIL-EATS
<br>
<br>
FROG-LIVES_IN
<br>
<br>
KOALA-LIVES_IN
<br>
<br>
SNAIL-LIVES_IN
<br>
<br>
TOAD-EATS
<br>
<br>
TOAD-LIVES_IN
<br>
<br>
<br>
</ul>
</I> <P>

<I>Exercise 20: Which of the two networks performs the memory task better?  Why?</i><P>

<I>Exercise 21: Give the algebraic equation that describes the matrix memory
formed from the three pairs of associates:
<br>
<ul>
<B>M = </B>
</ul>
</I>

<I>Exercise 22: Give the equations that describe each of the cued recall
tests from question 19.  Use the similarity measures from the table
above to simplify each equation to a weighted sum of the target patterns.

<ul>
FROG-EATS
<br>
<br>
KOALA-EATS
<br>
<br>
SNAIL-EATS
<br>
<br>
FROG-LIVES_IN
<br>
<br>
KOALA-LIVES_IN
<br>
<br>
SNAIL-LIVES_IN
<br>
<br>
TOAD-EATS
<br>
<br>
TOAD-LIVES_IN
<br>
<br>
<br>
</ul>
<P></I>

In this section, we have been looking at the way in which tensors of
rank one, two and three can be used to store information. In the next
section, we will examine the Matrix Model, which uses precisely this
mechanism to explain the nature of human memory.<P>

<br><br><br>
<A NAME="TheMatrixModel">
<H2>The Matrix Model</H2></A>

 
The Matrix Model of Memory was developed by Humphreys, Bain and Pike
(1989) and Pike (1984) to provide a coherent theoretical account of a
range of different memory tasks, including episodic tasks, such as
recognition and recall, and semantic tasks, such as familiarity rating
and indirect production tasks.  It is a distributed associative model
in which items are modelled and stored as vectors of feature weights or
elements, just as was the case in the previous section.  Elements
within each vector contribute conjointly to the representation of
items.  Thus memory representations are not located at specific points
within a memory network, or within specific memory systems.  Instead
they are conceptualised as unique patterns of activation over a common
set of elements. Typically, these patterns are thought to be <i>sparse</I>
representations meaning that only a few of the elements are active.<P>

<A NAME="Memory Representations"><H3>Memory Representations</H3></A>

The memory representations in the Matrix Model include items, contexts
or, combinations of items and contexts (associations).<P>

<OL>
<LI><B>Items</B> - Items can be any sort of stimuli including words, 
pictures, melodies etc. For the most part, however, the experiments to
which the model has been applied use words. Each
item is modelled as a vector of feature weights.   Feature weights are
used to specify the degree to which certain features form part of an
item.  There are two possible levels of vector representation for items.
These are:<P>

<OL>
<LI>modality specific peripheral representations
	(e.g., graphemic or phonemic representations of words)
<LI>modality independent central representations
	(e.g., semantic representations of words).

</OL><P>
Item vectors are distinguished by subscripts (e.g. <b>a<sub>i</sub></b>). A
distractor vector is indicated by a <b>d</b>.<P>

<LI><B>Contexts</B> - To distinguish between episodic and non-episodic
tasks the Matrix Model assumes the episode or context in which items
are studied is also represented by a vector of feature weights. In
episodic tasks this context vector must be reinstated so that it may be
used as a cue to the memory system. The context vector is represented
by an <b>x</b>.<P>

<LI><B>Associations</B> - While individual items and contexts are
represented as single vectors  (<b>a</b>, <b>b</b>, <b>x</b>),
associations between items and contexts are represented by matrices
derived from the matrix product of  these vectors.  The resulting
matrix product represents the association (or binding) between either
items, or between items and context. The memory of the matrix Model is
formed by adding these associations together. The model posits a number
of different kinds of associations including:<P>

<OL>
<LI>Two-way associations between a single item (<b>a</b>) and a context (<b>x</b>,
e.g. bacon x breakfast this morning) are represented as a
context-to-item association  (<b>x a</b>),  where <P>

<b>x</b>  = n element column vector <BR>
<b>a</b>  = n element row vector<P>

<LI>Associations between a list of items (<b>a<sub>1</sub>,
a<sub>2</sub></b>,...,<b>a<sub>k</sub></b>) and a context (<b>x</b>)
are represented by multiplying each of the item vectors by the context
vector and summing the resulting matrices.  This sum represents the
memory of the study list (<b>E</b>).<P>

<b>E</b> = <b>x a<sub>1</sub></b> + <b>x a<sub>2</sub></b> + ... + <b>x a<sub>k</sub> </b><P>


<LI>Three-way associations between a list of word pairs
(<b>a<sub>1</sub> b<sub>1</sub>, a<sub>2</sub> b<sub>2</sub></b>, ...
<b>a<sub>k</sub> b<sub>k</sub></b>) and context (<b>x</b>, e.g., bacon x dog x
breakfast this morning) are represented by the rank three tensor
(<b>x a<sub>j</sub> b<sub>j</sub></b>), where<P>

<b>x</b>  = n element column vector<BR>
<b>a<sub>j</sub></b> = n element row vector<BR>
<b>b<sub>j</sub></b> = n element orthogonal vector<P>

The resulting associations can be summed to form the memory for the
list (<b>E</b>).<P>

<b>E</b> =  <IMG align=middle SRC="sumjk.gif"><b>x a<sub>j</sub> b<sub>j</sub></b><P>

Note: the type of vector (i.e., row, column, orthogonal) can also be
inferred from the order of the vector symbols, where:  1st vector =
column vector;  2nd vector = row vector; and 3rd vector = orthogonal.<P>

<LI>Pre-existing memories (<b>S</b>) are added to list memories
(<b>E</b>) because test performance can be influenced by both list
memories and pre-existing memories.<P>

<b>M</b> = <IMG align=middle SRC="sumjk.gif"><b>x a<sub>j</sub> b<sub>j</sub></b>  + <b>S</b> <P>

&nbsp;
&nbsp;
&nbsp;
</OL><P>

<A NAME="Accessing Memory Representations"><H3>Accessing Memory Representations</H3></A>

Having constructed the memory matrix, we can now see how the Matrix
Model goes about accessing this representation at test for a number of
different tasks. All retrieval in the Matrix Model is <b>direct</b>.
The memory matrix is presented with cues and access occurs in
parallel.  There is no sequential search process. Presenting the model
with a cue involves taking the inner product (or dot product) of the
cue vector with the memory matrix.<P>

One of the strengths of the Matrix Model is in the number of a ways in
which information from the model can be accessed. In the introductory
section, two dimensions on which tasks can differ were outlined. 
The first was the matching/retrieval dimension. Matching tasks are
those based on a continuous form of information that typically require
either a yes/no answer or a rating response (e.g. recognition).
Retrieval tasks, by contrast, require a specific item to be returned
(e.g. cued recall). This distinction is captured in the Matrix Model by
the nature of the tensor that results once all cues have been applied.
If the resultant tensor is a scalar we are dealing with a matching
process. This scalar can be compared against criteria to determine a
yes/no or rating value.  If the resultant tensor is a vector then we
have a retrieval process. The vector can be compared against all item
vectors with the item being the output of the process. The next sections,
goes through the mathematics of recognition and cued recall with a 
list associate demonstrating how matching and retrieval tasks 
are accomplished within the model.<P>

The second task dimension discussed in the introduction focussed on the
the episodic/semantic dimension. Episodic tasks refer to a specific
context, whereas in semantic (generalized) tasks information is
integrated over a large number of experiences. The Matrix Model
captures this distinction.  In episodic tasks, a reinstated context
vector is used as a cue. In semantic (generalized) tasks, a vector
which is equally similar to all contexts is used so as to average over
all experiences with the cue items (typically, this is a vector with
all components set to 1/n where n is the dimension of the vector). The
section entitled "Episodic versus Semantic Memory: Cuing with the
Context Vector" describes an experiment designed to demonstrated the
importance of the distinction and leads you through the process of
modelling this experiment using the BrainWave simulator.<P>


<A NAME="MatchingVersusRetrievalTasksScalarOrVectorOutput"><H3>Matching Versus Retrieval Tasks: Scalar or Vector Output</H3></A>

In this section, a matching task, namely recognition, and a retrieval
task, namely cued recall with a list associate, are compared within
the Matrix Model framework.<P>

<H4>Recognition</H4>

Recognition involves a matching process, where the overall similarity
between the test cues (<b>x</b> and <b>a<sub>i</sub></b>) and memory
(<b>M</b>) is calculated.  Because this is an episodic task, the test
cues involve both word cues and a context cue.  This episodic matching
process is accomplished by combining the test cues into an associative
matrix (<b>x a<sub>i</sub></b>) and determining a dot product between: <P>

<ol>
<li>the cue matrix (<b>x a<sub>i</sub></b>), and  <BR>
<li>the memory matrix (<b>M</b> =  <IMG align=middle SRC="sumjk.gif">  <b>x a<sub>j</sub></b> + <b>S</b>). <P>
</ol>

[Note: Because the dot product operation is associative, the results
are identical regardless of whether you form a combined <b>x
a<sub>i</sub></b> matrix and then take a dot product or take the dot
product of each of the cues with the memory matrix progressively.]<P>

<u>Studied Test Word (a<sub>i</sub>)</u> <P>

<b>xa<sub>i</sub> . M = xa<sub>i</sub> . (<IMG align=middle SRC="sumjk.gif"> xa<sub>j</sub> + S</b>) <BR>
	=  <IMG align=middle SRC="sumjk.gif"> <b>x a<sub>i</sub> . x a<sub>j</sub> + x a<sub>i</sub> . S</b> <BR>
	=  <IMG align=middle SRC="sumjk.gif"> <b>(x . x) (a<sub>i</sub> . a<sub>j</sub>) + x a<sub>i</sub> . S</b> <BR>
	= <b>(x . x) (a<sub>i</sub> . a<sub>i</sub> ) + <IMG align=middle SRC="sumji.gif"> (x . x) (a<sub>i</sub> . a<sub>j</sub>) + xa<sub>i</sub> . S</b> <P>
 
Inserting the expected matching value:<P>

<ul>
E[<b>x a<sub>i</sub> . M</b>] = c s + (k - 1) c m + g<P>
</UL>

where <br>

<UL>
c = similarity between the study and test context (assumed to large)<BR>
s = similarity between the same word encoded at study and test (assumed to be large) <BR>
m = similarity between different words at study and test (assumed to be small) <BR>
g = contribution of pre-existing memories<P>
</UL>


<u>Non studied Test Word (d)</u> <P>

<b>x d . M = x d . (<IMG align=middle SRC="sumjk.gif"> xa<sub>j</sub>+ S)</b><BR>
=  <IMG align=middle SRC="sumjk.gif"> <b>xd . xa<sub>j</sub> + xd . S</b> <BR>
=  <IMG align=middle SRC="sumjk.gif"> <b>(x . x) (d . a<sub>j</sub>) + xd . S</b> <P>
where <P>
<ul>
E[<b>x d . M</b>] = c m k + g<P>
</UL>
 
Note that the matching operations in the above equations can be
collapsed down into several components, including :<P>

<ol>
<li>a match between the test cue and the pre-experimental memories 
(i.e.,  <b>x a<sub>i</sub> . S</b>  or  <b>x d . S</b>), and <BR>
<li>a match between the test cue and the experimental memories <BR>
(i.e., <IMG align=middle SRC="sumjk.gif">  <b>x a<sub>i</sub> . x a<sub>j</sub></b> or <IMG align=middle SRC="sumjk.gif"> <b>x d . x a<sub>j</sub></b> ) <P>
</ol><P>
 
The match between the test cue and the experimental memories can
further be collapsed down into :<P>

<ol>
<li>a match between the context on study and test occasions  
		(<b>x . x</b> = c), and <BR>
<li>a match between the study and test items 
		(<b>a<sub>i</sub> . a<sub>i</sub></b> = s  and  <b>a<sub>i</sub> . a<sub>j</sub></b> = m) or (<b>d . a<sub>j</sub></b> = m) <P>
</OL><P>

Thus the final dot product derived from these equations, represents
the match of the contexts on the study and test occasions (c), weighted
by the match of the items on the study and test occasions (s and m).
Consequently, memories that are conjointly defined by context and test
cues will be weighted more heavily than items not studied in that
context.  This mechanism enables the model to avoid interference (large
weights) from other items studied in the same context and also from
previous contexts in which items have appeared.<P>


<H4>Cued Recall with a List Associate</H4>

Cued recall with a list associate involves a subject studying a list of
pairs. At test they are given an item and are required to produce the
word with which it was paired at study. This is an important task
because it can be used to demonstrate that three way association are
necessary to model human memory. Simple associations two-way
associations between items are insufficient (Humphreys, Bain & Pike
1989).<P>

For this reason, cued recall with a list associate is modelled using
rank three tensors that associate word pairs (<b>a<sub>1</sub>
b<sub>1</sub></b>, <b>a<sub>2</sub> b<sub>2</sub></b>,...
<b>a<sub>k</sub> b<sub>k</sub></b>) and context (<b>x</b>). The tensor
is formed by taking the outer product of the context vector <b>x</b> and the two item vectors, <b>a<sub>j</sub></b> and <b>b<sub>j</sub></b>.<P>

<b>M</b> = <IMG align=middle SRC="sumjk.gif"> <b>x a<sub>j</sub>
b<sub>j</sub>  + S</b> <P>

Subjects are then asked to recall list targets (<b>b<sub>i</sub></b>) at test,
using list associates (<b>a<sub>i</sub></b>) and context (<b>x</b>) as cues.  The
retrieval cues (<b>x</b> and <b>a<sub>j</sub></b>) are combined to form an
associative matrix cue (<b>x a<sub>i</sub></b>).  Retrieval then involves the
pre-multiplication of the rank three tensor (<b>M</b>) by the retrieval cue
(<b>x a<sub>i</sub></b>).<P>

<b>x a<sub>i</sub> . M = <IMG align=middle SRC="sumjk.gif"> x a<sub>i</sub> .  x a<sub>j</sub> b<sub>j</sub> + S</b> <BR>
= <IMG align=middle SRC="sumjk.gif">[(<b>x a<sub>i</sub></b>)(<b>x a<sub>j</sub></b>)] <b>b<sub>j</sub></b> + <b>x a<sub>i</sub> . S</b><BR>
= <IMG align=middle SRC="sumjk.gif">[(<b>x . x</b>) (<b>a<sub>i</sub> .  a<sub>j</sub></b>)] <b>b<sub>j</sub></b> + <b>x a<sub>i</sub> . S</b><BR>
= (<b>x . x</b>) (<b>a<sub>i</sub> . a<sub>i</sub></b>) <b>b<sub>i</sub></b>  + <IMG align=middle SRC="sumji.gif">  (<b>x . x</b>) (<b>a<sub>i</sub> . a<sub>j</sub></b>) <b>b<sub>j</sub></b> + <b>x a<sub>i</sub> . S</b><P>

Inserting the expected values: <P>

E[<b>x a<sub>i</sub> . M</b>] = c s <b>b<sub>i</sub></b> + c m <IMG align=middle SRC="sumji.gif"><b>b<sub>j</sub></b>  + <b>x a<sub>i</sub> . S</b><P>

The end product (matrix product) of this process will comprise a target
vector of feature weights.  This featural information can be used to
produce a word or item response.<P>



The target vector is weighted by: <P>

<ol>
<li>the similarity of the context on the study and test occasions  
	( <b>x . x</b> = c), and <BR>
<li>the similarity of the list cue on the study and test occasions 
	(<b>a<sub>i</sub> . a<sub>i</sub></b> = s) and (<b>a<sub>i</sub> . a<sub>j</sub></b> = m) <P>
</ol>
 
Note that the weights for the same associate (s) will be greater than
the weights for different associates (m) making the resulting vector
look more like the correct associate (on average) than any other item.
Noise will also be generated by the pre-existing memories. The
assumption is that, in general, the similarity of the pre-existing
contexts and the current context will be small leading to low levels of
interference. Of course, if a recent context also included the cue word
then much more interference will be generated because the context
vectors will be more similar.

<!--
<A NAME="RecallInTheAbsenceOfRecognition">
<H2>Recall in the Absence of Recognition</H2></A>

<h3>The Task and Phenomenon</h3>

The relationship between recall and recognition has been central to the
study of episodic memory for several decades. Most people have the
intuition that recalling a specific item is more difficult than simply
recognizing that you have seen an item, and often this is the case (I
know I've seen that person before, but I can't remember their name).
However, under some circumstances recall can be better than recognition
(Tulving & Thompson 1973; Watkins & Tulving 1975).  The discovery of
recognition failure of recallable words has had an important impact on
the development of memory theory. In particular, models in which
recognition is a simple subprocess of recall (the generate - recognize
models) underwent substantial modification as a consequence. Before
looking more closely at the implications for memory modelling, however,
we will describe the sort of experiment that has demonstrated recall
without recognition as exemplified by Watkins and Tulving (1975,
experiment one). <P>

The basic methodology of the Watkins and Tulving experiment is presented
in table 1.<P>

Table 1: Basic methodology: Schematized Sequence of Procedures. Reproduced 
from Watkins and Tulving (1975).
<center>
<table>
<tr><td width=40><u>Step</U><td width=350><u>Procedure</U><td><u>Example</U>
<tr><td>1a<td>List 1 Presented<td><i>badge</I> - BUTTON
<tr><td>1b<td>Cued recall of List 1<td><i>badge</i> - <U><b>button</B></U>
<tr><td>2a<td>List presented<td><i>preach</I> - RANT
<tr><td>2b<td>Cued recall of List 2<td><i>preach</i> - <u><b>rant</B></u>
<tr><td>3<td>List 3 presented<td><i>glue</I> - CHAIR
<tr><td>4a<td>Free-association stimuli presented<td>table 
<b><u>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</u> </b>
<b><u>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</u> </b>
<b><u>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</u> </b>
<b><u>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</u> </b>
<tr><td>4b<td>Free-association responses made<td>table 
<tr><td>5a<td>Recognition test sheets presented<td>DESK TOP CHAIR
<tr><td>5b<td>Recognized items circled<td>DESK |<u>TOP</u>| CHAIR
<tr><td>5c<td>Recognition confidence of circled items attempted<td>DESK |<u>TOP</u>|<sub>1</sub> CHAIR
<tr><td>5d<td>Recall of list cues of circled items attempted<td>TOP - can't recall
<tr><td>6<td>Cued recall of List 3<td><i>glue</I> - <B><u>chair</u></B>
</table></center><P>


Steps one and two involve cued recall tests designed to influence how
subjects encode lists of word pairs. Word pairs such as <I>glue</I> -
CHAIR are presented and subjects are asked to learn them for a later
test. The first word (<i>glue</I>) is designated the cue and the second
word (CHAIR) is designated the target. Typically, subjects must encode
the two words <I>interactively</I> for cued recall performance to be
good.  These first two steps are practise trials aimed at making sure
their encodings are strong.<P>

The third step is the critical study list. It is similar to the first
two study lists. This time, however, instead of an immediate cued
recall test, subjects were asked to free associate to a set of
words that did not appear on the study list (step four). These words
were chosen so that it was likely that subjects would respond with one
of the target words.<P>

The fifth step was the recognition test. Subjects were given groups of
three words and asked to chose the word which appeared as a target in
the study list (step three). Then they had to rate their confidence and
try to recall the cue word that appeared with that target.  Finally,
in step six, subjects were given a cued recall test for the study list.
<P>



Table 1: The Proportions of Targets Recognized and Recalled (Watkins & Tulving 1975)
<center>
<table>
<tr><td><td>Recognized<td>Not Recognized
<tr><td>Recalled<td align=right>.25<td align=right>.24
<tr><td>Not Recalled<td align=right>.14<td align=right>.36
</table><P>
</center>


Table 1 shows the proportion of items recalled and recognized. The key point 
is that half of the items that were recalled were not recognized. This
occurs despite the fact that the recognition test comes first, so that
forgetting should affect recall more than recognition.<P>

-->
<P>
In the last two sections we have seen how, in a mathematical sense, the
Matrix Model distinguishes between matching and retrieval tasks.  In
the next section, we will examine the episodic/semantic distinction
by using the Matrix Model to simulate data generated by Bain &
Humphreys (1989).


<A NAME="EpisodicVersusSemanticMemoryCuingWithTheContextVector">
<H3>Episodic versus Semantic Memory: Cuing with the Context Vector</h3></A>

Bain & Humphreys (1989, pg. 229) report an experiment which clearly
demonstrates the difference between episodic and semantic matching
tasks by reinstating the context during some, but not all, of the test
conditions.  Subjects were given a set of words and asked to produce a
synonym for each.  One week later the same subjects were given a
passage containing unhighlighted target words, and asked to read the
text and then answer questions on it.  Half of the target words were
common to both training stages.  In addition to the test items already
mentioned (synonym, passage, or both), words which appeared in neither
training stage were also included as test items.  Each set of test
items contained equal numbers of high and low frequency words.<p>

The subjects were grouped into three test conditions.  Group A was
asked to give a general familiarity rating for the words (a generalized
matching condition).  Group B was asked to recognise which words had
been in the synonym generation task (an episodic matching condition).
Group C was asked to recognise which words had been in the passage
reading task (also an episodic matching condition).  The mean
recognition and familiarity ratings are displayed in Figure 11.<p>


<center>
<img src="memfig1.gif">
</center><P>

Figure 11: Mean ratings for three tasks as a function of presentation
list(s) and word frequency. (a) Familiarity Rating Task (b) Recognition
of Synonym Task words and (c) Recognition of Passage Task words. Note
that in generalized familiarity task ratings depended only on the
frequency of the word. For the episodic tasks, however, the lists in
which the subjects were exposed to the word are critical.<P>

As Figure 11 shows, subjects performing the episodic matching tasks were
affected by the training context indicated in the task instructions,
while subjects performing the general matching task were not influenced
by the prior training conditions.  Furthermore, the subjects did not
have trouble reinstating the synonym context as opposed to the passage
context, and vice versa.<P>

These results suggest that subjects are able to distinguish episodic
and semantic (or generalized) memory tasks quite well. One explanation
is that the episodic and semantic memory systems are located in two
different compartments in the brain. In the generalized familiarity
task, subjects access the semantic store, in the episodic recognition
task subjects access the episodic store. This may well be the case,
however, Humphreys, Bain and Pike (1989) showed using the Matrix model
that it need not be. The episodic/semantic distinction can be captured
in a single coherent memory system by assuming differences in the types
of cues supplied.<P>

In the following exercises, the Matrix Model will be used to
demonstrate how the difference between generalized familiarity and
episodic recognition can be captured. To simplify the modelling process
we assume a design similar to that employed by Bain and Humphreys
(1989), but in which only one study list is presented. What we are
looking for is a difference in the pattern of results for target and
distractor words when asking for generalized familiarity versus
episodic recognition. The key distinction, from the model's point of
view, is in the nature of the context cue. In episodic recognition it
will be assumed that the context cue is the same as that at study. In
contrast, when modelling generalized familiarity the context cue will
be a a vector in which all components are 0.1. This context vector will
be similar to all of the pre-experimental contexts and the study
context to approximately the same degree and will therefore produce an
output which is approximately the mean of all exposures - not just the
study list exposures.<P>

<I>Exercise 23: Load the simulator, BrainWave. From the NETWORKS menu -
select Familiarity vs Recognition.  This network contains three sets of
units - the input units, which will contain the context vectors, the
output units, which will contain the items to which a context is
associated and the match units, which contain the item to be tested.
Weights are connected between the input units and the output units.
What rank tensor does this network implement?</i><p>

Above the units is a global value called "Dot Product". This global
value indicates the dot product of the output units and the match units
and is updated when you click on cycle. It is this value which will
indicate the strength of a match in both the episodic recognition and
generalized familiarity conditions.<P>

In addition, there are three collections of pattern sets. The
pre-experimental sets contain the input/output pairs representing the
subjects experience before entering the experiment. Each context is
different indicating that subjects pre-experimental experience with
words arises from many different contexts.  Each context vector has
just three units active and these units are active to different
degrees. The same is true for the output patterns which represent the
words. However, some of the word patterns are repeated representing the
difference between high and low frequency words. The high frequency
words are repeated three times while the low frequency words appear
just once. Note that real words occur much more often. We have
decreased the numbers here to facilitate modelling. It is important to
consider, however, what effect increasing the numbers of presentations
would have. A later exercise will be directed towards this question.
In the pre-experimental output set (as well as the match and
experimental output sets), the words are followed by a tag such as hft
or lfd. The hf or lf stands for high frequency and low frequency
respectively, and the t or d stands for target or distractor. This tag
just allows you to easily remember the type of each word without having
to cycle through the relevant pattern sets.<P>

<I>Exercise 24: Click through the pre-experimental output set. How many
presentations are there? How many unique words are there?<P></I>

The experimental set represents a subject's experience during the study
list.  At study, words are all presented the same number of times and
in the experimental output set each word appears just once. In all
cases the study context is the same. Note that only target words
appear in the experimental list.<P>

<I>Exercise 25: Click through the experimental output set.  How many
words are there?<P></I>

The final collection of pattern sets are those that will be used for
testing the network. The input set contains the Study Context pattern
and the Generalized Context pattern. When testing episodic recognition
the Study Context pattern should be selected, when testing generalized
familiarity the Generalized Context pattern should be selected. The
output set contains no patterns because these sets will only be used for
cycling, not for learning. The match set contains a copy of each of the
words - both the targets and the distractors.<P>

<I>Exercise 26: Click through the match set.  How many
words are there?<P></I>


Now we are ready to train and test the system. Train the network for one
epoch with the Pre-experimental input and output sets and then for one
epoch with the Experimental input and output sets (If you are learning 
for a second time remember to reset the weights - Actions Menu -
so that current learning doesn't accumulate with the prior learning).<p>

To test whether the network is familiar with a word in the study
context, or is familiar with a word generally (it can be both): select
the test output set; select the word from the match set; select either
the Study Context or the General Context from the test input set and
cycle once.<p>

<I>Exercise 27: Simulate the generalized familiarity task and fill in the
the dot product values in Table 1 below.<p></I>


Table 1:  Generalized Familiarity Task: Dot Product Values<P>

<center>
<table width="90%" border>
<tr><td width="25%" colspan=2 align=center>High Frequency Target<td width="25%" colspan=2 align=center>Low Frequency Target<td width="25%" colspan=2 align=center>High Frequency Distractor<td width="25%" colspan=2 align=center>Low Frequency Distractor
<tr><td width="12.5%">child<td width="12.5%">&nbsp;<td width="12.5%">avery<td width="12.5%">&nbsp;<td width="12.5%">horse<td width="12.5%">&nbsp;<td width="12.5%">crept<td width="12.5%">&nbsp;
<tr><td>phone<td>&nbsp;<td>elope<td>&nbsp;<td>space<td>&nbsp;<td>flank<td>&nbsp;
<tr><td>woman<td>&nbsp;<td>adage<td>&nbsp;<td>eight<td>&nbsp;<td>broth<td>&nbsp;
<tr><td>light<td>&nbsp;<td>dally<td>&nbsp;<td>sound<td>&nbsp;<td>envoy<td>&nbsp;
<tr><td>visit<td>&nbsp;<td>graft<td>&nbsp;<td>april<td>&nbsp;<td>aural<td>&nbsp;
<tr><td>green<td>&nbsp;<td>banjo<td>&nbsp;<td>leave<td>&nbsp;<td>debit<td>&nbsp;
<tr><td>river<td>&nbsp;<td>fidel<td>&nbsp;<td>table<td>&nbsp;<td>guise<td>&nbsp;
<tr><td>MEANS<td>&nbsp;<td><td>&nbsp;<td><td>&nbsp;<td><td>&nbsp;
</table>
</center><P>

<I>Exercise 28: Simulate the episodic recognition task and fill in the
the dot product values in Table 2 below.<p></I>


Table 2:  Episodic Recognition Task: Dot Product Values<P>

<center>
<table width="90%" border>
<tr><td width="25%" colspan=2 align=center>High Frequency Target<td width="25%" colspan=2 align=center>Low Frequency Target<td width="25%" colspan=2 align=center>High Frequency Distractor<td width="25%" colspan=2 align=center>Low Frequency Distractor
<tr><td width="12.5%">child<td width="12.5%">&nbsp;<td width="12.5%">avery<td width="12.5%">&nbsp;<td width="12.5%">horse<td width="12.5%">&nbsp;<td width="12.5%">crept<td width="12.5%">&nbsp;
<tr><td>phone<td>&nbsp;<td>elope<td>&nbsp;<td>space<td>&nbsp;<td>flank<td>&nbsp;
<tr><td>woman<td>&nbsp;<td>adage<td>&nbsp;<td>eight<td>&nbsp;<td>broth<td>&nbsp;
<tr><td>light<td>&nbsp;<td>dally<td>&nbsp;<td>sound<td>&nbsp;<td>envoy<td>&nbsp;
<tr><td>visit<td>&nbsp;<td>graft<td>&nbsp;<td>april<td>&nbsp;<td>aural<td>&nbsp;
<tr><td>green<td>&nbsp;<td>banjo<td>&nbsp;<td>leave<td>&nbsp;<td>debit<td>&nbsp;
<tr><td>river<td>&nbsp;<td>fidel<td>&nbsp;<td>table<td>&nbsp;<td>guise<td>&nbsp;
<tr><td>MEANS<td>&nbsp;<td><td>&nbsp;<td><td>&nbsp;<td><td>&nbsp;
</table>
</center><P>

<I>Exercise 29: Produce graphs similar to those in figure 11 for the
mean values of the dot products. That is, plot the mean dot product
values for targets and distractors for both low and high frequency
words in the generalized familiarity condition on one graph, and the
mean dot product values for targets and distractors for both low and
high frequency words in the episodic recognition condition on another
graph. Are the generalized familiarity graphs flatter than the episodic
recognition graphs? Why?<P></I>

<I>Exercise 30: In the generalized familiarity graph the model's
results tend not to be as flat as the subject's data. Why might this
be the case, and does it represent a refutation of the model? (Hint:
consider the nature of pre-experimental experience).</I><P>


<A NAME="ObjectiveChecklist"><H2>Objective Checklist</H2></A>

In this chapter, we have been looking at the Matrix Model of long term
memory. The following is a check list of skills and knowledge which you
should obtain while working on this chapter. Go through the list
and tick off those things you are confident you can do. For any item
outstanding, you should refer back to the appropriate section or consult
your tutor.<P>

<ul>
<li>understand the distributed representation of items and associations
<br>
<li>calculate the vector memory values when two patterns are superimposed, in terms of:
<ul>
<li>network weights,
<br>
<li>Cartesian co-ordinates,
<br>
<li>vector addition.
</ul>
<li>explain the difference between matching and retrieval tasks and model
this difference in the Matrix Model
<Li>explain the difference between episodic and semantic tasks and model
this difference in the Matrix Model
</ul>

<A NAME="References"><h2>References</H2></A>

Bain, J.D., & Humphreys, M.S. (1989). Instructional reinstatement of
context: The forgotten prerequisite. In K. McConkey and A. Bennett
(Eds.), Proceedings of the XXIV International Congress of Psychology,
Vol. 3. Elsevier, North-Holland.<P>

Halford, G. S., Wiles, J., Humphreys, M. S., & Wilson, W. H. (1992).
Parallel distributed processing approaches to creative reasoning:
Tensor models of memory and analogy. unpublished manuscript.  <P>

Humphreys, M.S., Bain, J.D., & Burt, J.S. (1989). Episodically unique
and generalized memories: Applications to human and animal amnesics.
In  S. Lewandowsky, J.C. Dunn & K. Kirsner (Eds.) Implicit Memory:
Theoretical Issues. (pp. 139-158). Erlbaum Associates: Hillsdale, N.J.<P>

Humphreys, M.S., Bain, J.D., & Pike, R. (1989). Different ways to cue a
coherent memory system: A theory for episodic, semantic and procedural
tasks. Psychological Review, 96, 208-233.<P>

Pike, R. (1984). A comparison of convolution and matrix distributed
memory systems.  Psychological Review, 91, 281-294.<P>

Wiles, J., & Humphreys, M.S. (1993). Using artificial neural networks
to model implicit and explicit memory. In P.Graf & M. Masson (Eds.)
Implicit Memory: New Directions in Cognition, Development, and
Neuropsychology. (pp. 141-166). Erlbaum: Hillsdale, New Jersey.<P>


</BODY>
</HTML>

