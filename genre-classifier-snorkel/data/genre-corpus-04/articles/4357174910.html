<!-- <DOCUMENT>
	<FILE>
		4357174910.html
	</FILE>
	<URL>
		http://www.ai.univie.ac.at/~harald/handbook.html
	</URL>
	<TITLE>
		Linguistic Fundamentals and Computational Morphology
	</TITLE>
	<GENRE>
		articles
	</GENRE>
	<PLAINTEXT>
 Computational Morphology Computational Morphology Author: Harald Trost Abstract Computational morphology deals with the processing of words and word forms, in both their graphemic, i.e., written form, and their phonemic, i.e., spoken form. It has a wide range of practical applications. Probably every one of you has already come across some of them. Ever used spelling correction? Or wondered about some strange hyphenation in a newspaper article? This is computational morphology at work. To solve such seemingly simple tasks often poses hard problems for a computer program. This section shall provide you with some insights into why this is so and what techniques are available to tackle these tasks. 1 Introduction Natural languages have intricate systems to create words and word forms from smaller units in a systematic way. The part of linguistics dealing with these phenomena is morphology. This chapter starts with a quick overview over this fascinating field. It continues with applications of computational morphology. The rest is devoted to processing techniques. Computational morphology has evolved from very modest beginnings using full form lexica or some ad-hoc concatenation techniques to the much more powerful tools available today. The chapter concludes with a number of examples for encoding morphological phenomena from different languages using these tools. 2 Linguistic fundamentals What is morphology all about? A simple answer is that morphology deals with words. In formal language words are just arbitrary strings denoting constants or variables. Nobody would care about a morphology of formal languages. In natural languages the picture is very different. Every human language contains some hundred thousands of words. And continuously new words are integrated while others are drifting out of use. This infinity of words is produced from a finite collection of smaller units. The task of morphology is to find and describe the mechanisms behind this process. The basic building blocks in morphology are MORPHEMES . They are defined as the smallest unit in language to which a meaning may be assigned or, alternatively, as the minimal unit of grammatical analysis. Morphemes are abstract entities that express basic features. Either semantic concepts denoting entities or relationships in our world like door, blue or take . Such morphemes are called roots. Or syntactic features like past or plural . Their realisation as part of a word is called MORPH . Often, there is a one-to-one relation, e.g., the morpheme door is realized as the morph door . With take , on the other hand, we find the two possibilities take and took . In such a case we speak of allomorphs. Plural in English is usually expressed by the morph &#151;s . There are exceptions though: in oxen plural is expressed through the morph &#151;en , in men by stem vowel alteration. All these different forms are allomorphs of the plural morpheme. A basic distinction is the one between bound and free morphs. A free morph may form a word on its own, e.g., the morph door . We call such words monomorphemic because they consist of a single morph. Bound morphs, on the other hand, occur only in combination with other forms. All affixes are bound morphs. For example, the word doors consists of the free morph door and the bound morph -s . Words may also consist of free morphs only, e.g., tearoom , or bound morphs only, e.g., aggression. Every language typically contains some ten thousand morphs. This is a magnitude below the number of words. Strict rules govern the combination of these morphs to words (cf. 2.4). This way of structuring the lexicon makes the cognitive load of remembering so many words much easier. 2.1 What is a word? Surprisingly, there is no easy answer to this question. One can easily spot &#132;words&quot; in a text because they are separated from each other by blanks or punctuation. However, if you record ordinary speech you will find out that there are no breaks between words. But, we could isolate units which occur over and over again in speech, but in different combinations. So the notion of &#132;word&quot; makes sense. But how do we define it? We may look at &#132;words&quot; from different perspectives. To syntax &#132;words&quot; are the units that make up sentences. Words are grouped according to their function in the sentential structure. Each groups gets a tag&#150;usually called part-of-speech or word category&#150;and grammar deals with these tags only, omitting the details of specific words. Morphology, on the other hand, is concerned with the inner structure of &#132;words&quot;. It tries to uncover the rules that govern the formation of words from smaller units. We notice that words that convey the same meaning look differently depending on their syntactic context. Take, e.g., the words degrade, degrades, degrading, and degraded . We can think of those as different forms of the same &#132;word&quot;. We call the part that carries the meaning of those forms a base form. In our example this is the form degrade . All other forms in this example are produced by adding a suffix. A wide range of other possibilities will be shown in section 2.3 . All the different forms of a word together are called its paradigm . The part of morphology governing the production of these forms is called inflection . Base forms in English are at the same time always word forms in their own right, e.g., the base form degrade is also present tense, active voice, non 3 rd person singular. In other languages we find a slightly different situation. In Italian nouns are marked for gender and number. Different affixes are used to signal masculine and feminine on the one hand and singular an plural on the other hand. S INGULAR P LURAL M ASCULINE pomodor o pomodor i &#145;tomato&#146; F EMININE cipoll a cipoll e &#145;onion&#146; Neither of the two forms of a noun can function as the base form. Instead, we must assume that the base form is what is left over after removing the respective suffixes, i.e., pomodor- and cipoll-. Such base forms that cannot occur as word forms in their pure form are called stems. Base forms themselves are not necessarily atomic. By comparing degrade to downgrade, retrograde and upgrade on the one hand and decompose, decrease and deport on the other hand we can see that it is composed of the morphs de- and grade . The morpheme carrying the central meaning is often called the root of the word. A root may combine with suffixes (cf. 2.2.2.2) or other roots (cf. 2.2.2.3) to form new base forms. Finally, we can describe &#132;word&quot; from a phonological perspective. Important for morphology is that phonological units define the range for phonological processes. Often, the phonological word is identical to the morphological word but sometimes boundaries differ. For example, the morphophonological process of final devoicing in German (cf. 2.3.2.2) works on syllable structure. Let&#146;s look at two words derived from the root lieb . The word be+lieb+ig (arbitrary) is realized as / b&#180;li혱bik / because it is a single phonological word. On the other hand, lieb+lich (lovely) is realized as / li혱pliC /. Here, the last consonant of the root is devoiced because the two morphs are separated by a phonological word boundary. 2.2 Functions of morphology How much and what sort of information is expressed by morphology differs widely between languages. Information that in some languages is expressed by syntax is expressed morphologically in others. Take, e.g., the expression of future tense: English uses an auxiliary verb construction, Spanish a suffix. I speak - hablo I will speak - hablar&eacute; Also, some type of information may be present in one language while and missing in another. In many languages plural marking for nouns is mandatory. In Japanese it is absent. book - hon books - hon The means for encoding information also vary widely. Most common is the use of different types of affixes. Traditionally, linguists discriminate between the following types of languages with regard to morphology: Isolating languages (e.g. Mandarin Chinese): there are no bound forms, e.g., no affixes that can be attached to a word. The only morphological operation is composition. Agglutinative languages (e.g. Ugro-Finnic and Turkic languages): all bound forms are either prefixes or suffixes, i.e., they are added to a stem like beads on a string. Every affix represents a distinct morphological feature. Every feature is expressed by exactly one affix. Inflectional languages (e.g. Indo-European languages): distinct features are merged into a single bound form (a so-called portmanteau morph). The same underlying feature may be expressed differently, depending on the paradigm Polysynthetic languages (e.g. Inuit languages): these languages express more of syntax in morphology than other languages, e.g., verb arguments are incorporated into the verb. This classification is quite artificial. Real languages rarely fall cleanly into one of the above classes, e.g., even Mandarin has a few suffixes. Moreover, this classification mixes the aspect of what is expressed morphologically and the means for expressing it. 2.2.1 Inflection Inflection is required in particular syntactic contexts. It does not change the part-of-speech category but the grammatical function. The different forms of a word produced by inflection form its paradigm. Inflection is complete , i.e., with rare exceptions all the forms of its paradigm exist for a specific word. Regarding inflection, words can be categorized in three classes: Particles or not-inflecting words: they occur in just one form. In English, prepositions, adverbs, conjunctions and articles are particles; Verbs or words following conjugation; Nominals or words following declination, i.e., nouns, adjectives, and pronouns. Conjugation is mainly concerned with defining tense and aspect and agreement features like person and number. Take for example the German verb &#145;lesen&#146; (to read). German verb forms come in present and past tense, indicative or subjunctive. P RESENT P AST I NDICATIVE I NDICATIVE S UBJUNCTIVE S UBJUNCTIVE S INGULAR P LURAL S INGULAR P LURAL S INGULAR P LURAL S INGULAR P LURAL 1 st P ERSON lese lesen lese lesen las lasen l&auml;se l&auml;sen 2 nd P ERSON liest lest lesest leset last last l&auml;sest l&auml;set 3 rd P ERSON liest lesen lese lesen las lasen l&auml;se l&auml;sen P ARTICIPLE lesend gelesen I MPERATIVE lies lest I NFINITIVE lesen Declination marks various agreement features like number (singular, plural, dual, etc.), case (as governed by verbs and prepositions, or to mark various kinds of semantic relations), gender (male, female, neuter), and comparison. 2.2.2 Derivation and Compounding In contrast to inflection which produces different forms of the same word derivation and compounding are processes that create new words . Thus, derivation and compounding have nothing to do with morphosyntax. They are a means to extend our lexicon in an economic and principled way. In derivation, a different word--often of a different part-of-speech category--is produced by adding a bound morph to a stem. Derivation is incomplete, i.e., a derivational morph cannot be applied to all words of the appropriate class. For example, in German the very productive derivational suffix -bar can be applied to many but not all verbs to produce adjectives: essen &#145;to drink&#146; - ess bar &#145;eatable&#146; h&ouml;ren &#145;to hear&#146 - h&ouml;r bar &#145;audible&#146; absehen &#145;to conceive&#146; - abseh bar &#145;conceivable&#146; sehen &#145;to see&#146; - *seh bar &#145;visible&#146; Application of a derivational morpheme may be restricted to a certain subclass. For example, application of the English derivational suffix -ity is restricted to stems of Latin origin, while the suffix -ness can apply to a wider range: rare - rar ity - ?rare ness red - *redd ity - red ness grave - grav ity - grave ness weird - *weird ity - weird ness Derivation can be applied recursively, i.e., words that are already the product of derivation can undergo the process again. That way a potentially infinite number of words can be produced. Take, for example, the following chain of derivations: hospital &#151; hospital ize &#151; hospitaliz ation &#151; pseudo hospitalization Semantic interpretation of the derived word is often difficult. While a derivational suffix can usually be given a unique semantic meaning many of the derived words may still resist compositional interpretation. This may be due to lexicalization, i.e. a form is no more transparent because, or ambiguity of the underlying base form. For a more detailed discussion see Trost (1993). While inflectional and derivational morphology are mediated by the attachment of a bound morph to a base form, compounding is the joining of two or more base forms to form a new word. Most common is just setting two words one after the other, as in state monopoly, bedtime or red wine . In some cases parts are joined by a linking morphem (usually the remnant of case marking) as in bul l&#146; s eye or German Lieb es lied (love-song). The last part of a compound usually defines its morphosyntactic properties. Semantic interpretation is even more difficult than with derivation. Almost any semantic relationship may hold between the components of a compound: Wienerschnitzel &#9;&#145;cutlet made in Viennese style &#146; Schweineschnitzel &#9;&#145;cutlet made of pork&#146; Kinderschnitzel &#9;&#145;cutlet made for children&#146; The boundary between derivation and compounding is fuzzy. Historically, most derivational suffixes developed from words frequently used in compounding. An obvious example is the &#151;ful suffix as in hopeful, wishful, thankful . Phrases and compounds cannot always be distinguished. The English expression red wine in its written form could be both.&#9;In spoken language the stress pattern differs: red w&iacute;ne vs. r&eacute;d wine . In German phrases are morphologically marked, while compounds are not: roter Wein vs. Rotwein . But for verb compounds the situation is similar to English: zu Hause bleiben vs. zuhausebleiben. 2.3 What constitutes a morph? Every word form must at the core contain some root form. This root can (must) then be complemented with additional morphs. How are morphs realized? Obviously, a morph must somehow be recognizable in the phonetic or orthographic pattern constituing the word. The most common type of morph is a continuous sequence of phonemes. All roots and affixes are of this form. A complex word can then be analyzed as a series of morphs concatenated together. Agglutinative languages function almost exclusively this way. But there are surprisingly many other possibilities. 2.3.1 Affixation An affix is a bound morph that is realised as a sequence of phonemes (or graphemes). The by far most common types of affixes are prefixes and suffixes. Many languages have only these two types of affixes. Among them is English (at least under standard morphological analyses). A prefix is an affix that is attached in front of a stem. An example is the English negative marker un- attached to adjectives: common&#9; un common A suffix is an affix that is attached after a stem. Take, e.g., the English plural marker &#151;s : shoe&#9;&#9;shoe s Across languages suffixation is far more frequent than prefixation. Also, certain kinds of morphological information are never expressed via prefixes, e.g., nominal case marking. Many computational systems for morphological analysis and generation assume a model of morphology based on prefixation and suffixation only. A circumfix is the combination of a prefix and a suffix which together express some feature. Both theoretically and from a computational point of view a circumfix can be viewed as really two affixes applied one after the other. In German, the circumfixes ge--t and ge--n form the past participle of verbs: sag en &#9;&#9;&#145; to say&#146;&#9;&#9; ge sag t &#9;&#9;&#145; said&#146; lauf en &#9;&#145; to run&#146;&#9;&#9; ge lauf en &#9;&#9;&#145; run&#146; An infix is an affix where the placement is defined in terms of some phonological condition(s). These might result in the infix appearing within the root to which it is affixed. In Bontoc , a Philippine language, the infix -um- turns adjectives and nouns into verbs (Fromkin and Rodman 1983). The infix attaches after the initial consonant: Reduplication is a border case of affixation. The form of the affix is a function of the stem to which it is attached, i.e., it copies (some portion of) the stem. Reduplication may be complete or partial. In the latter case it may be prefixal, infixal or suffixal. Reduplication can include phonological alteration on the copy or the original. In Javanese complete reduplication is used to express the habitual-repetitive. In case the second vowel is non-/a/, the first vowel in the copy is made nonlow (changing /a/ to /o/ and / E / to /e/) and the second becomes /a/. When the second vowel is /a/, the copy remains unchanged while in the original the /a/ is changed to / E / (Kiparsky 1987): Partial reduplication is more common. In Yidin y , an Australian language, prefixal reduplication is used for plural marking. Reduplication involves copying the &#145;minimal word&#146; (Nash 1980). An example for infixal reduplication is the frequentative in Amharic , a semitic language spoken in Ethiopia (Rose 2000). From a computational point of view one property of reduplication is especially important: Since reduplication involves copying it cannot&#150;at least in the general case&#150;completely be described with the use of finite-state methods. 2.3.2 Root-and-template morphology Semitic languages (at least according to standard analyses) exhibit a very peculiar type of morphology: A so-called root, consisting of two to four consonants, conveys the basic semantic meaning. A vowel pattern marks information about voice and aspect. A derivational template gives the class of the word (traditionally called binyan ). In Arabic verb stems are constructed this way. The root ktb (write) produces--among others--the following stems: Template Vovel pattern a (active) ui (passive) CVCVC katab kutib &#145;write&#146; CVCCVC kattab kuttib &#145;cause to write&#146; CVVCVC ka:tab ku:tib &#145;correspond&#146; tVCVVCVC taka:tab tuku:tib &#145;write each other&#146; nCVVCVC nka:tab nku:tib &#145;subscribe&#146; CtVCVC ktatab ktutib &#145;write&#146; stVCCVC staktab stuktib &#145;dictate&#146; 2.3.3 Modification in phonetic substance This term subsumes processes which do neither introduce new nor remove existing segments. Morphs are not realized as any string of phonemes, but as a change of phonetic properties or an alteration of the prosodic shape. Ablaut refers to vowel alternations inherited from Indo-European. It is a pure example of vowel modification as a morphological process. Examples are strong verbs in Germanic languages like English (e.g., sw i m &#151; sw a m &#151; sw u m). In Icelandic this process is still more common and more regular than in most other Germanic languages. The following example is from Sproat (1992, p.62): Umlaut has its origin in a phonological process, whereby root vowels were assimilated to a high-front suffix vowel. When this suffix vowel was lost later on, the change in the root vowel became the sole remaining mark of the morphological feature originally signalled by the suffix. In German the plural of nouns may be marked by umlaut (sometimes in combination with a suffix), whereby in the stem vowel the feature back is changed to front : Another possibility to realize a morpheme is to alter the prosodic shape. Tone modification can be used to signal certain morphological features. In Ngbaka , spoken in the Democratic Republic of Congo, tense-aspect contrasts are expressed by four different tonal variants (Nida 1949): A morpheme may be realised by a stress shift. English noun-verb derivation sometimes uses a pattern where the stress is shifted from the first to the second syllable: N OUN V ERB &eacute;xport exp&oacute;rt r&eacute;cord rec&oacute;rd c&oacute;nvict conv&iacute;ct&#9; 2.3.4 Suppletion Total modification is a process occurring sporadically and idiosyncratically within inflectional paradigms. It is usually associated with forms that are used very frequently. Examples in English are went , the past tense of go , and the forms of to be: am, are, is, was and were . 2.3.5 Zero Morphology Sometimes a morphological operation has no phonological expression whatsoever. Examples are found in many languages. English noun-to-verb derivation is often not explicitly marked: man&#9;&#9;The man smiled.&#9; Man the boats. house &#9;He buys a house .&#9;They house in a cave. A possible analysis is to assume a zero morph which attaches to the noun to form a verb: book+&Oslash; V. Another possibility is to assume two independent lexical items disregarding any morphological relationship. 2.4 The structure of words: Morphotactics Somehow morphs must be put together to form words. A word grammar is determining the way this has to be done. This part of morphology is called morphotactics . As we have seen, the most usual way is simple concatenation. Let&#180;s have a look at the constraints involved. What are the conditions governing the ordering of morphemes in pseudohospitalization ? (1)&#9;*hospitalationizepseudo, *pseudoizehospitalation (2)&#9;*pseudohospitalationize In (1) an obvious restriction is violated: pseudo- is a prefix and must appear ahead of the stem, -ize and &#151;ation are suffixes and must appear after the stem. The violation in (2) is less obvious. In addition to the pure ordering requirements there are also rules governing to which types of stems an affix may attach: &#151;ize attaches to nouns and produces verbs, &#151;ation attaches to verbs and produces nouns. One possibility to describe the word formation process is to assume a functor-argument structure. Affixes are functors that pose restrictions on their (single) argument. That way a binary tree is constructed. Prefixes induce right branching and suffixes left branching. Fig. 1: The internal structure of the word pseudohospitalization In figure 1 the functor pseudo - takes a nominal argument to form a noun, &#151;ize a nominal argument to form a verb, and &#151;ation a verbal argument to form a noun. This description renders two different possible structures for pseudohospitalization . The one given in figure 1 and a second one where pseudo- combines first directly with hospital . We may or may not accept this ambiguity. To avoid the second reading we could state a lexical constraint that a word with the head pseudo- cannot serve as an argument anymore. 2.4.1 Constraints on affixes Affixes is that they attach to specific categories only. This is an example for a syntactic restriction. Restrictions may also be of a phonological, semantic or purely lexical nature. A semantic restriction on the English adjectival prefix un- prevents its attachment to an adjective that already has a negative meaning: unhappy&#9; *unsad unhealthy&#9; *unill unclean&#9; *undirty The fact that in English some suffixes may only attach to words of Latin origin (cf. 2.2.2) is an example for a lexical restriction. 2.4.2 Morphological vs. phonological structure In some cases there is a mismatch between the phonological and the morphological structure of a word. One example is comparative formation with the suffix &#151;er in English. Roughly, there is a phonological rule that prevents attaching this suffix to words that consist of more than two syllables: great&#9;&#9; greater tall&#9;&#9; taller happy &#9; happier competent&#9; *competenter elegant&#9; *eleganter If we want to stick to the above rule unrulier has to be explained with a structure where the prefix un- is attached to rulier . But, from a morphological point of view, the adjective ruly does not exist, only the negative form unruly . This implies that the suffix &#151;er is attached to unruly . We end up with an obvious mismatch! Another potential problem is cliticization. A clitic is a syntactically separate word phonologically realized as an affix. The phenomenon is quite common across languages. In English auxiliaries have contracted forms that function as affixes: he shall return -> he&#146;ll return In German prepositions can combine with the definite article an dem Tisch -> am Tisch in das Haus -> ins Haus In Italian personal pronouns can be attached to the verb. In this process the ordering of constituents is also altered. ce ne facciamo -> facciamocene 2.5 The Influence of Phonology Morphotactics is responsible to govern the rules for the combination of morphs into larger entities. One could assume that this is all a system needs to know to break down words into their component morphemes. But there is another aspect that makes things more complicated: Phonological rules may apply and change the shape of morphs. To deal with these changes and their underlying reasons is the area of morphophonology. 2.5.1 Phonology vs. orthography Most applications of computational morphology deal with text rather than speech. But, written language is rarely a true phonemic description. For some languages, e.g., Finnish, Spanish or Turkish orthography is a good approximation for a phonetic transcription. English, on the other hand, has very poor correspondence between writing and pronounciation. As a result, we often have to deal with orthography rather than phonology. A good example are English plural rules (cf. 2.4.1). 2.5.2 Local phenomena We have shown that, by and large, words are composed by concatenating morphs. In many cases this concatenation process will induce some phonological change in the vicinity of the morph boundary. Assimilation is a process where the two segments at a morph boundary influence each other, resulting in some feature change that makes them more similar. Take, for example, the English in- prefix where the n changes to m before labials: &lt;in+feasible&gt; -&gt; infeasible &lt;in+mature&gt; -&gt; i m mature &lt;in+probable&gt; -&gt; i m probable &lt;in+secure&gt;&#9; -&gt; insecure Another possibility is epenthesis (insertion) or elision (deletion) of a segment under certain (phonological) conditions. Take for example the English plural formation: &lt;cat+s&gt; -&gt; cats &lt;door+s&gt; -&gt; doors &lt;dish+s&gt; -&gt; dish e s &lt;bliss+s&gt; -&gt; bliss e s &lt;match+s&gt; -&gt; match e s &lt;fox+s&gt; -&gt; fox e s In this case the rule requires the insertion of an / &#180; / between / s /, / z /, / S /, or / Z / and another / s /. On the other hand, in German the suffix &#151;st attached to stems ending in / s / looses its starting segment / s /: &lt;leb+st&gt; -&gt; lebst &lt;sag+st&gt; -&gt; sagst &lt;ras+st&gt; -&gt; rast &lt;trotz+st&gt; -&gt; trotzt &lt;hex+st&gt; -&gt; hext We see that the change is not purely phonologically motivated. The same condition, namely two adjoining / s / phonemes leads to different results: Either the epenthesis of an / &#180; / between the two, or the elision of the second / s /. Moreover, the notion of insertion or deletion is purely descriptive. Phonological theory may explain the underlying processes completely different. Nonetheless, this is the view most often taken by work in computational morphology. 2.5.3 Long-distance effects Most common is vowel harmony but there are olso examples of consonant harmony. Vowel harmony is a phonological process where the leftmost (in rare cases the rightmost) vowel in a word influences all the following (preceding) vowels. Among the languages exhibiting vowel harmony are Finnish, Hungarian, Turkic and many African languages. Let&#146;s have a look at vowel harmony in Turkish . The nine vowels of the Turkish language can be specified the following way: i I &uuml u e a &ouml; o H IGH + + + + B ACK + + + + R OUND + + + + Turkish has two different harmony rules, called small and large respectively: e i &ouml; &uuml; -> e e i -> i &ouml; &uuml -> &uuml; a I o u -> a a I -> I o u -> u Only the stem vowel in a word is lexically determined. All the following vowels are realized in accordance to the harmony rules. For example, the root ev (house) induces either an e or an i in the attached suffixes. Which one of the two is realized is a property of the respective suffix. In this example the plural suffix follows the &#132;small&quot; harmony and the genitive suffix the &#132;large&quot; harmony rule.&#9; 3 Applications of Computational Morphology Computational morphology has many practical applications. Besides low-level applications, computational morphology contributes to many speech and language processing systems. 3.1 Low-level applications Hyphenation is almost exclusively done automatically. Although the task seems at first glance extremely simple only a human expert can achieve a 100% success rate. Segmenting words correctly into their morphs helps to solve the task. The major problem are spurious segmentations. Spelling correction is another low-level application. Just comparing input against a list of word forms has a number of drawbacks. Such a list will never contain all the words occurring in a text and enlarging the list has the negative side effect of including more and more obscure words that will match with typos thus preventing their detection. Most systems use a root lexicon, plus a relatively small set of affixes and simple rules to cover morphotactics. Stemmers are used in information retrieval to reduce as many related words and word forms as possible to a common canonical form which can then be used in the retrieval process. One should note that this canonical form is not necessarily the base form. The main requirement is&#150;like in all the above tasks&#150;robustness. Another application is to segment text in Chinese, Japanes or Korean. In these languages words in a sentence are not separated by blanks or punctuation marks. Morphological analysis can be used to perform the task of word separation. A related problem is the inputting of Japanese text. Japanese is written with a combination of two independent character sets. Kanji, the morphemic Chinese characters are used for open-class morphemes (verbs, nouns and adjectives). Kana has (about 50) syllabic characters and is mainly used for closed-class morphemes although in principle all Japanese words can be written exclusively in kana. Since there are several thousand kanji characters, many Japanese text input systems use kana-kanji conversion. The text is typed in kana and the relevant portions are subsequently converted to kanji. The mapping from kana to kanji is quite ambigous. A combination of statistical and morphological methods is applied to solve that task. 3.2 Natural language applications An obvious application area for morphological components are more general natural language processing systems involving parsing and/or generating natural language utterances in written or spoken form. There is a wide range of such applications from message and information extraction to dialog systems and machine translation. For many current applications, only inflectional morphology is considered. In a parser, morphological analysis of words is an important prerequisite for syntactic analysis. Properties of a word the parser needs to know are its part-of-speech category and the morphosyntactic information encoded in the particular word form. Another important task is lemmatization, i.e., finding the corresponding dictionary form for a given input word, because for many applications a lemma lexicon is used to provide more detailed syntactic (e.g, valency) and semantic information for a deep analysis. In generation, on the other hand, the task is to produce the correct word form from the base form plus the relevant set of morphosyntactic features. 3.3 Speech applications A text-to-speech system takes (electronically stored) text as input and produces speech from it. Morphological analysis helps to solve two different tasks in such systems. One is to guide the grapheme-to-phoneme conversion. Characters are often ambiguous with respect to their translation into phonemes. Finding out the underlying morphological structure is necessary for solving the task correctly. The sequence th , is usually pronounced as / D / or / T / in English. In the word hothouse we need to know the morph structure &lt;hot+house&gt; to correctly pronounce the th sequence as / th /. A less obvious application is the use of morphological analysis to help in determining the part-of-speech category of words. This is an important prerequisite of syntactic analysis which is the basis for coming up with a correct prosody. Speech recognition is a field where morphological analysis will become ever more important. At the moment most available systems make use of full form lexicons and perform their analysis on a word basis. Increasing demands on the lexicon size on the one hand and the need to limit the necessary training time on the other hand will make morph-based recognition systems more attractive. 4 Computational Morphology The most basic task in computational morphology is to take a string of characters or phonemes as input and deliver an analysis as output. The input could, for example be the English word form in (1). One possible output could be the string of underlying morphemes as in (2), another one a morphosyntactic interpretation as in (3). incompatibilities in+con+patible+ity+s incompatibility+NounPlural Let&#146;s start with the task of mapping (1) to (3). The easiest way to achieve a result is to have a long list of pairs where the left side represents some word form and the right side its interpretation. This is basically the notion of full form lexicon. Its advantages are simplicity and applicability to all possible phenomena. The main disadvantages are redundancy and inability to cope with forms not contained in the lexicon. Less redundant are so-called lemma lexica . A lemma is a canonical form taken as the representative for all the different forms of a paradigm. Usually, the base form is selected as this canonical form. An interpretation algorithm relates every form to its lemma plus delivering a morphosyntactic interpretation. As a default, forms are expected to be string concatenations of base form (= lemma) and affixes. Affixes must be stored in a separate repository together with the relevant morphotactic information about how they may combine with other forms. Interpretation then simply means finding a sequence of affixes and a base form that conforms to morphotactics. For different reasons a given word form may not conform to this simple picture: With very frequently used words we often find suppletion, e.g., to go has the completely unrelated form went . One clearly needs some exception handling mechanism to cope with suppletion. A possible solution is to have secondary entries where you store suppleted forms together with their morphosyntactic information. These secondary forms are then linked to the corresponding primary form, i.e., the lemma. Morphs are realised in a non-concatenative way, e.g., tense of strong verbs in English: give &#151; gave - given , find - found &#151; found In languages like English, where these phenomena affect only a fairly small and closed set of words these forms can be treated like suppletion. Alternatively, some exception handling mechanism (usually developed ad-hoc and language-specific) is applied. Due to phonological rules a word form may exhibit some change in shape, e.g., in English suffixes starting with s (plural of nouns, 3 rd person marker, superlative marker) may not directly follow stems ending in a sybillant (e.g., dish &#151; dishes) If morphophonological processes in a language are few and local the lemma lexicon approach can still be successful. In our example it suffices to assume two plural endings: -s and &#151;es . For all base forms it must be specified whether the former or the latter of the two endings may be attached. Apart from the obvious limitations with regard to the treatment of morphophonological rules on a more general scale the approach has some other inherent restrictions. The algorithm is geared towards analysis. For generation purposes, one needs a completely different algorithm and data. Interpretation algorithms are language-specific because they encode both the basic concatenation algorithm and the specific exception-handling mechanism. The approach was developed for morphosyntactic analysis. An extension to handle more generally the segmenting of word forms into morphs is difficult to achieve. 4.1 Finite-state Morphology Because most morphological phenomena can be described with regular expressions the use of finite-state techniques for morphological components is common. In particular, when morphotactics is seen as a simple concatenation of morphs it can straightforwardly be described by a finite automata. It was not so obvious though how to describe non-concatenative phenomena like vowel harmony, root-and-template morphology or infixation in such a framework. 4.1.1 Two-level morphology In this section we describe a system where morphophonology is taken care of by a separate mechanism that is well integrated with the morphotactical component. It has the further advantages of being non-directional (applicable to analysis and generation) and language-independent (because of its purely declarative specification of language-specific data). Rules for the description of morphophonological phenomena are standard in generative phonology. There, the derivation of a word form from its lexical structure is performed by the successive application of phonological rules creating a multi-step process involving several intermediate levels of representation. Such an approach may be suited for generation but leads to problems if applied to analysis. Since the ordering of rule application influences the result it is difficult to reverse the process. Several proposals were made on how to restrict rules and their application to overcome these problems. Two-level morphology is a an attempt to overcome these problems. Originally proposed by Kimmo Koskenniemi (1984) it has since been implemented in a number of different systems and applied to a wide range of natural languages. 4.1.1.1 Two-level rules As the name suggests two levels--called lexical level and surface level--suffice to describe the phonology (or orthography) of a natural language. On the surface level words appear just as they are pronounced (or written) in ordinary language, with the important exception of the null character which will be described later on. On the lexical level, the alphabet includes special symbols--so-called diacritics--which are mainly used to represent features that are no phonemes (or graphemes) but nevertheless constitute necessary phonological information. The diacritics '+' and '#&#145; are used to indicate morph and word boundary respectively. The two levels are linked by a set of pairs of lexical and surface characters constituting possible mappings between lexical and surface characters. Pairs are written as lexical character - colon - surface character (e.g. a:a or +:0 ). To any of these pairs rules may be attached to restrict their applicability. Pairs with no attached rules are applied by default. Rules serve to licence the application of a pair in a certain phonological context. They are viewed as constraints on the mapping between the surface and the lexical form of morphs. Accordingly, they are applied in parallel and not one after the other like in generative phonology. Since no ordering of the rules is involved this is a completely declarative way of description. A rule consists of the following parts: A substitution that indicates the affected character pair. left and right context define the phonological conditions for the substitution. One of four available operators defines the status of the rule: The context restriction operator restricts the substitution of the lexical character to exactly this context (it may not occur anywhere else). The is a combination of the former two, i.e., the substitution must take place in exactly this context and nowhere else. The fourth operator / Let's look at a simple epenthesis rule: (1a)&#9;+:e &lt;= s x z [ { s c } h ] : _ s ; It specifies that a lexical morph boundary (indicated by '+' ) between s, x, z, sh, or ch on the left side and an s on the right side must correspond to surface level e . By convention a pair with identical lexical and surface character may be denoted by just a single character. Curly brackets indicate a set of alternatives, square brackets a sequence. Rule (1a) makes no statements about other contexts where '+' may map to an 'e' . The rule covers some of the cases where an 'e' is inserted between stem and an inflectional morph starting with 's' (plural morpheme, 3 rd person marker, superlative) in English. By default a morph boundary will map to the null character, but in the given specific context it maps to 'e' . The following example shall demonstrate the application of this rule (Vertical bars denote a default pairing, numbers the application of the corresponding rule): #bliss+s#&#9;&#9;#fox+s#&#9;#dish+s#&#9;#watch+s# ||||||1||&#9;&#9;||||1||&#9;|||||1||&#9;||||||1||&#9; 0blisses0&#9;&#9;0foxes0&#9;0dishes0&#9;0watches0 Obviously, (1a) does not capture all the cases where epenthesis of 'e' occurs. For example, the forms spies, shelves or potatoes are not covered. A more complete rule is: (1b)&#9;+:e &lt;=&gt; {s x z [ { s c} h:h ] :v [ C y: ] [ C o ] } _ s ; Formally, rule (1b) defines exactly all the contexts where '+' maps to an 'e' (because of the use of the &#164; operator). It also makes use of some additional writing conventions. A colon followed by a character denotes the set of all pairs with that surface character. Accordingly, a character followed by a colon means the set of all pairs with that lexical character. Sets of characters can be globally defined and given names. The C stands for the set of English consonants (i.e., b:b, c:c, d:d,...). To cope with the spies example we need another rule which licences the mapping from 'y' to 'i'. (2) y:i &lt;=&gt; C _ { +:e [ +: e ] } ; V C+ _ +: C ; Rule (2) specifies two distinct contexts. If either of them is satisfied the substitution must occur, i.e., contexts are OR-connected. The ' + ' operator in the second context indicates at least one occurrence of the preceding sign (accordingly, the operator '*' has the reading arbitrarily many occurrences ). V stands for the set of vowels. Rules (1) and (2) in combination now correctly map spies with spy+s . Jointly with rule (3) for the mapping from 'f' to 'v' (1) takes also care of forms like shelves and potatoes : (3) f:v &lt;= { e l } _ +: s ; V _ e +: s; Let&#146;s see how the three rules interact to produce the expected results: #spy+s#&#9;#toy+s#&#9;#shelf+s#&#9;#wife+s#&#9;#potato+s# |||21||&#9;|||||||&#9;|||||31||&#9;|||3||||&#9;|||||||1|| 0spies0&#9;0toy0s0&#9;0shelves0&#9;0wive0s0&#9;0potatoes0 A given pair of lexical and surface strings can only map if they are of equal length. There is no possibility of omitting or inserting a character in one of the levels. On the other hand, elision and epenthesis are common phonological phenomena. To cope with these, the null character (written as 0) is included in both the surface and the lexical alphabet. The null character is taken to be contained in the surface string for the purpose of mapping lexical to surface string and vice versa but it does not show up in the output or input of the system. Diacritics are mapped to the null character by default. Any other mapping of a diacritic has to be licensed by a rule. Assumption of the explicit null character is essential for processing. A mapping between a lexical and a surface string presupposes that for every position a character pair exists. This implies that both strings are of equal length (nulls are considered as characters in this respect). Rules can either be directly interpreted or compiled into finite state transducers. The use of finite state machinery allows for very efficient implementation. For a more in-depth discussion of implementational aspects consult chapter 37 and Beesley and Karttunen (2000). One subtle difference between direct rule interpretation and transducers occurs in the repeated application of the same rule to one string. The transducer implicitly extends the phonological context to the whole string. It must therefore explicitly take care of overlapping right and left contexts (e.g., in (1) the pair s:s constitutes both a left and right context). With direct interpretation a new instance of the rule is activated every time the left context is found in the string and overlapping must not be treated explicitly. 4.1.1.2 The continuation lexicon Up to now we have only described the rule part of two-level morphology which is responsible for taking care of morphonological phenomena. It is complemented by a partitioned lexicon of morphs (or words) that takes care of word formation by affixation. The lexicon consists of (non-disjunctive) sublexica, so-called continuation classes. For every morph, a set of legal continuation classes is specified. This set defines which sublexicon must be searched for continuations. The class of morphs which can start a word is stored in the so-called "init lexicon" . The whole process is equivalent to stepping through a finite automaton. A successful match can be taken as a move from some state x of the automaton to some other state y . Lexical entries can be thought of as arcs of the automaton: a sublexicon is a collection of arcs having a common from state. The lexicon in two-level morphology is used for two purposes: one is to describe which combinations of morphs are legal words of the language, the other one is to act as a filter whenever a surface word form shall be mapped to a lexical form. Its use for the second task is crucial because otherwise there would be no way to limit the insertion of the null character. To enable fast access, lexicons are organized in the form of a letter trie (Fredkin, 1960). Such a structure is well suited for an incremental (letter-by-letter) search because at every point in the trie exactly those continuations leading to legal morphs are available. With every node which represents a legal morph its continuation classes are stored. In recognition we can now make use of that structure. Search starts at the root of the trie. Each character which is proposed must be matched against the lexicon. Only if that character is a legal continuation at that node in the trie it may be considered as a possible mapping. In recent implementations the lexicon and the two-level rules are collapsed into a single, large transducer, resulting in a very compact and efficient system 4.1.2 Related Formalisms Black et al. (1987) note the inelegance of Koskenniemi's formalism when describing a phonological (or orthographic) change affecting sequences of characters. They propose a rule format consisting of a surface string (called LHS for left hand side ), an operator ( &#139; or 횧) and a lexical string (called RHS for right hand side ). LHS and RHS must be of equal length. Surface-to-lexical rules ( 횧 ) request that there exists a partition of the surface string where each part is the LHS of a rule and the lexical string the concatenation of the corresponding RHSs. Lexical-to-surface rules ( &#139; ) request that any substring of a lexical string which equals a RHS of a rule must correspond to the surface string of the LHS of the same rule. The rules in (4) are equivalent to rule (1a). (4) ses =&gt; s+s ses &lt;= s+s shes =&gt; sh+s shes &lt;= sh+s xes =&gt; x+s xes &lt;= x+s zes =&gt; z+s zes &lt;= z+s ches =&gt; ch+s ches &lt;= ch+s These rules collapse context and substitution into one undistinguishable unit. Instead of regular expressions only strings are allowed. One drawback is that surface-to-lexical rules may not overlap. If two different changes happen to occur close to each other they must be captured in a single rule. Also, long-distance phenomena like vowel harmony cannot be described in this scheme. Ruessink (1989) removes this problem by introducing contexts again. Both LHS and RHS may come with a left and right context. LHS and RHS may also be of different length, doing away with the null character. Though he gives no account of the complexity of his algorithm one can suspect that it is in general less constrained than the Koskenniemi system. An inherently difficult problem for two-level morphology is the root-and-template morphology of Semitic languages. One solution is the introduction of multi-tape formalisms as first described in the seminal paper by Kay (1987). The best-documented current system is SEMHE described in Kiraz (1996, 1997). SEMHE is based on Ruessink&#146;s formalism with the extension of using three input tapes: one each for the root, the vowel pattern and the template. Another extension to the formalism is realized in X2MorF (Trost 1992). In the standard system, morphologically motivated phenomena like umlaut must be described by introducing some pseudosegmental material in the lexical level (see, e.g., 2.4.3.3). In X2MorF an additional morphological context is available to describe such phenomena more naturally. 4.2 Alternative formalisms Alternative proposals for morphological systems have been made in computational linguistics. They include so-called paradigmatic morphology described in Calder (1989) and the DATR system (Evans and Gazdar 1996). Common to both is the idea to introduce some default mechanism which makes it possible to define a hierarchically structured lexicon where general information is stored at a very high level. Lower in the hierarchy this information can be overwritten. Both systems seem to be more concerned with morphosyntax than with morphonology. It is an open question if these approaches could somehow be combined with two-level rules. 4.3 Examples 4.3.1 Vowel harmony in Finnish Finnish has eight vowels. They are classified into back+ (a, o, u), back- (&auml;, &ouml;, y) and neutral (e, i). In a Finnish word vowels must be either all back+ or all back - (disregarding neutral vowels). V = {a, o, u, &auml;, &ouml;, y, e, i} Vb = {a, o, u}&#9;Vf = {&auml;, &ouml;, y} [1] {A:a|O:o|U:u} 횧 =:Vb =:(-Vf)* _; [2]&#9;{A:&auml;|O:&ouml;|U:y} 횧 {#|=:Vf} =:(-Vb)* _; #taivas+tA#&#9;&#9;#puhelin+tA#&#9;&#9;#syy+tA# |||||||||1|&#9;&#9;||||||||||1|&#9;&#9;||||||2| 0taivas0ta0&#9;&#9;0puhelin0ta0&#9;&#9;0syy0t&auml;0 4.3.2 Final devoicing in (spoken) German Final devoicing is a morphophonological process where a voiced consonant is devoiced when it occurs in final position in the syllable. Take for example the root / ra혱:d / (wheel). The singular form is realized as /ra:t/, while in the plural form / re혱:d&aring; / the consonant stays voiced. This phenomenon is not reflected in the orthography where always the voiced consonant is kept. [1] Cx:Cy &#164; _ #:0 ; &#9;&#9; where Cx in (b d g) &#9;&#9;&#9; Cy in (p t k) matched; #l o혱 b#&#9;&#9;#r a혱 d#&#9;&#9;#we:g#&#9;&#9;#we:g+e# ||| 1|&#9;&#9;||| 1|&#9;&#9;||| 1|&#9;&#9;||| |||| 0l o혱 p0&#9;&#9;0r a혱 t0&#9;&#9;0we:k0&#9;&#9;0we:g0e0&#9; The two-level rule realises b, d and g as their voiceless counterparts p, t, and k respectively whenever directly followed by a boundary. While the original linguistic motivation behind two-level morphology was SPE and two-level rules were designed to describe morphophonology the mechanism can deal with a much wider range of phenomena. 4.3.3 Umlaut in German German umlaut is used to mark--among other morphosyntactic features&#150;plural. V = {a, &auml;, e, i, o, &ouml;, u, &uuml;, A:a, A:&auml;, O:o, O:&ouml;, U:u, U:&uuml;} [1] {A:&auml;|O:&ouml;|U:&uuml;} 횧 _ ?* $ :0; All stem vowels eligible for umlaut are realized by a vowel underspecified for the back/front distinction at the lexical level. A pseudo-ending $ is used to trigger the rule application, thus realizing the umlaut. In all other cases the default pairing is used. This way a morphological property is described as a morphophonological process. The ?* signifies zero or more occurrences of anything. Mutter &AElig; M &uuml; tter&#9;&#9;Garten &AElig; G &auml; rten&#9;&#9;Hof &AElig; H &ouml; f e #mUtter+ $ #&#9;&#9;#gArten+ $ #&#9;&#9;&#9;#hOf+ $ e# ||1|||||||&#9;&#9;||1|||||||&#9;&#9;&#9;||1||||| 0m&uuml;tter000&#9;&#9;0g&auml;rten000&#9;&#9;&#9;0h&ouml;f00e0 4.3.4 Reduplication and Infixation in Tagalog In this (simplified) example from Tagalog we shall see how two-level rules can be used to describe reduplication and infixation. V = {a, i, u, E} C = {p t k b d g m n N s l r w y R} The rule for infix insertion. On the lexical level, the prefix X is assumed. While the X is not realized on the surface it triggers the insertion of &#151; In- between initial consonant and following vowel. [1] X:0 횧 _ +:0 C 0:i 0:n V:V; pili &AElig; p in ili &#9;&#9;tahi &AElig; t in ahi&#9;&#9; #X+p00ili#&#9;&#9;#X+t00ahi#&#9; |1||||||||&#9;&#9;|1||||||||&#9; 000pinili0&#9;&#9;000tinahi0&#9; The rules for reduplication of the first (open) syllable. The R copies the initial consonant, the E the following consonant. The rule also takes care of the case where the infix is inserted as well: [2] R:Cx 횧 _ (0:i 0:n) E:V +:0 :Cx; &#9;&#9;where Cx in (p p:m t t:n k k: N ); [3] E:Vx 횧 R:C (0:i 0:n) _ +:0 C Vx; &#9;&#9;where Vx in (a i u); pili &AElig; pi pili &#9;&#9;tahi &AElig; ta tahi&#9;&#9; #RE+pili#&#9;&#9;#RE+tahi#&#9;&#9; |23||||||&#9;&#9;|23||||||&#9;&#9; 0pi0pili0&#9;&#9;0ta0tahi0&#9; &#9; pili &AElig; pini pili&#9;&#9;tahi &AElig; tina tahi #X+R00E+pili#&#9;&#9;#X+R00E+tahi# |1|2||3||||||&#9;&#9;|1|2||3|||||| 000pini0pili0&#9;&#9;000tina0tahi0 5 Further reading and relevant resources The most comprehensive book about computational morphology is Richard Sproat&#146;s book Morphology and Computation (Sproat 1992). It gives a concise introduction into morphology with examples from various languages and a good overview of applications of computational linguistics. On the methodological side it concentrates on finite-state morphology omitting other paradigms. Computational Morphology (Black et al. 1992) gives a more in-depth description of finite-state morphology but concentrates exclusively on English. An excellent overview of morphology with examples from diverse languages is found in the Handbook of Morphology (Spencer and Zwicky 1998). To get some hands-on experience with morpological processing connect to RXRC Europe and Lingsoft . A free downloadable version of a two-level morphology is available from SIL . References Beesley K.R. and Karttunen L. 2000. Finite-State Morphology: Xerox Tools and Techniques . Cambridge University Press, Cambridge. Black A.W., Ritchie G.D., Pulman S.G., Russell G.J. 1987. Formalisms for Morphographemic Description, Proc. 3rd European ACL, pp11-18, Kopenhagen. Calder J. 1989. Paradigmatic Morphology, Proc. 4th European ACL, pp58-65, Manchester. Chomsky N. and Halle M.: The Sound Pattern of English , Harper &amp; Row, Hagerstown/London/New York, 1968. Evans R., Gazdar G. 1996 DATR,: A Language for Lexical Knowledge Representation, Computational Linguistics 22(2)167-216. Fredkin E. 1960. Trie Memory, Communications ACM 3, pp490-499. Fromkin V., Rodman R. 1983. An Introduction to Language. Holt, Rinehart &amp; Winston, New York. Kay M. 1987. Noncatenative finite-state morphology, in Proc. of the 3rd Conference of the European Chapter of the ACL, Copenhagen, Denmark, pp2-10. Kiparsky P. 1987. The Phonology of Reduplication. Manuscript. Stanford University. Kiraz G.A. 1996. SEMHE: A Generalized Two-Level System, in Proc. of 34th Annual Meeting of the Association for Computational Linguistics, Morgan Kaufmann, Los Altos, pp159-166. Kiraz G.A. 1997. Compiling Regular Formalisms with Rule Features into Finite-State Automata, in Cohen P.R., Wahlster W.(eds.), Proc. 35th Annual Meeting of the Association for Computational Linguistics, Morgan Kaufmann, Los Altos, pp329-336. Koskenniemi K. 1984. A General Computational Model for Word-Form Recognition and Production, Proceedings 10th International Conference on Computational Linguistics, Stanford, CA. Nash D. 1980. Topics in Warlpiri Grammar, PhD Thesis, MIT, Cambridge, MA. Nida E. 1949. Morphology: The Descriptive Analysis of Words. University of Michigan Press. Ritchie G.D., Russel G.J., Black A.W., Pulman S.G. 1991. Computational Morphology , MIT Press, Cambridge. Rose S. 2000. Triple Take: Tigre and the case of internal reduplication. Studies in Afroasiatic Grammar . Ruessink H. 1989. Two-Level Formalisms, Working Papers in Natural Language Processing 5, Rijksuniversiteit Utrecht. Spencer A., Zwicky A. (eds.) 1998. The Handbook of Morphology, Basil Blackwell, Oxford. Sproat R.W ., 1992. Morphology and Computation , MIT Press, Cambridge, MA. Trost H. 1992. X2MORPH: A Morphological Component Based on Augmented Two-Level Morphology, in Proc. 12 th Intenational Joint Conference on Artificial Intelligence, Sydney, Morgan Kaufmann, San Mateo, pp.1024-1030. Trost H. 1993. Coping With Derivation in a Morphological Component, in Proc. 6 th Conference of the European Chapter of the Association for Computational Linguistics, Utrecht, pp368-376. 
	</PLAINTEXT>
	<CONTENT>
-->
<HTML>
<HEAD>
<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso-8859-1">
<TITLE>Computational Morphology</TITLE>
</HEAD>
<BODY bgcolor="#ffffff">

<table width="100%">
  <tr>
     <td valign="top"><A NAME="Top"><IMG ALIGN=middle SRC="/icons/large/oefailogo1-t.gif" ></A></td>
     <td align="middle"  valign="middle">
            <H1><FONT COLOR="#0100ff">Computational Morphology</Font></H1>
  <a href="http:/welcome.html"><IMG BORDER=0 SRC="http:/icons/medium/butHome.gif" ALT="Back"></a>
  <a href="http:/oefai/"><IMG BORDER=0 SRC="http:/icons/medium/butOFAI.gif" ALT="횜FAI"></a>
  <a href="http:/imkai/"><IMG BORDER=0 SRC="http:/icons/medium/butIMKAI.gif" ALT="IMKAI"></a> <a href="http:/oefai/nlu/nlu.html"><IMG BORDER=0 SRC="http:/oefai/nlu/nlu.gif" ALT="IMKAI"></a>

  <dt><IMG BORDER=0 SRC="http:/icons/lines/oline.gif" ALT="">
</TD></TR></TABLE>

<H3 ALIGN="CENTER">Author: Harald Trost</H3>

<DIR><H3>Abstract</H3>

<P ALIGN="JUSTIFY">Computational morphology deals with the processing of words and word forms, in both their graphemic, i.e., written form, and their phonemic, i.e., spoken form. It has a wide range of practical applications. Probably every one of you has already come across some of them. Ever used spelling correction? Or wondered about some strange hyphenation in a newspaper article? This is computational morphology at work. To solve such seemingly simple tasks often poses hard problems for a computer program. This section shall provide you with some insights into why this is so and what techniques are available to tackle these tasks.
</DIR>

<A NAME="Introduction"><H3>1 Introduction</H3></A>
<P ALIGN="JUSTIFY">Natural languages have intricate systems to create words and word forms from smaller units in a systematic way. The part of linguistics dealing with these phenomena is morphology. This chapter starts with a quick overview over this fascinating field. It continues with applications of computational morphology. The rest is devoted to processing techniques. Computational morphology has evolved from very modest beginnings using full form lexica or some ad-hoc concatenation techniques to the much more powerful tools available today. The chapter concludes with a number of examples for encoding morphological phenomena from different languages using these tools.

<A NAME="Fundamentals"><H3>2 Linguistic fundamentals</H3></A>

<P ALIGN="JUSTIFY">What is morphology all about? A simple answer is that morphology deals with words. In formal language words are just arbitrary strings denoting constants or variables. Nobody would care about a morphology of formal languages. In natural languages the picture is very different. Every human language contains some hundred thousands of words. And continuously new words are integrated while others are drifting out of use. This infinity of words is produced from a finite collection of smaller units. The task of morphology is to find and describe the mechanisms behind this process.
</P>
<P ALIGN="JUSTIFY">The basic building blocks in morphology are <SAMP>MORPHEMES</SAMP>. They are defined as the smallest unit in language to which a meaning may be assigned or, alternatively, as the minimal unit of grammatical analysis. Morphemes are abstract entities that express basic features. Either semantic concepts denoting entities or relationships in our world like <I>door, blue </I>or<I> take</I>. Such morphemes are called roots. Or syntactic features like <I>past</I> or <I>plural</I>. 
</P>
<P ALIGN="JUSTIFY">Their realisation as part of a word is called <SAMP>MORPH</SAMP>. Often, there is a one-to-one relation, e.g., the morpheme <I>door </I>is realized as the morph <I>door</I>. With <I>take</I>, on the other hand, we find the two possibilities <I>take </I>and <I>took</I>. In such a case we speak of allomorphs. Plural in English is usually expressed by the morph <I>&#151;s</I>. There are exceptions though: in <I>oxen</I> plural is expressed through the morph <I>&#151;en</I>, in <I>men</I> by stem vowel alteration. All these different forms are allomorphs of the plural morpheme.
</P>
<P ALIGN="JUSTIFY">A basic distinction is the one between bound and free morphs. A free morph may form a word on its own, e.g., the morph <I>door</I>. We call such words monomorphemic because they consist of a single morph. Bound morphs, on the other hand, occur only in combination with other forms. All affixes are bound morphs. For example, the word <I>doors</I> consists of the free morph <I>door</I> and the bound morph <I>-s</I>. Words may also consist of free morphs only, e.g., <I>tearoom</I>, or bound morphs only, e.g., <I>aggression.</P>
</I>
<P ALIGN="JUSTIFY">Every language typically contains some ten thousand morphs. This is a magnitude below the number of words. Strict rules govern the  combination of these morphs to words (cf. 2.4). This way of structuring the lexicon makes the cognitive load of remembering so many words much easier.

<A NAME="Word"><H4>2.1 What is a word?</H4></A>

<P ALIGN="JUSTIFY">Surprisingly, there is no easy answer to this question. One can easily spot &#132;words&quot; in a text because they are separated from each other by blanks or punctuation. However, if you record ordinary speech you will find out that there are no breaks between words. But, we could isolate units which occur over and over again in speech, but in different combinations. So the notion of &#132;word&quot; makes sense. But how do we define it?
</P>
<P ALIGN="JUSTIFY">We may look at &#132;words&quot; from different perspectives. To syntax &#132;words&quot; are the units that make up sentences. Words are grouped according to their function in the sentential structure. Each groups gets a tag&#150;usually called part-of-speech or word category&#150;and grammar deals with these tags only, omitting the details of  specific words. 
</P>
<P ALIGN="JUSTIFY">Morphology, on the other hand, is concerned with the inner structure of &#132;words&quot;. It tries to uncover the rules that govern the formation of words from smaller units. We notice that words that convey the same meaning look differently depending on their syntactic context. Take, e.g., the words <I>degrade, degrades, degrading, </I>and <I>degraded</I>. We can think of those as different forms of the same &#132;word&quot;. We call the part that carries the meaning of those forms a base form. In our example this is the form <I>degrade</I>. All other forms in this example are produced by adding a suffix. A wide range of other possibilities will be shown in <A HREF="#Morphshape">section 2.3</A>. All the different forms of a word together are called its <I>paradigm</I>. The part of morphology governing the production of these forms is called 
<A HREF="#Inflection"><I>inflection </I></A>.
</P>
<P ALIGN="JUSTIFY">Base forms in English are at the same time always word forms in their own right, e.g., the base form <I>degrade</I> is also present tense, active voice, non 3<SUP>rd</SUP> person singular. In other languages we find a slightly different situation. In <I>Italian</I> nouns are marked for gender and number. Different affixes are used to signal masculine and feminine on the one hand and singular an plural on the other hand.

<TABLE ALIGN="CENTER">
<TR><TD></TD><TD></B>S<SAMP>INGULAR</SAMP></TD><TD>P<SAMP>LURAL</SAMP><TD></TR>
<TR><TD>M<SAMP>ASCULINE</SAMP></TD> 
    <TD>pomodor<U>o</U></TD>
    <TD>pomodor<U>i</U></TD>
    <td>&#145;tomato&#146;</TD>
</TR>
<TR><TD>F<SAMP>EMININE</SAMP></TD>
    <TD>cipoll<U>a</U></TD>
    <TD>cipoll<U>e</U><TD>&#145;onion&#146;</TD>
</TR>
</TABLE>

<P ALIGN="JUSTIFY">Neither of the two forms of a noun can function as the base form. Instead, we must assume that the base form is what is left over after removing the respective suffixes, i.e.,  <I>pomodor-</I> and <I>cipoll-. </I>Such base forms that cannot occur as word forms in their pure form are called stems.</P>
<P ALIGN="JUSTIFY">Base forms themselves are not necessarily atomic. By comparing <I>degrade</I> to <I>downgrade, retrograde</I> and <I>upgrade</I> on the one hand and <I>decompose, decrease</I> and <I>deport</I> on the other hand we can see that it is composed of the morphs <I>de-</I> and <I>grade</I>. The morpheme carrying the central meaning is often called the root of the word. A root may combine with suffixes (cf. 2.2.2.2) or other roots (cf. 2.2.2.3) to form new base forms.</P>
<P ALIGN="JUSTIFY">Finally, we can describe &#132;word&quot; from a phonological perspective. Important for morphology is that phonological units define the range for phonological processes. Often, the phonological word is identical to the morphological word but sometimes boundaries differ. For example, the morphophonological process of final devoicing in German (cf. 2.3.2.2) works on syllable structure. Let&#146;s look at two words derived from the root <I>lieb</I>. The word <I>be+lieb+ig</I> (arbitrary) is realized as /</FONT><FONT FACE="SILDoulosIPA-Regular" SIZE=3>b&#180;li혱bik</FONT><FONT FACE="Times" SIZE=3>/ because it is a single phonological word. On the other hand, <I>lieb+lich</I> (lovely) is realized as /</FONT><FONT FACE="SILDoulosIPA-Regular" SIZE=3>li혱pliC</FONT><FONT FACE="Times" SIZE=3>/. Here, the last consonant of the root is devoiced because the two morphs are separated by a phonological word boundary.
</P>

<A NAME="Functions"><H4>2.2 Functions of morphology</H4></A>

<P ALIGN="JUSTIFY">How much and what sort of information is expressed by morphology differs widely between languages. Information that in some languages is expressed by syntax is expressed morphologically in others. Take, e.g., the expression of future tense: English uses an auxiliary verb construction, Spanish a suffix.
</P>

<TABLE ALIGN="CENTER">
<TR><TD>I speak</TD><TD>-</TD><TD>hablo</TR>
<TR><TD>I will speak</TD><TD>-</TD><TD>hablar&eacute;<TD></TR>
</TABLE>

<P ALIGN="JUSTIFY">Also, some type of information may be present in one language while and missing in another. In many languages plural marking for nouns is mandatory. In Japanese it is absent.
</P>

<TABLE ALIGN="CENTER">
<TR><TD>book</TD><TD>-</TD><TD>hon<TD></TR>
<TR><TD>books</TD><TD>-</TD><TD>hon<TD></TR>
</TABLE>

<P ALIGN="JUSTIFY">The means for encoding information also vary widely. Most common is the use of different types of affixes. Traditionally, linguists discriminate between the following types of languages with regard to morphology:
</P>
<UL>
<LI>Isolating languages (e.g. Mandarin Chinese): there are no bound forms, e.g., no affixes  that can be attached to a word. The only morphological operation is composition.</LI>
<LI>Agglutinative languages (e.g. Ugro-Finnic and Turkic languages): all bound forms are either prefixes or suffixes, i.e., they are added to a stem like beads on a string. Every affix represents a distinct morphological feature. Every feature is expressed by exactly one affix.</LI>
<LI>Inflectional languages (e.g. Indo-European languages): distinct features are merged into a single bound form (a so-called portmanteau morph). The same underlying feature may be expressed differently, depending on the paradigm</LI>
<LI>Polysynthetic languages (e.g. Inuit languages): these languages express more of syntax in morphology than other languages, e.g., verb arguments are incorporated into the verb.</LI>
</UL>
<P ALIGN="JUSTIFY">This classification is quite artificial. Real languages rarely fall cleanly into one of the above classes, e.g., even Mandarin has a few suffixes. Moreover, this classification mixes the aspect of what is expressed morphologically and the means for expressing it.</P>

<A NAME="Inflection"><H4>2.2.1 Inflection</H4></A>

<P ALIGN="JUSTIFY">Inflection is required in particular syntactic contexts. It does not change the part-of-speech category but the grammatical function. The different forms of a word produced by inflection form its paradigm. Inflection is <I>complete</I>, i.e., with rare exceptions all the forms of its paradigm exist for a specific word. Regarding inflection, words can be categorized in three classes: 
</P>
<UL>
<LI>Particles or not-inflecting words: they occur in just one form. In English, prepositions, adverbs, conjunctions and articles are particles;</LI>
<LI>Verbs or words following conjugation;</LI>
<LI>Nominals or words following declination, i.e., nouns, adjectives, and pronouns.</LI>
</UL>

<P ALIGN="JUSTIFY">Conjugation is mainly concerned with defining tense and aspect and agreement features like person and number. Take for example the <I>German</I> verb &#145;lesen&#146;  (to read). German verb forms come in present and past tense, indicative or subjunctive. 
</P>

<TABLE ALIGN="CENTER">
<TR><TD></TD>
    <TD>P<SAMP>RESENT</SAMP></TD><TD></TD><TD></TD><TD></TD>
    <TD>P<SAMP>AST</SAMP></TD><TD></TD><TD></TD><TD></TD></TR>
<TR><TD></TD>
    <TD>I<SAMP>NDICATIVE</SAMP></TD><TD></TD><TD>I<SAMP>NDICATIVE</SAMP></TD><TD></TD>
    <TD>S<SAMP>UBJUNCTIVE</SAMP></TD><TD></TD><TD>S<SAMP>UBJUNCTIVE</SAMP></TD><TD></TD></TR>
<TR><TD></TD>
    <TD>S<SAMP>INGULAR</SAMP></TD><TD>P<SAMP>LURAL</SAMP></TD><TD>S<SAMP>INGULAR</SAMP></TD><TD>P<SAMP>LURAL</SAMP></TD>
    <TD>S<SAMP>INGULAR</SAMP></TD><TD>P<SAMP>LURAL</SAMP></TD><TD>S<SAMP>INGULAR</SAMP></TD><TD>P<SAMP>LURAL</SAMP></TD></TR>
<TR><TD>1<SUP>st</SUP> P<SAMP>ERSON</SAMP></TD>
    <TD>lese</TD><TD>lesen</TD><TD>lese</TD><TD>lesen</TD>
    <TD>las</TD><TD>lasen</TD><TD>l&auml;se</TD><TD>l&auml;sen</TD></TR>
<TR><TD>2<SUP>nd</SUP> P<SAMP>ERSON</SAMP></TD>
    <TD>liest</TD><TD>lest</TD><TD>lesest</TD><TD>leset</TD>
    <TD>last</TD><TD>last</TD><TD>l&auml;sest</TD><TD>l&auml;set</TD></TR>
<TR><TD>3<SUP>rd</SUP> P<SAMP>ERSON</SAMP></TD>
    <TD>liest</TD><TD>lesen</TD><TD>lese</TD><TD>lesen</TD>
    <TD>las</TD><TD>lasen</TD><TD>l&auml;se</TD><TD>l&auml;sen</TD></TR>
<TR><TD>P<SAMP>ARTICIPLE</SAMP></TD>
    <TD>lesend</TD><TD></TD><TD></TD><TD></TD>
    <TD>gelesen</TD><TD></TD><TD></TD><TD></TD></TR>
<TR><TD>I<SAMP>MPERATIVE</SAMP></TD>
    <TD>lies</TD><TD>lest</TD><TD></TD><TD></TD>
    <TD></TD><TD></TD><TD></TD><TD></TD></TR>
<TR><TD>I<SAMP>NFINITIVE</SAMP></TD>
    <TD>lesen</TD><TD></TD><TD></TD><TD></TD>
    <TD></TD><TD></TD><TD></TD><TD></TD></TR>
</TABLE>

<P ALIGN="JUSTIFY">Declination marks various agreement features like number (singular, plural, dual, etc.), case (as governed by verbs and prepositions, or to mark various kinds of semantic relations), gender (male, female, neuter), and comparison.</P>

<A NAME="Derivation"><H4>2.2.2 Derivation and Compounding</H4></A>

<P ALIGN="JUSTIFY">In contrast to inflection which produces different forms of the same word derivation and compounding are processes that create <I>new words</I>. Thus, derivation and compounding have nothing to do with morphosyntax. They are a means to extend our lexicon in an economic and principled way.
</P>
<P ALIGN="JUSTIFY">In derivation, a different word--often of a different part-of-speech category--is produced by adding a bound morph to a stem. Derivation is incomplete, i.e., a derivational morph cannot be applied to all words of the appropriate class. For example, in German the very productive derivational suffix <I>-bar</I> can be applied to many but not all verbs to produce adjectives:
</P>

<TABLE ALIGN="CENTER">
<TR><TD>essen</TD><TD>&#145;to drink&#146;</TD><TD>-</TD><TD>ess<U>bar</U></TD><TD>&#145;eatable&#146;</TD></TR>
<TR><TD>h&ouml;ren</TD><TD>&#145;to hear&#146</TD><TD>-</TD><TD>h&ouml;r<U>bar</U></TD><TD>&#145;audible&#146;</TD></TR>
<TR><TD>absehen</TD><TD>&#145;to conceive&#146;</TD><TD>-</TD><TD>abseh<U>bar</U></TD><TD>&#145;conceivable&#146;</TD></TR>
<TR><TD>sehen</TD><TD>&#145;to see&#146;</TD><TD>-</TD><TD>*seh<U>bar</U></TD><TD>&#145;visible&#146;</TD></TR>
</TABLE>

<P ALIGN="JUSTIFY">Application of a derivational morpheme may be restricted to a certain subclass. For example, application of the English derivational suffix <I>-ity</I> is restricted to stems of Latin origin, while the suffix <I>-ness</I> can apply to a wider range:
</P>

<TABLE ALIGN="CENTER">
<TR><TD>rare</TD><TD>-</TD><TD>rar<U>ity</U></TD><TD>-</TD><TD>?rare<U>ness</U></TD></TR>
<TR><TD>red</TD><TD>-</TD><TD>*redd<U>ity</U></TD><TD>-</TD><TD>red<U>ness</U></TD></TR>
<TR><TD>grave</TD><TD>-</TD><TD>grav<U>ity</U></TD><TD>-</TD><TD>grave<U>ness</U></TD></TR>
<TR><TD>weird</TD><TD>-</TD><TD>*weird<U>ity</U></TD><TD>-</TD><TD>weird<U>ness</U></TD></TR>
</TABLE>

<P ALIGN="JUSTIFY">Derivation can be applied recursively, i.e., words that are already the product of derivation can undergo the process again. That way a potentially infinite number of words can be produced. Take, for example, the following chain of derivations:
</P>
<DIR>
<P>hospital &#151; hospital<U>ize</U> &#151; hospitaliz<U>ation</U> &#151; <U>pseudo</U>hospitalization</P>
</DIR>

<P ALIGN="JUSTIFY">Semantic interpretation of the derived word is often difficult. While a derivational suffix can usually be given a unique semantic meaning many of the derived words may still resist compositional interpretation. This may be due to lexicalization, i.e. a form is no more transparent because, or ambiguity of the underlying base form. For a more detailed discussion see Trost (1993).
</P>
<P ALIGN="JUSTIFY">While inflectional and derivational morphology are mediated by the attachment of a bound morph to a base form, compounding is the joining of two or more base forms to form a new word. Most common is just setting two words one after the other, as in <I>state monopoly, bedtime</I> or <I>red wine</I>. In some cases parts are joined by a linking morphem (usually the remnant of case marking) as in <I>bul<U>l&#146;</U>s eye</I> or German <I>Lieb<U>es</U>lied</I> (love-song).
</P>
<P ALIGN="JUSTIFY">The last part of a compound usually defines its morphosyntactic properties. Semantic interpretation is even more difficult than with derivation. Almost any semantic relationship may hold between the components of a compound:
</P>

<TABLE ALIGN="CENTER">
<TR><TD>Wienerschnitzel</TD><TD>&#9;&#145;cutlet made in Viennese <U>style</U>&#146;</TD></TR>
<TR><TD>Schweineschnitzel</TD><TD>&#9;&#145;cutlet made <U>of</U> pork&#146;</TD></TR>
<TR><TD>Kinderschnitzel</TD><TD>&#9;&#145;cutlet made <U>for</U> children&#146;</TD></TR>
</TABLE>

<P ALIGN="JUSTIFY">The boundary between derivation and compounding is fuzzy. Historically, most derivational suffixes developed from words frequently used in compounding. An obvious example is the <I>&#151;ful</I> suffix as in <I>hopeful, wishful, thankful</I>.</P>

<P ALIGN="JUSTIFY">Phrases and compounds cannot always be distinguished. The English expression <I>red wine</I> in its written form could be both.&#9;In spoken language the stress pattern differs: <I>red w&iacute;ne</I> vs. <I>r&eacute;d wine</I>. In German phrases are morphologically marked, while compounds are not: <I>roter Wein  </I>vs. <I>Rotwein</I>. But for verb compounds the situation is similar to English: <I>zu Hause bleiben</I>  vs. <I>zuhausebleiben.</I>
</P>

<A NAME="Morphshape"><H4>2.3 What constitutes a morph?</H4></A>

<P ALIGN="JUSTIFY">Every word form must at the core contain some root form. This root can (must) then be complemented with additional morphs. How are morphs realized? Obviously, a morph must somehow be recognizable in the phonetic or orthographic pattern constituing the word. The most common type of morph is a continuous sequence of phonemes. All roots and affixes are of this form. A complex word can then be analyzed as a series of morphs concatenated together. Agglutinative languages function almost exclusively this way. But there are surprisingly many other possibilities. </P>

<A NAME="Affix"><H4>2.3.1 Affixation</H4></A>

<P ALIGN="JUSTIFY">An affix is a bound morph that is realised as a sequence of phonemes (or graphemes). The by far most common types of affixes are prefixes and suffixes. Many languages have only these two types of affixes. Among them is English (at least under standard morphological analyses).
</P>
<P ALIGN="JUSTIFY">A prefix is an affix that is attached in front of a stem. An example is the English negative marker <I>un-</I> attached to adjectives:</P>

<DIR><P>common&#9;<U>un</U>common</P></DIR>

<P ALIGN="JUSTIFY">A suffix is an affix that is attached after a stem. Take, e.g., the English plural marker <I>&#151;s</I>:</P>

<DIR><P>shoe&#9;&#9;shoe<U>s</P></DIR>

</U><P ALIGN="JUSTIFY">Across languages suffixation is far more frequent than prefixation. Also, certain kinds of morphological information are never expressed via prefixes, e.g., nominal case marking. Many computational systems for morphological analysis and generation assume a model of morphology based on prefixation and suffixation only. 
</P>
<P ALIGN="JUSTIFY">A circumfix is the combination of a prefix and a suffix which together express some feature. Both theoretically and from a computational point of view a circumfix can be viewed as really two affixes applied one after the other.
</P>
<P ALIGN="JUSTIFY">In <I>German,</I>  the circumfixes ge--t and ge--n form the past participle of verbs:
</P>

<TABLE ALIGN="CENTER">
<TR><TD>sag<I>en</I>&#9;&#9;&#145;</TD>
    <TD>to say&#146;&#9;&#9;</TD>
    <TD><U>ge</U>sag<U>t</U>&#9;&#9;&#145;</TD>
    <TD>said&#146;</TD></TR>
<TR><TD>lauf<I>en</I>&#9;&#145;</TD>
    <TD>to run&#146;&#9;&#9;</TD>
    <TD><U>ge</U>lauf<U>en</U>&#9;&#9;&#145;</TD>
    <TD>run&#146;</TD></TR>
</TABLE>

<P ALIGN="JUSTIFY">An infix is an affix where the placement is defined in terms of some phonological condition(s). These might result in the infix appearing within the root to which it is affixed. In <I>Bontoc</I>, a Philippine language, the infix <I>-um- </I>turns adjectives and nouns into verbs (Fromkin and Rodman 1983). The infix attaches after the initial consonant:</P>

<DIR><P ALIGN="CENTER"><IMG SRC="Image3.gif"></P></DIR>

<P ALIGN="JUSTIFY">Reduplication is a border case of affixation. The form of the affix is a function of the stem to which it is attached, i.e., it copies (some portion of) the stem. Reduplication may be complete or partial. In the latter case it may be prefixal, infixal or suffixal. Reduplication can include phonological alteration on the copy or the original.</P>
<P ALIGN="JUSTIFY">In <I>Javanese</I> complete reduplication is used to express the habitual-repetitive. In case the second vowel is non-/a/, the first vowel in the copy is made nonlow (changing /a/ to /o/ and /</FONT><FONT FACE="SILDoulosIPA-Regular" SIZE=3>E</FONT><FONT FACE="Times" SIZE=3>/ to /e/) and the second becomes /a/. When the second vowel is /a/, the copy remains unchanged while in the original the /a/ is changed to /</FONT><FONT FACE="SILDoulosIPA-Regular" SIZE=3>E</FONT><FONT FACE="Times" SIZE=3>/ (Kiparsky 1987):</P>

<P ALIGN="CENTER"><IMG SRC="Image4.gif"></P>

<P ALIGN="JUSTIFY">Partial reduplication is more common. In <I>Yidin<SUP>y</I></SUP>, an Australian language, prefixal reduplication is used for  plural marking. Reduplication involves copying the &#145;minimal word&#146; (Nash 1980).

<P ALIGN="CENTER"><IMG SRC="Image5.gif">

<P ALIGN="JUSTIFY">An example for infixal reduplication is the frequentative in <I>Amharic</I>, a semitic language spoken in Ethiopia  (Rose 2000). </P>

<P ALIGN="CENTER"><IMG SRC="Image6.gif"></P>

<P ALIGN="JUSTIFY">From a computational point of view one property of reduplication is especially important: Since reduplication involves copying it cannot&#150;at least in the general case&#150;completely be described with the use of finite-state methods.</P>

<H4>2.3.2 Root-and-template morphology</H4>

<P ALIGN="JUSTIFY">Semitic languages (at least according to standard analyses) exhibit a very peculiar type of morphology: A so-called root, consisting of two to four consonants, conveys the basic semantic meaning. A vowel pattern marks information about voice and aspect. A derivational template gives the class of the word (traditionally called <I>binyan</I>).
</P>
<P ALIGN="JUSTIFY">In <I>Arabic</I> verb stems are constructed this way. The root <I>ktb</I> (write) produces--among others--the following stems:
</P>

<TABLE ALIGN="CENTER">
<TR><TD>Template</TD><TD>Vovel pattern</TD></TR>
<TR><TD></TD><TD>a (active)</TD><TD>ui (passive)</TD></TR>
<TR><TD>CVCVC</TD><TD>katab</TD><TD>kutib</TD><TD>&#145;write&#146;</TD></TR>
<TR><TD>CVCCVC</TD><TD>kattab</TD><TD>kuttib</TD><TD>&#145;cause to write&#146;</TD></TR>
<TR><TD>CVVCVC</TD><TD>ka:tab</TD><TD>ku:tib</TD><TD>&#145;correspond&#146;</TD></TR>
<TR><TD>tVCVVCVC</TD><TD>taka:tab</TD><TD>tuku:tib</TD><TD>&#145;write each other&#146;</TD></TR>
<TR><TD>nCVVCVC</TD><TD>nka:tab</TD><TD>nku:tib</TD><TD>&#145;subscribe&#146;</TD></TR>
<TR><TD>CtVCVC</TD><TD>ktatab</TD><TD>ktutib</TD><TD>&#145;write&#146;</TD></TR>
<TR><TD>stVCCVC</TD><TD>staktab</TD><TD>stuktib</TD><TD>&#145;dictate&#146;</TD></TR>
</TABLE>

<H4>2.3.3 Modification in phonetic substance</H4>

<P ALIGN="JUSTIFY">This term subsumes processes which do neither introduce new nor remove existing segments. Morphs are not realized as any string of phonemes, but as a change of phonetic properties or an alteration of the prosodic shape.
</P>
<P ALIGN="JUSTIFY">Ablaut refers to vowel alternations inherited from Indo-European. It is a pure example of vowel modification as a morphological process. Examples are strong verbs in Germanic languages like English (e.g., sw<U>i</U>m &#151; sw<U>a</U>m &#151; sw<U>u</U>m). In <I>Icelandic</I> this process is still more common and more regular than in most other Germanic languages. The following example is from Sproat (1992, p.62):

<P ALIGN="CENTER"><IMG SRC="Image7.gif"></P>

<P ALIGN="JUSTIFY">Umlaut has its origin in a phonological process, whereby root vowels were assimilated to a high-front suffix vowel. When this suffix vowel was lost later on, the change in the root vowel became the sole remaining mark of the morphological feature originally signalled by the suffix.</P>
<P ALIGN="JUSTIFY">In <I>German</I>  the plural of nouns may be marked by umlaut (sometimes in combination with a suffix), whereby in the stem vowel the feature <I>back</I> is changed to <I>front</I>:</P>

<P ALIGN="CENTER"><IMG SRC="Image8.gif"></P>

<P ALIGN="JUSTIFY">Another possibility to realize a morpheme is to alter the prosodic shape. Tone modification can be used to signal certain morphological features.</P>
<P ALIGN="JUSTIFY">In <I>Ngbaka</I>, spoken in the Democratic Republic of Congo, tense-aspect contrasts are expressed by four different tonal variants (Nida 1949):</P>

<P ALIGN="CENTER"><IMG SRC="Image9.gif"></P>

<P ALIGN="JUSTIFY">A morpheme may be realised by a stress shift. <I>English</I> noun-verb derivation sometimes uses a pattern where the stress is shifted from the first to the second syllable:
</P>

<TABLE ALIGN="CENTER">
<TR><TD>N<SAMP>OUN</SAMP></TD><TD>V<SAMP>ERB</SAMP></TD></TR>
<TR><TD>&eacute;xport</TD><TD>exp&oacute;rt</TD></TR>
<TR><TD>r&eacute;cord</TD><TD>rec&oacute;rd</TD></TR>
<TR><TD>c&oacute;nvict</TD><TD>conv&iacute;ct&#9;</TD></TR>
</TABLE>

<H4>2.3.4 Suppletion </H4>

<P ALIGN="JUSTIFY">Total modification is a process occurring sporadically and idiosyncratically within inflectional paradigms. It is usually associated with forms that are used very frequently. Examples in English are <I>went</I>, the past tense of <I>go</I>, and the forms <I>of to be: am, are, is, was</I> and<I> were</I>.</P>

<H4>2.3.5 Zero Morphology</H4>
<P ALIGN="JUSTIFY">Sometimes a morphological operation has no phonological expression whatsoever. Examples are found in many languages.</P>
<P ALIGN="JUSTIFY">English noun-to-verb derivation is often not explicitly marked: </P><DIR>

<P>man&#9;&#9;The <U>man</U> smiled.&#9;<U>Man</U> the boats.</P>
<P>house  &#9;He buys a <U>house</U>.&#9;They <U>house</U>  in a cave.</P></DIR>

<P ALIGN="JUSTIFY">A possible analysis is to assume a zero morph which attaches to the noun to form a verb: book+&Oslash;<SUB>V. </SUB>Another possibility is to assume two independent lexical items disregarding any morphological relationship. </P>

<H4>2.4 The structure of words: Morphotactics</H4>

</B></FONT><FONT FACE="Times" SIZE=3><P ALIGN="JUSTIFY">Somehow morphs must be put together to form words. A word grammar is determining the way this has to be done. This part of morphology is called <I>morphotactics</I>. As we have seen, the most usual way is simple concatenation. Let&#180;s have a look at the constraints involved. What are the conditions governing the ordering of morphemes in <I>pseudohospitalization</I>?</P><DIR>

<P ALIGN="JUSTIFY">(1)&#9;*hospitalationizepseudo, *pseudoizehospitalation</P>
<P ALIGN="JUSTIFY">(2)&#9;*pseudohospitalationize</P></DIR>

<P ALIGN="JUSTIFY">In (1) an obvious restriction is violated: <I>pseudo-</I> is a prefix and must appear ahead of the stem, <I>-ize</I> and <I>&#151;ation</I> are suffixes and must appear after the stem. The violation in (2) is less obvious. In addition to the pure ordering requirements there are also rules governing to which types of stems an affix may attach:  <I>&#151;ize</I> attaches to nouns and produces verbs, <I>&#151;ation</I> attaches to verbs and produces nouns.</P>
<P ALIGN="JUSTIFY">One possibility to describe the word formation process is to assume a functor-argument structure. Affixes are functors that pose restrictions on their (single) argument. That way a binary tree is constructed. Prefixes induce right branching and suffixes left branching.</P>
<P ALIGN="CENTER"><IMG SRC="Image1.gif" WIDTH=266 HEIGHT=132></P>
<P ALIGN="CENTER">Fig. 1: The internal structure of the word <I>pseudohospitalization</P>
</I><P ALIGN="JUSTIFY">In figure 1 the functor <I>pseudo</I>- takes a nominal argument to form a noun, <I>&#151;ize</I> a  nominal argument to form a verb, and <I>&#151;ation</I> a verbal argument to form a noun. This description renders two different possible structures for <I>pseudohospitalization</I>. The one given in figure 1 and a second one where <I>pseudo-</I> combines first directly with <I>hospital</I>. We may or may not accept this ambiguity. To avoid the second reading we could state a lexical constraint that a word with the head <I>pseudo-</I> cannot serve as an argument anymore.</P>

<H4>2.4.1 Constraints on affixes</H4>

<P ALIGN="JUSTIFY">Affixes is that they attach to specific categories only. This is an example for a syntactic restriction. Restrictions may also be of a phonological, semantic or purely lexical nature. A semantic restriction on the English adjectival prefix <I>un-</I> prevents its attachment to an adjective that already has a negative meaning:</P>

<TABLE ALIGN="CENTER">
<TR><TD>unhappy&#9;</TD><TD>*unsad</TD></TR>
<TR><TD>unhealthy&#9;</TD><TD>*unill</TD></TR>
<TR><TD>unclean&#9;</TD><TD>*undirty</TD></TR>
</TABLE>

<P ALIGN="JUSTIFY">The fact that in English some suffixes may only attach to words of Latin origin (cf. 2.2.2) is an example for a lexical restriction.</P>

<H4>2.4.2 Morphological vs. phonological structure</H4>

</B><P ALIGN="JUSTIFY">In some cases there is a mismatch between the phonological and the morphological structure of a word. One example is comparative formation with the suffix <I>&#151;er</I> in English. Roughly, there is a phonological rule that prevents attaching this suffix to words that consist of more than two syllables:</P>

<TABLE ALIGN="CENTER">
<TR><TD>great&#9;&#9;</TD><TD>greater</TD></TR>
<TR><TD>tall&#9;&#9;</TD><TD>taller</TD></TR>
<TR><TD>happy &#9;</TD><TD>happier</TD></TR>
<TR><TD>competent&#9;</TD><TD>*competenter</TD></TR>
<TR><TD>elegant&#9;</TD><TD>*eleganter</TD></TR>
</TABLE>

<P ALIGN="JUSTIFY">If we want to stick to the above rule <I>unrulier</I> has to be explained with a structure where the prefix <I>un-</I> is attached to <I>rulier</I>. But, from a morphological point of view, the adjective <I>ruly</I> does not exist, only the negative form <I>unruly</I>. This implies that the suffix <I>&#151;er</I> is attached to <I>unruly</I>. We end up with an obvious mismatch!</P>
<P ALIGN="JUSTIFY">Another potential problem is cliticization. A clitic is a syntactically separate word phonologically realized as an affix. The phenomenon is quite common across languages.

<UL>
<LI>In English auxiliaries have contracted forms that function as affixes:
<BR>
<I>he shall return -> he&#146;ll return</I>
</LI>
<LI>In German prepositions can combine with the definite article
<BR>
<I>an dem Tisch -> am Tisch
<BR>
in das Haus -> ins Haus</I></LI>
<LI>In Italian personal pronouns can be attached to the verb. In this process the ordering of constituents is also altered.
<BR>
<I>ce ne facciamo -> facciamocene</I>
</LI>
</UL>

<H4>2.5 The Influence of Phonology</H4>

<P ALIGN="JUSTIFY">Morphotactics is responsible to govern the rules for the combination of morphs into larger entities. One could assume that this is all a system needs to know to break down words into their component morphemes. But there is another aspect that makes things more complicated: Phonological rules may apply and change the shape of morphs. To deal with these changes and their underlying reasons is the area of morphophonology.</P>

<H4>2.5.1 Phonology vs. orthography</H4>

<P ALIGN="JUSTIFY">Most applications of computational morphology deal with text rather than speech. But, written language is rarely a true phonemic description. For some languages, e.g., Finnish, Spanish or Turkish orthography is a good approximation for a phonetic transcription. English, on the other hand, has very poor correspondence between writing and pronounciation. As a result, we often have to deal with orthography rather than phonology. A good example are English plural rules (cf. 2.4.1).</P>

<H4>2.5.2 Local phenomena</H4>

</B><P ALIGN="JUSTIFY">We have shown that, by and large, words are composed by concatenating morphs. In many cases this concatenation process will induce some phonological change in the vicinity of the morph boundary.</P>
<P ALIGN="JUSTIFY">Assimilation is a process where the two segments at a morph boundary influence each other, resulting in some feature change that makes them more similar. Take, for example, the English <I>in-</I> prefix where the <I>n</I> changes to <I>m</I> before labials:</P>

<TABLE ALIGN="CENTER">
<TR><TD>&lt;in+feasible&gt;</TD><TD>-&gt;</TD><TD>infeasible</TD></TR>
<TR><TD>&lt;in+mature&gt;</TD><TD>-&gt;</TD><TD>i<U>m</U>mature</TD></TR>
<TR><TD>&lt;in+probable&gt;</TD><TD>-&gt;</TD><TD>i<U>m</U>probable</TD></TR>
<TR><TD>&lt;in+secure&gt;&#9;</TD><TD>-&gt;</TD><TD>insecure</TD></TR>
</TABLE>

<P ALIGN="JUSTIFY">Another possibility is epenthesis (insertion) or elision (deletion) of a segment under certain (phonological) conditions. Take for example the English plural formation:</P>

<TABLE ALIGN="CENTER">
<TR><TD>&lt;cat+s&gt;</TD><TD>-&gt;</TD><TD>cats</TD></TR>
<TR><TD>&lt;door+s&gt;</TD><TD>-&gt;</TD><TD>doors</TD></TR>
<TR><TD>&lt;dish+s&gt;</TD><TD>-&gt;</TD><TD>dish<U>e</U>s</TD></TR>
<TR><TD>&lt;bliss+s&gt;</TD><TD>-&gt;</TD><TD>bliss<U>e</U>s</TD></TR>
<TR><TD>&lt;match+s&gt;</TD><TD>-&gt;</TD><TD>match<U>e</U>s</TD></TR>
<TR><TD>&lt;fox+s&gt;</TD><TD>-&gt;</TD><TD>fox<U>e</U>s</TD></TR>
</TABLE>

<P ALIGN="JUSTIFY">In this case the rule requires the insertion of an /</FONT><FONT FACE="SILDoulosIPA-Regular" SIZE=3>&#180;</FONT><FONT FACE="Times" SIZE=3>/ between /</FONT><FONT FACE="SILDoulosIPA-Regular" SIZE=3>s</FONT><FONT FACE="Times" SIZE=3>/, /</FONT><FONT FACE="SILDoulosIPA-Regular" SIZE=3>z</FONT><FONT FACE="Times" SIZE=3>/, /</FONT><FONT FACE="SILDoulosIPA-Regular" SIZE=3>S</FONT><FONT FACE="Times" SIZE=3>/, or /</FONT><FONT FACE="SILDoulosIPA-Regular" SIZE=3>Z</FONT><FONT FACE="Times" SIZE=3>/ and another /</FONT><FONT FACE="SILDoulosIPA-Regular" SIZE=3>s</FONT><FONT FACE="Times" SIZE=3>/. On the other hand, in German the suffix &#151;st attached to stems ending in /</FONT><FONT FACE="SILDoulosIPA-Regular" SIZE=3>s</FONT><FONT FACE="Times" SIZE=3>/ looses its starting segment /</FONT><FONT FACE="SILDoulosIPA-Regular" SIZE=3>s</FONT><FONT FACE="Times" SIZE=3>/:</P>

<TABLE ALIGN="CENTER">
<TR><TD>&lt;leb+st&gt;</TD><TD>-&gt;</TD><TD>lebst</TD></TR>
<TR><TD>&lt;sag+st&gt;</TD><TD>-&gt;</TD><TD>sagst</TD></TR>
<TR><TD>&lt;ras+st&gt;</TD><TD>-&gt;</TD><TD>rast</TD></TR>
<TR><TD>&lt;trotz+st&gt;</TD><TD>-&gt;</TD><TD>trotzt</TD></TR>
<TR><TD>&lt;hex+st&gt;</TD><TD>-&gt;</TD><TD>hext</TD></TR>
</TABLE>

<P ALIGN="JUSTIFY">We see that the change is not purely phonologically motivated. The same condition, namely two adjoining /</FONT><FONT FACE="SILDoulosIPA-Regular" SIZE=3>s</FONT><FONT FACE="Times" SIZE=3>/ phonemes leads to different results: Either the epenthesis of an /</FONT><FONT FACE="SILDoulosIPA-Regular" SIZE=3>&#180;</FONT><FONT FACE="Times" SIZE=3>/ between the two, or the elision of the second /</FONT><FONT FACE="SILDoulosIPA-Regular" SIZE=3>s</FONT><FONT FACE="Times" SIZE=3>/. Moreover, the notion of insertion or deletion is purely descriptive. Phonological theory may explain the underlying processes completely different. Nonetheless, this is the view most often taken by work in computational morphology.</P>

<H4>2.5.3 Long-distance effects</H4>

</B><P ALIGN="JUSTIFY">Most common is vowel harmony but there are olso examples of consonant harmony. Vowel harmony is a phonological process where the leftmost (in rare cases the rightmost) vowel in a word influences all the following (preceding) vowels. Among the languages exhibiting vowel harmony are Finnish, Hungarian, Turkic and many African languages.</P>
<P ALIGN="JUSTIFY">Let&#146;s have a look at vowel harmony in <I>Turkish</I>. The nine vowels of the Turkish language can be specified the following way:
</P>

<TABLE ALIGN="CENTER">
<TR><TD></TD><TD>i</TD><TD><FONT FACE="Courier" SIZE=2>I</FONT></TD><TD>&uuml</TD><TD>u</TD><TD>e</TD><TD>a</TD><TD>&ouml;</TD><TD>o</TD></TR>
<TR><TD>H<SAMP>IGH</SAMP></TD><TD>+</TD><TD>+</TD><TD>+</TD><TD>+</TR>
<TR><TD>B<SAMP>ACK</SAMP></TD><TD>+</TD><TD></TD><TD>+</TD><TD></TD><TD>+</TD><TD><T/D><TD>+</TD></TR>
<TR><TD>R<SAMP>OUND</SAMP></TD><TD></TD><TD></TD><TD>+</TD><TD>+</TD><TD></TD><TD></TD><TD>+</TD><TD>+</TD></TR>
</TABLE>

<P ALIGN="JUSTIFY">Turkish has two different harmony rules, called <I>small</I> and <I>large </I>respectively: 

<FONT FACE="Courier" SIZE=3>
<TABLE ALIGN="CENTER">
<TR><TD>e i &ouml; &uuml;</TD><TD> -> </TD><TD>e&nbsp;&nbsp;&nbsp;&nbsp;</TD>
    <TD>e i</TD><TD> -> </TD><TD>i</TD></TR>
<TR><TD></TD><TD></TD><TD></TD><TD>&ouml; &uuml</TD><TD> -> </TD><TD>&uuml;</TD></TR>
<TR><TD>a <FONT FACE="Courier" SIZE=2>I</FONT>o u</TD><TD> -> </TD><TD>a</TD>
    <TD>a <FONT FACE="Courier" SIZE=2>I</FONT></TD><TD> -> </TD><TD><FONT FACE="Courier" SIZE=2>I</FONT></TD></TR>
<TR><TD></TD><TD></TD><TD></TD><TD>o u</TD><TD> -> </TD><TD>u</TD></TR>
</TABLE>
</FONT>

<P ALIGN="JUSTIFY">Only the stem vowel in a word is lexically determined.  All the following vowels are realized in accordance to the harmony rules. For example, the root <I>ev</I> (house) induces either an <I>e</I> or an <I>i</I> in the attached suffixes. Which one of the two is realized is a property of the respective suffix.</P>

<P ALIGN="CENTER"><IMG SRC="Image10.gif"></P>

<P ALIGN="JUSTIFY">In this example the plural suffix follows the &#132;small&quot; harmony and the genitive suffix the &#132;large&quot; harmony rule.&#9;</P>

<H3>3 Applications of Computational Morphology</H3>

</B></FONT><FONT FACE="Times" SIZE=3><P ALIGN="JUSTIFY">Computational morphology has many practical applications. Besides low-level applications, computational morphology contributes to many speech and language processing systems.</P>

<H4>3.1 Low-level applications</H4>

</B></FONT><FONT FACE="Times" SIZE=3><P ALIGN="JUSTIFY">Hyphenation is almost exclusively done automatically. Although the task seems at first glance extremely simple only a human expert can achieve a 100% success rate. Segmenting words correctly into their morphs helps to solve the task. The major problem are spurious segmentations.</P>
<P ALIGN="JUSTIFY">Spelling correction is another low-level application. Just comparing input against a list of word forms has a number of drawbacks. Such a list will never contain all the words occurring in a text and enlarging the list has the negative side effect of including more and more obscure words that will match with typos thus preventing their detection. Most systems use a root lexicon, plus a relatively small set of affixes and simple rules to cover morphotactics.</P>
<P ALIGN="JUSTIFY">Stemmers are used in information retrieval to reduce as many related words and word forms as possible to a common canonical form which can then be used in the retrieval process. One should note that this canonical form is not necessarily the base form. The main requirement is&#150;like in all the above tasks&#150;robustness.</P>
<P ALIGN="JUSTIFY">Another application is to segment text in Chinese, Japanes or Korean. In these languages words in a sentence are not separated by blanks or punctuation marks. Morphological analysis can be used to perform the task of word separation. </P>
<P ALIGN="JUSTIFY">A related problem is the inputting of Japanese text. Japanese is written with a combination of two independent character sets. Kanji, the morphemic Chinese characters are used for open-class morphemes (verbs, nouns and adjectives). Kana has (about 50) syllabic characters and is mainly used for closed-class morphemes although in principle all Japanese words can be written exclusively in kana.</P>
<P ALIGN="JUSTIFY">Since there are several thousand kanji characters, many Japanese text input systems use kana-kanji conversion. The text is typed in kana and the relevant portions are subsequently converted to kanji. The mapping from kana to kanji is quite ambigous. A combination of statistical and morphological methods is applied to solve that task.</P>

<H4>3.2 Natural language applications</H4>

</FONT><FONT FACE="Times" SIZE=3><P ALIGN="JUSTIFY">An obvious application area for morphological components are more general natural language processing systems involving parsing and/or generating natural language utterances in written or spoken form. There is a wide range of such applications from message and information extraction to dialog systems and machine translation. For many current applications, only inflectional morphology is considered.</P>
<P ALIGN="JUSTIFY">In a parser, morphological analysis of words is an important prerequisite for syntactic analysis. Properties of a word the parser needs to know are its part-of-speech category and the morphosyntactic information encoded in the particular word form. Another important task is lemmatization, i.e., finding the corresponding dictionary form for a given input word, because for many applications a lemma lexicon is used to provide more detailed syntactic (e.g, valency) and semantic information for a deep analysis.</P>
<P ALIGN="JUSTIFY">In generation, on the other hand, the task is to produce the correct word form from the base form plus the relevant set of morphosyntactic features.</P>

<H4>3.3 Speech applications</H4>

<P ALIGN="JUSTIFY">A text-to-speech system takes (electronically stored) text as input and produces speech from it. Morphological analysis helps to solve two different tasks in such systems. One is to guide the grapheme-to-phoneme conversion. Characters are often ambiguous with respect to their translation into phonemes. Finding out the underlying morphological structure is necessary for solving the task correctly. The sequence <I>th</I>, is usually pronounced as /</FONT><FONT FACE="SILDoulosIPA-Regular" SIZE=3>D</FONT><FONT FACE="Times" SIZE=3>/ or /</FONT><FONT FACE="SILDoulosIPA-Regular" SIZE=3>T</FONT><FONT FACE="Times" SIZE=3>/ in English. In the word <I>hothouse</I> we need to know the morph structure &lt;hot+house&gt; to correctly pronounce the <I>th</I> sequence as /</FONT><FONT FACE="SILDoulosIPA-Regular" SIZE=3>th</FONT><FONT FACE="Times" SIZE=3>/.</P>
<P ALIGN="JUSTIFY">A less obvious application is the use of morphological analysis to help in determining the part-of-speech category of words. This is an important prerequisite of syntactic analysis which is the basis for coming up with a correct prosody.</P>
<P ALIGN="JUSTIFY">Speech recognition is a field where morphological analysis will become ever more important. At the moment most available systems make use of full form lexicons and perform their analysis on a word basis. Increasing demands on the lexicon size on the one hand and the need to limit the necessary training time on the other hand will make morph-based recognition systems more attractive.</P>

<H3>4 Computational Morphology</H3>

<P ALIGN="JUSTIFY">The most basic task in computational morphology is to take a string of characters or phonemes as input and deliver an analysis as output. The input could, for example be the English word form in (1). One possible output could be the string of underlying morphemes as in (2), another one a morphosyntactic interpretation as in (3).</P>
<OL>

</FONT><FONT FACE="Times"><P ALIGN="JUSTIFY"><LI>incompatibilities</LI></P>
<P ALIGN="JUSTIFY"><LI>in+con+patible+ity+s</LI></P>
<P ALIGN="JUSTIFY"><LI>incompatibility+NounPlural</LI></P></OL>

</FONT><FONT FACE="Times" SIZE=3><P ALIGN="JUSTIFY">Let&#146;s start with the task of mapping (1) to (3). The easiest way to achieve a result is to have a long list of pairs where the left side represents some word form and the right side its interpretation. This is basically the notion of full form lexicon. Its advantages are simplicity and applicability to all possible phenomena. The main disadvantages are redundancy and inability to cope with forms not contained in the lexicon.</P>
<P ALIGN="JUSTIFY">Less redundant are so-called lemma lexica<I>. </I>A lemma is a canonical form taken as the representative for all the different forms of a paradigm. Usually, the base form is selected as this canonical form. An interpretation algorithm relates every form to its lemma plus delivering a morphosyntactic interpretation. As a default, forms are expected to be string concatenations of base form (= lemma) and affixes. Affixes must be stored in a separate repository together with the relevant morphotactic information about how they may combine with other forms. Interpretation then simply means finding a sequence of affixes and a base form that conforms to morphotactics. For different reasons a given word form may not conform to this simple picture:</P>

<UL>
<LI>With very frequently used words we often find suppletion, e.g., <I>to go</I> has the completely unrelated form <I>went</I>.</LI>
</UL>

<P ALIGN="JUSTIFY">One clearly needs some exception handling mechanism to cope with suppletion. A possible solution is to have secondary entries where you store suppleted forms together with their morphosyntactic information. These secondary forms are then linked to the corresponding primary form, i.e., the lemma.</P>

<UL>
<LI>Morphs are realised in a non-concatenative way, e.g., tense of strong verbs in English: <I>give &#151; gave - given</I>, <I>find - found &#151; found</LI>
</UL>

</I><P ALIGN="JUSTIFY">In languages like English, where these phenomena affect only a fairly small and closed set of words these forms can be treated like suppletion. Alternatively, some exception handling mechanism (usually developed ad-hoc and language-specific) is applied.</P>

<UL>
<LI>Due to phonological rules a word form may exhibit some change in shape, e.g., in English suffixes starting with <I>s</I> (plural of nouns, 3<SUP>rd</SUP> person marker, superlative marker) may not directly follow stems ending in a sybillant (e.g., dish &#151; dishes)</LI>
</UL>

<P ALIGN="JUSTIFY">If morphophonological processes in a language are few and local the lemma lexicon approach can still be successful. In our example it suffices to assume two plural  endings: <I>-s</I> and <I>&#151;es</I>. For all base forms it must be specified whether the former or the latter of the two endings may be attached.</P>
<P ALIGN="JUSTIFY">Apart from the obvious limitations with regard to the treatment of morphophonological rules on a more general scale the approach has some other inherent restrictions.
</P>

<UL>
<LI>The algorithm is geared towards analysis. For generation purposes, one needs a completely different algorithm and data.</LI>
<LI>Interpretation algorithms are language-specific because they encode both the basic concatenation algorithm and the specific exception-handling mechanism.</LI>
<LI>The approach was developed for morphosyntactic analysis. An extension to handle more generally the segmenting of word forms into morphs is difficult to achieve.</LI>
</UL>

<H4>4.1 Finite-state Morphology</H4>

<P ALIGN="JUSTIFY">Because most morphological phenomena can be described with regular expressions the use of finite-state techniques for morphological components is common. In particular, when morphotactics is seen as a simple concatenation of morphs it can straightforwardly be described by a finite automata.</P>
<P ALIGN="JUSTIFY">It was not so obvious though how to describe non-concatenative phenomena like vowel harmony, root-and-template morphology or infixation in such a framework.</P>

<H4>4.1.1 Two-level morphology</H4>

<P ALIGN="JUSTIFY">In this section we describe a system where morphophonology is taken care of by a separate mechanism that is well integrated with the morphotactical component. It has the further advantages of being non-directional (applicable to analysis and generation) and language-independent (because of its purely declarative specification of language-specific data).</P>
<P ALIGN="JUSTIFY">Rules for the description of morphophonological phenomena are standard in generative phonology.  There, the derivation of a word form from its lexical structure is performed by the successive application of phonological rules creating a multi-step process involving several intermediate levels of representation. Such an approach may be suited for generation but leads to problems if applied to analysis. Since the ordering of rule application influences the result it is difficult to reverse the process.</P>
<P ALIGN="JUSTIFY">Several proposals were made on how to restrict rules and their application to overcome these problems. Two-level morphology is a an attempt to overcome these problems. Originally proposed by Kimmo Koskenniemi (1984) it has since been implemented in a number of different systems and applied to a wide range of natural languages.</P>

<H4>4.1.1.1 Two-level rules </H4>

<P ALIGN="JUSTIFY">As the name suggests two levels--called lexical level and surface level--suffice to describe the phonology (or orthography) of a natural language. On the surface level words appear just as they are pronounced (or written) in ordinary language, with the important exception of the null character which will be described later on.  On the lexical level, the alphabet includes special symbols--so-called diacritics--which are mainly used to represent features that are no phonemes (or graphemes) but nevertheless constitute necessary phonological information.  The diacritics <I>'+'</I> and <I>'#&#145;</I> are used to indicate morph and word boundary respectively.</P>
<P ALIGN="JUSTIFY">The two levels are linked by a set of pairs of lexical and surface characters constituting possible mappings between lexical and surface characters.  Pairs are written as lexical character - colon - surface character (e.g. <I>a:a </I>or<I> +:0</I>).  To any of these pairs rules may be attached to restrict their applicability.  Pairs with no attached rules are applied by default.  Rules serve to licence the application of a pair in a certain phonological context. They are viewed as constraints on the mapping between the surface and the lexical form of morphs.  Accordingly, they are applied in parallel and not one after the other like in generative phonology. Since no ordering of the rules is involved this is a completely declarative way of description.</P>
<P ALIGN="JUSTIFY">A rule consists of the following parts:

<UL>
<LI>A substitution that indicates the affected character pair. </LI>
<LI>left and right context define the phonological conditions for the substitution. </LI>
<LI>One of four available operators defines the status of the rule:</FONT><FONT FACE="Symbol" SIZE=3> </FONT><FONT FACE="Times" SIZE=3> The context restriction operator <= makes the substitution of the lexical character obligatory in the context defined by that rule (other phonological contexts are not affected). The surface coercion operator => restricts the substitution of the lexical character to exactly this context (it may not occur anywhere else).  The <=> is a combination of the former two, i.e., the substitution must take place in exactly this context and nowhere else. The fourth operator /<= states prohibitions, i.e., the substitution may not take place in this context.</LI>
</UL>
<P ALIGN="JUSTIFY">Let's look at a simple epenthesis rule:</P>

<DIR><P> (1a)&#9;+:e &lt;= s x z [ { s c } h ] :  _ s ;</P></DIR>

<P ALIGN="JUSTIFY">It specifies that a lexical morph boundary (indicated by  <I>'+'</I>) between <I>s, x, z, sh, </I>or <I>ch</I> on the left side and an <I>s</I> on the right side must correspond to surface level <I>e</I>. By convention a pair with identical lexical and surface character may be denoted by just a single character. Curly brackets indicate a set of alternatives, square brackets a sequence.</P>
<P ALIGN="JUSTIFY">Rule (1a) makes no statements about other contexts where <I>'+'</I> may map to an <I>'e'</I>.  The rule covers some of the cases where an <I>'e'</I> is inserted between stem and an inflectional morph starting with <I>'s'</I> (plural morpheme, 3<SUP>rd</SUP> person marker, superlative) in English. By default a morph boundary will map to the null character, but in the given specific context it maps to <I>'e'</I>. The following example shall demonstrate the application of this rule (Vertical bars denote a default pairing, numbers the application of the corresponding rule): 
</P>
<DIR>
<FONT FACE="Courier" SIZE=3>
<P>#bliss+s#&#9;&#9;#fox+s#&#9;#dish+s#&#9;#watch+s#
<BR>||||||1||&#9;&#9;||||1||&#9;|||||1||&#9;||||||1||&#9;
<BR>0blisses0&#9;&#9;0foxes0&#9;0dishes0&#9;0watches0</DIR>

</FONT><FONT FACE="Times" SIZE=3><P ALIGN="JUSTIFY">Obviously, (1a) does not capture all the cases where epenthesis of 'e' occurs. For example, the forms <I>spies, shelves</I> or<I> potatoes </I>are not covered.  A more complete rule is:</P>

<DIR><P>
(1b)&#9;+:e &lt;=&gt; {s x z [ { s c} h:h ] :v [ C y: ] [ C o ] } _ s ;
</P></DIR>

<P ALIGN="JUSTIFY">Formally, rule (1b) defines exactly all the contexts where '+' maps to an 'e' (because of the use of the </FONT><FONT FACE="Symbol" SIZE=2>&#164;</FONT><FONT FACE="Times" SIZE=3> operator). It also makes use of some additional writing conventions. A colon followed by a character denotes the set of all pairs with that surface character. Accordingly, a character followed by a colon means the set of all pairs with that lexical character.  Sets of characters can be globally defined and given names.  The C stands for the set of English consonants (i.e., b:b, c:c, d:d,...).  To cope with the <I>spies</I> example we need another rule which licences the mapping from 'y' to 'i'.
</P>
<DIR>
<TABLE>
<TR><TD>(2)</TD><TD>y:i</TD><TD> &lt;=&gt; </TD><TD> C _  { +:e  [ +:  e ]  } ;</TD></TR>
<TR><TD></TD><TD></TD><TD></TD><TD>V C+ _  +:  C ;</TD></TR>
</TABLE>
</DIR>

</FONT><P ALIGN="JUSTIFY">Rule (2)  specifies two distinct contexts. If either of them is satisfied the substitution must occur, i.e., contexts are OR-connected. The '</FONT><FONT FACE="Times" SIZE=1>+</FONT><FONT FACE="Times" SIZE=3>' operator in the second context indicates <I>at least one occurrence</I> of the preceding sign (accordingly, the operator '*' has the reading <I>arbitrarily many occurrences</I>).  V stands for the set of vowels. Rules (1) and (2) in combination now correctly map <I>spies</I> with <I>spy+s</I>. Jointly with rule (3) for the mapping from 'f' to 'v' (1) takes also care of forms like <I>shelves</I> and  <I>potatoes</I>: </P>

<DIR><TABLE>
<TR><TD>(3)</TD><TD> f:v </TD><TD>&lt;= </TD><TD> { e l } _ +:  s ;</TD></TR>
<TR><TD></TD><TD></TD><TD></TD><TD>V _ e +: s;</TD></TR>
</DIR></TABLE>

</FONT><FONT FACE="Times" SIZE=3><P ALIGN="JUSTIFY">Let&#146;s see how the three rules interact to produce the expected results:
</P>
<DIR>
<FONT FACE="Courier" SIZE=3>
<P>#spy+s#&#9;#toy+s#&#9;#shelf+s#&#9;#wife+s#&#9;#potato+s#
<BR>|||21||&#9;|||||||&#9;|||||31||&#9;|||3||||&#9;|||||||1||
<BR>0spies0&#9;0toy0s0&#9;0shelves0&#9;0wive0s0&#9;0potatoes0</DIR>

</FONT><FONT FACE="Times" SIZE=3><P ALIGN="JUSTIFY">A given pair of lexical and surface strings can only map if they are of equal length.  There is no possibility of omitting or inserting a character in one of the levels.  On the other hand, elision and epenthesis are common phonological phenomena.  To cope with these, the null character (written as 0) is included in both the surface and the lexical alphabet.  The null character is taken to be contained in the surface string for the purpose of mapping lexical to surface string and vice versa but it does not show up in the output or input of the system. Diacritics are mapped to the null character by default.  Any other mapping of a diacritic has to be licensed by a rule.</P>
<P ALIGN="JUSTIFY">Assumption of the explicit null character is essential for processing. A mapping between a lexical and a surface string presupposes that for every position a character pair exists.  This implies that both strings are of equal length (nulls are considered as characters in this respect). Rules can either be directly interpreted or compiled into finite state transducers. The use of finite state machinery allows for very efficient implementation. For a more in-depth discussion of implementational aspects consult chapter 37 and Beesley and Karttunen (2000).</P>
<P ALIGN="JUSTIFY">One subtle difference between direct rule interpretation and transducers occurs in the repeated application of the same rule to one string.  The transducer implicitly extends the phonological context to the whole string.  It must therefore explicitly take care of overlapping right and left contexts (e.g., in (1) the pair <I>s:s</I> constitutes both a left and right context).  With direct interpretation a new instance of the rule is activated every time the left context is found in the string and overlapping must not be treated explicitly.</P>

<H4>4.1.1.2 The continuation lexicon</H4>

<P ALIGN="JUSTIFY">Up to now we have only described the rule part of two-level morphology which is responsible for taking care of morphonological phenomena.  It is complemented by a partitioned lexicon of morphs (or words) that takes care of word formation by affixation.  The lexicon consists of (non-disjunctive) sublexica, so-called continuation classes. For every morph, a set of legal continuation classes is specified.  This set defines which sublexicon must be searched for continuations.  The class of morphs which can start a word is stored in the so-called <I>"init lexicon"</I>.</P>
<P ALIGN="JUSTIFY">The whole process is equivalent to stepping through a finite automaton.  A successful match can be taken as a move from some state <I>x</I> of the automaton to some other state <I>y</I>. Lexical entries can be thought of as arcs of the automaton: a sublexicon is a collection of arcs having a common <I>from</I> state.</P>
<P ALIGN="JUSTIFY">The lexicon in two-level morphology is used for two purposes: one is to describe which combinations of morphs are legal words of the language, the other one is to act as a filter whenever a surface word form shall be mapped to a lexical form.  Its use for the second task is crucial because otherwise there would be no way to limit the insertion of the null character.</P>
<P ALIGN="JUSTIFY">To enable fast access, lexicons are organized in the form of a letter trie (Fredkin, 1960).  Such a structure is well suited for an incremental (letter-by-letter) search because at every point in the trie exactly those continuations leading to legal morphs are available.  With every node which represents a legal morph its continuation classes are stored.  In recognition we can now make use of that structure.  Search starts at the root of the trie.  Each character which is proposed must be matched against the lexicon.  Only if that character is a legal continuation at that node in the trie it may be considered as a possible mapping.</P>
<P ALIGN="JUSTIFY">In recent implementations the lexicon and the two-level rules are collapsed into a single, large transducer, resulting in a very compact and efficient system</P>

<H4>4.1.2 Related Formalisms</H4>

<P ALIGN="JUSTIFY">Black et al. (1987) note the inelegance of Koskenniemi's formalism when describing a phonological (or orthographic) change affecting sequences of characters. They propose a rule format consisting of a surface string (called LHS for <I>left hand side</I>), an operator (</FONT><FONT FACE="Symbol" SIZE=2>&#139; </FONT><FONT FACE="Times" SIZE=3>or </FONT><FONT FACE="Symbol" SIZE=2>횧) </FONT><FONT FACE="Times" SIZE=3>and a lexical string (called RHS for <I>right hand side</I>).  LHS and RHS must be of equal length.  Surface-to-lexical rules (</FONT><FONT FACE="Symbol" SIZE=2>횧</FONT><FONT FACE="Times" SIZE=3>) request that there exists a partition of the surface string where each part is the LHS of a rule and the lexical string the concatenation of the corresponding RHSs.  Lexical-to-surface rules (</FONT><FONT FACE="Symbol" SIZE=2>&#139;</FONT><FONT FACE="Times" SIZE=3>) request that any substring of a lexical string which equals a RHS of a rule must correspond to the surface string of the LHS of the same rule. The rules in (4) are equivalent to rule (1a).</P>

<DIR>
<TABLE>
<TR><TD>(4)</TD><TD>ses =&gt; s+s</TD><TD>ses &lt;= s+s</TD><TD>shes =&gt; sh+s</TD><TD>shes &lt;= sh+s</TD><TD>xes =&gt; x+s </TD><TD>xes &lt;= x+s</TD></TR>
<TR><TD></TD><TD>zes =&gt; z+s</TD><TD>zes &lt;= z+s</TD><TD>ches =&gt; ch+s</TD><TD>ches &lt;= ch+s</TD></TR></DIR>
</TABLE>

<P ALIGN="JUSTIFY">These rules collapse context and substitution into one undistinguishable unit.  Instead of regular expressions only strings are allowed. One drawback is that surface-to-lexical rules may not overlap.  If two different changes happen to occur close to each other they must be captured in a single rule.  Also, long-distance phenomena like vowel harmony cannot be described in this scheme. Ruessink (1989) removes this problem by introducing contexts again.  Both LHS and RHS may come with a left and right context. LHS and RHS may also be of different length, doing away with the null character. Though he gives no account of the complexity of his algorithm one can suspect that it is in general less constrained than the Koskenniemi system. </P>
<P ALIGN="JUSTIFY">An inherently difficult problem for two-level morphology is the root-and-template morphology of Semitic languages. One solution is the introduction of multi-tape formalisms as first described in the seminal paper by Kay (1987). The best-documented current system is SEMHE described in Kiraz (1996, 1997). SEMHE is based on Ruessink&#146;s formalism with the extension of using three input tapes: one each for the root, the vowel pattern and the template.</P>
<P ALIGN="JUSTIFY">Another extension to the formalism is realized in X2MorF  (Trost 1992). In the standard system, morphologically motivated phenomena like umlaut must be described by introducing some pseudosegmental material in the lexical level (see, e.g., 2.4.3.3). In X2MorF an additional morphological context is available to describe such phenomena more naturally.</P>

<H3>4.2 Alternative formalisms</H3>

</B></FONT><FONT FACE="Times" SIZE=3><P ALIGN="JUSTIFY">Alternative proposals for morphological systems have been made in computational linguistics. They include so-called paradigmatic morphology described in Calder (1989) and the DATR system (Evans and Gazdar 1996).  Common to both is the idea to introduce some default mechanism which makes it possible to define a hierarchically structured lexicon where general information is stored at a very high level.  Lower in the hierarchy this information can be overwritten.  Both systems seem to be more concerned with morphosyntax than with morphonology.  It is an open question if these approaches could somehow be combined with two-level rules.  </P>

<H3>4.3 Examples</H3>

<H4>4.3.1 Vowel harmony in Finnish</H4>

<P ALIGN="JUSTIFY">
Finnish has eight vowels. They are classified into <I>back+</I> (a, o, u), <I>back-</I> (&auml;, &ouml;, y) and neutral (e, i). In a Finnish word vowels must be either all back+ or all back<I>-</I> (disregarding neutral vowels).
</P>
<DIR>
<FONT FACE="Courier" SIZE=3><P>V = {a, o, u, &auml;, &ouml;, y, e, i}</P>
<P>Vb = {a, o, u}&#9;Vf = {&auml;, &ouml;, y}</P>
<P>[1] {A:a|O:o|U:u} </FONT><FONT FACE="Symbol" SIZE=3>횧</FONT><FONT FACE="Courier" SIZE=3> =:Vb =:(-Vf)* _;</P>
<P>[2]&#9;{A:&auml;|O:&ouml;|U:y} </FONT><FONT FACE="Symbol" SIZE=3>횧</FONT><FONT FACE="Courier" SIZE=3> {#|=:Vf} =:(-Vb)* _;</P>
<P> #taivas+tA#&#9;&#9;#puhelin+tA#&#9;&#9;#syy+tA#
<BR>|||||||||1|&#9;&#9;||||||||||1|&#9;&#9;||||||2|
<BR>0taivas0ta0&#9;&#9;0puhelin0ta0&#9;&#9;0syy0t&auml;0
</FONT>
</DIR>

<H4>4.3.2 Final devoicing in (spoken) <I>German</I></H4>

<P ALIGN="JUSTIFY">Final devoicing is a morphophonological process where a voiced consonant is devoiced when it occurs in final position in the syllable. Take for example the root /<FONT FACE="SILDoulosIPA-Regular" SIZE=3>ra혱:d</FONT>/ (wheel). The singular form is realized as /ra:t/, while in the plural form /<FONT FACE="SILDoulosIPA-Regular" SIZE=3>re혱:d&aring;</FONT>/ the consonant stays voiced. This phenomenon is not reflected in the orthography where always the voiced consonant is kept.

<DIR>
<FONT FACE="Courier" SIZE=3><P>[1] Cx:Cy </FONT><FONT FACE="Symbol" SIZE=3>&#164;</FONT><FONT FACE="Courier" SIZE=3> _ #:0 ; </P>
<DIR>
<P>&#9;&#9;        where Cx in (b d g) </P>
<P>&#9;&#9;&#9;         Cy in (p t k)  matched;</P>
</DIR>

<P>#l</FONT><FONT FACE="SILManuscriptIPA-Regular" SIZE=3>o혱</FONT><FONT FACE="Courier" SIZE=3>b#&#9;&#9;#r</FONT><FONT FACE="SILManuscriptIPA-Regular" SIZE=3>a혱</FONT><FONT FACE="Courier" SIZE=3>d#&#9;&#9;#we:g#&#9;&#9;#we:g+e#
<BR>||| 1|&#9;&#9;||| 1|&#9;&#9;||| 1|&#9;&#9;||| ||||
<BR>0l</FONT><FONT FACE="SILManuscriptIPA-Regular" SIZE=3>o혱</FONT><FONT FACE="Courier" SIZE=3>p0&#9;&#9;0r</FONT><FONT FACE="SILManuscriptIPA-Regular" SIZE=3>a혱</FONT><FONT FACE="Courier" SIZE=3>t0&#9;&#9;0we:k0&#9;&#9;0we:g0e0&#9;</P>
</DIR>

<P ALIGN="JUSTIFY">The two-level rule realises b, d and g as their voiceless counterparts p, t, and k respectively whenever directly followed by a boundary. </P>
<P ALIGN="JUSTIFY">While the original linguistic motivation behind two-level morphology was SPE and two-level rules were designed to describe morphophonology the mechanism can deal with a much wider range of phenomena.</P>

<H4>4.3.3 Umlaut in <I>German</I></H4>

<P ALIGN="JUSTIFY">German<I> </I>umlaut is used to mark--among other morphosyntactic features&#150;plural. 
</P>
<DIR>
<FONT FACE="Courier" SIZE=3><P>V = {a, &auml;, e, i, o, &ouml;, u, &uuml;, A:a, A:&auml;, O:o, O:&ouml;, U:u, U:&uuml;}
</P>
<P>[1] {A:&auml;|O:&ouml;|U:&uuml;} </FONT><FONT FACE="Symbol" SIZE=3>횧</FONT><FONT FACE="Courier" SIZE=3> _ ?* $:0;</P>
</FONT>
</DIR>
<P ALIGN="JUSTIFY">All stem vowels eligible for umlaut are realized by a vowel underspecified for the back/front distinction at the lexical level. A pseudo-ending $ is used to trigger the rule application, thus realizing the umlaut. In all other cases the default pairing is used. This way a morphological property is described as a morphophonological process. The ?* signifies zero or more occurrences of anything.
</P>
<DIR>
<FONT FACE="Helvetica" SIZE=3><P>Mutter </FONT><FONT FACE="Symbol" SIZE=3>&AElig;</FONT><FONT FACE="Helvetica" SIZE=3> M<U>&uuml;</U>tter&#9;&#9;Garten </FONT><FONT FACE="Symbol" SIZE=3>&AElig;</FONT><FONT FACE="Helvetica" SIZE=3> G<U>&auml;</U>rten&#9;&#9;Hof </FONT><FONT FACE="Symbol" SIZE=3>&AElig;</FONT><FONT FACE="Helvetica" SIZE=3> H<U>&ouml;</U>f<U>e</P>
</B></U></FONT>
<FONT FACE="Courier" SIZE=3><P>#mUtter+$#&#9;&#9;#gArten+$#&#9;&#9;&#9;#hOf+$e#
<BR>||1|||||||&#9;&#9;||1|||||||&#9;&#9;&#9;||1|||||
<BR>0m&uuml;tter000&#9;&#9;0g&auml;rten000&#9;&#9;&#9;0h&ouml;f00e0
</FONT>
</DIR>

<H4>4.3.4 Reduplication and Infixation in <I>Tagalog</I></H4>

In this (simplified) example from <I>Tagalog</I> we shall see how two-level rules can be used to describe reduplication and infixation.
<P>
<DIR>
<FONT FACE="Courier" SIZE=3><P>V = {a, i, u, E}
<BR>C = {p t k b d g m n N s l r w y R}
</DIR>
</FONT>

<P ALIGN="JUSTIFY">The rule for infix  insertion. On the lexical level, the prefix X is assumed. While the X is not realized on the surface it triggers the insertion of &#151;<I>In-</I> between initial consonant and following vowel.</P>

<DIR>
<FONT FACE="Courier" SIZE=3><P>[1] X:0 </FONT><FONT FACE="Symbol" SIZE=3>횧</FONT><FONT FACE="Courier" SIZE=3> _ +:0 C 0:i 0:n V:V;</P>
</FONT><B><FONT FACE="Helvetica" SIZE=3>
<P>pili </FONT><FONT FACE="Symbol" SIZE=3>&AElig;</FONT><FONT FACE="Helvetica" SIZE=3> p<U>in</U>ili &#9;&#9;tahi </FONT><FONT FACE="Symbol" SIZE=3>&AElig;</FONT><FONT FACE="Helvetica" SIZE=3> t<U>in</U>ahi&#9;&#9; </B></P>
</FONT>

<FONT FACE="Courier" SIZE=3><P>#X+p00ili#&#9;&#9;#X+t00ahi#&#9;
<BR>|1||||||||&#9;&#9;|1||||||||&#9;
<BR>000pinili0&#9;&#9;000tinahi0&#9;</FONT>
</DIR>

<P ALIGN="JUSTIFY">The rules for reduplication of the first (open) syllable. The R copies the initial consonant, the E the following consonant. The rule also takes care of the case where the infix is inserted as well:
<P>
<DIR>
</FONT><FONT FACE="Courier" SIZE=3>
[2] R:Cx </FONT><FONT FACE="Symbol" SIZE=3>횧</FONT><FONT FACE="Courier" SIZE=3> _ (0:i 0:n) E:V +:0 :Cx;</P>
<P>&#9;&#9;where Cx in (p p:m t t:n k k:</FONT><FONT FACE="SILManuscriptIPA-Regular" SIZE=3>N</FONT><FONT FACE="Courier" SIZE=3>);</P>
<P>[3] E:Vx </FONT><FONT FACE="Symbol" SIZE=3>횧</FONT><FONT FACE="Courier" SIZE=3> R:C (0:i 0:n) _ +:0 C Vx;</P>
<P>&#9;&#9;where Vx in (a i u);</P>

</FONT><B><FONT FACE="Helvetica" SIZE=3><P>pili</FONT><FONT FACE="Geneva" SIZE=3> </FONT><FONT FACE="Symbol" SIZE=3>&AElig;</FONT><FONT FACE="Geneva" SIZE=3> </FONT><U><FONT FACE="Helvetica" SIZE=3>pi</U>pili &#9;&#9;tahi </FONT><FONT FACE="Symbol" SIZE=3>&AElig;</FONT><FONT FACE="Geneva" SIZE=3> </FONT><U><FONT FACE="Helvetica" SIZE=3>ta</U>tahi&#9;&#9;</P>

</B></FONT><FONT FACE="Courier" SIZE=3>
<P>#RE+pili#&#9;&#9;#RE+tahi#&#9;&#9;
<BR>|23||||||&#9;&#9;|23||||||&#9;&#9;
<BR>0pi0pili0&#9;&#9;0ta0tahi0&#9;

<P>&#9;</FONT><B><FONT FACE="Helvetica" SIZE=3>pili</FONT><FONT FACE="Geneva" SIZE=3> </FONT><FONT FACE="Symbol" SIZE=3>&AElig; </FONT><U><FONT FACE="Helvetica" SIZE=3>pini</U>pili&#9;&#9;tahi </FONT><FONT FACE="Symbol" SIZE=3>&AElig;</FONT><FONT FACE="Geneva" SIZE=3> </FONT><U><FONT FACE="Helvetica" SIZE=3>tina</U>tahi
</P>
</B></FONT><FONT FACE="Courier" SIZE=3>
<P>#X+R00E+pili#&#9;&#9;#X+R00E+tahi#
<BR>|1|2||3||||||&#9;&#9;|1|2||3||||||
<BR>000pini0pili0&#9;&#9;000tina0tahi0
</FONT>
</DIR>

<H3>5 Further reading and relevant resources</H3>

<P ALIGN="JUSTIFY">The most comprehensive book about computational morphology is Richard Sproat&#146;s  book <I>Morphology and Computation</I> (Sproat 1992). It gives a concise introduction into morphology with examples from various languages and a good overview of applications of computational linguistics.  On the methodological side it concentrates on finite-state morphology omitting other paradigms. <I>Computational Morphology </I>(Black et al. 1992) gives a more in-depth description of finite-state morphology but concentrates exclusively on English. An excellent overview of morphology with examples from diverse languages is found in the <I>Handbook of Morphology </I>(Spencer and Zwicky 1998). </P>
<P ALIGN="JUSTIFY">To get some hands-on experience with morpological processing connect to 
<a href="http://www.rxrc.xerox.com/research/mltt/">RXRC Europe</A> 
and <a href="http://www.lingsoft.fi/">Lingsoft</A>. A free downloadable 
version of a two-level morphology is available from 
<a href="http://www.sil.org/pckimmo">SIL</A>.

<H3>References</H3>

<OL>
<LI>Beesley K.R. and Karttunen L. 2000. <I>Finite-State Morphology: Xerox Tools and Techniques</I>. Cambridge University Press, Cambridge.</LI>
<LI>Black A.W., Ritchie G.D., Pulman S.G., Russell G.J. 1987. Formalisms for Morphographemic  Description,  Proc. 3rd European ACL, pp11-18, Kopenhagen.</LI>
<LI>Calder J. 1989. Paradigmatic Morphology, Proc. 4th European ACL, pp58-65, Manchester.</LI>
<LI>Chomsky N. and Halle M.: <I>The Sound Pattern of English</I>, Harper &amp; Row, Hagerstown/London/New York, 1968.</LI>
<LI>Evans R., Gazdar G. 1996 DATR,: A Language for Lexical Knowledge Representation, <I>Computational Linguistics </I>22(2)167-216.</LI>
<LI>Fredkin E. 1960. Trie Memory, <I>Communications ACM</I> 3, pp490-499.</LI>
<LI>Fromkin V., Rodman R. 1983. An Introduction to Language. Holt, Rinehart &amp; Winston, New York.</LI>
<LI>Kay M. 1987. Noncatenative finite-state morphology, in Proc. of the 3rd Conference of the European Chapter of the ACL, Copenhagen, Denmark, pp2-10.</LI>
<LI>Kiparsky P. 1987. The Phonology of Reduplication. Manuscript. Stanford University.</LI>
<LI>Kiraz G.A. 1996. SEMHE: A Generalized Two-Level System, in Proc. of 34th Annual Meeting of the Association for Computational Linguistics, Morgan Kaufmann, Los Altos, pp159-166. </LI>
<LI>Kiraz G.A. 1997. Compiling Regular Formalisms with Rule Features into Finite-State Automata, in Cohen P.R., Wahlster W.(eds.), Proc. 35th Annual Meeting of the Association for Computational Linguistics, Morgan Kaufmann, Los Altos, pp329-336. </LI>
<LI>Koskenniemi  K. 1984.  A  General  Computational  Model  for   Word-Form Recognition   and   Production,   Proceedings 10th International Conference on Computational Linguistics,  Stanford, CA.</LI>
<LI>Nash D. 1980. Topics in Warlpiri Grammar, PhD Thesis, MIT, Cambridge, MA.</LI>
<LI>Nida E. 1949. Morphology: The Descriptive Analysis of Words. University of Michigan Press.</LI>
<LI>Ritchie G.D., Russel G.J., Black A.W., Pulman S.G. 1991. <I>Computational Morphology</I>, MIT Press, Cambridge.</LI>
<LI>Rose S. 2000. Triple Take: Tigre and the case of internal reduplication. <I>Studies in Afroasiatic Grammar</I>.</LI>
<LI>Ruessink H. 1989. Two-Level Formalisms, Working Papers in Natural Language Processing 5, Rijksuniversiteit Utrecht.</LI>
<LI>Spencer A., Zwicky A. (eds.) 1998. The Handbook of Morphology, Basil Blackwell, Oxford.</LI>
<LI>Sproat R.W<I>., </I>1992.<I> Morphology and Computation</I>, MIT Press, Cambridge, MA.</LI>
<LI>Trost H. 1992. X2MORPH: A Morphological Component Based on Augmented Two-Level Morphology, in Proc. 12<SUP>th</SUP> Intenational Joint Conference on Artificial Intelligence, Sydney, Morgan Kaufmann, San Mateo, pp.1024-1030.</LI>
<LI>Trost H. 1993. Coping With Derivation in a Morphological Component, in Proc. 6<SUP>th</SUP> Conference of the European Chapter of the Association for Computational Linguistics, Utrecht, pp368-376.</LI>
</OL>
</BODY>
</HTML>

