<!-- <DOCUMENT>
	<FILE>
		2956742436.html
	</FILE>
	<URL>
		http://www.mirror.ac.uk/sites/ubmail.ubalt.edu/~harsham/Business-stat/opre504.htm#rmeanmodemed
	</URL>
	<TITLE>
		Dr. Arsham's Statistics Site
	</TITLE>
	<GENRE>
		articles
	</GENRE>
	<PLAINTEXT>
 Dr. Arsham's Statistics Site Statistical Thinking for Managerial Decision Making Asia-Pacific Mirror Site Europe Mirror Site Middle East Mirror Site South America Mirror Site UK Mirror Site USA Site This Web site is a course in statistics appreciation; i.e., acquiring a feeling for the statistical way of thinking. It is an introductory course in statistics that is designed to provide you with the basic concepts and methods of statistical analysis for decision making under uncertainties. Materials in this Web site are tailored to meet your needs in making good decisions by fostering statistical thinking. The cardinal objective for this Web site is to increase the extent to which statistical thinking is merged with managerial thinking for decision making under uncertainty. Professor Hossein Arsham MENU Chapter 1: Towards Statistical Thinking for Decision Making Chapter 2: Descriptive Sampling Data Analysis Chapter 3: Probability for Statistical Inference and Modeling Chapter 4: Necessary Conditions for Statistical Decision Making Chapter 5: Estimators and Their Qualities Chapter 6: Hypothesis Testing: Rejecting a Claim Chapter 7: Hypotheses Testing for Means and Proportions Chapter 8: Tests for Statistical Equality of Two or More Populations Chapter 9: Applications of the Chi-square Statistic Chapter 10: Regression Modeling and Analysis Chapter 11: Unified Views of Statistical Decision Technologies Chapter 12: Visualization of Statistics Chapter 13: Index Numbers with Applications Companion Sites: JavaScript E-labs Learning Objects , Europe Mirror Site Collection . Excel For Introductory Statistical Analysis , Europe Mirror Site . Frequently Asked Questions: A Statistical Why? List (Word.Doc) Statistical Data Analysis , Asia-Pacific Mirror Site , Europe Mirror Site . Time Series Analysis and Business Forecasting , Europe Mirror Site . Computers and Computational Statistics , Europe Mirror Site . Questionnaire Design and Surveys Sampling , Europe Mirror Site . Probabilistic Modeling , Europe Mirror Site , Versión en Español . Systems Simulation , Europe Mirror Site , South Africa Mirror Site . Probability and Statistics Resources , Europe Mirror Site . To search the site , try E dit | F ind in page [Ctrl + f]. Enter a word or phrase in the dialogue box, e.g. " parameter" or " probability" . If the first appearance of the word/phrase is not what you are looking for, try F ind Next . Towards Statistical Thinking for Decision Making Introduction The Birth of Probability and Statistics Statistical Modeling for Decision-Making under Uncertainties Statistical Decision-Making Process What is Business Statistics? Common Statistical Terminology with Applications Descriptive Sampling Data Analysis Greek Letters Commonly Used in Statistics Type of Data and Levels of Measurement Why Statistical Sampling? Sampling Methods Representative of a Sample: Measures of Central Tendency Selecting Among the Mean, Median, and Mode Specialized Averages: The Geometric & Harmonic Means Histogramming: Checking for Homogeneity of Population How to Construct a BoxPlot Measuring the Quality of a Sample Selecting Among the Measures of Dispersion Shape of a Distribution Function: The Skewness-Kurtosis Chart A Numerical Example &amp; Discussions The Two Statistical Representations of a Population Empirical (i.e., observed) Cumulative Distribution Function Probability for Statistical Inference and Modeling Introduction Probability, Chance, Likelihood, and Odds How to Assign Probabilities General Laws of Probability Mutually Exclusive versus Independent Events What Is so Important About the Normal Distributions? What Is a Sampling Distribution? What Is The Central Limit Theorem? What Is "Degrees of Freedom"? Applications of and Conditions for Using Statistical Tables Beta Density Function Binomial Probability Function Chi-square Density Function Exponential Density Function F-Density Function Gamma Density Function Log-normal Density Function Multinomial Probability Function Normal Density Function Poisson Probability Function Student T-Density Function Triangular Density Function Uniform Density Function Necessary Conditions for Statistical Decision Making Introduction Measure of Surprise for Outlier Detection Homogeneous Population (Don't mix apples and oranges) Test for Randomness Test for Normality Estimators and Their Qualities Introduction Qualities of a Good Estimator Statistics with Confidence What Is the Margin of Error? Bias Reduction Techniques: Bootstrapping and Jackknifing Prediction Intervals What Is a Standard Error? Sample Size Determination Revising the Expected Value and the Variance Subjective Assessment of Several Estimates Hypothesis Testing: Rejecting a Claim Introduction Managing the Producer's or the Consumer's Risk Classical Approach to Testing Hypotheses The Meaning and Interpretation of P-values (what the data say) Blending the Classical and the P-value Based Approaches in Test of Hypotheses Bonferroni Method for Multiple P-Values Procedure Power of a Test and the Size Effect Parametric vs. Non-Parametric vs. Distribution-free Tests Hypotheses Testing for Means and Proportions Introduction Single Population t-Test Two Independent Populations When Should We Pool Variance Estimates? Non-parametric Multiple Comparison Procedures The Before-and-After Test ANOVA for Normal but Condensed Data Sets ANOVA for Dependent Populations Tests for Statistical Equality of Two or More Populations Introduction Equality of Two Normal Populations Testing a Shift in Normal Populations Analysis of Variance (ANOVA) Equality of Proportions in Several Populations Distribution-free Equality of Two Populations Comparison of Two Random Variables Applications of the Chi-square Statistic Introduction Test for Crosstable Relationship Identical Populations Test for Crosstable Data Test for Equality of Several Population Proportions Test for Equality of Several Population Medians Goodness-of-Fit Test for Probability Mass Functions Compatibility of Multi-Counts Necessary Conditions in Applying the Above Tests Testing the Variance: Is the Quality that Good? Testing the Equality of Multi-Variances Correlation Coefficients Testing Regression Modeling and Analysis Introduction Regression Modeling Selection Process Covariance and Correlation Pearson, Spearman, and Point-biserial Correlations Correlation, and Level of Significance Independence vs. Correlated How to Compare Two Correlation Coefficients Planning, Development, and Maintenance of a Model Conditions and the Check-list for Linear Models Analysis of Covariance: Comparing the Slopes Residential Properties Appraisal Application Unified Views of Statistical Decision Technologies Introduction Hypothesis Testing with Confidence Regression Analysis, ANOVA, and Chi-square Test Regression Analysis, ANOVA, T-test, and Coefficient of Determination Relationships among Distributions and Unification of Statistical Tables Visualization of Statistics: Analytic-Geometry & Statistics Introduction Mean and Median Geometric Mean Variance, Covariance, and Correlation Coefficient Index Numbers with Applications Introduction The Geometric Mean Ratio Indexes Composite Index Numbers Variation Index as a Quality Indicator Labor Force Unemployment Index Seasonal Index and Deseasonalizing Data Statistical Technique and Index Numbers Introduction to Statistical Thinking for Decision Making This site builds up the basic ideas of business statistics systematically and correctly. It is a combination of lectures and computer-based practice, joining theory firmly with practice. It introduces techniques for summarizing and presenting data, estimation, confidence intervals and hypothesis testing. The presentation focuses more on understanding of key concepts and statistical thinking, and less on formulas and calculations, which can now be done on small computers through user-friendly Statistical JavaScript Applets , etc. Today's good decisions are driven by data. In all aspects of our lives, and importantly in the business context, an amazing diversity of data is available for inspection and analytical insight. Business managers and professionals are increasingly required to justify decisions on the basis of data . They need statistical model-based decision support systems. Statistical skills enable them to intelligently collect, analyze and interpret data relevant to their decision-making. Statistical concepts and statistical thinking enable them to: solve problems in a diversity of contexts. add substance to decisions. reduce guesswork. This Web site is a course in statistics appreciation; i.e., acquiring a feel for the statistical way of thinking. It hopes to make sound statistical thinking understandable in business terms. An introductory course in statistics, it is designed to provide you with the basic concepts and methods of statistical analysis for processes and products. Materials in this Web site are tailored to help you make better decisions and to get you thinking statistically. A cardinal objective for this Web site is to embed statistical thinking into managers , who must often decide with little information. In competitive environment, business managers must design quality into products, and into the processes of making the products. They must facilitate a process of never-ending improvement at all stages of manufacturing and service. This is a strategy that employs statistical methods, particularly statistically designed experiments , and produces processes that provide high yield and products that seldom fail. Moreover, it facilitates development of robust products that are insensitive to changes in the environment and internal component variation. Carefully planned statistical studies remove hindrances to high quality and productivity at every stage of production. This saves time and money. It is well recognized that quality must be engineered into products as early as possible in the design process. One must know how to use carefully planned, cost-effective statistical experiments to improve, optimize and make robust products and processes. Business Statistics is a science assisting you to make business decisions under uncertainties based on some numerical and measurable scales. Decision making processes must be based on data, not on personal opinion nor on belief. The Devil is in the Deviations: Variation is inevitable in life! Every process, every measurement, every sample has variation. Managers need to understand variation for two key reasons. First, so that they can lead others to apply statistical thinking in day-to-day activities and secondly, to apply the concept for the purpose of continuous improvement. This course will provide you with hands-on experience to promote the use of statistical thinking and techniques to apply them to make educated decisions, whenever you encounter variation in business data. You will learn techniques to intelligently assess and manage the risks inherent in decision-making. Therefore, remember that: Just like weather, if you cannot control something, you should learn how to measure and analyze it, in order to predict it, effectively . If you have taken statistics before, and have a feeling of inability to grasp concepts, it may be largely due to your former non-statistician instructors teaching statistics. Their deficiencies lead students to develop phobias for the sweet science of statistics . In this respect, Professor Herman Chernoff (1996) made the following remark: "Since everybody in the world thinks he can teach statistics even though he does not know any, I shall put myself in the position of teaching biology even though I do not know any" Inadequate statistical teaching during university education leads even after graduation, to one or a combination of the following scenarios: In general, people do not like statistics and therefore they try to avoid it. There is a pressure to produce scientific papers, however often confronted with "I need something quick." At many institutes in the world, there are only a few (mostly 1) statisticians, if any at all. This means that these people are extremely busy. As a result, they tend to advise simple and easy to apply techniques, or they will have to do it themselves. Communication between a statistician and decision-maker can be difficult. One speaks in statistical jargon; the other understands the monetary or utilitarian benefit of using the statistician's recommendations. Plugging numbers into the formulas and crunching them have no value by themselves. You should continue to put effort into the concepts and concentrate on interpreting the results. Even when you solve a small size problem by hand, I would like you to use the available computer software and Web-based computation to do the dirty work for you. You must be able to read the logical secret in any formulas not memorize them. For example, in computing the variance, consider its formula. Instead of memorizing, you should start with some why: i. Why do we square the deviations from the mean. Because, if we add up all deviations, we get always zero value. So, to deal with this problem, we square the deviations. Why not raise to the power of four (three will not work)? Squaring does the trick; why should we make life more complicated than it is? Notice also that squaring also magnifies the deviations; therefore it works to our advantage to measure the quality of the data. ii. Why is there a summation notation in the formula. To add up the squared deviation of each data point to compute the total sum of squared deviations. iii. Why do we divide the sum of squares by n-1. The amount of deviation should reflect also how large the sample is; so we must bring in the sample size. That is, in general, larger sample sizes have larger sum of square deviation from the mean. Why n-1 not n? The reason for n-1 is that when you divide by n-1, the sample's variance provides an estimated variance much closer to the population variance, than when you divide by n. You note that for large sample size n (say over 30), it really does not matter whether it is divided by n or n-1. The results are almost the same, and they are acceptable. The factor n-1 is what we consider as the "degrees of freedom". This example shows how to question statistical formulas, rather than memorizing them. In fact, when you try to understand the formulas, you do not need to remember them, they are part of your brain connectivity. Clear thinking is always more important than the ability to do arithmetic . When you look at a statistical formula, the formula should talk to you, as when a musician looks at a piece of musical-notes, he/she hears the music. computer-assisted learning: The computer-assisted learning provides you a "hands-on" experience which will enhance your understanding of the concepts and techniques covered in this site. Java, once an esoteric programming language for animating Web pages, is now a full-fledged platform for building JavaScript E-labs' learning objects with useful applications. As you used to do experiments in physics labs to learn physics, computer-assisted learning enables you to use any online interactive tool available on the Internet to perform experiments. The purpose is the same; i.e., to understand statistical concepts by using statistical applets which are entertaining and educating. The appearance of computer software, JavaScript Applets, Statistical Demonstration Applets, and Online Computation are the most important events in the process of teaching and learning concepts in model-based, statistical decision making courses. These e-lab Technologies allow you to construct numerical examples to understand the concepts, and to find their significance for yourself. Unfortunately, most classroom courses are not learning systems. The way the instructors attempt to help their students acquire skills and knowledge has absolutely nothing to do with the way students actually learn. Many instructors rely on lectures and tests, and memorization. All too often, they rely on "telling." No one remembers much that's taught by telling, and what's told doesn't translate into usable skills. Certainly, we learn by doing, failing, and practicing until we do it right. The computer assisted learning serves this purpose. A course in appreciation of statistical thinking gives business professionals an edge. Professionals with strong quantitative skills are in demand. This phenomenon will grow as the impetus for data-based decisions strengthens and the amount and availability of data increases. The statistical toolkit can be developed and enhanced at all stages of a career. Decision making process under uncertainty is largely based on application of statistics for probability assessment of uncontrollable events (or factors), as well as risk assessment of your decision. The main objective for this course is to learn statistical thinking; to emphasize more on concepts, and less theory and fewer recipes, and finally to foster active learning using the useful and interesting Web-sites. It is already a known fact that "Statistical thinking will one day be as necessary for efficient citizenship as the ability to read and write." So, let's be ahead of our time. Further Readings: Chernoff H., A Conversation With Herman Chernoff, Statistical Science , Vol. 11, No. 4, 335-350, 1996. Churchman C., The Design of Inquiring Systems , Basic Books, New York, 1971. Early in the book he stated that knowledge could be considered as a collection of information, or as an activity, or as a potential. He also noted that knowledge resides in the user and not in the collection. Rustagi M., et al. (eds.), Recent Advances in Statistics: Papers in Honor of Herman Chernoff on His Sixtieth Birthday , Academic Press, 1983. The Birth of Probability and Statistics The original idea of "statistics" was the collection of information about and for the "state". The word statistics derives directly, not from any classical Greek or Latin roots, but from the Italian word for state . The birth of statistics occurred in mid-17 th century. A commoner, named John Graunt, who was a native of London, began reviewing a weekly church publication issued by the local parish clerk that listed the number of births, christenings, and deaths in each parish. These so called Bills of Mortality also listed the causes of death. Graunt who was a shopkeeper organized this data in the form we call descriptive statistics, which was published as Natural and Political Observations Made upon the Bills of Mortality . Shortly thereafter he was elected as a member of Royal Society. Thus, statistics has to borrow some concepts from sociology, such as the concept of Population . It has been argued that since statistics usually involves the study of human behavior, it cannot claim the precision of the physical sciences. Probability has much longer history. Probability is derived from the verb to probe meaning to "find out" what is not too easily accessible or understandable. The word "proof" has the same origin that provides necessary details to understand what is claimed to be true. Probability originated from the study of games of chance and gambling during the 16 th century. Probability theory was a branch of mathematics studied by Blaise Pascal and Pierre de Fermat in the seventeenth century. Currently in 21 st century, probabilistic modeling is used to control the flow of traffic through a highway system, a telephone interchange, or a computer processor; find the genetic makeup of individuals or populations; quality control; insurance; investment; and other sectors of business and industry. New and ever growing diverse fields of human activities are using statistics; however, it seems that this field itself remains obscure to the public. Professor Bradley Efron expressed this fact nicely: During the 20 th Century statistical thinking and methodology have become the scientific framework for literally dozens of fields including education, agriculture, economics, biology, and medicine, and with increasing influence recently on the hard sciences such as astronomy, geology, and physics. In other words, we have grown from a small obscure field into a big obscure field. Further Readings: Daston L., Classical Probability in the Enlightenment , Princeton University Press, 1988. The book points out that early Enlightenment thinkers could not face uncertainty. A mechanistic, deterministic machine, was the Enlightenment view of the world. Gillies D., Philosophical Theories of Probability , Routledge, 2000. Covers the classical, logical, subjective, frequency, and propensity views. Hacking I., The Emergence of Probability , Cambridge University Press, London, 1975. A philosophical study of early ideas about probability, induction and statistical inference. Hald A., A History of Probability and Statistics and Their Applications before 1750 , Wiley, 2003. Peters W., Counting for Something: Statistical Principles and Personalities , Springer, New York, 1987. It teaches the principles of applied economic and social statistics in a historical context. Featured topics include public opinion polls, industrial quality control, factor analysis, Bayesian methods, program evaluation, non-parametric and robust methods, and exploratory data analysis. Porter T., The Rise of Statistical Thinking , 1820-1900, Princeton University Press, 1986. The author states that statistics has become known in the twentieth century as the mathematical tool for analyzing experimental and observational data. Enshrined by public policy as the only reliable basis for judgments as the efficacy of medical procedures or the safety of chemicals, and adopted by business for such uses as industrial quality control, it is evidently among the products of science whose influence on public and private life has been most pervasive. Statistical analysis has also come to be seen in many scientific disciplines as indispensable for drawing reliable conclusions from empirical (i.e., observed) results. This new field of mathematics found so extensive a domain of applications. Stigler S., The History of Statistics: The Measurement of Uncertainty Before 1900 , U. of Chicago Press, 1990. It covers the people, ideas, and events underlying the birth and development of early statistics. Tankard J., The Statistical Pioneers , Schenkman Books, New York, 1984. This work provides the detailed lives and times of theorists whose work continues to shape much of the modern statistics. Statistical Modeling for Decision-Making under Uncertainties: From Data to the Instrumental Knowledge In this diverse world of ours, no two things are exactly the same. A statistician is interested in both the differences and the similarities ; i.e., both departures and patterns. The actuarial tables published by insurance companies reflect their statistical analysis of the average life expectancy of men and women at any given age. From these numbers, the insurance companies then calculate the appropriate premiums for a particular individual to purchase a given amount of insurance. Exploratory analysis of data makes use of numerical and graphical techniques to study patterns and departures from patterns. The widely used descriptive statistical techniques are: Frequency Distribution ; Histograms; Boxplot; Scattergrams and Error Bar plots; and diagnostic plots. In examining distribution of data, you should be able to detect important characteristics, such as shape, location, variability, and unusual values. From careful observations of patterns in data, you can generate conjectures about relationships among variables. The notion of how one variable may be associated with another permeates almost all of statistics, from simple comparisons of proportions through linear regression. The difference between association and causation must accompany this conceptual development. Data must be collected according to a well-developed plan if valid information on a conjecture is to be obtained. The plan must identify important variables related to the conjecture, and specify how they are to be measured. From the data collection plan, a statistical model can be formulated from which inferences can be drawn. As an example of statistical modeling with managerial implications , such as "what-if" analysis , consider regression analysis. Regression analysis is a powerful technique for studying relationship between dependent variables (i.e., output, performance measure) and independent variables (i.e., inputs, factors, decision variables). Summarizing relationships among the variables by the most appropriate equation (i.e., modeling) allows us to predict or identify the most influential factors and study their impacts on the output for any changes in their current values. Frequently, for example the marketing managers are faced with the question, What Sample Size Do I Need? This is an important and common statistical decision, which should be given due consideration, since an inadequate sample size invariably leads to wasted resources. The sample size determination section provides a practical solution to this risky decision . Statistical models are currently used in various fields of business and science. However, the terminology differs from field to field . For example, the fitting of models to data, called calibration, history matching, and data assimilation, are all synonymous with parameter estimation. Your organization database contains a wealth of information, yet the decision technology group members tap a fraction of it. Employees waste time scouring multiple sources for a database. The decision-makers are frustrated because they cannot get business-critical data exactly when they need it. Therefore, too many decisions are based on guesswork, not facts . Many opportunities are also missed, if they are even noticed at all. Knowledge is what we know. Information is the communication of knowledge. In every knowledge exchange, there is a sender and a receiver. The sender makes common what is private, does the informing, the communicating. Information can be classified as explicit and tacit forms. The explicit information can be explained in structured form, while tacit information is inconsistent and fuzzy to explain. Data is known to be crude information and not knowledge by itself. The sequence from data to knowledge is: from Data to Information, from Information to Facts, and finally, from Facts to Knowledge . Data becomes information, when it becomes relevant to your decision problem. Information becomes fact, when the data can support it. Facts are what the data reveals. However the decisive instrumental knowledge is expressed together with some statistical degree of confidence . Fact becomes knowledge, when it is used in the successful completion of a decision process. Knowledge needs wisdom. Wisdom is the power to put our time and our knowledge to the proper use. Once you have a massive amount of facts integrated as knowledge, then your mind will be superhuman in the same sense that mankind with writing is superhuman compared to mankind before writing. The following figure illustrates the statistical thinking process based on data in constructing statistical models for decision making under uncertainties. The above figure depicts the fact that as the exactness of a statistical model increases, the level of improvements in decision-making increases. That's why we need Business Statistics. Statistics arose from the need to place knowledge on a systematic evidence base. This required a study of the laws of probability, the development of measures of data properties and relationships, and so on. Statistical inference aims at determining whether any statistical significance can be attached that results after due allowance is made for any random variation as a source of error. Intelligent and critical inferences cannot be made by those who do not understand the purpose, the conditions, and applicability of the various techniques for judging significance. The purpose of statistical thinking is to get acquainted with the statistical techniques, to be able to execute procedures using available JavaScript Applets, and to be conscious of the conditions and limitations of various techniques. Statistical Decision-Making Process Unlike the deterministic decision-making process, such as linear optimization by solving systems of equations and in decision making under pure uncertainty , the variables are often more numerous and more difficult to measure and control. However, the steps are the same. They are: Simplification Building a decision model Testing the model Using the model to find the solution: It is a simplified representation of the actual situation It need not be complete or exact in all respects It concentrates on the most essential relationships and ignores the less essential ones. It is more easily understood than the empirical (i.e., observed) situation, and hence permits the problem to be solved more readily with minimum time and effort. It can be used again and again for similar problems or can be modified. Fortunately the probabilistic and statistical methods for analysis and decision making under uncertainty are more numerous and powerful today than ever before. The computer makes possible many practical applications. A few examples of business applications are the following: An auditor can use random sampling techniques to audit the accounts receivable for clients. A plant manager can use statistical quality control techniques to assure the quality of his production with a minimum of testing or inspection. A financial analyst may use regression and correlation to help understand the relationship of a financial ratio to a set of other variables in business. A market researcher may use test of significace to accept or reject the hypotheses about a group of buyers to which the firm wishes to sell a particular product. A sales manager may use statistical techniques to forecast sales for the coming year. Questions Concerning Statistical the Decision-Making Process: Objectives or Hypotheses: What are the objectives of the study or the questions to be answered? What is the population to which the investigators intend to refer their findings? Statistical Design: Is the study a planned experiment (i.e., primary data ), or an analysis of records ( i.e., secondary data )? How is the sample to be selected? Are there possible sources of selection, which would make the sample atypical or non-representative? If so, what provision is to be made to deal with this bias? What is the nature of the control group, standard of comparison, or cost? Remember that statistical modeling means reflections before actions . Observations: Are there clear definition of variables, including classifications, measurements (and/or counting), and the outcomes? Is the method of classification or of measurement consistent for all the subjects and relevant to Item No. 1.? Are there possible biased in measurement (and/or counting) and, if so, what provisions must be made to deal with them? Are the observations reliable and replicable (to defend your finding)? Analysis: Are the data sufficient and worthy of statistical analysis? If so, are the necessary conditions of the methods of statistical analysis appropriate to the source and nature of the data? The analysis must be correctly performed and interpreted. Conclusions: Which conclusions are justifiable by the findings? Which are not? Are the conclusions relevant to the questions posed in Item No. 1? Representation of Findings: The finding must be represented clearly, objectively, in sufficient but non-technical terms and detail to enable the decision-maker (e.g., a manager) to understand and judge them for himself? Is the finding internally consistent; i.e., do the numbers added up properly? Can the different representation be reconciled? Managerial Summary: When your findings and recommendation(s) are not clearly put, or framed in an appropriate manner understandable by the decision maker, then the decision maker does not feel convinced of the findings and therefore will not implement any of the recommendations. You have wasted the time, money, etc. for nothing. Further Readings: Corfield D., and J. Williamson, Foundations of Bayesianism , Kluwer Academic Publishers, 2001. Contains Logic, Mathematics, Decision Theory, and Criticisms of Bayesianism. Lapin L., Statistics for Modern Business Decisions , Harcourt Brace Jovanovich, 1987. Pratt J., H. Raiffa, and R. Schlaifer, Introduction to Statistical Decision Theory , The MIT Press, 1994. What is Business Statistics? The main objective of Business Statistics is to make inferences (e.g., prediction, making decisions) about certain characteristics of a population based on information contained in a random sample from the entire population. The condition for randomness is essential to make sure the sample is representative of the population . Business Statistics is the science of good' decision making in the face of uncertainty and is used in many disciplines, such as financial analysis, econometrics, auditing, production and operations, and marketing research. It provides knowledge and skills to interpret and use statistical techniques in a variety of business applications. A typical Business Statistics course is intended for business majors, and covers statistical study, descriptive statistics (collection, description, analysis, and summary of data), probability, and the binomial and normal distributions, test of hypotheses and confidence intervals, linear regression, and correlation. Statistics is a science of making decisions with respect to the characteristics of a group of persons or objects on the basis of numerical information obtained from a randomly selected sample of the group. Statisticians refer to this numerical observation as realization of a random sample. However, notice that one cannot see a random sample. A random sample is only a sample of a finite outcomes of a random process. At the planning stage of a statistical investigation, the question of sample size (n) is critical. For example, sample size for sampling from a finite population of size N, is set at: N ½ +1, rounded up to the nearest integer. Clearly, a larger sample provides more relevant information, and as a result a more accurate estimation and better statistical judgement regarding test of hypotheses. Activities Associated with the General Statistical Thinking Click on the image to enlarge it and THEN print it The above figure illustrates the idea of statistical inference from a random sample about the population . It also provides estimation for the population's parameters ; namely the expected value µ x , the standard deviation, and the cumulative distribution function (cdf) F x , s and their corresponding sample statistics, mean , sample standard deviation S x , and empirical (i.e., observed) cumulative distribution function (cdf), respectively. The major task of statistics is to study the characteristics of populations whether these populations are people, objects, or collections of information. For two major reasons, it is often impossible to study an entire population: The process would be too expensive or too time-consuming. The process would be destructive. In either case, we would resort to looking at a sample chosen from the population and trying to infer information about the entire population by only examining the smaller sample. Very often the numbers which interest us most about the population are the mean m and standard deviation s . Any number -- like the mean or standard deviation -- which is calculated from an entire population, is called a Parameter . If the very same numbers are derived only from the data of a sample, then the resulting numbers are called Statistics . Frequently, Greek letters represent parameters and Latin letters represent statistics (as shown in the above Figure). Statistics is a tool that enables us to impose order on the disorganized cacophony of the real world of modern society. The business world has grown both in size and competition. Corporate executive must take risk in business , hence the need for business statistics. Business statistics has grown with the art of constructing charts and tables! It is a science of basing decisions on numerical data in the face of uncertainty. Business statistics is a scientific approach to decision making under risk. In practicing business statistics, we search for an insight, not the solution. Our search is for the one solution that meets all the business's needs with the lowest level of risk. Business statistics can take a normal business situation, and with the proper data gathering, analysis, and re-search for a solution, turn it into an opportunity. While business statistics cannot replace the knowledge and experience of the decision maker, it is a valuable tool that the manager can employ to assist in the decision making process in order to reduce the inherent risk. Business Statistics provides justifiable answers to the following concerns for every consumer and producer: What is your or your customer's, Expectation of the product/service you sell or that your customer buys? That is, what is a good estimate for m ? Given the information about your, or your customer's, expectation, what is the Quality of the product/service you sell or that you customer buys. That is, what is a good estimate for s ? Given the information about your or your customer's expectation, and the quality of the product/service you sell or you customer buy, how does the product/service compare with other existing similar types? That is, comparing several m 's, and several s 's . Common Statistical Terminology with Applications Like all profession, also statisticians have their own keywords and phrases to ease a precise communication. However, one must interpret the results of any decision making in a language that is easy for the decision-maker to understand. Otherwise, he/she does not believe in what you recommend, and therefore does not go into the implementation phase. This lack of communication between statisticians and the managers is the major roadblock for using statistics. Population: A population is any entire collection of people, animals, plants or things on which we may collect data. It is the entire group of interest, which we wish to describe or about which we wish to draw conclusions. In the above figure the life of the light bulbs manufactured say by GE, is the concerned population. Qualitative and Quantitative Variables: Any object or event, which can vary in successive observations either in quantity or quality is called a "variable." Variables are classified accordingly as quantitative or qualitative. A qualitative variable, unlike a quantitative variable does not vary in magnitude in successive observations. The values of quantitative and qualitative variables are called "Variates" and "Attributes", respectively. Variable: A characteristic or phenomenon, which may take different values, such as weight, gender since they are different from individual to individual. Randomness: Randomness means unpredictability. The fascinating fact about inferential statistics is that, although each random observation may not be predictable when taken alone, collectively they follow a predictable pattern called its distribution function. For example, it is a fact that the distribution of a sample average follows a normal distribution for sample size over 30. In other words, an extreme value of the sample mean is less likely than an extreme value of a few raw data. Sample: A subset of a population or universe. An Experiment: An experiment is a process whose outcome is not known in advance with certainty. Statistical Experiment: An experiment in general is an operation in which one chooses the values of some variables and measures the values of other variables, as in physics. A statistical experiment, in contrast is an operation in which one take a random sample from a population and infers the values of some variables. For example, in a survey, we "survey" i.e. "look at" the situation without aiming to change it, such as in a survey of political opinions. A random sample from the relevant population provides information about the voting intentions. In order to make any generalization about a population, a random sample from the entire population; that is meant to be representative of the population, is often studied. For each population, there are many possible samples. A sample statistic gives information about a corresponding population parameter . For example, the sample mean for a set of data would give information about the overall population mean m . It is important that the investigator carefully and completely defines the population before collecting the sample, including a description of the members to be included. Example: The population for a study of infant health might be all children born in the U.S.A. in the 1980's. The sample might be all babies born on 7 th of May in any of the years. An experiment is any process or study which results in the collection of data, the outcome of which is unknown. In statistics, the term is usually restricted to situations in which the researcher has control over some of the conditions under which the experiment takes place. Example: Before introducing a new drug treatment to reduce high blood pressure, the manufacturer carries out an experiment to compare the effectiveness of the new drug with that of one currently prescribed. Newly diagnosed subjects are recruited from a group of local general practices. Half of them are chosen at random to receive the new drug, the remainder receives the present one. So, the researcher has control over the subjects recruited and the way in which they are allocated to treatment. Design of experiments is a key tool for increasing the rate of acquiring new knowledge. Knowledge in turn can be used to gain competitive advantage, shorten the product development cycle, and produce new products and processes which will meet and exceed your customer's expectations. Primary data and Secondary data sets: If the data are from a planned experiment relevant to the objective(s) of the statistical investigation, collected by the analyst, it is called a Primary Data set. However, if some condensed records are given to the analyst, it is called a Secondary Data set. Random Variable: A random variable is a real function (yes, it is called " variable", but in reality it is a function) that assigns a numerical value to each simple event. For example, in sampling for quality control an item could be defective or non-defective, therefore, one may assign X=1, and X = 0 for a defective and non-defective item, respectively. You may assign any other two distinct real numbers, as you wish; however, non-negative integer random variables are easy to work with. Random variables are needed since one cannot do arithmetic operations on words; the random variable enables us to compute statistics, such as average and variance. Any random variable has a distribution of probabilities associated with it. Probability: Probability (i.e., probing for the unknown) is the tool used for anticipating what the distribution of data should look like under a given model. Random phenomena are not haphazard: they display an order that emerges only in the long run and is described by a distribution . The mathematical description of variation is central to statistics. The probability required for statistical inference is not primarily axiomatic or combinatorial, but is oriented toward describing data distributions. Sampling Unit: A unit is a person, animal, plant or thing which is actually studied by a researcher; the basic objects upon which the study or experiment is executed. For example, a person; a sample of soil; a pot of seedlings; a zip code area; a doctor's practice. Parameter: A parameter is an unknown value, and therefore it has to be estimated. Parameters are used to represent a certain population characteristic. For example, the population mean m is a parameter that is often used to indicate the average value of a quantity. Within a population, a parameter is a fixed value that does not vary. Each sample drawn from the population has its own value of any statistic that is used to estimate this parameter. For example, the mean of the data in a sample is used to give information about the overall mean m in the population from which that sample was drawn. Statistic: A statistic is a quantity that is calculated from a sample of data. It is used to give information about unknown values in the corresponding population. For example, the average of the data in a sample is used to give information about the overall average in the population from which that sample was drawn. A statistic is a function of an observable random sample. It is therefore an observable random variable . Notice that, while a statistic is a "function" of observations, unfortunately, it is commonly called a random "variable" not a function. It is possible to draw more than one sample from the same population, and the value of a statistic will in general vary from sample to sample. For example, the average value in a sample is a statistic. The average values in more than one sample, drawn from the same population, will not necessarily be equal. Statistics are often assigned Roman letters (e.g. and s), whereas the equivalent unknown values in the population (parameters ) are assigned Greek letters (e.g., µ, s ). The word estimate means to esteem, that is giving a value to something. A statistical estimate is an indication of the value of an unknown quantity based on observed data. More formally, an estimate is the particular value of an estimator that is obtained from a particular sample of data and used to indicate the value of a parameter. Example: Suppose the manager of a shop wanted to know m , the mean expenditure of customers in her shop in the last year. She could calculate the average expenditure of the hundreds (or perhaps thousands) of customers who bought goods in her shop; that is, the population mean m . Instead she could use an estimate of this population mean m by calculating the mean of a representative sample of customers. If this value were found to be $ 25, then $ 25 would be her estimate. There are two broad subdivisions of statistics: Descriptive Statistics and Inferential Statistics as described below. Descriptive Statistics: The numerical statistical data should be presented clearly, concisely, and in such a way that the decision maker can quickly obtain the essential characteristics of the data in order to incorporate them into decision process. The principal descriptive quantity derived from sample data is the mean ( ), which is the arithmetic average of the sample data. It serves as the most reliable single measure of the value of a typical member of the sample. If the sample contains a few values that are so large or so small that they have an exaggerated effect on the value of the mean, the sample is more accurately represented by the median -- the value where half the sample values fall below and half above. The quantities most commonly used to measure the dispersion of the values about their mean are the variance s 2 and its square root , the standard deviation s. The variance is calculated by determining the mean, subtracting it from each of the sample values (yielding the deviation of the samples), and then averaging the squares of these deviations. The mean and standard deviation of the sample are used as estimates of the corresponding characteristics of the entire group from which the sample was drawn. They do not , in general, completely describe the distribution (F x ) of values within either the sample or the parent group; indeed, different distributions may have the same mean and standard deviation. They do, however, provide a complete description of the normal distribution, in which positive and negative deviations from the mean are equally common, and small deviations are much more common than large ones. For a normally distributed set of values, a graph showing the dependence of the frequency of the deviations upon their magnitudes is a bell-shaped curve. About 68 percent of the values will differ from the mean by less than the standard deviation, and almost 100 percent will differ by less than three times the standard deviation. Inferential Statistics: Inferential statistics is concerned with making inferences from samples about the populations from which they have been drawn. In other words, if we find a difference between two samples, we would like to know, is this a "real" difference (i.e., is it present in the population) or just a "chance" difference (i.e. it could just be the result of random sampling error). That's what tests of statistical significance are all about. Any inferred conclusion from a sample data to the population from which the sample is drawn must be expressed in a probabilistic term. Probability is the language and a measuring tool for uncertainty in our statistical conclusions. Inferential statistics could be used for explaining a phenomenon or checking for validity of a claim. In these instances, inferential statistics is called Exploratory Data Analysis or Confirmatory Data Analysis , respectively. Statistical Inference: Statistical inference refers to extending your knowledge obtained from a random sample from the entire population to the whole population. This is known in mathematics as Inductive Reasoning , that is, knowledge of the whole from a particular. Its main application is in hypotheses testing about a given population. Statistical inference guides the selection of appropriate statistical models. Models and data interact in statistical work. Inference from data can be thought of as the process of selecting a reasonable model, including a statement in probability language of how confident one can be about the selection. Normal Distribution Condition: The normal or Gaussian distribution is a continuous symmetric distribution that follows the familiar bell-shaped curve. One of its nice features is that, the mean and variance uniquely and independently determines the distribution. It has been noted empirically that many measurement variables have distributions that are at least approximately normal. Even when a distribution is non-normal, the distribution of the mean of many independent observations from the same distribution becomes arbitrarily close to a normal distribution, as the number of observations grows large. Many frequently used statistical tests make the condition that the data come from a normal distribution. Estimation and Hypothesis Testing: Inference in statistics are of two types. The first is estimation , which involves the determination, with a possible error due to sampling, of the unknown value of a population characteristic, such as the proportion having a specific attribute or the average value m of some numerical measurement. To express the accuracy of the estimates of population characteristics, one must also compute the standard errors of the estimates. The second type of inference is hypothesis testing . It involves the definitions of a hypothesis as one set of possible population values and an alternative, a different set. There are many statistical procedures for determining, on the basis of a sample, whether the true population characteristic belongs to the set of values in the hypothesis or the alternative. Statistical inference is grounded in probability, idealized concepts of the group under study, called the population, and the sample. The statistician may view the population as a set of balls from which the sample is selected at random, that is, in such a way that each ball has the same chance as every other one for inclusion in the sample. Notice that to be able to estimate the population parameters , the sample size n must be greater than one. For example, with a sample size of one, the variation (s 2 ) within the sample is 0/1 = 0. An estimate for the variation ( s 2 ) within the population would be 0/0, which is indeterminate quantity, meaning impossible. Greek Letters Commonly Used as Statistical Notations We use Greek letters as scientific notations in statistics and other scientific fields to honor the ancient Greek philosophers who invented science and scientific thinking. Before Socrates, in 6 th Century BC, Thales and Pythagoras, amomg others, applied geometrical concepts to arithmetic, and Socrates is the inventor of dialectic reasoning. The revival of scientific thinking (initiated by Newton's work) was valued and hence reappeared almost 2000 years later. Greek Letters Commonly Used as Statistical Notations alpha beta ki-sqre delta mu nu pi rho sigma tau theta a b c 2 d m n p r s t q Note: ki-square (ki-sqre, Chi-square), c 2 , is not the square of anything, its name implies Chi-square (read, ki-square). Ki does not exist in statistics. I'm glad that you're overcoming all the confusions that exist in learning statistics. Type of Data and Levels of Measurement Information can be collected in statistics using qualitative or quantitative data. Qualitative data , such as eye color of a group of individuals, is not computable by arithmetic relations. They are labels that advise in which category or class an individual, object, or process fall. They are called categorical variables. Quantitative data sets consist of measures that take numerical values for which descriptions such as means and standard deviations are meaningful. They can be put into an order and further divided into two groups: discrete data or continuous data. Discrete data are countable data and are collected by counting , for example, the number of defective items produced during a day's production. Continuous data are collected by measuring and are expressed on a continuous scale. For example, measuring the height of a person. Among the first activities in statistical analysis is to count or measure: Counting/measurement theory is concerned with the connection between data and reality. A set of data is a representation (i.e., a model) of the reality based on numerical and measurable scales. Data are called "primary type" data if the analyst has been involved in collecting the data relevant to his/her investigation. Otherwise, it is called "secondary type" data. Data come in the forms of N ominal, O rdinal, I nterval, and R atio (remember the French word NOIR for the color black). Data can be either continuous or discrete. Levels of Measurements _________________________________________ Nominal Ordinal Interval/Ratio Ranking? no yes yes Numerical difference no no yes Both the zero point and the units of measurement are arbitrary on the Interval scale. While the unit of measurement is arbitrary on the Ratio scale, its zero point is a natural attribute. The categorical variable is measured on an ordinal or nominal scale. Counting/measurement theory is concerned with the connection between data and reality. Both statistical theory and counting/measurement theory are necessary to make inferences about reality. Since statisticians live for precision, they prefer Interval/Ratio levels of measurement. For a good business application of discrete random variables, visit Markov Chain Calculator and Large Markov Chain Calculator . Why Statistical Sampling? Sampling is the selection of part of an aggregate or totality known as population , on the basis of which a decision concerning the population is made. The following are the advantages and/or necessities for sampling in statistical decision making: Cost: Cost is one of the main arguments in favor of sampling, because often a sample can furnish data of sufficient accuracy and at much lower cost than a census. Accuracy: Much better control over data collection errors is possible with sampling than with a census, because a sample is a smaller-scale undertaking. Timeliness: Another advantage of a sample over a census is that the sample produces information faster. This is important for timely decision making. Amount of Information: More detailed information can be obtained from a sample survey than from a census, because it take less time, is less costly, and allows us to take more care in the data processing stage. Destructive Tests: When a test involves the destruction of an item under study, sampling must be used. Statistical sampling determination can be used to find the optimal sample size within an acceptable cost. Further Reading: Thompson S., Sampling , Wiley, 2002. Sampling Methods From the food you eat to the television you watch, from political elections to school board actions, much of your life is regulated by the results of sample surveys. A sample is a group of units selected from a larger group (the population). By studying the sample, one hopes to draw valid conclusions about the larger group. A sample is generally selected for study because the population is too large to study in its entirety. The sample should be representative of the general population. This is often best achieved by random sampling. Also, before collecting the sample, it is important that one carefully and completely defines the population, including a description of the members to be included. A common problem in business statistical decision-making arises when we need information about a collection called a population but find that the cost of obtaining the information is prohibitive. For instance, suppose we need to know the average shelf life of current inventory. If the inventory is large, the cost of checking records for each item might be high enough to cancel the benefit of having the information. On the other hand, a hunch about the average shelf life might not be good enough for decision-making purposes. This means we must arrive at a compromise that involves selecting a small number of items and calculating an average shelf life as an estimate of the average shelf life of all items in inventory. This is a compromise, since the measurements for a sample from the inventory will produce only an estimate of the value we want, but at substantial savings. What we would like to know is how "good" the estimate is and how much more will it cost to make it "better". Information of this type is intimately related to sampling techniques . This section provides a short discussion on the common methods of business statistical sampling. Cluster sampling can be used whenever the population is homogeneous but can be partitioned. In many applications the partitioning is a result of physical distance. For instance, in the insurance industry, there are small "clusters" of employees in field offices scattered about the country. In such a case, a random sampling of employee work habits might not required travel to many of the "clusters" or field offices in order to get the data. Totally sampling each one of a small number of clusters chosen at random can eliminate much of the cost associated with the data requirements of management. Stratified sampling can be used whenever the population can be partitioned into smaller sub-populations, each of which is homogeneous according to the particular characteristic of interest. If there are k sub-populations and we let N i denote the size of sub-population i, let N denote the overall population size, and let n denote the sample size, then we select a stratified sample whenever we choose: n i = n(N i /N) items at random from sub-population i, i = 1, 2, . . . . , k. The estimates is: s = S W t . t , over t = 1, 2, ..L (strata), and t is S X it /n t . Its variance is: S W 2 t /(N t -n t )S 2 t /[n t (N t -1)] Population total T is estimated by N. s ; its variance is S N 2 t (N t -n t )S 2 t /[n t (N t -1)]. Random sampling is probably the most popular sampling method used in decision making today. Many decisions are made, for instance, by choosing a number out of a hat or a numbered bead from a barrel, and both of these methods are attempts to achieve a random choice from a set of items. But true random sampling must be achieved with the aid of a computer or a random number table whose values are generated by computer random number generators. A random sampling of size n is drawn from a population size N. The unbiased estimate for variance of is: Var( ) = S 2 (1-n/N)/n, where n/N is the sampling fraction. For sampling fraction less than 10% the finite population correction factor (N-n)/(N-1) is almost 1. The total T is estimated by N &#180; , its variance is N 2 Var( ). For 0, 1, (binary) type variables, variation in estimated proportion p is: S 2 = p(1-p) &#180; (1-n/N)/(n-1). For ratio r = S x i / S y i = / , the variation for r is: [(N-n)(r 2 S 2 x + S 2 y -2 r Cov(x, y)]/[n(N-1) 2 ]. Determination of sample sizes (n) with regard to binary data: Smallest integer greater than or equal to: [t 2 N p(1-p)] / [t 2 p(1-p) + a 2 (N-1)], with N being the size of the total number of cases, n being the sample size, a the expected error, t being the value taken from the t-distribution corresponding to a certain confidence interval, and p being the probability of an event. Cross-Sectional Sampling: Cross-Sectional study the observation of a defined population at a single point in time or time interval. Exposure and outcome are determined simultaneously. What is a statistical instrument? A statistical instrument is any process that aim at describing a phenomena by using any instrument or device, however the results may be used as a control tool. Examples of statistical instruments are questionnaire and surveys sampling. What is grab sampling technique? The grab sampling technique is to take a relatively small sample over a very short period of time, the result obtained are usually instantaneous. However, the Passive Sampling is a technique where a sampling device is used for an extended time under similar conditions. Depending on the desirable statistical investigation, the passive sampling may be a useful alternative or even more appropriate than grab sampling. However, a passive sampling technique needs to be developed and tested in the field. Further Reading: Thompson S., Sampling , Wiley, 2002. Statistical Summaries Representative of a Sample: Measures of Central Tendency Summaries How do you describe the "average" or "typical" piece of information in a set of data? Different procedures are used to summarize the most representative information depending of the type of question asked and the nature of the data being summarized. Measures of location give information about the location of the central tendency within a group of numbers. The measures of location presented in this unit for ungrouped (raw) data are the mean, the median, and the mode. Mean: The arithmetic mean (or the average, simple mean) is computed by summing all numbers in an array of numbers (x i ) and then dividing by the number of observations (n) in the array. Mean = = S X i /n, the sum is over all i's. The mean uses all of the observations, and each observation affects the mean. Even though the mean is sensitive to extreme values; i.e., extremely large or small data can cause the mean to be pulled toward the extreme data; it is still the most widely used measure of location. This is due to the fact that the mean has valuable mathematical properties that make it convenient for use with inferential statistical analysis. For example, the sum of the deviations of the numbers in a set of data from the mean is zero, and the sum of the squared deviations of the numbers in a set of data from the mean is the minimum value. You might like to use Descriptive Statistics Applet to compute the mean. Weighted Mean: In some cases, the data in the sample or population should not be weighted equally, rather each value should be weighted according to its importance. Median: The median is the middle value in an ordered array of observations. If there is an even number of observations in the array, the median is the average of the two middle numbers. If there is an odd number of data in the array, the median is the middle number. The median is often used to summarize the distribution of an outcome. If the distribution is skewed , the median and the interquartile range (IQR) may be better than other measures to indicate where the observed data are concentrated. Generally, the median provides a better measure of location than the mean when there are some extremely large or small observations; i.e., when the data are skewed to the right or to the left. For this reason, median income is used as the measure of location for the U.S. household income. Note that if the median is less than the mean, the data set is skewed to the right. If the median is greater than the mean, the data set is skewed to the left. For normal population, the sample median is distributed normally with m = the mean, and standard error of the median ( p /2) ½ times standard error of the mean. The mean has two distinct advantages over the median. It is more stable, and one can compute the mean based of two samples by combining the two means. Mode: The mode is the most frequently occurring value in a set of observations. Why use the mode? The classic example is the shirt/shoe manufacturer who wants to decide what sizes to introduce. Data may have two modes. In this case, we say the data are bimodal , and sets of observations with more than two modes are referred to as multimodal . Note that the mode is not a helpful measure of location, because there can be more than one mode or even no mode. When the mean and the median are known, it is possible to estimate the mode for the unimodal distribution using the other two averages as follows: Mode &#187; 3(median) - 2(mean) This estimate is applicable to both grouped and ungrouped data sets. Whenever, more than one mode exist, then the population from which the sample came is a mixture of more than one population. However, notice that a Uniform distribution has uncountable number of modes having equal density value; therefore it is considered as a homogeneous population. Almost all standard statistical analyses are conditioned on the assumption that the population is homogeneous. Notice that Excel has very limited statistical capability. For example, it displays only one mode , the first one. Unfortunately, this is very misleading. However, you may find out if there are others by inspection only, as follow: Create a frequency distribution, invoke the menu sequence: Tools, Data analysis, Frequency and follow instructions on the screen. You will see the frequency distribution and then find the mode visually. Unfortunately, Excel does not draw a Stem and Leaf diagram. All commercial off-the-shelf software, such as SAS and SPSS , display a Stem and Leaf diagram, which is a frequency distribution of a given data set. Selecting Among the Mode, Median, and Mean It is a common mistake to specify the wrong index for central tenancy. The first consideration is the type of data, if the variable is categorical, the mode is the single measure that best describes that data. The second consideration in selecting the index is to ask whether the total of all observations is of any interest. If the answer is yes, then the mean is the proper index of central tendency. If the total is of no interest, then depending on whether the histogram is symmetric or skewed one must use either mean or median, respectively. In all cases the histogram must be unimodal. However, notice that, e.g., a Uniform distribution has uncountable number of modes having equal density value; therefore it is considered as a homogeneous population. Notice also that: |Mean - Median| £ s The main characteristics of these three statistics are tabulated below: The Main Characteristics of the Mode, the Median, and the Mean Fact No. The Mode The Median The Mean 1 It is the most frequent value in the distribution; it is the point of greatest density. It is the value of the middle point of the array (not midpoint of range), such that half the item are above and half below it. It is the value in a given aggregate which would obtain if all the values were equal. 2 The value of the mode is established by the predominant frequency, not by the value in the distribution. The value of the media is fixed by its position in the array and doesn't reflect the individual value. The sum of deviations on either side of the mean are equal; hence, the algebraic sum of the deviation is equal zero. 3 It is the most probable value, hence the most typical. The aggregate distance between the median point and all the value in the array is less than from any other point. It reflect the magnitude of every value. 4 A distribution may have 2 or more modes. On the other hand, there is no mode in a rectangular distribution. Each array has one and only one median. An array has one and only one mean. 5 The mode does nott reflect the degree of modality. It cannot be manipulated algebraically: medians of subgroups cannot be weighted and combined. Means may be manipulated algebraically: means of subgroups may be combined when properly weighted. 6 It cannot be manipulated algebraically: modes of subgroups cannot be combined. It is stable in that grouping procedures do not affect it appreciably. It may be calculated even when individual values are unknown, provided the sum of the values and the sample size n are known. 7 It is unstable that it is influenced by grouping procedures. Value must be ordered, and may be grouped, for computation. Values need not be ordered or grouped for this calculation. 8 Values must be ordered and group for its computation. It can be compute when ends are open It cannot be calculated from a frequency table when ends are open. 9 It can be calculated when table ends are open. It is not applicable to qualitative data. It is stable in that grouping procedures do not seriously affected it. The Descriptive Statistics JavaScript provides a complete set of information about all statistics that you ever need. You might like to use it to perform some numerical experimentation for validating the above assertions for a deeper understanding. Specialized Averages: The Geometric & Harmonic Means The Geometric Mean: The geometric mean (G) of n non-negative numerical values is the n th root of the product of the n values. If some values are very large in magnitude and others are small, then the geometric mean is a better representative of the data than the simple average. In a "geometric series", the most meaningful average is the geometric mean (G). The arithmetic mean is very biased toward the larger numbers in the series. An Application: Suppose sales of a certain item increase to 110% in the first year and to 150% of that in the second year. For simplicity, assume you sold 100 items initially. Then the number sold in the first year is 110 and the number sold in the second is 150% x 110 = 165. The arithmetic average of 110% and 150% is 130% so that we would incorrectly estimate that the number sold in the first year is 130 and the number in the second year is 169. The geometric mean of 110% and 150% is G = (1.65) 1/2 so that we would correctly estimate that we would sell 100 (G) 2 = 165 items in the second year. The Harmonic Mean: The harmonic mean (H) is another specialized average, which is useful in averaging variables expressed as rate per unit of time, such as mileage per hour, number of units produced per day. The harmonic mean (H) of n non-zero numerical values x(i) is: H = n/[ S (1/x(i)]. An Application: Suppose 4 machines in a machine shop are used to produce the same part. However, each of the four machines takes 2.5, 2.0, 1.5, and 6.0 minutes to make one part, respectively. What is the average rate of speed? The harmonic means is: H = 4/[(1/2.5) + (1/2.0) + 1/(1.5) + (1/6.0)] = 2.31 minutes. If all machines working for one hour, how many parts will be produced? Since four machines running for one hour represent 240 minutes of operating time, then: 240 / 2.31 = 104 parts will be produced. The Order Among the Three Means: If all the three means exist, then the Arithmetic Mean is never less than the other two, moreover, the Harmonic Mean is never larger than the other two. You might like to use The Other Means JavaScript applet in performing some numerical experimentation for validating the above assertions for a deeper understanding. Further Reading: Langley R., Practical Statistics Simply Explained , 1970, Dover Press. Histogramming: Checking for Homogeneity of Population A histogram is a graphical presentation of an estimate for the density (for continuous random variables ) or probability mass function (for discrete random variables) of the population. The geometric feature of histogram enables us to find out useful information about the data, such as: The location of the "center" of the data. The degree of dispersion. The extend to which its is skewed, that is, it does not fall off systemically on both side of its peak. The degree of peakedness. How steeply it rises and falls. The mode is the most frequently occurring value in a set of observations. Data may have two modes. In this case, we say the data are bimodal , and sets of observations with more than two modes are referred to as multimodal . Whenever, more than one mode exist, then the population from which the sample came is a mixture of more than one population. Almost all standard statistical analyses are conditioned on the assumption that the population is homogeneous, meaning that its density (for continuous random variables) or probability mass function (for discrete random variables) is unimodal. However, notice that, e.g., a Uniform distribution has uncountable number of modes having equal density value; therefore it is considered as a homogeneous population. To check the unimodality of sampling data, one may use the histogramming process. Number of Class Intervals in a Histogram: Before we can construct our frequency distribution we must determine how many classes we should use. This is purely arbitrary, but too few classes or too many classes will not provide as clear a picture as can be obtained with some more nearly optimum number. An empirical (i.e., observed) relationship, known as Sturge's rule, may be used as a useful guide to determine the optimal number of classes (k) is given by k = the smallest integer greater than or equal to 1 + 3.332 Log(n) where k is the number of classes, Log is in base 10, and n is the total number of the numerical values which comprise the data set. Therefore, class width is: (highest value - lowest value) / (1 + 3.332 Logn) where n is the total number of items in the data set. The following JavaScript produces a histogram based on this rule: Test for Homogeneity of a Population . To have an "optimum" you need some measure of quality -- presumably in this case, the "best" way to display whatever information is available in the data. The sample size contributes to this; so the usual guidelines are to use between 5 and 15 classes, with more classes, if you have a larger sample. You should take into account a preference for tidy class widths, preferably a multiple of 5 or 10, because this makes it easier to understand. Beyond this it becomes a matter of judgement. Try out a range of class widths, and choose the one that works best. This assumes you have a computer and can generate alternative histograms fairly readily. There are often management issues that come into play as well. For example, if your data is to be compared to similar data -- such as prior studies, or from other countries -- you are restricted to the intervals used therein. If the histogram is very skewed, then unequal classes should be considered. Use narrow classes where the class frequencies are high, wide classes where they are low. The following approaches are common: Let n be the sample size, then the number of class intervals could be Min {n ½ , 10 Log(n) }. The Log is the logarithm in base 10. Thus for 200 observations you would use 14 intervals but for 2000 you would use 33. Alternatively , Find the range (highest value - lowest value). Divide the range by a reasonable interval size: 2, 3, 5, 10 or a multiple of 10. Aim for no fewer than 5 intervals and no more than 15. One of the main applications of histogramming is to Test for Homogeneity of a Population . The unimodality of the histogram is a necessary condition for the homogeneity of population to make any statistical analysis meaningful. However, notice that, e.g., a Uniform distribution has uncountable number of modes having equal density value; therefore it is considered as a homogeneous population. Further Reading: Efron B., and R. Tibshirani, An Introduction to the Bootstrap , Chapman &amp; Hall (now the CRC Press), 1994. Contains a tedious test for multimodality that is based on the Gaussian kernel density estimates and then test for multimodality by using the window-size approach. How to Construct a BoxPlot A BoxPlot is a graphical display that has many characteristics. It includes the presence of possible outliers . It illustrates the range of data. It shows a measure of dispersion such as the upper quartile, lower quartile and interquartile range (IQR) of the data set as well as the median as a measure of central location, which is useful for comparing sets of data. It also gives an indication of the symmetry or skewness of the distribution. The main reason for the popularity of boxplots is that they offer much of information in a compact way. Steps to Construct a BoxPlot: Horizontal lines are drawn at the smallest observation (A), lower quartile. And another from the upper quartile (D), and the largest observation (E). Vertical lines to produce the box join these horizontal lines at points (B, and D). A vertical line is drawn at the median point (C), as shown on the above Figure. For a deeper understanding, you may like using graph paper , and Descriptive Sampling Statistics Applet in constructing the BoxPlots for some sets of data; e.g., from your textbook. Measuring the Quality of a Sample Average by itself is not a good indication of quality. You need to know the variance to make any educated assessment. We are reminded of the dilemma of the six-foot tall statistician who drowned in a stream that had an average depth of three feet. Statistical measures are often used for describing the nature and extent of differences among the information in the distribution. A measure of variability is generally reported together with a measure of central tendency. Statistical measures of variation are numerical values that indicate the variability inherent in a set of data measurements. Note that a small value for a measure of dispersion indicates that the data are concentrated around the mean; therefore, the mean is a good representative of the data set. On the other hand, a large measure of dispersion indicates that the mean is not a good representative of the data set. Also, measures of dispersion can be used when we want to compare the distributions of two or more sets of data. Quality of a data set is measured by its variability: Larger variability indicates lower quality. That is why high variation makes the manager very worried. Your job, as a statistician, is to measure the variation , and if it is too high and unacceptable, then it is the job of the technical staff, such as engineers, to fix the process. Decision situations with complete lack of knowledge, known as the flat uncertainty , have the largest risk. For simplicity, consider the case when there are only two outcomes, one with probability of p. Then, the variation in the outcomes is p(1-p). This variation is the largest if we set p = 50%. That is, equal chance for each outcome. In such a case, the quality of information is at its lowest level. Remember, quality of information and variation are inversely related . The larger the variation in the data, the lower the quality of the data (i.e., information): the Devil is in the Deviations. The four most common measures of variation are the range , variance , standard deviation , and coefficient of variation . Range: The range of a set of observations is the absolute value of the difference between the largest and smallest values in the data set. It measures the size of the smallest contiguous interval of real numbers that encompasses all of the data values. It is not useful when extreme values are present. It is based solely on two values, not on the entire data set. In addition, it cannot be defined for open-ended distributions such as Normal distribution. Notice that, when dealing with discrete random observations , some authors define the range as: Range = Largest value - Smallest value + 1. A normal distribution does not have a range. A student said, "since the tails of a normal density function never touch the x-axis and since for an observation to contribute to forming such a curve, very large positive and negative values must exist" Yet such remote values are always possible, but increasingly improbable. This encapsulates the asymptotic behavior of normal density very well. Therefore, in spite of this behavior, it is useful and applicable to a wide range of decision-making situations. Quartiles: When we order the data, for example in ascending order, we may divide the data into quarters, Q1Q4, known as quartiles. The first Quartile (Q1) is that value where 25% of the values are smaller and 75% are larger. The second Quartile (Q2) is that value where 50% of the values are smaller and 50% are larger. The third Quartile (Q3) is that value where 75% of the values are smaller and 25% are larger. Percentiles: Percentiles have a similar concept and therefore, are related; e.g., the 25 th percentile corresponds to the first quartile Q1, etc. The advantage of percentiles is that they may be subdivided into 100 parts. The percentiles and quartiles are most conveniently read from a cumulative distribution function. Interquartiles Range: The interquartile range (IQR) describes the extent for which the middle 50% of the observations scattered or dispersed. It is the distance between the first and the third quartiles: IQR = Q3 - Q1, which is twice the Quartile Deviation . For data that are skewed , the relative dispersion , similar to the coefficient of variation (C.V.) is given (provided the denominator is not zero) by the Coefficient of Quartile Variation : CQV = (Q3-Q1) / (Q3 + Q1). Note that almost all statistics that we have covered up to now can be obtained and understood deeply by graphical method using Empirical (i.e., observed) Cumulative Distribution Function (ECDF) JavaScript. However, the numerical Descriptive Statistics applet provides a complete set of information about all statistics that you ever need. The Duality between the ECDF and the Histogram: Notice that the empirical (i.e., observed) cumulative distribution function ( ECDF ) indicates by its height at a particular pointthat is numerically equal to the area in the corresponding histogram to the left of that point. Therefore, either or both could be used depending on the intended applications. Mean Absolute Deviation (MAD): A simple measure of variability is the mean absolute deviation: MAD = S |(x i - )| / n. The mean absolute deviation is widely used as a performance measure to assess the quality of the modeling, such forecasting techniques . However, MAD does not lend itself to further use in making inference; moreover, even in the error analysis studies, the variance is preferred since variances of independent (i.e., uncorrelated) errors are additive; however MAD does not have such a nice feature. The MAD is a simple measure of variability, which unlike range and quartile deviation, takes every item into account, and it is simpler and less affected by extreme deviations. It is therefore often used in small samples that include extreme values . The mean absolute deviation theoretically should be measured from the median, since it is at its minimum; however, it is more convenient to measure the deviations from the mean. As a numerical example, consider the price (in $ ) of same item at 5 different stores: $ 4.75, $ 5.00, $ 4.65, $ 6.10, and $ 6.30. The mean absolute deviation from the mean is $ 0.67, while from the median is $ 0.60, which is a better representative of deviation among the prices. Variance: An important measure of variability is variance. Variance is the average of the squared deviations of each observation in the set from the arithmetic mean of all of the observations. Variance = S (x i - ) 2 / (n - 1), where n is at least 2. The variance is a measure of spread or dispersion among values in a data set. Therefore, the greater the variance, the lower the quality . The variance is not expressed in the same units as the observations. In other words, the variance is hard to understand because the deviations from the mean are squared, making it too large for logical explanation. This problem can be solved by working with the square root of the variance, which is called the standard deviation. Standard Deviation: Both variance and standard deviation provide the same information; one can always be obtained from the other . In other words, the process of computing a standard deviation always involves computing a variance. Since standard deviation is the square root of the variance, it is always expressed in the same units as the raw data: Standard Deviation = S = (Variance) ½ For large data sets (say, more than 30), approximately 68% of the data are contained within one standard deviation of the mean, 95% fall within two standard deviations. 97.7% (or almost 100% ) of the data are contained within within three standard deviations (S) from the mean. You may use Descriptive Statistics Applet to compute the mean, and standard deviation. The Mean Square Error (MSE) of an estimate is the variance of the estimate plus the square of its bias; therefore, if an estimate is unbiased, then its MSE is equal to its variance, as it is the case in the ANOVA table. Coefficient of Variation : Coefficient of Variation (CV) is the absolute relative deviation with respect to size , provided is not zero, expressed in percentage: CV =100 |S/ | % CV is independent of the unit of measurement. In estimation of a parameter, when its CV is less than 10%, the estimate is assumed acceptable. The inverse of CV; namely, 1/CV is called the Signal-to-noise Ratio . The coefficient of variation is used to represent the relationship of the standard deviation to the mean, telling how representative the mean is of the numbers from which it came. It expresses the standard deviation as a percentage of the mean; i.e., it reflects the variation in a distribution relative to the mean. However, confidence intervals for the coefficient of variation are rarely reported. One of the reasons is that the exact confidence interval for the coefficient of variation is computationally tedious. Note that, for a skewed or grouped data set, the coefficient of quartile variation: V Q = 100(Q 3 - Q 1 )/(Q 3 + Q 1 )% is more useful than the CV. You may use Descriptive Statistics Applet to compute the mean, standard deviation and the coefficient of variation. Variation Ratio for Qualitative Data: Since the mode is the most frequently used measure of central tendency for qualitative variables, variability is measured with reference to the mode. The statistic that describes the variability of quantitative data is the Variation Ratio (VR): VR = 1 - f m /n, where f m is the frequency of the mode, and n is the total number of scores in the distribution. Z Score: how many standard deviations a given point (i.e., observation) is above or below the mean. In other words, a Z score represents the number of standard deviations that an observation (x) is above or below the mean. The larger the Z value, the further away a value will be from the mean . Note that values beyond three standard deviations are very unlikely. Note that if a Z score is negative, the observation (x) is below the mean. If the Z score is positive, the observation (x) is above the mean. The Z score is found as: Z = (x - ) / standard deviation of X The Z score is a measure of the number of standard deviations that an observation is above or below the mean. Since the standard deviation is never negative, a positive Z score indicates that the observation is above the mean, a negative Z score indicates that the observation is below the mean. Note that Z is a dimensionless value, and therefore is a useful measure by which to compare data values from two different populations, even those measured by different units. Z-Transformation: Applying the formula z = (X - m ) / s will always produce a transformed variable with a mean of zero and a standard deviation of one. However, the shape of the distribution will not be affected by the transformation. If X is not normal, then the transformed distribution will not be normal either. One of the nice features of the z-transformation is that the resulting distribution of the transformed data has an identical shape but with mean zero, and standard deviation equal to 1. One can generalize this data transformation to have any desirable mean and standard deviation other than 0 and 1, respectively. Suppose we wish the transformed data to have the mean and standard deviation of M and D, respectively. For example, in the SAT Scores, they are set at M = 500, and D=100. The following transformation should be applied: Z = (standard Z) &#180; D + M Suppose you have two data sets with very different scales (e.g., one has very low values, another very high values). If you wish to compare these two data sets, due to differences in scales, the statistics that you generate are not comparable. It is a good idea to use the Z-transformation of both original data sets and then make any comparison. You have heard the terms z value, z test, z transformation, and z score . Do all of these terms mean the same thing? Certainly not: The z value refers to the critical value (a point on the horizontal axes) of the Normal (0, 1) density function, for a given area to the left of that z-value. The z test refers to the procedures for testing the equality of mean (s) of one (or two) population(s). The z score of a given observation x, in a sample of size n, is simply (x - average of the sample) divided by the standard deviation of the sample. One must be careful not to mistake z scores for the Standard Scores. The z transformation of a set of observations of size n is simply (each observation - average of all observations) divided by the standard deviation among all observations. The aim is to produce a transformed data set with a mean of zero and a standard deviation of one. This makes the transformed set dimensionless and manageable with respect to its magnitudes. It is used also in comparing several data sets that have been measured using different scales of measurements. Pearson coined the term "standard deviation" sometime near 1900. The idea of using squared deviations goes back to Laplace in the early 1800's. Finally, notice again, that the transforming raw scores to z scores do NOT normalize the data. Computation of Descriptive Statistics for Grouped Data: One of the most common ways to describe a single variable is with a frequency distribution. A histogram is a graphical presentation of an estimate for the frequency distribution of the population. Depending upon the particular variable, all of the data values may be represented, or you may group the values into categories first (e.g., by age). It would usually not be sensible to determine the frequencies for each value. Rather, the values are grouped into ranges, and the frequency is then determined.). Frequency distributions can be depicted in two ways: as a table or as a graph that is often referred to as a histogram or bar chart. The bar chart is often used to show the relationship between two categorical variables. Grouped data is derived from raw data, and it consists of frequencies (counts of raw values) tabulated with the classes in which they occur. The Class Limits represent the largest (Upper) and lowest (Lower) values which the class will contain. The formulas for the descriptive statistic becomes much simpler for the grouped data, as shown below for Mean, Variance, Standard Deviation, respectively, where (f) is for the frequency of each class, and n is the total frequency: Selecting Among the Quartile Deviation, Mean Absolute Deviation, and Standard Deviation A general guideline for selecting a suitable statistic in describing the dispersion in a population includes consideration of the following factors: The concept of dispersion required by the problem. Is a single pair of values adequate, such as the two extremes or the two quartiles (range or Q)? The type of data available. If they are few in numbers, or contain extreme value, avoid the standard deviation. If they are generally skewed, avoid the mean absolute deviation as well. If they have a gap around the quartile, the quartile deviation should be avoided. The peculiarity of the dispersion measures themselves. These are summarized under "The Main Characteristics of the Quartile Deviation, the Mean Absolute Deviation, and the Standard deviation" below. The Main Characteristics of the Quartile Deviation, the Mean Absolute Deviation, and the Standard Deviation Fact No. The Quartile Deviation The Mean Absolute Deviation The Standard Deviation 1 The quartile deviation is also easy to calculate and to understand. However, it is unreliable if there are gaps in the data around the quartiles. The mean absolute deviation has the advantage of giving equal weight to the deviation of every value form the mean or median. The standard deviation is usually more useful and better adapted to further analysis than the mean absolute deviation. 2 It depends on only 2 values, which include the middle half of the items. Therefore, it is a more sensitive measure of dispersion than those described above and ordinarily has a smaller sampling error. It is more reliable as an estimator of the population dispersion than other measures, provided the distribution is normal. 3 It is usually superior to the range as a rough measure of dispersion. It is also easier to compute and to understand and is less affected by extreme values than the standard deviation. It is the most widely used measure of dispersion and the easiest to handle algebraically. 4 It may be determined in an open-end distribution, or one in which the data may be ranked but not measured quantitatively. Unfortunately, it is difficult to handle algebraically, since minus signs must be ignored in its computation. Compared with the others, it is harder to compute and more difficult to understand. 5 It also useful in badly skewed distributions or those in which other measures of dispersion would be warped by extreme values. Its main application is in modeling accuracy for comparative forecasting techniques. It is generally affected by extreme values that may be due to skewness of data You might like to use the Descriptive Sampling Statistics JavaScript applet in performing some numerical experimentation for validating the above assertions for a deeper understanding. Shape of a Distribution Function: The Skewness-Kurtosis Chart The pair of statistical measures, skewness and kurtosis, are measuring tools, which is used in selecting a distribution(s) to fit your data. To make an inference with respect to the population distribution, you may first compute skewness and kurtosis from your random sample from the entire population. Then, locating a point with these coordinates on the widely used skewness-kurtosis chart , guess a couple of possible distributions to fit your data. Finally, you might use the goodness-of-fit test to rigorously come up with the best candidate fitting your data. Removing outliers improves the accuracy of both skewness and kurtosis. Skewness: Skewness is a measure of the degree to which the sample population deviates from symmetry with the mean at the center. Skewness = S (x i - ) 3 / [ (n - 1) S 3 ], n is at least 2. Skewness will take on a value of zero when the distribution is a symmetrical curve. A positive value indicates the observations are clustered more to the left of the mean with most of the extreme values to the right of the mean. A negative skewness indicates clustering to the right. In this case we have: Mean £ Median £ Mode. The reverse order holds for the observations with positive skewness. Kurtosis: Kurtosis is a measure of the relative peakedness of the curve defined by the distribution of the observations. Kurtosis = S (x i - ) 4 / [ (n - 1) S 4 ], n is at least 2. Standard normal distribution has kurtosis of +3. A kurtosis larger than 3 indicates the distribution is more peaked than the standard normal distribution. Coefficient of Excess Kurtosis = Kurtosis - 3. A value of less than 3 for kurtosis indicates that the distribution is flatter than the standard normal distribution. It can be shown that, Kurtosis - Skewness 2 is greater than or equal to 1, and Kurtosis is less than or equal to the sample size n . These inequalities hold for any probability distribution having finite skewness and kurtosis. In the Skewness-Kurtosis Chart , you notice two useful families of distributions, namely the beta and gamma families. The Beta-Type Density Function: Since the beta density has both a shape and a scale parameter, it describes many random phenomena provided the random variable is between [0, 1]. For example, when both parameters are integer with random variables the result is the binomial Probability function. Applications: A basic distribution of statistics for variables bounded at both sides; for example x between [0, 1]. The beta density is useful for both theoretical and applied problems in many areas. Examples include distribution of proportion of population located between lowest and highest value in sample; distribution of daily per cent yield in a manufacturing process; description of elapsed times to task completion (PERT). There is also a relationship between the Beta and Normal distributions. The conventional calculation is that given a PERT Beta with highest value as b, lowest as a, and most likely as m, the equivalent normal distribution has a mean and mode of (a + 4M + b)/6 and a standard deviation of (b - a)/6. Comments: Uniform, right triangular, and parabolic distributions are special cases. To generate beta, generate two random values from a gamma, g 1 , g 2 . The ratio g 1 /(g 1 +g 2 ) is distributed like a beta distribution. The beta distribution can also be thought of as the distribution of X1 given (X1+X2), when X1 and X2 are independent gamma random variables. Gamma-Type Density Function: Some random variables are always non-negative. The density function associated with these random variables often is adequately modeled as the gamma density function. The Gamma-Type Density Function has both a shape and a scale parameter. With both the shape and scale parameters equal to 1, the result is the exponential density function. Chi-square is also a special case of gamma density function with shape parameter equal to 2. Applications: A basic distribution of statistics for variables bounded at one side ; for example x greater than or equal to zero. The gamma density gives distribution of time required for exactly k independent events to occur, assuming events take place at a constant rate. Used frequently in queuing theory, reliability, and other industrial applications. Examples include distribution of time between re-calibrations of instrument that needs re-calibration after k uses; time between inventory restocking, time to failure for a system with standby components. Comments: Erlangian, Exponential, and Chi-square distributions are special cases. The negative binomial is an analog to gamma distribution with discrete random variable . What is the distribution of the product of sample observations from the uniform (0, 1) random? Like many problems with products, this becomes a familiar problem when turned into a problem about sums. If X is uniform (for simplicity of notation make it U(0,1)), Y=-log(X) is exponentially distributed, so the log of the product of X1, X2, ... Xn is the sum of Y1, Y2, ... Yn which has a gamma (scaled Chi-square) distribution. Thus, it is a gamma density with shape parameter n and scale 1. The Log-normal Density Function: Permits representation of a random variable whose logarithm follows a normal distribution. The ratio of two log-normally random variables is also log-normal. Applications: Model for a process arising from many small multiplicative errors. Appropriate when the value of an observed variable is a random proportion of the previously observed value. Applications: Examples include distribution of sizes from a breakage process; distribution of income size, inheritances and bank deposits; distribution of various biological phenomena; life distribution of some transistor types. The lognormal distribution is widely used in situations where values are positively skewed (where the distribution has a long right tail; negatively skewed distributions have a long left tail; a normal distribution has no skewness). Examples of data that "fit" a lognormal distribution include financial security valuations or real estate property valuations. Financial analysts have observed that the stock prices are usually positively skewed, rather than normally (symmetrically) distributed. Stock prices exhibit this trend because the stock price cannot fall below the lower limit of zero but may increase to any price without limit. Similarly, healthcare costs illustrate positive skewness since unit costs cannot be negative. For example, there can't be negative cost for services in a capitation contract. This distribution accurately describes most healthcare data. In the case where the data are log-normally distributed, the Geometric Mean acts as a better data descriptor than the mean. The more closely the data follow a log-normal distribution, the closer the geometric mean is to the median, since the log re-expression produces a symmetrical distribution. Further Reading: Snell J., Introduction to Probability , Random House, 1987. Read section 4.2 for a link between beta and F distributions (with the advantage that tables are easy to find). Tabachnick B., and L. Fidell, Using Multivariate Statistics , HarperCollins, 1996. Has a good discussion on applications and significance tests for skewness and kurtosis. Numerical Example and Discussions A Numerical Example: Given the following, small (n = 4) data set, compute the descriptive statistics: x 1 = 1, x 2 = 2, x 3 = 3, and x 4 = 6. i x i ( x i - ) ( x i - ) 2 ( x i - ) 3 ( x i - ) 4 1 1 -2 4 -8 16 2 2 -1 1 -1 1 3 3 0 0 0 0 4 6 3 9 27 81 Sum 12 0 14 18 98 The mean is 12 / 4 = 3; the variance is s 2 = 14 / 3 = 4.67; the standard deviation is s = (14/3) 0.5 = 2.16; the skewness is 18 / [3 (2.16) 3 ] = 0.5952, and finally, the kurtosis is 98 / [3 (2.16) 4 ] = 1.5. You might like to use Descriptive Statistics Applet to check your hand computation. A Short Discussion on the Descriptive Statistic: Deviations about the mean m of a distribution is the basis for most of the statistical tests we will learn. Since we are measuring how much a set of scores is dispersed about the mean m , we are measuring variability . We can calculate the deviations about the mean m and express it as variance s 2 or standard deviation s . It is very important to have a firm grasp of this concept because it will be a central concept throughout your statistics course . Both variance s 2 and standard deviation s measure variability within a distribution. Standard deviation s is a number that indicates how much on average each of the values in the distribution deviates from the mean m (or center) of the distribution. Keep in mind that variance s 2 measures the same thing as standard deviation s (dispersion of scores in a distribution). Variance s 2 , however, is the average squared deviations about the mean m . Thus, variance s 2 is the square of the standard deviation s . The expected value and the variance of the statistic are m and s 2 /n, respectively. The expected value and variance of statistic S 2 are s 2 and 2 s 4 / (n-1), respectively. and S 2 are the best estimators for m and s 2 . They are Unbiased (you may update your estimate); Efficient (they have the smallest variation among other estimators); Consistent (increasing sample size provides a better estimate); and Sufficient (you do not need to have the whole data set; what you need are S x i and S x i 2 for estimations). Note also that the above variance S 2 is justified only in the case where the population distribution tends to be normal, otherwise one may use bootstrapping techniques. In general, it is believed that the pattern of mode, median, and mean go from lower to higher in positive skewed data sets, and just the opposite pattern in negative skewed data sets. However; for example, in the following 23 numbers, mean = 2.87, median = 3, but the data is positively skewed: 4, 2, 7, 6, 4, 3, 5, 3, 1, 3, 1, 2, 4, 3, 1, 2, 1, 1, 5, 2, 2, 3, 1 and, the following 10 numbers have mean = median = mode = 4, but the data set is left skewed: 1, 2, 3, 4, 4, 4, 5, 5, 6, 6. Note also, that most commercial software do not correctly compute skewness and kurtosis . There is no easy way to determine confidence intervals about a computed skewness or kurtosis value from a small to medium sample. The literature gives tables based on asymptotic methods for sample sets larger than 100 for normal distributions only. You may have noticed that using the above numerical example on some computer packages such as SPSS, the skewness and the kurtosis are different from what we have computed. For example, the SPSS output for the skewness is 1.190. However, for large a sample size n, the results are identical. Reference and Further Readings: David H., Early sample measures of variability, Statistical Science , Vol. 13, 1998, 368-377. This article provides a good historical account of statistical measures. Groeneveld R., A class of quantile measures for kurtosis, The American Statistician , 325, Nov. 1998. Lehmann E., Testing Statistical Hypotheses , 1996, Wiley. Exact confidence interval for the coefficient of variation is computationally tedious as shown in this book. The Two Statistical Representations of a Population The following figure depicts a typical relationship between the cumulative distribution function (cdf) and the density (for continuous random variables ), All characteristics of the population are well described by either of these two functions. The figure also illustrates their applications in determining the (lower) percentile measures denoted by P : P = P[ X £ x] = Probability that the random variable X is less than or equal to a given number x, among other useful information. Notice that the probability P is the area under the density function curve, while numerically equal to the height of cdf curve at point x. Both functions can be estimated by smoothing the empirical (i.e., observed) cumulative step-function , and smoothing the histogram constructed from a random sample. Empirical (i.e., observed) Cumulative Distribution Function The empirical cumulative distribution function (ECDF), also known as Ogive (pronounced o-jive), is used to graph cumulative frequency. The ogive is the estimator for the population's cumulative distribution function, which contains all the characteristic of the population. The empirical distribution is a staircase function with the location of the drops randomly placed. The size of the each stair at each point depends on the frequency of that point value, and it is equal to the frequency/n where n is the sample size. The sample size is the sum of all frequencies. Note that almost all statistics we have covered up to now can be obtained and understood more deeply by graph paper using Empirical Distribution Function JavaScript. You may like using this JavaScript in performing some numerical experimentation for a deeper understanding. Other widely used decision model based upon empirical cumulative distribution function (ECDF) as a measuring tool and decision procedure are the ABC Inventory Classification , Single-period Inventory Analysis (The Newsboy Model) , and determination of the Best Time to Replace Equipment . For other inventory decisions, visit the Inventory Control Models site. Introduction Modeling of a Data Set: Families of parametric distribution models are widely used to summarize a huge data set , to obtain predictions, assess goodness of fit, to estimate functions of the data not easily derived directly, or to render manageable random effects. The trustworthiness of the results obtained depends on the generality of the distribution family employed. Inductive Inference: This extension of our knowledge from a particular random sample to the population is called inductive inference. The main function of business statistics is the provision of techniques for making inductive inference and for measuring the degree of uncertainty of such inference. Uncertainty is measured in terms of probability statements, and that is the reason we need to learn the language of uncertainty and its measuring tool called probability. In contrast to the inductive inference, mathematics often uses deductive inference to prove theorems, while in empirical science, such as statistics, inductive inference is used to find new knowledge or to extend our knowledge. Further Readings : Brown B., F. Spears, and L. Levy, The log F: A distribution for all seasons, Computational Statistics , 17(1), 47-58, 2002. Probability, Chance, Likelihood, and Odds The concept of probability occupies an important place in the decision-making process under uncertainty, whether the problem is one faced in business, in government, in the social sciences, or just in one's own everyday personal life. In very few decision-making situations is perfect information -- all the needed facts -- available. Most decisions are made in the face of uncertainty. Probability enters into the process by playing the role of a substitute for certainty - a substitute for complete knowledge. Probability is especially significant in the area of statistical inference . Here the statistician's prime concern lies in drawing conclusions or making inferences from experiments which involve uncertainties. The concepts of probability make it possible for the statistician to generalize from the known (sample) to the unknown (population) and to place a high degree of confidence in these generalizations. Therefore, Probability is one of the most important tools of statistical inference . Probability has an exact technical meaning -- well, in fact it has several, and there is still debate as to which term ought to be used. However, for most events for which probability is easily computed; e.g., rolling of a die, the probability of getting a four [::], almost all agree on the actual value (1/6), if not the philosophical interpretation. A probability is always a number between 0 and 1. Zero is not "quite" the same thing as impossibility. It is possible that "if" a coin were flipped infinitely many times, it would never show "tails", but the probability of an infinite run of heads is 0. One is not "quite" the same thing as certainty but close enough. The word "chance" or "chances" is often used as an approximate synonym of "probability", either for variety or to save syllables. It would be better practice to leave "chance" for informal use, and say "probability" if that is what is meant. One occasionally sees "likely" and "likelihood"; however, these terms are used casually as synonyms for "probable" and "probability". Odds is a probabilistic concept related to probability. It is the ratio of the probability (p) of an event to the probability (1-p) that it does not happen: p/(1-p). It is often expressed as a ratio, often of whole numbers; e.g., "odds" of 1 to 5 in the die example above, but for technical purposes the division may be carried out to yield a positive real number (here 0.2). Odds are a ratio of nonevents to events. If the event rate for a disease is 0.1 (10 per cent), its nonevent rate is 0.9 and therefore its odds are 9:1. Another way to compare probabilities and odds is using "part-whole thinking" with a binary (dichotomous) split in a group. A probability is often a ratio of a part to a whole; e.g., the ratio of the part [those who survived 5 years after being diagnosed with a disease] to the whole [those who were diagnosed with the disease]. Odds are often a ratio of a part to a part; e.g., the odds against dying are the ratio of the part that succeeded [those who survived 5 years after being diagnosed with a disease] to the part that 'failed' [those who did not survive 5 years after being diagnosed with a disease]. Aside from their value in betting, odds allow one to specify a small probability (near zero) or a large probability (near one) using large whole numbers (1,000 to 1 or a million to one). Odds magnify small probabilities (or large probabilities) so as to make the relative differences visible. Consider two probabilities: 0.01 and 0.005. They are both small. An untrained observer might not realize that one is twice as much as the other. But if expressed as odds (99 to 1 versus 199 to 1) it may be easier to compare the two situations by focusing on large whole numbers (199 versus 99) rather than on small ratios or fractions. How to Assign Probabilities? Probability is an instrument to measure the likelihood of the occurrence of an event. There are five major approaches of assigning probability: Classical Approach, Relative Frequency Approach, Subjective Approach, Anchoring, and the Delphi Technique: Classical Approach : Classical probability is predicated on the condition that the outcomes of an experiment are equally likely to happen. The classical probability utilizes the idea that the lack of knowledge implies that all possibilities are equally likely. The classical probability is applied when the events have the same chance of occurring (called equally likely events), and the sets of events are mutually exclusive and collectively exhaustive. The classical probability is defined as: P(X) = Number of favorable outcomes / Total number of possible outcomes Relative Frequency Approach : Relative probability is based on accumulated historical or experimental data. Frequency-based probability is defined as: P(X) = Number of times an event occurred / Total number of opportunities for the event to occur. Note that relative probability is based on the ideas that what has happened in the past will hold. Subjective Approach : The subjective probability is based on personal judgment and experience. For example, medical doctors sometimes assign subjective probability to the length of life expectancy for a person who has cancer. Anchoring: is the practice of assigning a value obtained from a prior experience and adjusting the value in consideration of current expectations or circumstances The Delphi Technique: It consists of a series of questionnaires. Each series is one "round". The responses from the first "round" are gathered and become the basis for the questions and feedback of the second "round". The process is usually repeated for a predetermined number of "rounds" or until the responses are such that a pattern is observed. This process allows expert opinion to be circulated to all members of the group and eliminates the bandwagon effect of majority opinion. Delphi Analysis is used in decision making processes, in particular in forecasting. Several "experts" sit together and try to compromise on something upon which they cannot agree. Further Reading: Delbecq, A., Group Techniques for Program Planning , Scott Foresman, 1975. General Laws of Probability General Law of Addition: When two or more events will happen at the same time, and the events are not mutually exclusive, then: P (X or Y) = P (X) + P (Y) - P (X and Y) Notice that, the equation P (X or Y) = P (X) + P (Y) - P (X and Y), contains especial events: An event (X and Y) which is the intersection of set/events X and Y, and another event (X or Y) which is the union (i.e., either/or) of sets X and Y. Although this is very simple, it says relatively little about how event X influences event Y and vice versa. If P (X and Y) is 0, indicating that events X and Y do not intersect (i.e., they are mutually exclusive), then we have P (X or Y) = P (X) + P (Y). On the other hand if P (X and Y) is not 0, then there are interactions between the two events X and Y. Usually it could be a physical interaction between them. This makes the relationship P (X or Y) = P (X) + P (Y) - P (X and Y) nonlinear because the P(X and Y) term is subtracted from which influences the result. The above law is known also as the Inclusion-Exclusion Formula . It can be extended to more than two events. For example, for three events A, B, and C, it becomes: P(A or B or C) = P(A) + P(B) + P(C) - P(A and B) - P(A and C) - P(B and C) + P(A and B and C) Special Law of Addition: When two or more events will happen at the same time, and the events are mutually exclusive, then: P(X or Y) = P(X) + P(Y) General Law of Multiplication: When two or more events will happen at the same time, and the events are dependent, then the general rule of multiplicative law is used to find the joint probability: P(X and Y) = P(Y) &#180; P(X|Y), where P(X|Y) is a conditional probability. Multiplicative Law: When two or more events will happen at the same time, and the events are independent, then the special rule of multiplication law is used to find the joint probability: P(X and Y) = P(X) &#180; P(Y) Conditional Probability Law: A conditional probability is denoted by P(X|Y). This phrase is read: the probability that X will occur given that Y is known to have occurred. Conditional probabilities are based on knowledge of one of the variables. The conditional probability of an event, such as X, occurring given that another event, such as Y, has occurred is expressed as: P(X|Y) = P(X and Y) &#184; P(Y), provided P(Y) is not zero. Note that when using the conditional law of probability, you always divide the joint probability by the probability of the event after the word given . Thus, to get P(X given Y), you divide the joint probability of X and Y by the unconditional probability of Y. In other words, the above equation is used to find the conditional probability for any two dependent events. The simplest version of the Bayes' Theorem is: P(X|Y) = P(Y|X) &#180; P(X) &#184; P(Y) If two events, such as X and Y, are independent then: P(X|Y) = P(X), and P(Y|X) = P(Y) The Bayes' Law: P(X|Y) = [ P(X) &#180; P(Y|X) ] &#184; [P(X) &#180; P(Y|X) + P(not X) &#180; P(Y| not X)] Bayes' Law provides posterior probability [i.e, P(X|Y)] sharpening the prior probability [i.e., P(X)] by the availability of accurate and relevant information in probabilistic terms. An Application: Suppose two machines, A and B, produce identical parts. Machine A has probability 0.1 of producing a defective each time, whereas Machine B has probability 0.4 of producing a defective. Each machine produces one part. One of these parts is selected at random, tested, and found to be defective. What is the probability that it was produced by Machine B? Probability tree diagrams depict events or sequences of events as branches of a tree. Tree diagrams are useful for visualizing the conditional probabilities: The probabilities at the end of each branch are the probability that events leading to that end will happen simultaneously. The above tree diagram indicates that the probability of a part testing Good is 9/20 + 6/20 = 3/4, therefore the probability of Bad is 1/4. Thus, P(made by B | it is bad) = (4/20) / (1/4) = 4/5. Now using the Bayes' Law we are able to obtain useful information such as: P(it is bad | made by B) = 1/4(4/5) / [1/4(4/5) + 3/4(2/5)] = 2/5. Equivalently, using the above conditional probability, results in: P(it is bad | made by B) = P(it is bad &amp; made by B)/P(made by B) = (4/20)/(1/2) = 2/5. You may like using the Bayes' Revised Probability JavaScript. Mutually Exclusive versus Independent Events Mutually Exclusive (ME): Event A and B are ME if both cannot occur simultaneously. That is, P[A and B] = 0. Independency (Ind.): Events A and B are independent if having the information that B already occurred does not change the probability that A will occur. That is P[A given B occurred] = P[A]. If two events are ME they are also Dependent: P(A given B) = P[A and B] &#184; P[B], and since P[A and B] = 0 (by ME), then P[A given B] = 0. Similarly, If two events are Independent then they are also not ME. If two events are Dependent then they may or may not be ME. If two events are not ME, then they may or may not be Independent. The following Figure contains all possibilities. The notations used in this table are as follows: X means does not imply, question mark ? means it may or may not imply, while the check mark means it implies. Notice that the (probabilistic) pairwise independency and mutual independency for a collection of events A 1 ,..., A n are two different notions. What Is so Important About the Normal Distributions? The term "normal" possibly arose because of the various attempts made to establish this distribution as the underlying law governing all continuous variables. These attempts were based on false premises and consequently failed. Nonetheless, the normal distribution rightly occupies a preeminent place in the field of probability. In addition to portraying the distribution of many types of natural and physical phenomena (such as the heights of men, diameters of machined parts, etc.), it also serves as a convenient approximation of many other distributions which are less tractable. Most importantly, it describes the manner in which certain estimators of population characteristics vary from sample to sample and, thereby, serves as the foundation upon which much statistical inference from a random sample to population are made. Normal Distribution (called also Gaussian) curves, which have a bell-shaped appearance (it is sometimes even referred to as the "bell-shaped curves") are very important in statistical analysis. In any normal distribution is observations are distributed symmetrically around the mean, 68% of all values under the curve lie within one standard deviation of the mean and 95% lie within two standard deviations. There are many reasons for their popularity. The following are the most important reasons for its applicability: One reason the normal distribution is important is that a wide variety of naturally occurring random variables such as heights and weights of all creatures are distributed evenly around a central value, average, or norm (hence, the name normal distribution). Although the distributions are only approximately normal, they are usually quite close. Whenever there are too many factors influencing the outcome of a random outcome, then the underlying distribution is approximately normal. For example, the height of a tree is determined by the "sum" of such factors as rain, soil quality, sunshine, disease, etc. As Francis Galton wrote in 1889, "Whenever a large sample of chaotic elements are taken in hand and arranged in the order of their magnitude, an unsuspected and most beautiful form of regularity proves to have been latent all along." Almost all statistical tables are limited by the size of their parameters. However, when these parameters are large enough one may use normal distribution for calculating the critical values for these tables. For example, the F-statistic is related to standard normal z-statistic as follows: F = z 2 , where F has (d.f. 1 = 1, and d.f. 2 is the largest available in the F-table). For more, visit the Relationships among Common Distributions . Approximation of the binomial: For example, the normal distribution provides a very accurate approximation of the binomial when n is large and p is close to 1/2. Even if n is small and p is not extremely close to 0 or to 1, the approximation is adequate. In fact, the normal approximation of the binomial will be satisfactory for most purposes provided that np &gt; 5 and nq &gt; 5. Here is how the approximation is made. First, set m = np and s 2 = npq. To allow for the fact that the binomial is a discrete distribution, we conventionally use a continuity correction factor of 1/2 unit added to or subtracted from X on the grounds that the discrete value (x = a) should correspond on a continuous scale to (a - 1/2) &lt; x &lt; (a + 1/2). Then we compute the value of the standard normal variable by: z = [(a - 1/2) - m ]/ s OR z = [(a + 1/2) - m ]/ s Now one may used the standard normal table for the numerical values. An Application: The probability of a defective item coming off a certain assembly line is p = 0.25. A sample of 400 items is selected from a large lot of these items. What is the probability 90 or less items are defective? If the mean and standard deviation of a normal distribution are known, it is easy to convert back and forth from raw scores to percentiles . It has been proven that the underlying distribution is normal if and only if the sample mean is independent of the sample variance, this characterizes the normal distribution. Therefore many effective transformations can be applied to convert almost any shaped distribution into a normal one. The most important reason for popularity of normal distribution is the Central Limit Theorem (CLT) . The distribution of the sample averages of a large number of independent random variables will be approximately normal regardless of the distributions of the individual random variables. The Sampling distribution of normal populations provide more information than any other distributions. For example, the following standard (i.e., having the same unit as the data have) errors are readily available: Standard Error of the Median = ( p /2n) ½ S. Standard Error of the Standard Deviation = S/(2n) ½ . Therefore, the test statistic for the null hypothesis s = s 0 , is Z = (2n) ½ (S - s 0 )/ s 0 . Standard Error of the Variance = S 2 [(2/(n-1)] ½ . Standard Error of the Interquartiles Half-Range (Q) = 1.166Q/n ½ Standard Error of the Skewness = (6/n) ½ . Standard Error of the Skewness of Sample Mean = Skewness/n ½ Notice that the skewness in sampling distribution of the mean rapidly disappears as n gets larger. Standard Error of the Kurtosis = (24/n) ½ = 2 times the standard error of skewness. Standard Error of the Correlation (r) = [(1 - r 2 )/(n-1)] ½ . Moreover, Quartile deviation &#187; 2S/3, and, Mean absolute deviation &#187; 4S/5. The other reason the normal distributions are so important is that the normality condition is required by almost all kinds of parametric statistical tests . The Central Limit Theorem is a useful tool when you are dealing with a population with an unknown distribution. Often, you may analyze the mean (or the sum) of a sample of size n. For example instead of analyzing the weights of individual items you may analyze the batch of size n, that is, the packages each containing n items. What Is A Sampling Distribution? A sampling distribution describes probabilities associated with a statistic when a random sample is drawn from the entire population. The sampling distribution is the density (for a continuous statistic, such as an estimated mean), or probability function (for discrete statistic, such as an estimated proportion). Derivation of the sampling distribution is the first step in calculating a confidence interval or carrying out a hypothesis testing for a parameter . Example: Suppose that x1,.......,xn are a simple random sample from a normally distributed population with expected value m and known variance s 2 . Then, the sample mean is normally distributed with expected value m and variance s 2 /n. The main idea of statistical inference is to take a random sample from the entire particular population and then to use the information from the sample to make inferences about the particular population characteristics such as the mean m (measure of central tendency), the standard deviation (measure of dispersion, spread) s or the proportion of units in the population that have a certain characteristic. Sampling saves money, time, and effort. Additionally, a sample can provide, in some cases, as much or more accuracy than a corresponding study that would attempt to investigate an entire population. Careful collection of data from a sample will often provide better information than a less careful study that tries to look at everything. Often, one must also study the behavior of the mean of sample values taken from different specified populations; e.g., for comparison purposes. Because a sample examines only part of a population, the sample mean will not exactly equal the corresponding mean of the population m . Thus, an important consideration for those planning and interpreting sampling results is the degree to which sample estimates, such as the sample mean, will agree with the corresponding population characteristic. In practice, only one sample is usually taken. In some cases a small "pilot sample" is used to test the data-gathering mechanisms and to get preliminary information for planning the main sampling scheme. However, for purposes of understanding the degree to which sample means will agree with the corresponding population mean m , it is useful to consider what would happen if 10, or 50, or 100 separate sampling studies, of the same type, were conducted. How consistent would the results be across these different studies? If we could see that the results from each of the samples would be nearly the same (and nearly correct!), then we would have confidence in the single sample that will actually be used. On the other hand, seeing that answers from the repeated samples were too variable for the needed accuracy would suggest that a different sampling plan (perhaps with a larger sample size) should be used. A sampling distribution is used to describe the distribution of outcomes that one would observe from replication of a particular sampling plan. Know that estimates computed from one sample will be different from estimates that would be computed from another sample. Understand that estimates are expected to differ from the population characteristics (parameters) that we are trying to estimate, but that the properties of sampling distributions allow us to quantify, based on probability, how they will differ. Understand that different statistics have different sampling distributions with distribution shape depending on (a) the specific statistic, (b) the sample size, and (c) the parent distribution . Understand the relationship between sample size and the distribution of sample estimates. Understand that increasing the sample size can reduce the variability in a sampling distribution. See that in large samples, many sampling distributions can be approximated with a normal distribution. Sampling Distribution of the Mean and the Variance for Normal Populations: Given the random variable X is distributed normally with mean m and standard deviation s , then for a random sample of size n: The sampling distribution of [ - m ] &#180; n ½ &#184; s , is the standard normal distribution. The sampling distribution of [ - m ] &#180; n ½ &#184; S, is a t-distribution with parameter d.f. = n-1. The sampling distribution of [S 2 (n-1) &#184; s 2 ], is a c 2 distribution with parameter d.f. = n-1. For two independent samples, the sampling distribution of [S 1 2 / S 2 2 ], is an F distribution with parameters d.f. 1 = n 1 -1, and d.f. 2 = n 2 -1. What Is The Central Limit Theorem? The central limit theorem (CLT) is a "limit" that is "central" to statistical practice. For practical purposes, the main idea of the CLT is that the average (center of data) of a sample of observations drawn from some population is approximately distributed as a normal distribution if certain conditions are met. In theoretical statistics there are several versions of the central limit theorem depending on how these conditions are specified. These are concerned with the types of conditions made about the distribution of the parent population (population from which the sample is drawn) and the actual sampling procedure. One of the simplest versions of the central limit theorem stated by many textbooks is: if we take a random sample of size (n) from the entire population, then, the sample mean which is a random variable defined by: S x i / n, has a histogram which converges to a normal distribution shape if n is large enough . Equivalently, the sample mean distribution approaches to normal distribution as the sample size increases. Some students having difficulty reconciling their own understanding of the central limit theorem with some of the textbooks statements. Some textbooks do not emphasize the on the independent, random samples of fixed-size n (say more than 30). The shape of the sampling distributions for means - becomes increasingly normal as the sample size n becomes larger. The increasing sample size is what causes the distribution to become increasingly normal and the independence condition provides the &#214; n contraction of the standard deviation. The CLT for proportion data , such as binary 0, 1, again the sampling distribution-- while becoming increasingly "bell-shaped"-- remains confined to the domain [0,1]. This domain represents a dramatic difference from a normal distribution, with has an unbounded domain. However, as n increases without bound, the "width" of the bell becomes very small so that the CLT "still works". In applications of the central limit theorem to practical problems in statistical inference , however, we are more interested in how closely the approximate distribution of the sample mean follows a normal distribution for finite sample size, than in the limiting distribution itself. Sufficiently close agreement with a normal distribution allows us to use normal theory for making inferences about population parameters (such as the mean ) using the sample mean, irrespective of the actual form of the parent population. It can be shown that, if the parent population has mean m and a finite standard deviation s , then the sample mean distribution has the same mean m but with smaller standard deviation which is s divided by n ½ . You know by now that, whatever the parent population is, the standardized variable Z = (X - m )/ s will have a distribution with a mean m = 0 and standard deviation s =1 under random sampling. Moreover, if the parent population is normal, then Z is distributed exactly as the standard normal. The central limit theorem states the remarkable result that, even when the parent population is non-normal, the standardized variable is approximately normal if the sample size is large enough. It is generally not possible to state conditions under which the approximation given by the central limit theorem works and what sample sizes are needed before the approximation becomes good enough. As a general guideline, statisticians have used the prescription that, if the parent distribution is symmetric and relatively short-tailed, then the sample mean more closely approximates normality for smaller samples than if the parent population is skewed or long-tailed. Under certain conditions, in large samples, the sampling distribution of the sample mean can be approximated by a normal distribution. The sample size needed for the approximation to be adequate depends strongly on the shape of the parent distribution. Symmetry (or lack thereof) is particularly important. For a symmetric parent distribution, even if very different from the shape of a normal distribution, an adequate approximation can be obtained with small samples (e.g., 15 or more for the uniform distribution). For symmetric, short-tailed parent distributions, the sample mean more closely approximates normality for smaller sample sizes than if the parent population is skewed and long-tailed. In some extreme cases (e.g. binomial) sample sizes far exceeding the typical guidelines (e.g., over 30) are needed for an adequate approximation. For some distributions without first and second moments (e.g., one is known as the Cauchy distribution), the central limit theorem does not hold. For some distributions, extremely large (impractical) samples would be required to approach a normal distribution. In manufacturing, for example, when defects occur at a rate of less than 100 parts per million, using, a Beta distribution yields an honest Confidence Interval (CI) of total defects in the population. What Is "Degrees of Freedom"? Recall that in estimating the population's variance, we used (n-1) rather than n, in the denominator. The factor (n-1) is called "degrees of freedom." Estimation of the Population Variance: Variance in a population is defined as the average of squared deviations from the population mean. If we draw a random sample of n cases from a population where the mean is known, we can estimate the population variance in an intuitive way. We sum the deviations of scores from the population mean and divide this sum by n. This estimate is based on n independent pieces of information, and we have n degrees of freedom. Each of the n observations, including the last one, is unconstrained ('free' to vary). When we do not know the population's mean, we can still estimate the population variance; but, now we compute deviations around the sample mean. This introduces an important constraint because the sum of the deviations around the sample mean is known to be zero. If we know the value for the first (n-1) deviations, the last one is known. There are only n-1 independent pieces of information in this estimate of variance. If you study a system with n parameters x i , i =1..., n, you can represent it in an n-dimension space. Any point of this space shall represent a potential state of your system. If your n parameters could vary independently, then your system would be fully described in a n-dimension hyper-volume (for n over 3). Now, imagine you have one constraint between the parameters (an equation with your n parameters), then your system would be described by a (n-1)-dimension hyper-surface (for n over 3). For example, in three dimensional space, a linear relationship means a plane which is 2-dimensional. In statistics, your n parameters are your n data. To evaluate variance, you first need to infer the mean m . So when you evaluate the variance, you have one constraint on your system (which is the expression of the mean), and it remains only (n-1) degrees of freedom to your system. Therefore, we divide the sum of squared deviations by n-1, rather than by n, when we have sample data. On average, deviations around the sample mean are smaller than deviations around the population mean. This is because our sample mean is always in the middle of our sample scores; in fact, the minimum possible sum of squared deviations for any sample of numbers is around the mean for that sample of numbers. Thus, if we sum the squared deviations from the sample mean and divide by n, we have an underestimate of the variance in the population (which is based on deviations around the population mean). If we divide the sum of squared deviations by n-1 instead of n, our estimate is a bit larger, and it can be shown that this adjustment gives us an unbiased estimate of the population variance. However, for large n, say, over 30, it does not make too much difference if we divide by n, or n-1. Degrees of Freedom in ANOVA: You will see the key parse "degrees of freedom" also appearing in the Analysis of Variance (ANOVA) tables. If I tell you about 4 numbers, but don't say what they are, the average could be anything. I have 4 degrees of freedom in the data set. If I tell you 3 of those numbers, and the average, you can guess the fourth number. The data set, given the average, has 3 degrees of freedom. If I tell you the average and the standard deviation of the numbers, I have given you 2 pieces of information, and reduced the degrees of freedom from 4 to 2. You only need to know 2 of the numbers' values to guess the other 2. In an ANOVA table, degree of freedom (df) is the divisor in (Sum of Squared deviations)/df which will result in an unbiased estimate of the variance of a population. In general, a degree of freedom d.f. = N - k, where N is the sample size, and k is a small number, equal to the number of "constraints", the number of "bits of information" already "used up". As we will see in the ANOVA section, degree of freedom is an additive quantity; total amounts of it can be "partitioned" into various components. For example, suppose we have a sample of size 13 and calculate its mean, and then the deviations from the mean; only 12 of the deviations are free to vary. Once one has found 12 of the deviations, the thirteenth one is determined. In bivariate correlation or regression situations, k = 2. The calculation of the sample means of each variable "uses up" two bits of information, leaving N - 2 independent bits of information. In a one-way analysis of variance (ANOVA) with g groups, there are three ways of using the data to estimate the population variance. If all the data are pooled, the conventional SST/(n-1) would provide an estimate of the population variance. If the treatment groups are considered separately, the sample means can also be considered as estimates of the population mean, and thus SSb/(g - 1) can be used as an estimate. The remaining ("within-group", "error") variance can be estimated from SSw/(n - g). This example demonstrates the partitioning of d.f.: d.f. total = n - 1 = d.f.(between) + d.f.(within) = (g - 1) + (n - g). Therefore, the simple 'working definition' of d.f. is sample size minus the number of estimated parameters'. A more complete answer would have to explain why there are situations in which the degrees of freedom is not an integer. After we said all this, the best explanation, is mathematical in that we use d.f. to obtain an unbiased estimate . In summary, the concept of degrees of freedom is used for the following two different purposes: Parameter(s) of certain distributions, such as F and t-distribution, are called degrees of freedom. Most importantly, the degrees of freedom are used to obtain unbiased estimates for the population parameters. Applications of and Conditions for Using Statistical Tables Some widely used applications of the popular statistical tables can be categorized as follows: T - Table: Single Population µ Test . Two Independent Populations µ's Test . The Before-and-After µ's Test . Tests Concerning Regression Coefficients . Test Concerning Correlation . Conditions for using this table: Test for randomness of the data is needed before using this table. Test for normality condition of the population distribution is also needed if the sample size is small, or it may not be possible to invoke the central limit theorem. Z - Table: Test for Randomness . Tests concerning µ for one population or two populations based on their large-size, random sample(s), (say over 30) to invoke the central limit theorem. This includes test concerning proportions , with large-size, random sample size n (say over 30) to invoke distribution convergence results. To Compare Two Correlation Coefficients . Notes: As you know by now, in test of hypotheses concerning m , and construction of confidence interval for it, we start with s known, since the critical value (and the p-value) of the Z-Table distribution can be used. Considering the more realistic situations, when we don't know s , the T-Table is used. In both cases, we need to verify the normality condition of the population's distribution; however, if the sample size n is very large, we can in fact switch back to Z-Table by virtue of the central limit theorem. For perfectly normal populations, the t-distribution corrects for any errors introduced by estimating s with s when doing inference. Note also that, in hypothesis testing concerning the parameter of binomial and Poisson distributions for large sample sizes, the standard deviation is known under the null hypotheses. That's why you may use the normal approximations for both of these distributions. Conditions for using this table: Test for randomness of the data is needed before using this table. Test for normality condition of the population distribution is also needed if the sample size is small, or it may not be possible to invoke the Central Limit Theorem. Chi-square - Table: Test for Cross-table Relationship . Identical-Populations Test for Crosstable Data . Test for Equality of Several Population Proportions . Test for Equality of Several Population Medians . Goodness-of-Fit Test for Probability Mass Functions . Compatibility of Multi-Counts . Correlation-Coefficient Testing . Necessary Conditions in Applying the Above Tests . Testing the Variance: Is the Quality that Good? . Testing the Equality of Multi-Variances . Conditions for using this table: The necessary conditions for using this table for all the above tests, except for the last one, can be found at Conditions for the Chi-square Based Tests . The last application requires normality (condition) of the population distribution. F - Table: Multi-Means Comparisons: Analysis of Variance (ANOVA) . Tests Concerning Two Variances . Overall Assessment of Regression Models . Conditions for using this table: Tests for randomness of the data and normality (condition) of the populations are needed before using this table for ANOVA. Same conditions must be satisfied for the residuals in regression analysis. The following chart summarizes application of statistical tables with respect to test of hypotheses and construction of confidence intervals for mean m and variance s 2 in one population or the comparison of two or more populations. Selection of of an Appropriate Statistical Table Click on the image to enlarge it and THEN print it You may like using Online Statistical Computation in performing most of these tests. The P-values for the Popular Distributions Web site provides P-values useful in major statistical testing. The results are more accurate than those that can be obtained (by interpolation) from statistical tables of your textbook are. Further Reading: Evans M., N. Hastings, and B. Peacock, Statistical Distributions , Wiley, 2000. Kanji G., 100 Statistical Tests , Sage Publisher, 1995. Binomial Probability Function An important class of decision problems under uncertainty involves situations for which there are only two possible random outcomes. The binomial probability function gives probability of exact number of "successes" in n independent trials, when probability of success p on single trial is a constant. Each single trial is called a Bernoulli Trial satisfying the following conditions: Each trial results in one of two possible, mutually exclusive, outcomes. One of the possible outcomes is denoted (arbitrarily) as a success, and the other is denoted a failure. The probability of a success, denoted by p, remains constant from trial to trial. The probability of a failure, 1-p, is denoted by q. The trials are independent; that is, the outcome of any particular trial is not affected by the outcome of any other trial. The number of ways of getting r successes in n trials is: P (r successes in n trials) = n C r . p r . (1- p) (n-r) = n! / [r!(n-r)!] . [p r . (1- p) (n-r) ]. The mean and variance of random variable r, are np and np(1-p), respectively, where q = 1 - p. The skewness and kurtosis are (2q -1)/ (npq) ½ , and (1- 6pq)/(npq), respectively. From its skewness, we notice that the distribution is symmetric for p =1/2 and most skewed when p is 0 or 1. Its mode is within interval [(n+1)p -1, (n+1)p], therefore if (n+1) p is not an integer, then the mode is an integer within the interval. However if (n+1)p is an integer, then its probability function has two but adjacent modes: (n+1)p -1, and (n+1)p. Determination of probabilities for p over 0.5: The binomial tables in some textbooks are limited to deterring the probabilities for values of p up to 0.5. However, these tables can be used for values of p over 0.5. By recasting a problem in terms of p to 1 -p, and setting r to n-r, then the probability of obtaining r successes in n trials for a given value of p is equal to the probability of obtaining n-r failures in n trials with 1-p. An Application: A large shipment of purchased parts is received at a warehouse, and a sample of 10 parts is checked for quality. The manufacturer's claim is that at most 5% might be defective. What is the chance that the sample includes one defective? P (one defective out of ten) = {10! /[(1!)(9!)]}(0.05) 1 (0.95) 9 = 32%. Know that the binomial distribution is to satisfy the five following requirements: (1) each trial can have only two outcomes or its outcomes can be reduced to two categories which are called pass and fail, (2) there must be a fixed number of trials, (3) the outcome of each trail must be independent, (4) the probabilities must remain constant, (5) and the outcome of interest is the number of successes. Normal approximation for binomial: All binomial tables are limited in their scope; therefore it is necessary to use standard normal distribution in computing the binomial probabilities. The following numerical example illustrates how good the approximation could be. This provides an indication for real applications when n is beyond the given values in the available binomial tables. Numerical Example: A sample of 20 items are taken randomly from a manufacturing process with defective probability p = 0.40. What is the probability of obtaining exactly 5 defective? P (5 out of 20) = {20!/[(5!)(15!)]} &#180; (0.40) 5 (0.6) 15 = 7.5% Since the mean and standard deviation of distribution are: m = np = 8, and s = (npq) 1/2 = 2.19, respectively; therefore, the standardized observation for r = 5, by using the continuity factor (which always enlarges) are: z 1 = [(r-1/2) - m ] / s = (4.5 -8)/2.19 = -1.60, and z 2 = [(r+1/2) - m ] / s = (5.5 -8)/2.19 = -1.14. Therefore, the approximated P (5 out of 20) is P (z being within interval -1.60, -1.14). Now, by using the standard normal table, we obtain: P (5 out of 20) = 0.44520 - 0.37286 = 7.2% Comments: The approximation for binomial distribution is used frequently in quality control, reliability, survey sampling, and other industrial problems. You might like to use the Exact Confidence Interval Construction and Test of Hypothesis for Binomial Population , and Binomial Probability Function Applet JavaScript in performing some numerical experimentation for validating the above assertions for a deeper understanding. Exponential Density Function An important class of decision problems under uncertainty concerns the random durations between events. For example, the the length of time between breakdowns of a machine not exceeding a certain time interval, such as the copying machine in your office not breaking down during this week. Exponential distribution gives distribution of time between independent events occurring at a constant rate. Its density function is: f(t) = l exp(- l t), where l is the average number of events per unit of time, which is a positive number. The mean and the variance of the random variable t (time between events) are 1/ l , and 1/ l 2 , respectively. Applications include probabilistic assessment of the time between arrivals of patients to the emergency room of a hospital, and time between arrivals of ships at a particular port. Comments: Itis a special case of Gamma distribution. You might like to use Exponential Density Applet to perform your computations, and Lilliefors Test for Exponentiality to perform the goodness-of-fit test. F-Density Function The F distribution is the distribution of the ratio of two independent sampling (of size of n 1 , and n 2 , respectively) estimates of variance from standard normal distributions. It is also formed by the ratio of two independent chi-square variables divided by their respective independent degrees of freedom. Its main applications are in testing equality of two independent population variances based on two independent random samples, ANOVA , and regression analysis . You might like to use F-Density Function to obtain its P-values. Chi-square Density Function The probability density curve of a Chi-square distribution is an asymmetric curve stretching over the positive side of the line and having a long right tail. The form of the curve depends on the value of a parameter known as the degree of freedom (d.f.). The expected value of Chi-square statistic is its d.f., its variance is twice of its d.f., and its mode is equal to (d.f.- 2). Chi square Distribution relation to Normal Distribution: The Chi-square distribution is related to the sampling distribution of the variance when the sample is from a normal distribution. The sample variance is a sum of squares of standard normal variables N (0, 1). Hence, the of square of N (0,1) random variable is a Chi-square with 1 d.f.. Notice that the Chi-square is related to F-statistics as follows: F = Chi-square/d.f. 1 , where F has (d.f. 1 = d.f. of the Chi-square-table, and d.f. 2 is the largest available in the F-table) Similar to Normal random variable s, the Chi-square has the additive property. For example, for two independent Chi-square variables, their sum is also Chi-square with degrees of freedom equal to the sum of the d.f. of the individual d.f.s. Thus the unbiased sample variance for a sample of size n from N (0,1) is a sum of n-1 Chi-squares, each with d.f. = 1, hence Chi-square with d.f. = n-1. The most widely used applications of Chi-square distribution are: The Chi-square Test for Association which is a non-parametric test; therefore, it can be used for nominal data too. It is a test of statistical significance widely used bivariate tabular association analysis. Typically, the hypothesis is whether or not two populations are different in some characteristic or aspect of their behavior based on two random samples. This test procedure is also known as the Pearson Chi-square test. The Chi-square Goodness-of-Fit Test is used to test if an observed distribution conforms to any particular distribution. Calculation of this goodness-of-fit test is by comparison of observed data with data expected based on a particular distribution. You might like to use Chi-square Density to find its P-values. Multinomial Probability Function A multinomial random variable is an extended binomial. However, the difference is that in a multinomial case, there are more than two possible outcomes. There are a fixed number of independent outcomes, with a given probability for each outcome. The Expected Value (i.e., averages): Expected Value = m = S X i &#180; P i , the sum is over all i's. Expected value is known also as the First Moment , borrowed from Physics, because it is the point of balance where the data and the probabilities are the distances and the weights, respectively. The Variance is: Variance = s 2 = S [X i 2 &#180; P i ] - m 2 , the sum is over all i's. The variance is not expressed in the same units as the expected value. So, the variance is hard to understand and to explain as a result of the squared term in its computation. This can be alleviated by working with the square root of the variance, which is called the Standard (i.e., having the same unit as the data have) Deviation : Standard Deviation = s = (Variance) ½ Both variance and standard deviation provide the same information and, therefore, one can always be obtained from the other. In other words, the process of computing standard deviation always involves computing the variance. Since standard deviation is the square root of the variance, it is always expressed in the same units as the expected value. For the dynamic process, the Volatility as a measure for risk includes the time period over which the standard deviation is computed. The Volatility measure is defined as standard deviation divided by the square root of the time duration. Coefficient of Variation : Coefficient of Variation (CV) is the absolute relative deviation with respect to size provided is not zero, expressed in percentage: CV =100 | s / | % Notice that the CV is independent from the expected value measurement. The coefficient of variation demonstrates the relationship between standard deviation and expected value, by expressing the risk as a percentage of the expected value. The inverse of CV (namely 1/CV) is called the Signal-to-Noise Ratio . You might like to use Multinomial Applet for checking your computation and performing computer-assisted experimentation. An Application: Consider two investment alternatives, Investment I and Investment II with the characteristics outlined in the following table: - Two Investments - Investment I Investment II Payoff % Prob. Payoff % Prob. 1 0.25 3 0.33 7 0.50 5 0.33 12 0.25 8 0.34 Performance of Two Investments To rank these two investments under the Standard Dominance Approach in Finance , first we must compute the mean and standard deviation and then analyze the results. Using the Multinomial Applet for calculation, we notice that the Investment I has mean = 6.75% and standard deviation = 3.9%, while the second investment has mean = 5.36% and standard deviation = 2.06%. First observe that under the usual mean-variance analysis, these two investments cannot be ranked. This is because the first investment has the greater mean; it also has the greater standard deviation; therefore, the Standard Dominance Approach is not a useful tool here. We have to resort to the coefficient of variation (C.V.) as a systematic basis of comparison. The C.V. for Investment I is 57.74% and for Investment II is 38.43%. Therefore, Investment II has preference over the Investment I. Clearly, this approach can be used to rank any number of alternative investments. Notice that less variation in return on investment implies less risk. You might like to use this Applet in performing some numerical experimentation to: Show that E[aX + b] = aE(X) + b. Show that V[aX + b] = a 2 V(X). Show that: E(X 2 )= V(X) + (E(X)) 2 . Normal Density Function In the Descriptive Statistic Section of this Web site, we have been concerned with how empirical scores are distributed and how best to describe their distribution. We have discussed several different measures, but the mean m will be the measure that we use to describe the center of the distribution, and the standard deviation s will be the measure we use to describe the spread of the distribution. Knowing these two facts gives us ample information to make statements about the probability of observing a certain value within that distribution. If I know, for example, that the average Intelligence Quotient (I.Q.) score is 100 with a standard deviation of s = 20, then I know that someone with an I.Q. of 140 is very smart. I know this because 140 deviates from the mean m by twice the average amount as the rest of the scores in the distribution. Thus, it is unlikely to see a score as extreme as 140 because most of the I.Q. scores are clustered around 100 and only deviate 20 points from the mean m . Many applications arise from the central limit theorem (CLT). The CLT states that, average of values of n observations approaches normal distribution, irrespective of the form of original distribution under quite general conditions. Consequently, normal distribution is an appropriate model for many, but not all, physical phenomena, such as distribution of physical measurements on living organisms, intelligence test scores, product dimensions, average temperatures, and so on. Know that the Normal distribution is to satisfy seven requirements: (1) the graph should be bell shaped curve; (2) mean, median and mode are all equal; (3) mean, median and mode are located at the center of the distribution; (4) it has only one mode, (5) it is symmetric about mean, (6) it is a continuous function; (6) it never touches x-axis; and (7) the area under curve equals one. Many methods of statistical analysis presume normal distribution. When we know the mean and variance of a Normal then it allows us to find probabilities. So, if, for example, you knew some things about the average height of women in the nation, including the fact that heights are distributed normally, you could measure all the women in your extended family and find the average height. This enables you to determine a probability associated with your result, if the probability of getting your result, given your knowledge of women nationwide, is high. Then your family's female height cannot be said to be different from average. If that probability is low, then your result is rare (given the knowledge about women nationwide), and you can say your family is different. You have just completed a test of the hypothesis that the average height of women in your family is different from the overall average. The ratio of two independent observations from the standard normal is distributed as the Cauchy Distribution which has thicker tails than a normal distribution. It density function is f(x) = 1/[ p (1+x 2 )], for all real value x. You might like to use Standard Normal Applet instead of using tabular values from your textbook, and the well-known Lilliefors' Test for Normality to assess the goodness-of-fit. Poisson Probability Function Life is good for only two things, discovering mathematics and teaching mathematics. -- Simeon Poisson An important class of decision problems under uncertainty is characterized by the small chance of the occurrence of a particular event, such as an accident. Poisson probability function computes the probability of exactly x independent occurrences during a given period of time, if events take place independently and at a constant rate. Poisson probability function also represent number of occurrences over constant areas or volumes: Poisson probabilities are often used; for example in quality control, software and hardware reliability, insurance claim, number of incoming telephone calls, and queuing theory. An Application: One of the most useful applications of the Poisson distribution is in the field of queuing theory. In many situations where queues occur it has been shown that the number of people joining the queue in a given time period follows the Poisson model. For example, if the rate of arrivals to an emergency room is l per unit of time period (say 1 hr), then: P ( n arrivals) = l n e - l / n! The mean and variance of random variable n are both l . However if the mean and variance of a random variable have equal numerical values, then it is not necessary that its distribution is a Poisson. Its mode is within interval [ l -1, l ]. Applications: P ( 0 arrival) = e - l P ( 1 arrival) = l e - l / 1! P ( 2 arrival) = l 2 e - l / 2! and so on. In general: P ( n+1 arrivals ) = l P ( n arrivals ) / n. Normal approximation for Poisson: All Poisson tables are limited in their scope; therefore, it is necessary to use standard normal distribution in computing the Poisson probabilities. The following numerical example illustrates how good the approximation could be. Numerical Example: Emergency patients arrive at a large hospital at the rate of 0.033 per minute. What is the probability of exactly two arrivals during the next 30 minutes? The arrival rate during 30 minutes is l = (30)(0.033) = 1. Therefore, P (2 arrivals) = [1 2 /(2!)] e -1 = 18% The mean and standard deviation of distribution are: m = l = 1, and s = l 1/2 = 1, respectively; therefore, the standardized observation for n = 2, by using the continuity factor (which always enlarges) are: z 1 = [(r-1/2) - m ] / s = (1.5 -1)/1 = 0.5, and z 2 = [(r+1/2) - m ] / s = (2.5 -1)/1 = 1.5. Therefore, the approximated P (2 arrivals) is P (z being within the interval 0.5, 1.5). Now, by using the standard normal table, we obtain: P (2 arrivals) = 0.43319 - 0.19146 = 24% As you see the approximation is slightly overestimated, therefore the error is on the safe side. For large values of l , say over 20, one may use the Normal approximation to calculate Poisson probabilities. Notice that by taking the square root of a Poisson random variable , the transformed variable is more symmetric. This is a useful transformation in regression analysis of Poisson observations. You might like to use Poisson Probability Function Applet to perform your computation, and Testing Poisson to perform the goodness-of-fit test. Further Reading: Barbour et al. , Poisson Approximation , Oxford University Press, 1992. Student T-Density Function The t distributions were discovered in 1908 by William Gosset , who was a chemist and a statistician employed by the Guinness brewing company. He considered himself a student still learning statistics, so that is how he signed his papers as pseudonym "Student". Or, perhaps he used a pseudonym due to "trade secret" restrictions by Guinness. Note that there are different t-distributions; it is a class of distributions. When we speak of a specific t distribution, we have to specify the degrees of freedom. The t density curves are symmetric and bell-shaped like the normal distribution and have their peak at 0. However, the spread is more than that of the standard normal distribution. The larger the degrees of freedom, the closer the t-density is to the normal density. The shape of a t-distribution depends on a parameter called "degree-of-freedom". As the degree-of-freedom gets larger, the t-distribution gets closer and closer to the standard normal distribution. For practical purposes, the t-distribution is treated as the standard normal distribution when degree-of-freedom is greater than 30. Suppose we have two independent random variable s, one is Z, distributed as the standard normal distribution, while the other has a Chi-square distribution with (n-1) d.f.; then the random variable : (n-1)Z / c 2 has a t-distribution with (n-1) d.f. For large sample size (say, n over 30), the new random variable has an expected value equal to zero, and its variance is (n-1)/(n-3) which is close to one. Notice that the t- statistic is related to F-statistic as follow: F = t 2 , where F has (d.f. 1 = 1, and d.f. 2 = d.f. of the t-table) You might like to use Student t-Density to obtain its P-values. Triangular Density Function Triangular Density Function The triangular distribution shows the number of successes when you know the minimum, maximum, and most likely values. For example, you could describe the number of intakes seen per week when past intake data show the minimum, maximum, and most likely number of cases seen. It has a continuous probability distribution. The parameters for the triangular distribution are Minimum, Maximum, and Likeliest. There are three conditions underlying triangular distribution: The minimum number of items is fixed. The maximum number of items is fixed. The most likely number of items falls between the minimum and maximum values. These three parameters forming a triangular shaped distribution, which shows that values near the minimum and maximum are less apt to occur than those near the most likely value. Further Reading: Evans M., Hastings N., and B., Peacock, Triangular Distribution , Ch. 40 in Statistical Distributions, Wiley, pp. 187-188, 2000. Uniform Density Function The uniform density function gives the probability that observation will occur within a particular interval [a, b] when probability of occurrence within that interval is directly proportional to interval length. Its mean and variance are: m = (a+b)/2, s 2 = (b-a) 2 /12. Applications: Used to generate random numbers in sampling and Monte Carlo simulation. Comments: Special case of beta distribution. You might like to use Goodness-of-Fit Test for Uniform and performing some numerical experimentation for a deeper understanding of the concepts. Notice that any Uniform distribution has uncountable number of modes having equal density value; therefore it is considered as a homogeneous population. Further Reading: Balakrishnan N., and V. Nevzorov, A Primer on Statistical Distributions , Wiley, 2003. Necessary Conditions for Statistical Decision Making Introduction to Inferential Data Analysis Necessary Conditions: Do not just learn formulas and number-crunching. Learn about the conditions under which statistical testing procedures apply. The following conditions are common to almost all statistical tests: Any undetected outliers may have major impact and may influence the results of almost all statistical estimation and testing procedures. Homogeneous population. That is, there is not more than one mode. Perform Test for Homogeneity of a Population The sample must be random. Perform Test for Randomness . In addition to the Homogeneity requirement, each population has a normal distribution. Perform the Lilliefors' Test for Normality . Homogeneity of variances. Variation in each population is almost the same as in the other(s). Perform The Bartlett's Test . For two populations use the F-test. For 3 or more populations, there is a practical rule known as the "Rule of 2". In this rule, one divides the highest variance of a sample by the lowest variance of the other sample. Given that the sample sizes are almost the same, and the value of this division is less than 2, then the variations of the populations are almost the same. Notice: This important condition in analysis of variance (ANOVA and the t-test for mean differences) is commonly tested by the Levene test or its modified test known as the Brown-Forsythe test. Interestingly, both tests rely on the homogeneity of variances condition! These conditions are crucial, not for the method of computation, but for the testing using the resultant statistic. Otherwise, we can do ANOVA and regression without any assumptions, and the numbers come out the same. Simple computations give us least-square fits, partitions of variance, regression coefficients, and so on. We do need the above conditions when test of hypotheses are our main concern. Further Readings: Good Ph., and J. Hardin, Common Errors in Statistics , Wiley, 2003. Wang H., Improved confidence estimators for the usual one-sided confidence intervals for the ratio of two normal variances, Statistics &amp; Probability Letters , Vol. 59, No.3, 307-315, 2002. Measure of Surprise for Outlier Detection Robust statistical techniques are needed to cope with any undetected outliers; otherwise they are more likely to invalidate the conditions underlying statistical techniques , and they may seriously distort estimates and produce misleading conclusions in test of hypotheses. A common approach consists of assuming that contaminating models, different from the one generating the rest of the data, generate the (possible) outliers. Because of a potentially large variance, outliers could be the outcome of sampling errors or clerical errors such as recording data. Therefore, you must be very careful and cautious. Before declaring an observation "an outlier," find out why and how such observation occurred. It could even be an error at the data entering stage while using any computer package. In practice, any observation with a standardized value greater than 2.5 in absolute value is a candidate for being an outlier. In such a case, one must first investigate the source of the datum. If there is no doubt about the accuracy or veracity of the observation, then it should be removed, and the model should be refitted. Compute the mean ( ) and standard deviation (S) of the whole sample. Set limits for the mean : - k &#180; S, + k &#180; S. A typical value for k is 2.5 Remove all sample values outside the limits. Now, iterate through the algorithm, the sample set may reduce after removing the outliers by applying step 3. In most cases, we need to iterate through this algorithm several times until all outliers are removed. An Application: Suppose you ask ten of your classmates to measure a given length X. The results (in mm) are: 46, 48, 38, 45, 47, 58, 44, 45, 43, 44 Is 58 an outlier? Computing the mean and the variance of the ten measurement using the Descriptive Sampling Statistics JavaScript, are 45.8, and 5.1(after the needed adjustment), respectively. The Z-value for 58 is Z (58) = 2.4. Since the measurements, in general, follow a normal distribution, therefore, Probability [X as large as 2.4 times standard deviation] = 0.008, obtained by using the Standard Normal P-value JavaScript, or from the normal table in your textbook. According this probability, one expects only .09 of the ten measurements as bad as this one. This is a very rare event, however, in spite of such small probability, it has occurred, therefore, it might be an outlier. The next most suspected measurement is 38, is it an outlier? It is a question for you. A Notice: Outlier detection in the single population setting is not too difficult. Quite often, however, one can argue that the detected outliers are not really outliers, but form a second population . If this is the case, a data separation approach needs to be taken. You might like to use the Identification of Outliers JavaScript in performing some numerical experimentation for validating and for a deeper understanding of the concepts Further Reading: Rothamsted V., V. Barnett, and T. Lewis, Outliers in Statistical Data , Wiley, 1994. Homogeneous Population A homogeneous population is a statistical population which has a unique mode . Notice that, e.g., a Uniform distribution has uncountable number of modes having equal density value; therefore it is considered as a homogeneous population. To determine if a given population is homogeneous or not, construct the histogram of a random sample from the entire population. If there is more than one mode, then you have a mixture of two or more different populations. Know that to perform any statistical testing, you need to make sure you are dealing with a homogeneous population. One of the main applications of histogramming is to Test for Homogeneity of a Population . The unimodality of the histogram is a necessary condition for the homogeneity of a population in order to conduct any meaningful statistical analysis. However, notice that, e.g., a Uniform distribution has uncountable number of modes having equal density value; therefore it is considered as a homogeneous population. Test for Randomness: The Runs' Test A basic condition in almost all inferential statistics is that a set of data constitutes a random sample from a given homogeneous population. The condition of randomness is essential to make sure the sample is truly representitive of the population . The widely used test for randomness is the Runs test. A "run" is a maximal subsequence of like elements. Consider the following sequence (D for Defective items, N for Non-defective items) from a production line: DDDNNDNDNDDD. Number of runs is R = 7, with n 1 = 8, and n 2 = 4 which are number of D's and N's. A sequence is a random sequence if it is neither "over-mixed" nor "under-mixed". An example of over-mixed sequence is DDDNDNDNDNDD, with R = 9 while under-mixed looks like DDDDDDDDNNNN with R = 2. There the above sequence seems to be a random sequence. The Runs Tests, which is also known as Wald-Wolfowitz Test, is designed to test the randomness of a given sample at 100(1- a )% confidence level. To conduct a runs test on a sample, perform the following steps: Step 1: compute the mean of the sample. Step 2: going through the sample sequence, replace any observation with +, or - depending on whether it is above or below the mean. Discard any ties. Step 3: compute R, n 1 , and n 2 . Step 4: compute the expected mean and variance of R, as follows: a =1 + 2n 1 n 2 /(n 1 + n 2 ). s 2 = 2n 1 n 2 (2n 1 n 2 -n 1 - n 2 )/[[n 1 + n 2 ) 2 (n 1 + n 2 -1)]. Step 5: Compute z = (R- m )/ s . Step 6: Conclusion: If z &gt; Z a , then there might be cyclic, seasonality behavior (under-mixing). If z &lt; - Z a , then there might be a trend. If z &lt; - Z a/2 , or z &gt; Z a/2 , reject the randomness. Note: This test is valid for cases for which both n 1 and n 2 are large, say greater than 10. For small sample sizes, special tables must be used. For example, suppose for a given sample of size 50, we have R = 24, n 1 = 14 and n 2 = 36. Test for randomness at a = 0.05. The Plugging these into the above formulas we have a = 16.95, s = 2.473, and z = -2.0 From Z-table, we have Z = 1.645. Therefore, there might be a trend, which means that the sample is not random. You may use the following JavaScript to Test for Randomness . Test for Normality The standard test for normality is the Lilliefors' statistic. A histogram and normal probability plot will also help you distinguish between a systematic departure from normality when it shows up as a curve. Lilliefors' Test for Normality: This test is a special case of the Kolmogorov-Smirnov goodness-of-fit test , developed for testing the normality of population's distribution. When applying the Lilliefors test, a comparison is made between the standard normal cumulative distribution function , and a sample cumulative distribution function with standardized random variable . If there is a close agreement between the two cumulative distributions, the hypothesis that the sample was drawn from population with a normal distribution function is supported. If, however, there is a discrepancy between the two cumulative distribution functions too great to be attributed to chance alone, then the hypothesis is rejected. The difference between the two cumulative distribution functions is measured by the statistic D, which is the greatest vertical distance between the two functions. You might like to use the well-known Lilliefors' Test for Normality to assess the goodness-of-fit. Further Readings Thode T., Testing for Normality , Marcel Dekker, Inc., 2001. Contains the major tests for normality. Introduction to Estimation To estimate means to esteem (to give value to). An estimator is any quantity calculated from the sample data which is used to give information about an unknown quantity in the population. For example, the sample mean is an estimator of the population mean m . Results of estimation can be expressed as a single value; known as a point estimate, or a range of values, referred to as a confidence interval. Whenever we use point estimation, we calculate the margin of error associated with that point estimation. Estimators of population parameters are sometimes distinguished from the true value by using the symbol 'hat'. For example, true population standard deviation s is estimated from a sample population standard deviation. Again, the usual estimator of the population mean is = S x i / n, where n is the size of the sample and x 1 , x 2 , x 3 ,.......,x n are the values of the sample. If the value of the estimator in a particular sample is found to be 5, then 5 is the estimate of the population mean µ. Qualities of a Good Estimator A "Good" estimator is the one which provides an estimate with the following qualities: Unbiasedness: An estimate is said to be an unbiased estimate of a given parameter when the expected value of that estimator can be shown to be equal to the parameter being estimated. For example, the mean of a sample is an unbiased estimate of the mean of the population from which the sample was drawn. Unbiasedness is a good quality for an estimate, since, in such a case, using weighted average of several estimates provides a better estimate than each one of those estimates. Therefore, unbiasedness allows us to upgrade our estimates. For example, if your estimates of the population mean µ are say, 10, and 11.2 from two independent samples of sizes 20, and 30 respectively, then a better estimate of the population mean µ based on both samples is [20 (10) + 30 (11.2)] (20 + 30) = 10.75. Consistency: The standard deviation of an estimate is called the standard error of that estimate. The larger the standard error the more error in your estimate. The standard deviation of an estimate is a commonly used index of the error entailed in estimating a population parameter based on the information in a random sample of size n from the entire population. An estimator is said to be "consistent" if increasing the sample size produces an estimate with smaller standard error. Therefore, your estimate is "consistent" with the sample size. That is, spending more money to obtain a larger sample produces a better estimate. Efficiency: An efficient estimate is one which has the smallest standard error among all unbiased estimators. The "best" estimator is the one which is the closest to the population parameter being estimated. The Concept of Distance for an Estimator Click on the image to enlarge it and THEN print it The above figure illustrates the concept of closeness by means of aiming at the center for unbiased with minimum variance . Each dart board has several samples: The first one has all its shots clustered tightly together, but none of them hit the center. The second one has a large spread, but around the center. The third one is worse than the first two. Only the last one has a tight cluster around the center, therefore has good efficiency. If an estimator is unbiased, then its variability will determine its reliability. If an estimator is extremely variable, then the estimates it produces may not on average be as close to the population parameter as a biased estimator with small variance. The following chart depicts the quality of a few popular estimators for the population mean µ: The widely used estimator of the population mean µ is = S x i /n, where n is the size of the sample and x 1 , x 2 , x 3 ,......., x n are the values of the sample that have all of the above good properties. Therefore, it is a "good" estimator. If you want an estimate of central tendency as a parameter for a test or for comparison, then small sample sizes are unlikely to yield any stable estimate. The mean is sensible in a symmetrical distribution as a measure of central tendency; but, e.g., with ten cases, you will not be able to judge whether you have a symmetrical distribution. However, the mean estimate is useful if you are trying to estimate the population sum, or some other function of the expected value of the distribution. Would the median be a better measure? In some distributions (e.g., shirt size) the mode may be better. BoxPlot will indicate outliers in the data set. If there are outliers, the median is better than the mean as a measure of central tendency. You might like to use Descriptive Statistics Applet for obtaining "good" estimates. Further Readings Casella G., and R. Berger, Statistical Inference , Wadsworth Pub. Co., 2001. Lehmann E., and G. Casella, Theory of Point Estimation , Springer Verlag, New York, 1998. Statistics with Confidence In practice, a confidence interval is used to express the uncertainty in a quantity being estimated. There is uncertainty because inferences are based on a random sample of finite size from the entire population or process of interest. To judge the statistical procedure we can ask what would happen if we were to repeat the same study, over and over, getting different data (and thus different confidence intervals) each time. In most studies, investigators are usually interested in determining the size of difference of a measured outcome between groups, rather than a simple indication of whether or not it is statistically significant. Confidence intervals present a range of values, on the basis of the sample data, in which the value of such a difference may lie. Know that a confidence interval computed from one sample will be different from a confidence interval computed from another sample. Understand the relationship between sample size and width of confidence interval, moreover, know that sometimes the computed confidence interval does not contain the true value. Let's say you compute a 95% confidence interval for a mean m . The way to interpret this is to imagine an infinite number of samples from the same population, 95% of the computed intervals will contain the population mean m , and at most 5% will not. However, it is wrong to state, "I am 95% confident that the population mean m falls within the interval." Again, the usual definition of a 95% confidence interval is an interval constructed by a process such that the interval will contain the true value 95% of the time. This means that "95%" is a property of the process, not the interval. Is the probability of occurrence of the population mean greater in the confidence interval (CI) center and lowest at the boundaries? Does the probability of occurrence of the population mean in a confidence interval vary in a measurable way from the center to the boundaries? In a general sense, normality condition is assumed, and then the interval between CI limits is represented by a bell shaped t distribution. The expectation (E) of another value is highest at the calculated mean value, and decreases as the values approach the CI limits. Tolerance Interval and CI: A good approximation for the single measurement tolerance interval is n ½ times confidence interval of the mean. Statistics with Confidence Click on the image to enlarge it and THEN print it You need to use Sample Size Determination JavaScript at the design stage of your statistical investigation in decision making with specific subjective requirements. A Note on Multiple Comparison via Individual Intervals: Notice that, if the confidence intervals from two samples do not overlap, there is a statistically significant difference, say at 5%. However, the other way is not true; two confidence intervals can overlap even when there is a significant difference between them. As a numerical example, consider the means of two independent samples. Suppose their values are 10 and 22 with equal standard error of 4. The 95% confidence interval for the two statistics (using the critical value of 1.96) are: [2.2, 17.8] and [14.2, 29.8], respectively. As you see they display considerable overlap. However, the z-statistic for the two-population mean is: |22 -10|/(16 + 16) ½ = 2.12 which is clearly significant under the same conditions as applied for constructing the confidence intervals. One should examine the confidence interval for the difference explicitly. Even if the confidence intervals are overlapping, it is hard to find the exact overall confidence level. However, the sum of individual confidence levels can serve as an upper limit. This is evident from the fact that: P(A and B) £ P(A) + P(B). The Confidence Interval JavaScript demonstrates the precision vs confidence. Further Reading: Cohen J., Statistical Power Analysis for the Behavioral Sciences , L. Erlbaum Associates, 1988. Kraemer H., and S. Thiemann, How Many Subjects? Provides basic sample size tables , explanations, and power analysis. Murphy K., and B. Myors, Statistical Power Analysis , L. Erlbaum Associates, 1998. Provides a simple and general sample size determination for hypothesis tests. Newcombe R., Interval estimation for the difference between independent proportions: Comparison of eleven methods, Statistics in Medicine , 17, 873-890, 1998. Hahn G. and W. Meeker, Statistical Intervals: A Guide for Practitioners , Wiley, 1991. Schenker N., and J. Gentleman, On judging the significance of differences by examining the overlap between confidence intervals, The American Statistician , 55(2), 135-139, 2001. What Is the Margin of Error? Estimation is the process by which sample data are used to indicate the value of an unknown quantity in a population. Results of estimation can be expressed as a single value, known as a point estimate; or a range of values, referred to as a confidence interval. Whenever we use point estimation, we calculate the margin of error associated with that point estimate. For example, for the estimation of the population proportion, by the means of sample proportion (p), the margin of error is calculated often as follows: ±1.96 [p(1-p)/n] 1/2 In newspapers and television reports on public opinion polls, the margin of error often appears in a small font at the bottom of a table or screen. However, reporting the amount of error only, is not informative enough by itself, what is missing is the degree of the confidence in the findings. The more important missing piece of information is the sample size n; that is, how many people participated in the survey, 100 or 100000? By now, you know well that the larger the sample size the more accurate is the finding, right? The reported margin of error is the margin of "sampling error". There are many non-sampling errors that can and do affect the accuracy of polls. Here we talk about sampling error. The fact that sub-groups might have sampling error larger than the group, one must include the following statement in the report: "Other sources of error include, but are not limited to, individuals refusing to participate in the interview and inability to connect with the selected number. Every feasible effort was made to obtain a response and reduce the error, but the reader (or the viewer) should be aware that some error is inherent in all research." If you have a yes/no question in a survey, you probably want to calculate a proportion P of Yes's (or No's). In a simple random sample survey, the variance of p is p(1-p)/n, ignoring the finite population correction, for large n, say over 30. Now a 95% confidence interval is p - 1.96 [p(1-p)/n] 1/2 , p + 1.96 [p(1-p)/n] 1/2 . A conservative interval can be calculated, since p(1-p) takes its maximum value when p = 1/2. Replace 1.96 by 2, put p = 1/2 and you have a 95% consevative confidence interval of 1/n 1/2 . This approximation works well as long as p is not too close to 0 or 1. This useful approximation allows you to calculate approximate 95% confidence intervals. For continuous random variables, such as the estimation of the population mean m , the margin of error is calculated often as follows: ±1.96 S/n 1/2 . The margin of error can be reduced by one or a combination of the following strategies: Decreasing the confidence in the estimate -- an undesirable strategy since confidence relates to the chance of drawing the wrong conclusion (i.e., increases the Type II error). Reducing the standard deviation -- something we cannot do since it is usually a static property of the population. Increasing the sample size -- this provides more information for a better decision. You might like to use Descriptive Statistics Applet to check your computations, and Sample Size Determination JavaScript at the design stage of your statistical investigation in decision making with specific subjective requirements. Further Reading Levy P., and S. Lemeshow, Sampling of Populations: Methods and Applications , Wiley, 1999. Bias Reduction Techniques: Bootstrapping and Jackknifing Some inferencial statistical techniques do not require distributional assumptions about the statistics involved. These modern non-parametric methods use large amounts of computation to explore the empirical variability of a statistic, rather than making a priori assumptions about this variability, as is done in the traditional parametric t- and z- tests. Bootstrapping: Bootstrapping method is to obtain an estimate by combining estimators to each of many sub-samples of a data set. Often M randomly drawn samples of T observations are drawn from the original data set of size n with replacement, where T is less n. Jackknife Estimator: A jackknife estimator creates a series of estimate, from a single data set by generating that statistic repeatedly on the data set leaving one data value out each time. This produces a mean estimate of the parameter and a standard deviation of the estimates of the parameter. Monte Carlo simulation allows for the evaluation of the behavior of a statistic when its mathematical analysis is intractable. Bootstrapping and jackknifing allow inferences to be made from a sample when traditional parametric inference fails. These techniques are especially useful to deal with statistical problems, such as small sample size, statistics with no well-developed distributional theory, and parametric inference condition violations. Both are computer intensive. Bootstrapping means you take repeated samples from a sample and then make statements about a population. Bootstrapping entails sampling-with-replacement from a sample. Jackknifing involves systematically doing n steps, of omitting 1 case from a sample at a time, or, more generally, n/k steps of omitting k cases; computations that compare "included" vs. "omitted" can be used (especially) to reduce the bias of estimation. Both have applications in reducing bias in estimations. Resampling -- including the bootstrap, permutation, and other non-parametric tests -- is a method for hypothesis testing, confidence limits, and other applied problems in statistics and probability. It involves no formulas or tables. Following the first publication of the general technique (and the bootstrap) in 1969 by Julian Simon and subsequent independent development by Bradley Efron, resampling has become an alternative approach for testing hypotheses. There are other findings: "The bootstrap started out as a good notion in that it presented, in theory, an elegant statistical procedure that was free of distributional conditions. In practice the bootstrap technique doesn't work very well, and the attempts to modify it make it more complicated and more confusing than the parametric procedures that it was meant to replace." While resampling techniques may reduce the bias, they achieve this at the expense of increase in variance. The two major concerns are: The loss in accuracy of the estimate as measured by variance can be very large. The dimension of the data affects drastically the quality of the samples and therefore the estimates. Further Readings: Young G., Bootstrap: More than a Stab in the Dark?, Statistical Science , l9, 382-395, 1994. Provides the pros and cons on the bootstrap methods. Yatracos Y., Assessing the quality of bootstrap samples and of the bootstrap estimates obtained with finite resampling, Statistics and Probability Letters , 59, 281-292, 2002. Prediction Intervals In many application of business statistics, such as forecasting, we are interested in construction of a statistical interval for random variable , rather than a parameter of a population distribution. The Tchebysheff's inequality is often used to put bounds on the probability that a proportion of random variable X will be within k &gt; 1 standard deviation of the mean m for any probability distribution. In other words: P [|X - m | ³ k s ] £ 1/k 2 , for any k greater than 1 The symmetric property of Tchebysheff's inequality is useful; e.g., in constructing control limits in the quality control process. However, the limits are very conservative due to lack of knowledge about the underlying distribution. The above bounds can be improved (i.e., becomes tighter) if we have some knowledge about the population distribution. For example, if the population is homogeneous; that is, its distribution is unimodal; then, P [|X - m | ³ k s ] £ 1/(2.25k 2 ), for any k greater than 1. The above inequality is known as the Camp-Meidell inequality. Now, let X be a random variable distributed normally with estimated mean and standard deviation S, then a prediction interval for the sample mean with 100(1- a )% confidence level is: ± t a /2 &#180; S &#180; (1+1/n) 1/2 . This is the range of a random variable with 100(1- a )% confidence, using t-table. Relaxing the normality condition for sample-mean prediction interval, requires a large sample size, say n over 30. Further Readings: Grant E., and R. Leavenworth, Statistical Quality Control , McGraw-Hill, 1996. Ryan T., Statistical Methods for Quality Improvement , John Wiley &amp; Sons, 2000. A very good book for a starter. What Is a Standard Error? For statistical inference, namely statistical testing and estimation, one needs to estimate the population's parameter(s). Estimation involves the determination, with a possible error due to sampling, of the unknown value of a population parameter, such as the proportion having a specific attribute or the average value m of some numerical measurement. To express the accuracy of the estimates of population characteristics, one must also compute the standard errors of the estimates. These are measures of accuracy that determine the possible errors arising from the fact that the estimates are based on random samples from the entire population, and not on a complete population census. Standard error is a statistic indicating the accuracy of an estimate. That is, it tells us to assess how different the estimate (such as ) is from the population parameter (such as m ). It is therefore, the standard deviation of a sampling distribution of the estimator such as . The following is a collection of standard errors for the widely used statistics: Standard Error for the Mean is: S/n ½ . As one expects, the standard error decreases as the sample size increases. However the standard deviation of the estimate decreases by a factor of n ½ not n. For example, if you wish to reduce the error by 50%, the sample size must be 4 times n, which is expensive. Therefore, as an alternative to increasing sample size, one may reduce the error by obtaining "quality" data that provide a more accurate estimate. For a finite population of size N, the standard error of the sample mean of size n, is: S &#180; [(N -n)/(nN)] ½ . Standard Error for the Multiplication of Two Independent Means 1 &#180; 2 is: { 1 S 2 2 /n 2 + 2 S 1 2 /n 1 } ½ . Standard Error for Two Dependent Means 1 &#177; 2 is: {S 1 2 /n 1 + S 2 2 /n 2 + 2 r &#180; [(S 1 2 /n 1 )(S 2 2 /n 2 )] ½ } ½ . Standard Error for the Proportion P is: [P(1-P)/n] ½ Standard Error for P 1 &#177; P 2 , Two Dependent Proportions is: {[P 1 + P 2 - (P 1 -P 2 ) 2 ] / n} ½ . Standard Error of the Proportion (P) from a finite population is: [P(1-P)(N -n)/(nN)] ½ . The last two formulas for finite population are frequently used when we wish to compare a sub-sample of size n with a larger sample of size N, which contains the sub-sample. In such a comparison, it would be wrong to treat the two samples "as if" there were two independent samples. For example, in comparing the two means one may use the t-statistic but with the standard error: S N [(N -n)/(nN)] ½ as its denominator. Similar treatment is needed for proportions. Standard Error of the Slope (m) in Linear Regression is S res / S xx ½ , where S res is the residual' standard deviation. Standard Error of the Intercept (b) in Linear Regression is: S res [(S xx + n &#180; 2 ) /(n &#180; S xx ] ½ . Standard Error of the Predicted Value using a Linear Regression is: S y (1 - r 2 ) ½ . The term (1 - r 2 ) ½ is called the coefficient of alienation. Therefore if r = 0, the error of prediction is S y as expected. Standard Error of the Linear Regression is: S y (1 - r 2 ) ½ . Note that if r = 0, then the standard error reaches its maximum possible value, which is standard deviation in Y. Stability of an estimator: An estimator is stable if, by taking two different samples of the same size, they produce two estimates having "small" absolute difference. The stability of an estimator is measured by its reliability: Reliability of an estimator = 1 / (its standard error) 2 The larger the standard error, the less reliable is the estimate. Reliability of estimators is often used to select the "best" estimator among all unbiased estimators. Sample Size Determination At the planning stage of a statistical investigation, the question of sample size (n) is critical. This is an important matter NOT to be taken lightly. To take a larger sample than is needed to achieve the desired results is wasteful of resources, whereas very small samples often lead to what are no practical use of making good decisions. The main objective is to obtain both a desirable accuracy and a desirable confidence level with minimum cost. Students sometimes ask me, what fraction of the population do you need for good estimation? I answer, "It's irrelevant; accuracy is determined by sample size." This answer has to be modified if the sample is a sizable fraction of the population. The confidence level of conclusions drawn from a set of data depends on the size of the data set. The larger the sample, the higher is the associated confidence. However, larger samples also require more effort and resources. Thus, your goal must be to find the smallest sample size that will provide the desirable confidence. For an item scored 0 or 1, for no or yes, the standard error (SE) of the estimated proportion p, based on your random sample observations, is given by: SE = [p(1-p)/n] 1/2 where p is the proportion obtaining a score of 1, and n is the sample size. This SE is the standard deviation of the range of possible estimate values. The SE is at its maximum when p = 0.5, therefore the worst case scenario occurs when 50% are yes, and 50% are no. Under this extreme condition, the sample size, n, can then be expressed as the largest integer less than or equal to: n = 0.25/SE 2 To have some notion of the sample size, for example for SE to be 0.01 (i.e. 1%), a sample size of 2500 will be needed; 2%, 625; 3%, 278; 4%, 156, 5%, 100. Note, incidentally, that as long as the sample is a small fraction of the total population, the actual size of the population is entirely irrelevant for the purposes of this calculation. Pilot Studies: When the needed estimates for sample size calculation is not available from an existing database, a pilot study is needed for adequate estimation with a given precision. A pilot, or preliminary, sample must be drawn from the population, and the statistics computed from this sample are used in determination of the sample size. Observations used in the pilot sample may be counted as part of the final sample, so that the computed sample size minus the pilot sample size is the number of observations needed to satisfy the total sample size requirement. Sample Size with Acceptable Absolute Precision: The following present the widely used method for determining the sample size required for estimating a population mean and proportion. Let us suppose we want an interval that extends d unit on either side of the estimator. We can write d = Absolute Precision = (reliability coefficient) &#180; (standard error) = Z a /2 &#180; (S/n 1/2 ) Suppose, based on a pilot sample of size n, the estimated proportion is p, then the required sample size with the absolute error size not exceeding d , with 1- a confidence is: [t 2 n p(1-p)] / [t 2 p(1-p) - d 2 (n-1)], where t = t a/2 being the value taken from the t-table with parameter d.f. = n = n-1, corresponding to the desired 1- a confidence interval. For large pilot sample sizes (n), say over 30, the simplest sample size determinate is: [(Z a/2 ) 2 S 2 ] / d 2 for the Mean m [(Z a/2 ) 2 p(1-p)] / d 2 for the proportion, where d is the desirable margin of error (i.e., the absolute error), which is the half-length of the confidence interval with 100(1- a )% confidence interval. Sample Size with Acceptable Type I and Type II Errors: One may use the following sample size determinate, which is based on the size of type I and Type II errors: 2(Z a/2 + Z b/2 ) 2 S 2 / d 2 , where a and b are the desirable type I, and type II errors, respectively. S 2 is the variance obtained from the pilot run, and d is the difference between the null and alternative ( m 0 - m a ). Sample Size with Acceptable Relative Precision: You may use the following sample size determinate for a desirable relative error D in %, which requires an estimate of the coefficient of variation (CV in %) from a pilot sample with size over 30: [(Z a/2 ) 2 (C.V.) 2 ] / D 2 Sample Size Based on the Null and an Alternative: One may use power of the test to determine the sample size. The functional relation of the power and the sample size is known as the operating characteristic curve . On this curve, as sample size increases, the power function increases rapidly. Let d be such that: m a = m 0 + d is an alternative to represent departure from the null hypothesis. We wish to be reasonably confident to find evidence against the null, if in fact the particular alternative holds. That is, the type error b , is the probability of failing to find evidence at least at level of a , when the alternative holds. This implies Required sample size = (z 1 + z 2 ) S 2 / d 2 Where: z 1 = |mean - m 0 |/ SE, z 2 = |mean - m a |/ SE, the mean is the current estimate for m , and S is the current estimate for s . All of the above sample size determinates could also be used for estimating the mean of any unimodal population, with discrete or continuous random variables , provided the pilot run size (n) is larger than (say) 30. In estimating the sample size, when the standard deviation is not known, instead of S 2 one may use 1/4 of the range for sample size over 30 as a "good" estimate for the standard deviation. It is a good practice to compare the result with IQR/1.349. One may extend the sample size determination to other useful statistics, such as correlation coefficient (r) based on acceptable Type I and Type II errors: 2 + [(Z a/2 + Z b/2 ( 1- r 2 ) ½ )/r] 2 provided r is not equal to -1, 0, or 1. The aim of applying any one of the above sample size determinates is at improving your pilot estimates at feasible costs. You might like to use Sample Size Determination JavaScript to check your computations. Further Reading: Kish L., Survey Sampling , Wiley, 1995. Murphy K., and B. Myors, Statistical Power Analysis , L. Erlbaum Associates, 1998. Provides a simple and general sample size determination for hypothesis tests. Revising the Expected Value and the Variance Averaging Variances: What is the mean variance of k variances without regard to differences in their sample sizes? The answer is simply: Average of Variances = [ S S i 2 ] / k However, what is the variance of all k groups combined? The answer must consider the sample size n i of the ith group: Combined Group Variance = S n i [S i 2 + d i 2 ]/N, where d i = mean i - grand mean, and N = S n i , for all i = 1, 2, .., k. Notice that the above formula allows us to split up the total variance into its two component parts. This splitting process permits us to determine the extent to which the overall variation is inflated by the difference between group means. What the variation would be if all groups had the same mean? ANOVA is a well-known application of this concept where the equality of several means is tested. Subjective Mean and Variance: In many applications, we saw how to make decisions based on objective data; however, an informative decision-maker might be able to combine his/her subjective input and the two sources of information. Application: Suppose the following information is available from two independent sources: Revising the Expected Value and the Variance Estimate Source Expected value Variance Sales manager m 1 = 110 s 1 2 = 100 Market survey m 2 = 70 s 2 2 = 49 The combined expected value is: [ m 1 / s 1 2 + m 2 / s 2 2 ] / [1/ s 1 2 + 1/ s 2 2 ] The combined variance is: 2 / [1/ s 1 2 + 1/ s 2 2 ] For our application, using the above tabular information, the combined estimate of expected sales is 83.15 units with combined variance of 65.77. You might like to use Revising the Mean and Variance JavaScript in performing some numerical experimentation. You may apply it for validating the above example and for a deeper understanding of the concept where more than two sources of information are to be combined. Subjective Assessment of Several Estimates Based on Relative Precision In many cases, we may wish to compare several estimates of the same parameter. The simplest approach is to measure the closeness among the estimates in an attempt to determine that at least one of the estimates is more than r times the parameter away from the parameter, where r is a subjective, non-negative number less than one. You might like to use Subjective Assessment of Estimates JavaScript to isolate any inaccurate estimate. By repeating the same process you might be able to remove all inaccurate estimates. Further Reading: Tsao H. and T. Wright, On the maximum ratio: A tool for assisting inaccuracy assessment, The American Statistician , 37(4), 1983. Managing the Producer's or the Consumer's Risk The logic behind a statistical test of hypothesis is similar to the following logic. Draw two lines on a paper and determine whether they are of different lengths. You compare them and say, "Well, certainly they are not equal. Therefore they must be of different lengths. By rejecting equality, that is, the null hypothesis, you assert that there is a difference. The power of a statistical test is best explained by the overview of the Type I and Type II errors. The following matrix shows the basic representation of these errors. As indicated in the above matrix a Type-I error occurs when, based on your data, you reject the null hypothesis when in fact it is true. The probability of a type-I error is the level of significance of the test of hypothesis and is denoted by a . Type-I error is often called the producer's risk that consumers reject a good product or service indicated by the null hypothesis. That is, a producer introduces a good product, in doing so, he or she take a risk that consumer will reject it. A type II error occurs when you do not reject the null hypothesis when it is in fact false. The probability of a type-II error is denoted by b . The quantity 1 - b is known as the Power of a Test . A Type-II error can be evaluated for any specific alternative hypotheses stated in the form "Not Equal to" as a competing hypothesis. Type-II error is often called the consumer's risk for not rejecting possibly a worthless product or service indicated by the null hypothesis. Students often raise questions, such as what are the 'right' confidence intervals, and why do most people use the 95% level? The answer is that the decision-maker must consider both the Type I and II errors and work out the best tradeoff. Ideally one wishes to reduce the probability of making these types of error; however, for a fixed sample size, we cannot reduce one type of error without at the same time increasing the probability of another type of error. Nevertheless, to reduce the probabilities of both types of error simultaneously is to increase the sample size. That is, by having more information one makes a better decision . The following example highlights this concept. A electronics firm, Big Z, manufactures and sells a component part to a radio manufacturer, Big Y. Big Z consistently maintain a component part failure rate of 10% per 1000 parts produced. Here Big Z is the producer and Big Y is the consumer. Big Y, for reasons of practicality, will test sample of 10 parts out of lots of 1000. Big Y will adopt one of two rules regarding lot acceptance: Rule 1: Accept lots with one or fewer defectives; therefore, a lot has either 0 defective or 1 defective. Rule 2: Accept lots with two or fewer defectives; therefore, a lot has either 0,1, or 2 defective(s). On the basis of the binomial distribution, the P(0 or 1) is 0.7367. This means that, with a defective rate of .10, the Big Y will accept 74% of tested lots and will reject 26% of the lots even though they are good lots. The 26% is the producer's risk or the a level. This a level is analogous to a Type I error -- rejecting a true null. Or, in other words, rejecting a good lot. In this example, for illustration purposes, the lot represents a null hypothesis. The rejected lot goes back to the producer; hence, producer's risk. If Big Y is to take rule 2, then the producer's risk decreases. The P(0 or, or 1, or 2) is 0.9298 therefore, Big Y will accept 93% of all tested lots, and 7% will be rejected, even though the lot is acceptable. The primary reason for this is that, although the probability of defective is .10, the Big Y through rule 2 allows for a higher defective acceptance rate. Big Y increases its own risk (consumer's risk), as stated previously. Making Good Decision: Given that there is a relevant profit (which could be negative) for the outcome of your decision, and a prior probability (before testing) for the null hypothesis to be true, the objective is to make a good decision. Let us denote the profits for each cell in the decision table as $ a, $ b, $ c and $ d (column-wise), respectively. The expectation of profit is [ a a + (1- a )b], and + [(1- b )c + b d], depending whether the null is true. Now having a prior (i.e., before testing) subjective probability of p that the null is true, then the expected profit of your decision is: Net Profit = [ a a + (1- a )b]p + [(1- b )c + b d](1-p) - Sampling cost A good decision makes this profit as large as possible. To this end, we must suitably choose the sample size and all other factors in the above profit function. Note that, since we are using a subjective probability expressing the strength of belief assessment of the truthfulness of the null hypothesis, it is called a Bayesian Approach to statistical decision making, which is a standard approach in decision theory . You might like to use the Subjectivity in Hypothesis Testing JavaScript applet in performing some numerical experimentation for validating the above assertions for a deeper understanding. Further Reading: Cochran W., Planning and Analysis of Observational Studies , Wiley, 1983. Hypothesis Testing: Rejecting a Claim To perform a hypothesis test, one must be very specific about the test one wishes to perform. The null hypothesis must be clearly stated, and the data must be collected in a repeatable manner. If there is any subjectivity, the results are technically not valid. All of the analyses, including the sample size, significance level, the time, and the budget, must be planned in advance, or else the user runs the risk of "data diving". Hypothesis testing is mathematical proof by contradiction . For example, for a Student's t test comparing two groups, we assume that the two groups come from the same population (same means, standard deviations, and in general same distributions). Then we do our best to prove that this assumption is false. Rejecting H 0 means either H 0 is false, or a rare event as has occurred. The real question is in statistics not whether a null hypothesis is correct, but whether it is close enough to be used as an approximation. Test of Hypotheses Click on the image to enlarge it and THEN print it In most statistical tests concerning m , we start by assuming the s 2 , and the higher moments, such as skewness and kurtosis , are equal. Then, we hypothesize that the a 's are equal wich is null hypothesis. The "null" often suggests no difference between group means, or no relationship between quantitative variables, and so on. Then we test with a calculated t-value. For simplicity, suppose we have a two-sided test. If the calculated t is close to 0, we say "it is good", as we expected. If the calculated t is far from 0, we say, "the chance of getting this value of t, given my assumption that the populations are statistically the same, is so small that I will not believe the assumption. We will say that the populations are not equal; specifically the means are not equal." As an example, sketch a normal distribution with mean 1 - 2 and standard deviation s. If the null hypothesis is true, then the mean is 0. We calculate the 't' value, as per the equation. We look up a "critical" value of t. The probability of calculating a t value more extreme ( + or - ) than this, given that the null hypothesis is true, is equal or less than the a risk we used in pulling the critical value from the table. Mark the calculated t, and critical t (both sides) on the sketch of the distribution. Now, if the calculated t is more extreme than the critical value, we say, "the chance of getting this t, by shear chance, when the null hypothesis is true, is so small that I would rather say the null hypothesis is false, and accept the alternative, that the means are not equal." When the calculated value is less extreme than the calculated value, we say, "I could get this value of t by shear chance. I cannot detect a difference in the means of the two groups at the a significance level." In this test, we need (among others) the condition that the population variances (i.e., treatment impacts on central tendency but not variability) are equal. However, this test is robust to violations of that condition if n's are large and almost the same size. A counter example would be to try a t-test between (11, 12, 13) and (20, 30, 40). The pooled and unpooled tests both give t statistics of 3.10, but the degrees of freedom are different: d.f. = 4 (for pooled) or d.f. about 2 (for unpooled). Consequently the pooled test gives p = .036 and the unpooled p = .088. We could go down to n = 2 and get something still more extreme. You might like to use Online Statistical Computation , Testing the Mean , and Testing the Variance in performing more of these tests. You might need to use Sample Size Determination JavaScript at the design stage of your statistical investigation in decision making with specific subjective requirements. Classical Approach to Testing Hypotheses In this treatment there are two parties: One party (or a person) proposes the null hypothesis (the claim). Another party proposes an alternative hypothesis. A significance level a and a sample size n are agreed upon by both parties. The next step is to compute the relevant statistic based on the null hypothesis and the random sample of size n. Finally, one determines the rejection region . The conclusion based on this approach is as follows: If the computed statistic falls within the rejection region, then Reject the null hypothesis; otherwise Do Not Reject the null hypothesis (the claim). You may ask: How do you determine the critical value (such as z-value) for the rejection interval for one and two-tailed hypotheses?. What is the rule? First, you have to choose a significance level a . Knowing that the null hypothesis is always in "equality" form then, the alternative hypothesis has one of the three possible forms: "greater-than", "less-than", or "not equal to". The first two forms correspond to a one-tail hypothesis while the last one corresponds to a two-tail hypothesis. If your alternative is in the form of "greater-than" , then z is the value that gives you an area to the right tail of the distribution that is equal to a . If your alternative is in the form of "less-than" , then z is the value that gives you an area to the left tail of the distribution that is equal to a . If your alternative is in the form of "not equal to" , then there are two z values, one positive and the other negative. The positive z is the value that gives you an a /2 area to the right tail of the distribution. While, the negative z is the value that gives you an a /2 area to the left tail of the distribution. The above rule can be generalized and implemented for determining the critical value for any test of hypothesis, you must first master reading the statistical tables, because, as you see, not all tables in your textbook are presented in the same format. The Meaning and Interpretation of P-values (what the data say?) The p-value, which directly depends on a given sample attempts to provide a measure of the strength of the results of a test for the null hypothesis, in contrast to a simple reject or do not reject in the classical approach to the test of hypotheses. If the null hypothesis is true, and if the chance of random variation is the only reason for sample differences, then the p-value is a quantitative measure to feed into the decision-making process as evidence. The following table provides a reasonable interpretation of p-values: P-value Interpretation P &lt; 0.01 very strong evidence against H 0 0.01 £ P &lt; 0.05 moderate evidence against H 0 0.05 £ P &lt; 0.10 suggestive evidence against H 0 0.10 £ P little or no real evidences against H 0 This interpretation is widely accepted, and many scientific journals routinely publish papers using this interpretation for the result of a test of hypothesis. For the fixed-sample size, when the number of realizations is decided in advance, the distribution of p is uniform, assuming the null hypothesis is true. We would express this as P(p £ x) = x. That means the criterion of p £ 0.05 achieves a of 0.05. Understand that the distribution of p-values under null hypothesis H 0 is uniform, and thus does not depend on a particular form of the statistical test. In a statistical hypothesis test, the P value is the probability of observing a test statistic at least as extreme as the value actually observed, assuming that the null hypothesis is true. The value of p is defined with respect to a distribution. Therefore, we could call it "model-distribution hypothesis" rather than "the null hypothesis". In short, it simply means that, if the null had been true, the p-value is the probability against the null in that case. The p-value is determined by the observed value; however, this makes it difficult to even state the inverse of p. Finally, since the p-values are random variables , one cannot compare several p-values for any statistical conclusions (nor order them). This is a common mistake many people do, therefore, the above table is not intended for such a comparison. You might like to use The P-values for the Popular Distributions JavaScript. Further Readings: Arsham H., Kuiper's P-value as a Measuring Tool and Decision Procedure for the Goodness-of-fit Test, Journal of Applied Statistics , Vol. 15, No.3, 131-135, 1988. Good Ph.., Resembling Methods: A Practical Guide to Data Analysis , Springer Verlag, 1999. Blending the Classical and the P-value Based Approaches in Test of Hypotheses A p-value is a measure of how much evidence you have against the null hypothesis. Notice that the null hypothesis is always in = form, and does not contain any forms of inequalities. The smaller the p-value, the more evidence you have. In this setting, the p-value is based on the hull hypothesis and has nothing to do with an alternative hypothesis and therefore with the rejection region. In recent years, some authors try to use the mixture of the classical and the p-value approaches. It is based on the critical value obtained from given a , the computed statistics and the p-value. This is a blend of two different schools of thought. In this setting, some textbooks compare the p-value with the significance level to make decisions on a given test of hypothesis. The larger the p-value is when compared with a (in one-sided alternative hypothesis, and a /2 for the two sided alternative hypotheses), the less evidence we have for rejecting the null hypothesis. In such a comparison, if the p-value is less than some threshold (usually 0.05, sometimes a bit larger like 0.1 or a bit smaller like 0.01) then you reject the null hypothesis. The following deal with such a combined approach. Use of P-value and a : In this setting, we must also consider the alternative hypothesis in drawing the rejection region. There is only one p-value to compare with a (or a /2). Know that, for any test of hypothesis, there is only one p-value. The following outlines the computation of the p-value and the decision process involved in a given test of hypothesis: P-value for One-sided Alternative Hypotheses: The p-value is defined as the area under the right tail of distribution, if the rejection region in on the right tail; if the rejection region is on the left tail, then the p-value is the area under the left tail (in one-sided alternative hypotheses). P-value for Two-sided Alternative Hypotheses: If the alternative hypothesis is two-sided (that is, rejection regions are both on the left and on the right tails), then the p-value is the area under the right tail or to the left tail of the distribution, depending on whether the computed statistic is closer to the right rejection region or left rejection region. For symmetric densities (such as t-density), the left and right tails p-values are the same. However, for non-symmetric densities (such as Chi-square) use the smaller of the two. This makes the test more conservative. Notice that, for a two sided-test alternative hypotheses, the p-value is never greater than 0.5. After finding the p-value as defined here, you compare it with a pre-set a value for one-sided tests, and with a /2 for two sided-test. The larger the p-value is when compared with a (in one-sided alternative hypothesis, and a /2 for the two sided alternative hypotheses), the less evidence we have for rejecting the null hypothesis. To avoid looking-up the p-values from the limited statistical tables given in your textbook, most professional statistical packages such as SAS and SPSS provide the two-tailed p-value. Based on where the rejection region is, you must find out what p-value to use. Some textbooks have many misleading statements about p-value and its applications. For example, in many textbooks you find the authors double the p-value to compare it with a when dealing with the two-sided test of hypotheses. One wonders how they do it in the case when "their" p-value exceeds 0.5? Notice that, while it is correct to compare the p-value with a for a one sided tests of hypotheses a , for two-sided hypotheses, one must compare the p-value with a /2, NOT a with 2 times p-value, as some textbooks advise. While the decision is the same, there is a clear distinction here and an important difference, which the careful reader will note. How to set the appropriate a value? You may have wondered why a = 0.05 is so popular in a test of hypothesis. a = 0.05 is traditional for tests, but is arbitrary in its origins suggested by R.A. Fisher, who suggested it in the spirit of 0.05 being the biggest p-value at which one would think maybe the null hypothesis in a statistical experiment was to be considered false. This was also a tradeoff between "type I error" and "type II error"; that we do not want to accept the wrong null hypothesis, but we do not want to fail to reject the false null hypothesis, either. As a final note, the average of these two p-values is often called the mid-p value. Conversions from two-sided to one-sided probabilities: Let C be the probability for a two-sided confidence interval (CI) constructed for an estimate. The probability (C 1 ) that either the estimate is greater than the lower limit or that it is less than the upper limit can be computed by using: C 1 = C/2 + 1/2, for conversion to one-sided Numerical Example: Suppose you wish to convert a C = 90% two-sided CI into a one-sided, then C 1 = 0.90/2 + 1/2 = 95%. You might need to use Sample Size Determination JavaScript at the design stage of your statistical investigation in decision making with specific, subjective requirements. Bonferroni Method for Multiple P-Values Procedure One may combine several t-tests by using the Bonferroni method. It works reasonably well when there are only a few tests, but as the number of comparisons increases above 8, the value of 't' required to conclude that a difference exists becomes much larger than it really needs to be, and the method becomes over conservative. One way to make the Bonferroni t-test less conservative is to use the estimate of the population variance computed from within the groups in the analysis of variance. t = ( 1 - 2 )/ ( s 2 / n1 + s 2 / n2 ) 1/2 , where s 2 is the population variance computed within the groups. Hommel's Multiple P-Values Procedure: This test can be summarized as follows: Suppose we have n number of P-values: p(i), i =1, .., n, in ascending order corresponding to independent tests. Let j be the largest integer, such as: p(n-j+k) &gt; k a /j, for all k=1,.., ,j. If no such j exists, reject all hypotheses; otherwise, reject all hypotheses with p(i) £ a / j. This provides a strong control of the family-wise error rate at a level. There are other improvements on the Bonferroni adjustment when multiple tests are independent or positively dependent. However, the Hommel's method is the most powerful compared with other methods. Further Readings: Hommel G., Bonferroni procedures for logically related hypotheses, Journal of Statistical Planning and Inference , 82, 119-128, 1999. Kost J., and M. McDermott, Combining dependent P-values, Statistics and Probability Letters , 60, 183-190, 2002. Wasteful P., and S. Young, Resembling-Based Multiple Testing: Examples and Methods for P-Value Adjustment , Wiley, 1992. Wright S., Adjusted P-values for simultaneous inference, Biometrics , 48, 1005-1013, 1992. Power of a Test and the Size Effect The power of a test plays the same role in hypothesis testing that Standard Error played in estimation. It is a measuring tool for assessing the accuracy of a test or in comparing two competing test procedures. The power of a test is the probability of rejecting a false null hypothesis when the null hypothesis is false. This probability is inversely related to the probability of making a Type II error, not rejecting the null hypothesis when it is false . Recall that we choose the probability of making a Type I error when we set a . If we decrease the probability of making a Type I error, then we increase the probability of making a Type II error. Therefore, there are basically two errors possible when conducting a statistical analysis; type I error and and type II error: Type I error - (producer's) risk of rejecting the null hypothesis when it is in fact true. Type II error - (consumer's) risk of not rejecting the null hypothesis when it is in fact false. Power and Alpha ( a ): Thus, the probability of not rejecting a true null has the same relationship to Type I errors as the probability of correctly rejecting an untrue null does to Type II error. Yet, as I mentioned if we decrease the odds of making one type of error we increase the odds of making the other type of error. What is the relationship between Type I and Type II errors? For a fixed sample size, decreasing one type of error increases the size of the other one. Power and the Size Effect: Anytime we test whether a sample differs from a population, or whether two samples come from 2 separate populations, there is the condition that each of the populations we are comparing has its own mean and standard deviation (even if we do not know it). The distance between the two population means will affect the power of our test. This is known as the size of treatment , also known as the effect size , as shown in the following table with the three popular values for a : Power as a Function of a and the Size Effect a Size Effect 0.10 0.05 0.01 1.0 .22 .13 .03 2.0 .39 .26 .09 3.0 .59 .44 .20 4.0 .76 .64 .37 5.0 .89 .79 .57 6.0 .96 .91 .75 7.0 .99 .97 .88 Power and the Size of Variance s 2 : The greater the variance S 2 , the lower the power 1- b . Anything that effects the extent to which the two distributions share common values will increase b (the likelihood of making a Type II error) Power and the Sample Size: The smaller the sample sizes n, the lower the power. Very small n produces power so low that false hypotheses are accepted. The following is a list of four factors influencing the power: effect size (for example, the difference between the means) variance S 2 significance level a number of observations, or the sample size n In practice, the first three factors are often fixed. Only the sample size can be controlled by the statistician and that only within budget constraint. There exists a tradeoff between budget and achievement of desirable accuracy in any analysis. A Numerical Example: The power of a test is most easily understood by viewing it in the context of a composite test. A composite test requires the specification of a population mean as the alternative hypothesis. For example, using Z-test of hypothesis in the following Figure. The power is developed from specification of an alternative hypothesis such as m = 2.5, and m = 3. The resultant distribution under this alternative shifts to the right 2.5 units with the shaded area representing the power of the test, correctly rejecting a false null. Power of a Test Click on the image to enlarge it Not rejecting the null hypothesis when it is false is defined as a Type II error, and is denoted by the b region. In the above Figure this region lies to the left of the critical value. In the configuration shown in this Figure, b falls to the left of the critical value (and below the statistic's density (or probability) function under the alternative hypothesis H a ). The b is also defined as the probability of not-rejecting a false null hypothesis when it is false, also called a miss. Related to the value of b is the power of a test. The power is defined as the probability of rejecting the null hypothesis given that a specific alternative is true, and is computed as (1- a ). A Short Discussion: Consider testing a simple null versus simple alternative. In the Neyman-Pearson setup, an upper bound is set for the probability of a given Type I error ( a ), and then it is desirable to find tests with low probability of type II error ( b ) given this. The usual justification for this is that "we are more concerned about a Type I error, so we set an upper limit on the a that we can tolerate." I have seen this sort of reasoning in elementary texts and also in some advanced ones. It doesn't seem to make any sense. When the sample size is large, for most standard tests, the ratio b / a tends to 0. If we care more about Type I error than Type II error, why should this concern dissipate with increasing sample size? This is indeed a drawback of the classical theory of testing statistical hypotheses. A second drawback is that the choice lies between only two test decisions: reject the null or accept the null. It is worth considering approaches that overcome these deficiencies. This can be done, for example, by the concept of profile-tests at a 'level' a . Neither the Type I nor Type II error rates are considered separately, but they are the ratio of a correct decision. For example, we accept the alternative hypothesis H a and reject the null H 0 , if an event is observed which is at least a-times greater under H a than under H 0 . Conversely, we accept H 0 and reject H a , if an event is observed which is at least a-times greater under H 0 than under H a . This is a symmetric concept which is formulated within the classical approach. Power of Parametric versus Non-parametric Tests: As a general rule, for a given sample size n, the parametric tests are more powerful than their non-parametric counterparts. The primarily reason for this is that we have emphasized parametric tests. Moreover, among the parametric tests, those which use correlation are more powerful, such as the before-and-after test . This is known as a Variance Reduction Technique used in system simulation to increase the accuracy (i.e., reduce variation) without increasing the sample size. Correlation Coefficient as a Measuring Tool and Decision Criterion for the Effect Size: The correlation coefficient could be obtained and used as a measuring tool and decision criteron for the strength of the effect size based on the computed test-statistic for major hypothesis testing. The correlation coefficient r stands as a very useful and accessible index of the magnitude of effect. It is commonly accepted that the small, medium, and large effect sizes correspond to r-values over 0.1, 0.3, and 0.5, respectively. The following are needed transformation of some major inferential statistics to the r-value: For the t(df)-statistic: r = [t 2 /(t 2 + df)] ½ For the F(1,df 2 )-statistic: r = [F/(F + df)] ½ For the c 2 (1)-statistic: r = [ c 2 /n] ½ For the Standard Normal Z: r = (Z 2 /n) ½ You might like to use Sample Size Determination JavaScript at the design stage of your statistical investigation in decision making with specific subjective requirements. Further Reading: Murphy K., and B. Myors, Statistical Power Analysis , L. Erlbaum Associates, 1998. Parametric vs. Non-Parametric vs. Distribution-free Tests One must use a statistical technique called non-parametric if it satisfies at least one of the following five types of criteria: The data entering the analysis are enumerative; that is, counted data represent the number of observations in each category or cross-category. The data are measured and/or analyzed using a nominal scale of measurement. The data are measured and/or analyzed using an ordinal scale of measurement. The inference does not concern a parameter in the population distribution; for example, the hypothesis that a time-ordered set of observations exhibits a random pattern. The probability distribution of the statistic upon which the analysis is based is not dependent upon specific information or conditions (i.e., assumptions) about the population(s) from which the sample(s) are drawn, but only upon general assumptions, such as a continuous and/or symmetric population distribution. According to these creteria, the distinction of non-parametric is accorded either because of the level of measurement used or required for the analysis, as in types 1 through 3; the type of inference, as in type 4, or the generality of the assumptions made about the population distribution, as in type 5. For example, one may use the Mann-Whitney Rank Test as a non-parametric alternative to Students T-test when one does not have normally distributed data. Mann-Whitney: To be used with two independent groups (analogous to the independent groups t-test) Wilcoxon: To be used with two related (i.e., matched or repeated) groups (analogous to the related samples t-test) Kruskall-Wallis: To be used with two or more independent groups (analogous to the single-factor between-subjects ANOVA) Friedman: To be used with two or more related groups (analogous to the single-factor within-subjects ANOVA) Non-parametric vs. Distribution-free Tests: Non-parametric tests are those used when some specific conditions for the ordinary tests are violated. Distribution-free tests are those for which the procedure is valid for all different shape of the population distribution. For example, the Chi-square test concerning the variance of a given population is parametric since this test requires that the population distribution be normal. The Chi-square test of independence does not assume normality condition , or even that the data are numerical. The Kolmogorov-Smirnov test is a distribution-free test, which is applicable to comparing two populations with any distribution of continuous random variable. The following section is an interesting non-parametric procedure with various and useful applications. Comparison of Two Random Variables: Consider two independent observations X = (x 1 , x 2 ,, x r ) and Y = (y 1 , y 2 ,, y s ) for two random variables X and Y respectively. To estimate the reliability function: R = Pr (X > Y) One may use: The estimator RS = U/(r &#180; s), where U is the number of pairs (x i , y j ) such that x i > y j , for all i = 1, 2, ,r, and j = 1, 2,..,s. This estimator is an unbiased one with the minimum variance for R. It is important to know that the estimate has an upper limit, non-negative delta value for its accuracy: Pr{R &#179; RS - d } &#179; max {1- exp(-2n d 2 ), 4n d 2 /(1-4n d 2 )}. Application areas include the insurance ruin problem. Let random variable Y denote the claims per unit of time and let random variable X denote the return on investment (ROI) for the Insurance Company. Finally, let z denote the constant premium amount collected; then the probability that the insurance company will survive is: R = Pr [X + z > Y}. You might like to use the Kolmogorov-Smirnov Test for Two Populations and Comparing Two Random Variables in checking your computations and performing some numerical experiment for a deeper understanding of these concepts. Further Readings: Arsham H., A generalized confidence region for stress-strength reliability, IEEE Transactions on Reliability , 35(4), 586-589, 1986. Conover W., Practical Nonparametric Statistics , Wiley, 1998. Hollander M., and D. Wolfe, Nonparametric Statistical Methods , Wiley, 1999. Kotz S., Y. Lumelskii, and M. Pensky, The Stress-Strength Model and Its Generalizations: Theory and Applications , Imperial College Press, London, UK, 2003, distributed by World Scientific Publishing. Hypotheses Testing Remember that, in the t-tests for differences in means, there is a condition of equal population variances that must be examined. One way to test for possible differences in variances is to do an F test. However, the F test is very sensitive to violations of the normality condition ; i.e., if populations appear not to be normal, then the F test will tend to reject too often the null of no differences in population variances. You might like to use the following JavaScript to check your computations and to perform some statistical experiments for deeper understanding of these concepts: Testing the Mean . Testing the Variance . Testing Two Populations . Testing the Difference: The Before-and-After Test . ANOVA . For statistical equality of two populations, you might like to use the Kolmogorov-Smirnov Test . Single Population t-Test The purpose is to compare the sample mean with the given population mean. The aim is to judge the claimed mean value, based on a set of random observations of size n. A necessary condition for validity of the result is that the population distribution is normal, if the sample size n is small (say less than 30). The task is to decide whether to accept a null hypothesis: H 0 = m = m 0 or to reject the null hypothesis in favor of the alternative hypothesis: H a : m is significantly different from m 0 The testing framework consists of computing a the t-statistics: T = [( - m 0 ) n 1/2 ] / S Where is the estimated mean and S 2 is the estimated variance based on n random observations. The above statistic is distributed as a t-distribution with parameter d.f. = n = (n-1). If the absolute value of the computed T-statistic is "too large" compared with the critical value of the t-table, then one rejects the claimed value for the population's mean. This test could also be used for testing similar claims for other unimodal populations including those with discrete random variables , such as proportion, provided there are sufficient observations (say, over 30). You might like to use Testing the Mean JavaScript in checking your computations. and Sample Size Determination JavaScript at the design stage of your statistical investigation in decision making with specific subjective requirements The Procedure for Two Populations Independent Means Test Click on the image to enlarge it and THEN print it You might like to use JavaScript Testing Two Populations . When Should We Pool Variance Estimates? We should pool variance estimates only if there is a good reason for doing so, and then (depending on that reason) the conclusions might have to be made explicitly conditional on the validity of the equal-variance model. There are several different good reasons for pooling: (a) to get a single stable estimate from several relatively small samples, where variance fluctuations seem not to be systematic; or (b) for convenience, when all the variance estimates are near enough to equality; or (c) when there is no choice but to model variance (as in simple linear regression with no replicated X values), and deviations from the constant-variance model do not seem systematic; or (d) when group sizes are large and nearly equal, so that there is essentially no difference between the pooled and unpooled estimates of standard errors of pairwise contrasts, and degrees of freedom are nearly asymptotic. Note that this last rationale can fall apart for contrasts other than pairwise ones. One is not really pooling variance in case (d), rather one is merely taking a shortcut in the computation of standard errors of pairwise contrasts. If you calculate the test without the assumption, you have to determine the degrees of freedom (d.f.). The formula works in such a way that d.f. will be less if the larger sample variance is in the group with the smaller number of observations. This is the case in which the two tests will differ considerably. A study of the formula for the d.f. is most enlightening, and one must understand the correspondence between the unfortunate design (having the most observations in the group with little variance) and the low d.f. and accompanying large t-value. Example: When doing t tests for differences in means of populations (a classic independent samples case): For differences in means that do not make any assumption about equality of population variances, use the standard error formula: [S 2 1 /n 1 + S 2 2 /n 2 ] ½ , with d.f. = n = n 1 or n 2 whichever is smaller. With equal variances, use the statistics: with parameter d.f. = n = (n 1 + n 2 - 2), n 1 , for n 2 greater than or equal to 1, where the pooled variance is: If total N is less than 50 and one sample is 1/2 the size of the other (or less), and if the smaller sample has a standard deviation at least twice as large as the other sample, then apply the procedure given in item no. 1, but adjust d.f. parameter of the t-test to the largest integer less than or equal to: d.f. = n = A/(B +C), where: A = [S 2 1 /n 1 + S 2 2 /n 2 ] 2 , B = [S 2 1 /n 1 ] 2 / (n 1 -1), C = [S 2 2 /n 2 ] 2 / (n 2 -1) Otherwise, do not worry about the problem of having an actual a level that is much different than what you have set it to be. Statistics with Confidence Section is concerned with the construction of a confidence interval where the equality of variances condition is an important issue. The last approach, which is very general with conservative results, can be implemented using Testing Two Populations Applet. The Procedure for Two Dependent Means Test Click on the image to enlarge it and THEN print it You might like to use JavaScript Testing the Difference in Means: The Before-and-After Test and Paired Proportion Test for dependent proportions. Non-parametric Multiple Comparison Procedures Duncan's multiple-range test: This is one of the many multiple comparison procedures. It is based on the standardized range statistic by comparing all pairs of means while controlling the overall Type I error at a desirable level. While it does not provide interval estimates of the difference between each pair of means, it does indicate which means are significantly different from the others. For determining the significant differences between a single control group mean and the other means, one may use the Dunnett's multiple-comparison test. Introduction to Tests for Statistical Equality of Two or More Populations: Two random variables X and Y having distribution F X (x) and F Y (y) respectively, are said to be equivalent, or equal in law, or equal in distribution, if and only if they have the same distribution function. That is, F X (z) = F Y (z), for all z, There are different tests depending on the intended applications. The widely used tests for statistical equality of populations are as follow: Equality of Two Normal Populations: One may use the Z-test and F-test to check the equality of the means, and the equality of variances, respectively. Testing a Shift in Normal Populations: Often we are interested in testing for a given shift in a given population distribution, that is testing if a random variable Y is equal in distribution to another X + c for some constant c. In other words, the distribution of Y is the distribution of X shifted. In testing any shift in distribution one needs to test for normality first, and then testing the difference in expected values by applying the two-sided Z-test with the null hypothesis of: H 0 : m Y - m X = c. Analysis of Variance: Analysis of Variance (ANOVA) tests are designed for simultaneous testing of equality of three or more populations. The preconditions in applying ANOVA are normality of each population's distribution, and the equality of all variances simultaneously (not the pair-wise tests). Notice that ANOVA is an extension of item no. 1 in testing equality of more than two populations. It can be shown that if one applies ANOVA for testing the equality of two populations based on two independent samples with sizes of n1 and n2 form each population, respectively, then the results of both tests will be identical. Moreover, the test-statistic obtained by each test are directly related, i.e., F a , (1, n1+ n2 - 2) = t 2 a/2 , (n1+ n2 - 2) Equality of Proportions in Several Populations: This test is for discrete random variables. It is one of the many interesting chi-square applications . Distribution-free Equality of Two Populations: Whenever one is interested in testing the equality of two populations with a common continuous random variable, without any reference to the underlying distribution such as normality condition, one may use the distribution-free known as the K-S test. Non-parametric Comparison of Two Random Variables: Consider two independent observations X = (x 1 , x 2 ,, x r ) and Y = (y 1 , y 2 ,, y s ) for two independent populations with random variables X and Y, respectively. Often we are interested in estimating the Pr (X > Y). Equality of Two Normal Populations: The normal or Gaussian distribution is a continuous symmetric distribution that follows the familiar bell-shaped curve. One of its nice features is that, the mean and variance uniquely and independently determines the distribution. Therefore, for testing the statistical equality of two independent normal populations, one must first perform the Lilliefors' Test for Normality to assess this condition. Given that both populations are normally distributed, then one must performing two more tests, namely the test for equality of the two means and the test for equality of the two variances. Both of these tests can be carried out by using the Test of Hypotheses for Two Populations JavaScript Applets. Multi-Means Comparisons: Analysis of Variance (ANOVA) The tests we have learned up to this point allow us to test hypotheses that examine the difference between only two means. Analysis of Variance or ANOVA will allow us to test the difference between two or more means. ANOVA does this by examining the ratio of variability between two conditions and variability within each condition. For example, say we give a drug that we believe will improve memory to a group of people and give a placebo to another group of people. We might measure memory performance by the number of words recalled from a list we ask everyone to memorize. A t-test would compare the likelihood of observing the difference in the mean number of words recalled for each group. An ANOVA test, on the other hand, would compare the variability that we observe between the two conditions to the variability observed within each condition. Recall that we measure variability as the sum of the difference of each score from the mean. When we actually calculate an ANOVA we will use a short-cut formula Thus, when the variability that we predict between the two groups is much greater than the variability we don't predict within each group, then we will conclude that our treatments produce different results. An Illustrative Numerical Example for ANOVA Consider the following (small integers, indeed for illustration while saving space) random samples from three different populations. With the null hypothesis: H 0 : µ1 = µ2 = µ3, and the alternative: H a : at least two of the means are not equal. At the significance level a = 0.05, the critical value from F-table is F 0.05, 2, 12 = 3.89. Sum Mean Sample P1 2 3 1 3 1 10 2 Sample P2 3 4 3 5 0 15 3 Sample P3 5 5 5 3 2 20 4 Demonstrate that, SST=SSB+SSW. That is, the sum of squares total (SST) equals sum of squares between (SSB) the groups plus sum of squares within (SSW) the groups. Computation of sample SST: With the grand mean = 3, first, start with taking the difference between each observation and the grand mean, and then square it for each data point. Sum Sample P1 1 0 4 0 4 9 Sample P2 0 1 0 4 9 14 Sample P3 4 4 4 0 1 13 Therefore SST = 36 with d.f = (n-1) = 15-1 = 14 Computation of sample SSB: Second, let all the data in each sample have the same value as the mean in that sample. This removes any variation WITHIN. Compute SS differences from the grand mean. Sum Sample P1 1 1 1 1 1 5 Sample P2 0 0 0 0 0 0 Sample P3 1 1 1 1 1 5 Therefore SSB = 10, with d.f = (m-1)= 3-1 = 2 for m=3 groups. Computation of sample SSW: Third, compute the SS difference within each sample using their own sample means. This provides SS deviation WITHIN all samples. Sum Sample P1 0 1 1 1 1 4 Sample P2 0 1 0 4 9 14 Sample P3 1 1 1 1 4 8 SSW = 26 with d.f = 3(5-1) = 12. That is, 3 groups times (5 observations in each -1) Results are: SST = SSB + SSW, and d.f SST = d.f SSB + d.f SSW , as expected. Now, construct the ANOVA table for this numerical example by plugging the results of your computation in the ANOVA Table. Note that, the Mean Squares are the Sum of squares divided by their Degrees of Freedom. F-statistics is the ratio of the two Mean Squares. The ANOVA Table Sources of Variation Sum of Squares Degrees of Freedom Mean Squares F-Statistic Between Samples 10 2 5 2.30 Within Samples 26 12 2.17 Total 36 14 Conclusion: There is not enough evidence to reject the null hypothesis H 0 . The Logic behind ANOVA: First, let us try to explain the logic and then illustrate it with a simple example. In performing the ANOVA test, we are trying to determine if a certain number of population means are equal. To do that, we measure the difference of the sample means and compare that to the variability within the sample observations. That is why the test statistic is the ratio of the between-sample variation (MSB) and the within-sample variation (MSW). If this ratio is close to 1, there is evidence that the population means are equal. Here is a good application for you: Many people believe that men get paid more in the business world than women, simply because they are male. To justify or reject such a claim, you could look at the variation within each group (one group being women's salaries and the other group being men's salaries) and compare that to the variation between the means of randomly selected samples of each population. If the variation in the women's salaries is much larger than the variation between the men's and women's mean salaries, one could say that because the variation is so large within the women's group that this may not be a gender-related problem. Now, getting back to our numerical example of the drug treatment to improve memory vs the placebo. We notice that: given the test conclusion and the ANOVA test's conditions, we may conclude that these three populations are in fact the same population. Therefore, the ANOVA technique could be used as a measuring tool and statistical routine for quality control as described below using our numerical example. Construction of the Control Chart for the Sample Means: Under the null hypothesis, the ANOVA concludes that µ1 = µ2 = µ3; that is, we have a "hypothetical parent population." The question is, what is its variance? The estimated variance (i.e., the total mean squares) is 36 / 14 = 2.75. Thus, estimated standard deviation is = 1.60 and estimated standard deviation for the means is 1.6 / 5 ½ = 0.71. Under the conditions of ANOVA, we can construct a control chart with the warning limits = 3 ± 2(0.71); the action limits = 3 ± 3(0.71). The following figure depicts the control chart. You might like to use ANOVA: Testing Equality of Means , or ANOVA for your computations, and then to interpret the results in managerial (not technical) terms. You might need to use Sample Size Determination JavaScript at the design stage of your statistical investigation in decision making with specific subjective requirements. The Procedure for More Than Two Independent Means Test Click on the image to enlarge it and THEN print it ANOVA for Normal but Condensed Data Sets In testing the equality of several means, often the raw data are not available. In such a case, one must perform the needed analysis based on secondary data using the data summaries; namely, the triple-set: The sample sizes, the sample means, and the sample variances. Suppose one of the samples is of size n having the sample mean , and the sample variance S 2 . Let: y i = + (S 2 /n) ½ for all i = 1, 2, , n-1, and y n = n - (n - 1)y 1 Then, the new random data y i 's are surrogate data having the same mean and variance as the original data set. Therefore, by generating the surrogate data for each sample, one can perform the standard ANOVA test. The results are identical. You might like to use ANOVA for Condensed Data for your computation and experimentation. The JavaScript Subjective Assessment of Estimates tests the claim that at least the ratio of one estimate to the largest estimate is as large as a given claimed value. Further Reading: Larson D., Analysis of variance with just summary statistics as input, The American Statistician , 46(2), 151-152, 1992. ANOVA for Dependent Populations Populations can be dependent in either of the following ways: Every subject is tested in every experimental condition. This kind of dependency is called the repeated-measurement design. Subjects under different experimental conditions are related in some manner. This kind of dependency is called matched-subject designed. An Application: Suppose we are interested in studying the effect of alcohol on driving ability. Ten subjects are given three different alcohol levels and the number of driving errors are tabulated below: Mean 0 oz 2 3 1 3 1 4 1 3 2 1 2.1 2 oz 3 2 1 4 2 3 1 5 1 2 2.4 4 oz 3 1 2 4 2 5 2 4 3 2 3.1 The test null hypothesis is: H 0 : µ1 = µ2 = µ3, and the alternative: H a : at least two of the means are not equal. Using the ANOVA for Dependent Populations JavaScripts, we obtain the needed information in constructing the following ANOVA table: The ANOVA Table Sources of Variation Sum of Squares Degrees of Freedom Mean Squares F-Statistic Subjects 31.50 9 3.50 - Between 5.26 2 2.63 7.03 Within 6.70 18 0.37 Total 43.46 29 Conclusion: The p-value is P= 0.006, indicating a strong evidence against the null hypothesis. The means of the populations are not equal. Here, one may conclude that person who has consumed more than certain level of alcohol commits more driving errors. The Procedure for More Than Two Dependent Populations Test Click on the image to enlarge it and THEN print it A "block design sampling" implies studying more than two dependent populations. For testing the equality of means of more than two populations based on block design sampling, you may use Two-Way ANOVA Test JavaScript. In the case of having block design data with replications, use Two-Way ANOVA with Replications JavaScript to obtain the needed information for constructing the ANOVA tables. Test for Equality of Several Population Proportions The Chi-square test of homogeneity provides an alternative method for testing the null hypothesis that two population proportions are equal. Moreover, it extend, to several populations similar to the ANOVA test that compares several means. An Application: Suppose we wish to test the null hypothesis H 0 : P 1 = P 2 = ..... = P k That is, all three population proportions are almost identical. The sample data from each of the three populations are given in the following table: Test for homogeneity of Several Population Proportions Populations Yes No Total Sample I 60 40 100 Sample II 57 53 110 Sample III 48 72 120 Total 165 165 330 The Chi-square statistic is 8.95 with d.f. = (3-1)(3-1) = 4. The p-value is equal to 0.062, indicating that there is moderate evidence against the null hypothesis that the three populations are statistically identical. You might like to use Testing Proportions to perform this test. Distribution-free Equality of Two Populations For statistical equality of two populations, one may use the Kolmogorov-Smirnov Test (K-S Test) for two populations. The K-S test seeks differences between the two population's distribution function based on their two independent random samples. The test rejects the null hypothesis of no difference between the two populations if the difference between the two empirical distribution functions is "large". Prior to applying the K-S test it is necessary to arrange each of the two sample observations in a frequency table. The frequency table must have a common classification. Therefore the test is based on the frequency table, which belongs to the family of distribution-free tests. The K-STest process is as follows: Some k number of "classes" is selected, each typically covering a different but similar range of values. Some much larger number of independent observations (n 1 , and n 2 , both larger than 40) are taken. Each is measured and its frequency is recorded in a class. Based on the frequency table, the empirical cumulative distribution functions F1 i and F2 i for two sample populations are constructed, for i = 1, 2,..., k. The K-S statistic is the largest absolute difference between F1 i and F2 i ; i.e., K-S statistic = D = Maximum | F1 i - F2 i |, for all i = 1, 2, .., k. The critical values of K-S statistic can be found at Computers and Computational Statistics with Applications An Application: The daily sales of the two subsidiaries of The PC & Accessories Company are shown in the following table, with n1 = 44, and n2 = 54: Daily Sales at Two Branches Over 6 Months Sales ( $ 1000) Frequency I Frequency II 0 - 2 11 1 3 - 5 7 3 6 - 8 8 6 9 - 11 3 12 12 - 14 5 12 15 - 17 5 14 18 - 20 5 6 Sums 44 54 The manager of the first branch is claiming that "since the daily sales are random phenomena, my overall performance is as good as the other manager's performance." In other words: H 0 : The daily sales at the two stores are almost the same. H a : The performance of the managers is significantly different. Following the above process for this test, the K-S statistic is 0.421 with the p-value of 0.0009, indicating a strong evidence against the null hypothesis. There is enough evidence that the performance of the manager of the second branch is better. Introduction to Applications of the Chi-square Statistic The variance is not the only thing for which you use a Chi-square test for. The most widely used applications of Chi-square distribution are: The Chi-square Test for Association which is a non-parametric test; therefore, it can be used for nominal data too. It is a test of statistical significance widely used bivariate tabular association analysis. Typically, the hypothesis is whether or not two populations are different in some characteristic or aspect of their behavior based on two random samples. This test procedure is also known as the Pearson Chi-square test. The Chi-square Goodness-of-Fit Test is used to test if an observed distribution conforms to any particular distribution. Calculation of this goodness-of-fit test is by comparison of observed data with data expected based on a particular distribution. One of the disadvantages of some of the Chi-square tests is that they do not permit the calculation of confidence intervals; therefore, determination of the sample size is not readily available. Treatment of Cases with Many Categories: Notice that, although in the following section most of the crosstables have only two categories, it is always possible to convert cases with many categories into similar crosstables. To do so, one must consider all possible pairs of categories and their numerical values while constructing the equivalent "two-categories" crosstable. Test for Crosstable Relationship Crosstables: Often crosstables are used to test relationships among two categorical types of data, or independence of two variables, such as cigarette smoking and drug use. If you were to survey 1000 people on whether or not they smoke and whether or not they use drugs, you would get one of four answers: (no, no) (no, yes) (yes, no) (yes, yes) By compiling the number of people in each category, you can ultimately test whether drug usage is independent of cigarette smoking by using the Chi-square distribution (this is approximate, but works well). Again, the methodology for this is in your textbook. The degrees of freedom is equal to (number of rows-1)(number of columns -1). That is, these many numbers needed to fill in the entire body of the crosstable, the rest will be determined by using the given row sums and the column sums values. Do not forget the conditions for the validity of Chi-square test and related expected values greater than 5 in 80% or more of the cells. Otherwise, one could use an "exact" test, using either a permutation or resampling approach. Using Chi-square in a 2x2 table requires the Yates's correction. One first subtracts 0.5 from the absolute differences between observed and expected frequencies for each of the three genotypes before squaring, dividing by the expected frequency, and summing. The formula for the Chi-square value in a 2x2 table can be derived from the Normal Theory comparison of the two proportions in the table using the total incidence to produce the standard errors. The rationale of the correction is a better equivalence of the area under the normal curve and the probabilities obtained from the discrete frequencies. In other words, the simplest correction is to move the cut-off point for the continuous distribution from the observed value of the discrete distribution to midway between that and the next value in the direction of the null hypothesis expectation. Therefore, the correction essentially only applied to one d.f. tests where the "square root" of the Chi-square looks like a "normal/t-test" and where a direction can be attached to the 0.5 addition. Chi-square distribution is used as an approximation of the binomial distribution. By applying a continuity correction, we get a better approximation of the binomial distribution for the purposes of calculating tail probabilities. Given the following 2x2 table, one may compute some relative risk measures: a b c d The most usual measures are: Rate-difference: a/(a+c) - b/(b+d) Rate-ratio: (a/(a+c))/(b/(b+d)) Odds-ratio: ad/bc The rate difference and rate ratio are appropriate when you are contrasting two groups whose sizes (a+c and b+d) are given. The odds ratio is for when the issue is association rather than difference. The risk-ratio (RR) is the ratio of the proportion (a/(a+b)) to the proportion (c/(c+d)): RR = (a / (a + b)) / (c / (c + d)) RR is thus a measure of how much larger the proportion in the first row is compared to the second. RR value of &lt; 1.00 indicating a 'negative' association [a/(a+b) &lt; c/(c+d)], 1.00 indicating no association [a/(a+b) = c/(c+d)], and &gt; 1.00 indicating a 'positive' association [a/(a+b) &gt; c/(c+d)]. The further from 1.00 the RR is, the stronger the association. An Application: Suppose a counselor of a school in a small town is interested whether the curriculum chosen by students is related to the occupation of their parents. It is necessary to record the data as shown in the following contingency table with two rows (r1, r2) and three columns (c1, c2, c3): Relationship between occupation of parents and curriculum chosen by high school students Curriculum Chosen by Students Parental Occupation College prep Vocational General Totals Professional 12 2 6 6 6 8 20 Blue collar 20 Totals 18 8 14 Under the hypothesis that there is no relation, the expected (E) frequency would be: E i, j = ( S r i )( S c j )/N The Observed (O) and Expected (E) frequencies are recorded in the following table: Expected frequencies for the data. College prep Vocational General Totals Professional O = 12 E = 9 O = 2 E = 4 O = 6 E = 7 O = 6 E = 9 O = 6 E = 4 O = 8 E = 7 å O = 20 å E = 20 Blue collar å O = 20 å E = 20 Totals å O = 18 å E = 18 å O = 8 å E = 8 å O = 14 å E = 14 The quantity c 2 = S [(O - E ) 2 / E] is a measure of the degree of deviation between the Observed and Expected frequencies. If there is no relationship between the row variable and the column variable this measure will be very close to zero. Under the hypothesis that there is a relationship between the rows and the columns, this quantity has a Chi-square distribution with parameter equal to number of rows minus 1, multiplied by number of columns minus 1. For this numerical example we have: c 2 = S [(O - E ) 2 / E] = 30/7 = 4.3 with d.f. = (2-1)(3-1) = 2, that has the p-value of 0.14, suggesting little or no real evidences against the null hypothesis. The main question is how large is this measure. The maximum value of this measure is: c 2 max = N(A-1), where A is the number of rows or columns, whichever is smaller. For our numerical example it is, 40(2-1) = 40. The coefficient of determination which has a range of [0, 1], provides relative strength of relationship, computed as c 2 / c 2 max = 4.3/40 = 0.11 Therefore we conclude that the degree of association is only 11% which is fairly weak. Alternatively, you could also look at the contingency coefficient f statistic, which is: f = [ c 2 /(N + c 2 )] ½ = 0.31 This statistic ranges between 0 and 1 and can be interpreted like the correlation coefficient. This measure also indicates that the curriculum chosen by students is related to the occupation of their parents. You might like to use Chi-square Test for Crosstable Relationship in performing this test, and he P-values for the Popular Distributions Applet to findout the p-values of Chi-square statistic. Further Readings: Agresti A., Categorical Data Analysis , Wiley, 2002. Fleiss J., Statistical Methods for Rates and Proportions , Wiley, 1981. Identical Populations Test for Crosstable Data Test of homogeneity is much like the Test for Crosstable Relationship in that both deal with the cross-classification of nominal data; that is, r &#180; c tables. The method of computing Chi-square statistic is the same for both tests, with the same d.f. The two tests differ, however, in the following respect. The Test for Crosstable Relationship is made on data drawn from a single population (with fixed total) where one is concerned with whether one set of attributes is independent of another set. The test for homogeneity, on the other hand, is designed to test the null hypothesis that two or more random samples are drawn from the same population or from different populations , according to some criterion of classification applied to the samples. The homogeneity test is concerned with the question: Are the samples drawn form populations that are homogeneous (i.e., the same) with respect to some criterion of classification? In the crosstable for this test, either the row or the column categories may represent the populations from which the samples are drawn. An Application: Suppose a board of directors of a labor union wishes to survey the opinion of its members regarding a change in its constitution. The following table shows the result of the survey sent to three union locals: Reactions of A Sample of Three Locals Group Members Union Local Reaction A B C In Favor 18 22 10 7 14 9 5 4 11 Against No Response The problem is not to determine whether or not the union members are in favor of the change. The question is to test if there is a significant difference in the proportions of opinion of the three populations' members concerning the proposed change. The Chi-square statistic is 9.58 with d.f. = (3-1)(3-1) = 4. The p-value is equal to 0.048, indicating that there is moderate evidence against the null hypothesis that the three union locals are the same. You might like to use Populations Homogeneity Test to perfor this test. Further Readings: Agresti A., Categorical Data Analysis , Wiley, 2002. Clark Ch., and L. Schkade, Statistical Analysis for Administrative Decisions , South-Western Pub., 1979. Test for Equality of Several Population Medians Generally, the median provides a better measure of location than the mean when there are some extremely large or small observations; i.e., when the data are skewed to the right or to the left. For this reason, median income is used as the measure of location for the U.S. household income. Suppose we are interested in testing the equality of the medians of k number of populations with respect to the same continuous random variable . The first step in calculating the test statistic is to compute the common median of the k samples combined. Then, determine for each group the number of observations falling above and below the common median. The resulting frequencies are arranged in a 2 by k crosstable. If the k samples are, in fact, from populations with the same median, one expects about one half the score in each sample to be above the combined median and about one half to be below. In the case that some observations are equal to the combined median, one may drop those few observations, in constructing a 2 by k crosstable. Under this condition, now the Chi-square statistic may be computed and compared with the p-value of Chi-square distribution with d.f. = k-1. An illustrative application: Do public and private primary school teachers differ with respect to their salary? The data from a random sample are given in the following table (in thousands of dollars per year). Public Private Public Private 35 29 25 50 26 50 27 37 27 43 45 34 21 22 46 31 27 42 33 38 47 26 23 42 46 25 32 41 The test of hypothesis is: H 0 : The public and private school teachers' salaries are almost the same. The median of all data (i.e., combined) is 33.5. Now determine in each group the number of observations falling above and below the common median of 33.5. The resulting frequencies are shown in the following table: Crosstable for the public and private school teachers' Public Private Total Above median 6 8 14 Below median 10 4 14 Total 16 12 28 The Chi-square statistic based on this table is 2.33. The p-value for the computed test statistic with d.f. = (2-1)(2-1) = 1 is 0.127, therefore, we are unable to reject the null hypothesis. You might like to use Testing Medians to perform this test. Goodness-of-Fit Test for Probability Mass Functions There are other tests that might use the Chi-square, such as goodness-of-fit test for discrete random variables . Therefore, Chi-square is a statistical test that measures "goodness-of-fit". In other words, it measures how much the observed or actual frequencies differ from the expected or predicted frequencies. Using a Chi-square table will enable you to discover how significant the difference is. A null hypothesis in the context of the Chi-square test is the model that you use to calculate your expected or predicted values. If the value you get from calculating the Chi-square statistic is sufficiently high (as compared to the values in the Chi-square table), it tells you that your null hypothesis is probably wrong. Let Y 1 , Y 2 , . . ., Y n be a set of independent and identically distributed discrete random variables. Assume that the probability distribution of the Y i 's has the probability mass function f o (y). We can divide the set of all possible values of Y i , i = {1, 2, ..., n}, into m non-overlapping intervals D 1 , D 2 , ...., D m . Define the probability values p 1 , p 2 , ..., p m as; p 1 = P(Y i Î D 1 ) p 2 = P(Y i Î D 2 ) : p m = P(Y i Î D m ) Where the symbol Î means, "an element of". Since the union of the mutually exclusive intervals D 1 , D 2 , ...., D m is the set of all possible values for the Y i 's, ( p 1 + p 2 + .... + p m ) = 1. Define the set of discrete random variables X 1 , X 2 , ...., X m , where X 1 = number of Y i 's whose value Î D 1 X 2 = number of Y i 's whose value Î D 2 : : X m = number of Y i 's whose value Î D m and (X 1 + X 2 + .... + X m ) = n. Then the set of discrete random variables X 1 , X 2 , ...., X m will have a multinomial probability distribution with parameters n and the set of probabilities { p 1 , p 2 , ..., p m }. If the intervals D 1 , D 2 , ...., D m are chosen such that np i ³ 5 for i = 1, 2, ..., m, then; C = S (X i - np i ) 2 / np i . The sum is over i = 1, 2,..., m. The results is distributed as c 2 m-1 . For the goodness-of-fit sample test, we formulate the null and alternative hypothesis as H 0 : f Y (y) = f o (y) H a : f Y (y) ¹ f o (y) At the a level of significance, H 0 will be rejected in favor of H a if C = S (X i - np i ) 2 / np i is greater than c 2 m However, it is possible that in a goodness-of-fit test, one or more of the parameters of f o (y) are unknown. Then the probability values p 1 , p 2 , ..., p m will have to be estimated by assuming that H 0 is true and calculating their estimated values from the sample data. That is, another set of probability values p ' 1 , p ' 2 , ..., p ' m will need to be computed so that the values ( np ' 1 , np ' 2 , ..., np ' m ) are the estimated expected values of the multinomial random variable (X 1 , X 2 , ...., X m ). In this case, the random variable C will still have a Chi-square distribution, but its degrees of freedom will be reduced. In particular, if the probability function f o (y) has r unknown parameters, C = S (X i - np i ) 2 / np i is distributed as c 2 m-1-r . For this goodness-of-fit test, we formulate the null and alternative hypothesis as H 0 : f Y (y) = f o (y) H a : f Y (y) ¹ f o (y) At the a level of significance, H 0 will be rejected in favor of H a if C is greater than c 2 m-1-r . An Application: A die is thrown 300 times and the following frequencies are observed. Test the hypothesis that the die is fair at level 0.05. Under the null hypothesis that the die is fair, the expected frequencies are all equal to 300/6 = 50. Both the Observed (O) and Expected (E) frequencies are recorded in the following table together with the random variable Y that represents the number on each sides of the die: Goodness-of-fit Test For Discrete Variables Y 1 2 3 4 5 6 O 57 43 59 55 63 23 E 50 50 50 50 50 50 The quantity c 2 = S [(O - E ) 2 / E] = 22.04 is a measure of the goodness-of-fit. If there is a reasonably good fit to the hypothetical distribution, this measure will be very close to zero. Since c 2 n-1, 0.95 = 11.07, we reject the null hypothesis that the die is a fair one. You might like to use this JavaScript to perform this test. For statistical equality of two random variables characterizing two populations, you might like to use the Kolmogorov-Smirnov Test if you have two independent sets of random observations, one from each population. Compatibility of Multi-Counts Test In some applications, such as quality control, it is necessary to check if the process is under control. This can be done by testing if there are significant differences between number of "counts", taken over k equal-periods of times. The counts are supposed to have been obtained under comparable conditions. The null hypothesis is: H 0 : There is no significant difference between number of "counts" taken over k equal-periods of times. Under the null hypothesis, the statistic: S (N i - N) 2 /N has a Chi-square distribution with d.f. = k-1. Where i is the count's number, N i is its counts, and N = S N i /k. One may extend this useful test to where the duration of obtaining the i th count is t i . Then the above test statistic becomes: S [(N i - t i N) 2 / t i N] and has a Chi-square distribution with d.f. = k-1, where i is the count's number, N i is its counts, and N = S N i / S t i . You might like to use the Compatibility of Multi-Counts JavaScript to check your computations, and to perform some numerical experimentation for a deeper understanding of the concepts. Necessary Conditions for the Above Chi-square Based Testing Like any statistical test procedures, the Chi-square based testing must meet certain necessary conditions to apply; otherwise, any obtained conclusion might be wrong or misleading. This is true in particular for using the Chi-square-based test for cross-tabulated data. Necessary conditions for the Chi-square based tests for crosstable data are: Expected values greater than 5 in 80% or more of the cells. Moreover, if number of cells is fewer than 5, then all expected values must be greater than 5. An Example: Suppose the monthly number of accidents reported in a factory in three eight-hour shifts is 1, 7, and 7, respectively. Are the working conditions and the exposure to risk similar for all shifts? Clearly, the answer must be, No they are not. However, applying the goodness-of-fit, at 0.05, under the null hypothesis that there are no differences in the number of accidents in three shifts, one expects 5, 5, and 5 accidents in each shift. The Chi-square test statistic is: c 2 = S [(O - E ) 2 / E] = 4.8 However, since c 2 n-1, 0.95 = 5.99, there is no reason to reject that there is no difference, which is a very strange conclusion. What is wrong with this application? You might like to use this JavaScript to verify your computation. Testing the Variance: Is the Quality that Good? Suppose a population has a normal distribution. The manager is to test a specific claim made about the quality of the population by way of testing its variance s 2 . Among three possible scenarios, the interesting case is in testing the following null hypothesis based on a set of n random sample observations: H 0 : Variation is about the claimed value. H a : The variation is more than what is claimed, indicating the quality is much lower than expected. Upon computing the estimated variance S 2 based on n observations, then the statistic: c ½ = [(n-1) . s 2 ] / s 2 has a Chi-square distribution with degree of freedom n = n - 1. This statistic is then used for testing the above null hypothesis. You might like to use Testing the Variance JavaScript to check your computations. Testing the Equality of Multi-Variances The equality of variances across populations is called homogeneity of variances or homoscedasticity. Some statistical tests, such as testing equality of the means by the t-test and ANOVA, assume that the data come from populations that have the same variance, even if the test rejects the null hypothesis of equality of population means. If this condition of homogeneity of variance is not met, the statistical test results may not be valid. Heteroscedasticity refers to lack of homogeneity of variances. Bartlett's Test is used to test if k samples have equal variances. It compares the Geometric Mean of the group variances to the arithmetic mean; therefore, it is a Chi-square statistic with (k-1) degrees of freedom, where k is the number of categories in the independent variable. The test is sensitive to departures from normality. The sample sizes do not have to be equal but each must be at least 6. Just like the two population t-test, ANOVA can go wrong when the equality of variances condition is not met. The Bartlett test statistic is designed to test for equality of variances across groups against the alternative that variances are unequal for at least two groups. Formally, H 0 : All variances are almost equal. The test statistic: B = { S [(n i -1)LnS 2 ] S [(n i -1)LnS i 2 ]}/ C In the above, S i 2 is the variance of the ith group, n i is the sample size of the i th group, k is the number of groups, and S 2 is the pooled variance. The pooled variance is a weighted average of the group variances and is defined as: S 2 = { S [(n i -1)S i 2 ]} / S [(n i -1)], over all i = 1, 2,..,k and C = 1 + { S [1/(n i -1)] - 1/ S [1/(n i -1)] }/[3(k+1)]. You might like to use the Equality of Multi-Variances JavaScript tor check your computations, and to perform some numerical experimentation for a deeper understanding of the concepts. Rule of 2: For 3 or more populations, there is a practical rule known as the "Rule of 2". According to this rule, one divides the highest variance of a sample by the lowest variance of the other sample. Given that the sample sizes are almost the same, and the value of this division is less than 2, then, the variations of the populations are almost the same. Example: Consider the following three random samples from three populations, P1, P2, P3: Sample P1 Sample P2 Sample P3 25 17 8 25 21 10 20 17 14 18 25 16 13 19 12 6 21 14 5 15 6 22 16 16 25 24 13 10 23 6 N 10 10 10 Mean 16.90 19.80 11.50 Std.Dev. 7.87 3.52 3.81 SE Mean 2.49 1.11 1.20 The ANOVA Table Sources of Variation Sum of Squares Degrees of Freedom Mean Squares F-Statistic Between Samples 79.40 2 39.70 4.38 Within Samples 244.90 27 9.07 Total 324.30 29 With an F = 4.38 and a p-value of .023, we reject the null at a = 0.05. This is not good news, since ANOVA, like the two-sample t-test, can go wrong when the equality of variances condition is not met. Further Readings: Hand D., and C. Taylor, Multivariate Analysis of Variance and Repeated Measures , Chapman and Hall, 1987. Miller R. Jr, Beyond ANOVA: Basics of Applied Statistics , Wiley, 1986. Correlation Coefficients Testing The Fisher's Z-transformation is a useful tool in the circumstances in which two or more independent correlation coefficients are to be compared simultaneously. To perform such a test one may evaluate the Chi-square statistic: c 2 = S [(n i - 3).Z i 2 ] - [ S (n i - 3).Z i ] 2 / [ S (n i - 3)], the sums are over all i = 1, 2, .., k. Where the Fisher Z-transformation is Z i = 0.5[Ln(1+r i ) - Ln(1-r i )], provided | r i | &#185; 1. Under the null hypothesis: H 0 : All correlation coefficients are almost equal. The test statistic c 2 has (k-1) degrees of freedom, where k is the number of populations. An Application: Consider the following correlation coefficients obtained by random sampling form ten independent populations. Population P i Correlation r i Sample Size n i 1 0.72 67 2 0.41 93 3 0.57 73 4 0.53 98 5 0.62 82 6 0.21 39 7 0.68 91 8 0.53 27 9 0.49 75 10 0.50 49 Using the above formula c 2 -statistic = 19.916, that has a p-value of 0.02. Therefore, there is moderate evidence against the null hypothesis. In such a case, one may omit a few outliers from the group, then use the Test for Equality of Several Correlation Coefficients applet. Repeat this process until a possible homogeneous sub-group may emerge. You might need to use Sample Size Determination JavaScript at the design stage of your statistical investigation in decision making with specific subjective requirements. Regression Modeling and Analysis Many problems in analyzing data involve describing how variables are related. The simplest of all models describing the relationship between two variables is a linear, or straight-line, model. Linear regression is always linear in the coefficients being estimated, not necessarily linear in the variables. The simplest method of drawing a linear model is to "eye-ball" a line through the data on a plot, but a more elegant, and conventional method is that of least squares, which finds the line minimizing the sum of the vertical distances between observed points and the fitted line. Realize that fitting the "best" line by eye is difficult, especially when there is much residual variability in the data. Know that there is a simple connection between the numerical coefficients in the regression equation and the slope and intercept of the regression line. Know that a single summary statistic, like a correlation coefficient, does not tell the whole story. A scatterplot is an essential complement to examining the relationship between the two variables. Again, the regression line is a group of estimates for the variable plotted on the Y-axis. It has a form of y = b + mx, where m is the slope of the line. The slope is the rise over run. If a line goes up 2 for each 1 it goes over, then its slope is 2. Formulas and Notations: = S x(i)/n This is just the mean of the x values. = S y(i)/n This is just the mean of the y values. S xx = S (x(i) - ) 2 = S x(i) 2 - [ S x(i) ] 2 / n S yy = S (y(i) - ) 2 = S y(i) 2 - [ S y(i) ] 2 / n S xy = S (x(i) - )(y(i) - ) = S x(i).y(i) - [ S x(i) . S y(i)] / n Slope m = S xy / S xx Intercept, b = - m . y-predicted = yhat = mx + b. Residual = Error = y - yhat. S errors = S (y - yhat) 2 . Standard deviation of residuals = S res = [S errors / (n-2)] ½ . Standard error of the slope (m) = S res / S xx ½ . Standard error of the intercept (b) = S res [(S xx + n. 2 ) /(n.S xx ] ½ . The regression line goes through a point with coordinates of (mean of x values, mean of y values), known as the mean-mean point. If you plug each x in the regression equation, then you obtain a predicted value for y. The difference between the predicted y and the observed y is called a residual, or an error term. Some errors are positive and some are negative. The sum of squares of the errors plus the sum of squares of the estimates add up to the sum of squares of Y. The regression line is the line that minimizes the variance of the errors. The mean error is zero; so, this means that it minimizes the sum of the squares errors. The reason for finding the best fitting line is so that you can make a reasonable prediction of what y will be if x is known (not vise-versa). r 2 is the variance of the estimates divided by the variance of Y. r is the size of the slope of the regression line, in terms of standard deviations. In other words, it is the slope of the regression line if we use the standardized X and Y. It is how many standard deviations of Y you would go up, when you go one standard deviation of X to the right. Coefficient of Determination: Another measure of the closeness of the points to the regression line is the Coefficient of Determination: r 2 = S yhat yhat / S yy which is the amount of the squared deviation in Y, that is explained by the points on the least squares regression line. Homoscedasticity and Heteroscedasticity: Homoscedasticity (homo = same, skedasis = scattering) is a word used to describe the distribution of data points around the line of best fit. The opposite term is heteroscedasticity. Briefly, homoscedasticity means that data points are distributed equally about the line of best fit. Therefore, homoscedasticity means constancy of variance over all the levels of factors. Heteroscedasticity means that the data points cluster or clump above and below the line in a non-equal pattern. Standardized Regression Analysis: The scale of measurements used to measure X and Y has major impact on the regression equation and correlation coefficient. This impact is more drastic comparing two regression equations having different scales of measurement. To overcome these drawbacks, one must standardize both X and Y prior to constructing the regression and interpreting the results. In such a model, the slope is equal to the correlation coefficient r. Notice that the derivative of function Y with respect to dependent variable X is the correlation coefficient. Therefore, there is a nice similarity in the meaning of r in statistics and the derivative from calculus, in that its sign and its magnitude reveal the increasing/decreasing and the rate of change, as the derivative of a function do. In the usual regression modeling the estimated slope and intercept are correlated ; therefore, any error in estimating the slope influences the estimate of the intercept. One of the main advantages of using the standardized data is that the intercept is always equal to zero. Regression when both X and Y are random: Simple linear least-squares regression has among its conditions that the data for the independent (X) variables are known without error. In fact, the estimated results are conditioned on whatever errors happened to be present in the independent data sets. When the X-data have an error associated with them the result is to bias the slope downwards. A procedure known as Deming regression can handle this problem quite well. Biased slope estimates (due to error in X) can be avoided using Deming regression. If X and Y are random variables, then the correlation coefficient R is often referred to as the Coefficient of Reliability . The Relationship Between Slope and Correlation Coefficient: By a little bit of algebraic manipulation, one can show that the coefficient of correlation is related to the slope of the two regression lines: Y on X, and X on Y, denoted by m yx and m xy , respectively: R 2 = m yx . m xy Lines of regression through the origin: Often the conditions of a practical problem require that the regression line go through the origin (x = 0, y = 0). In such a case, the regression line has one parameter only, which is its slope: m = S (x i &#180; y i )/ S x i 2 Notice that, for the models with the omission of the intercept, it is generally agreed that R 2 should not be defined or even considered. Parabola models: Parabola regressions have three coefficients with a general form: Y = a + bX + cX 2 , where c = { S (x i - xbar) 2 &#215; y i - n[ S (x i - xbar) 2 &#215; S y i ]} / {n S (x i - xbar) 4 - [ S (x i - xbar) 2 ] 2 } b = [ S (x i - xbar) y i ]/[ S (x i - xbar) 2 ] - 2 &#215; c &#215; xbar a = { S y i - [c &#215; S (x i - xbar) 2 )}/n - (c &#215; xbar &#215; xbar + b &#215; xbar), where xbar is the mean of x i 's. Applications of quadratic regression include fitting the supply and demand curves in econometrics and fitting the ordering cost and holding cost functions in inventory control for finding the optimal ordering quantity. You might like to use Quadratic Regression JavaScript to check your hand computation. For higher degrees than quadratic, you may like to use the Polynomial Regressions JavaScript. Multiple Linear Regression: The objectives in a multiple regression problem are essentially the same as for a simple regression. While the objectives remain the same, the more predictors we have the calculations and interpretations become more complicated. With multiple regression, we can use more than one predictor. It is always best, however, to be parsimonious, that is to use as few variables as predictors as necessary to get a reasonably accurate forecast. Multiple regression is best modeled with commercial package such as SAS and SPSS . The forecast takes the form: Y = b 0 + b 1 X 1 + b 2 X 2 + . . .+ b n X n , where b 0 is the intercept, b 1 , b 2 , . . . b n are coefficients representing the contribution of the independent variables X 1 , X 2 ,..., X n . For small sample size, you may like to use the Multiple Linear Regression JavaScript. What Is Auto-Regression: In time series analysis and forecasting techniques, often linear regression is use to combine present and past values of an observation in order to forecast its future value. The model is called an autoregressive model. For details and implementation process visit Autoregressive Modeling JavaScript. What Is Logistic Regression: Standard logistic regression is a method for modeling binary data (e.g., does a person smoke or not?, does a person survive a disease, or not?). Polygamous logistic regression is a method for modeling more than two options (e.g., does a person take the bus, drive a car or take the subway? does an office use WordPerfect, Word, or other office-ware?). Why Linear Regression? The study of corn shell (i.e., ear of corn) height versus rainfall has shown to have the following regression curve: Clearly, the relationship is highly nonlinear; however, if we are interested in a "small" range (say, for a specific geographical area, like southern region of the state of Maryland) then the condition of linearity might be satisfactory. A typical application is depicted in the above figure where we are interested in predicting the height of corn in an area with rainfall in the range of [a, b]. Magnifying process of scale for this range allows us to fit a useful linear regression. If the range is not short enough, then one may sub-divide the range accordingly by applying the same process of fitting a few lines, one for each sub-interval. Structural Changes: When a regression model has been estimated using the available data set, an additional data set may sometimes become available. To test if previous model is still valid or the two separate models are equivalent or not, one may use the analysis of covariance testing described on this site. You might like to use the Regression Analysis JavaScript to check your computations and to perform some numerical experimentation for a deeper understanding of the concepts. Further Reading: Chatterjee S., B. Price, and A. Hadi, Regression Analysis by Example , Wiley, 1999. Regression Modeling Selection Process When you have more than one regression equation based on data, to select the "best model", you should compare: R-squares: That is, the percentage of variance [in fact, the sum of squares] in Y accounted for by variance in X captured by the model. When you want to compare models of different sizes (different numbers of independent variables (p) and/or different sample sizes n), you must use the Adjusted R-Square, because the usual r-square tends to grow with the number of independent variables. r 2 a = 1 - (n - 1)(1 - r 2 )/(n - p - 1) Standard deviation of error terms, i.e., observed y-value - predicted y-value for each x. Trends in errors as a function of control variable x. Systematic trends are not uncommon. The T-statistic of individual parameters. The values of the parameters and its content to content underpinnings. F df1 df2 value for overall assessment. Where df1 (numerator degrees of freedom) is the number of linearly independent predictors in the assumed model minus the number of linearly independent predictors in the restricted model; i.e., the number of linearly independent restrictions imposed on the assumed model, and df2 (denominator degrees of freedom) is the number of observations minus the number of linearly independent predictors in the assumed model. The observed F-statistic should exceed not merely the selected critical value of F-table, but at least four times the critical value. Regression Analysis Process Click on the image to enlarge it and THEN print it Finally in statistics for business, there exists an opinion that with more than 4 parameters, one can fit an elephant so that if one attempts to fit a regression funtion that depends on many parameters, the result should not be regarded as very reliable. Further Reading: Draper N., and H. Smith, Applied Regression Analysis , Wiley, 1998. Covariance and Correlation Suppose that X and Y are two random variables for the outcome of a random experiment. The covariance of X and Y is defined by Cov (X, Y) = E{[X - E(X)][Y - E(Y)]} and, given that the variances are strictly positive, the correlation of X and Y is defined by r (X, Y) = Cov(X, Y) / [sd(X) . sd(Y)] Correlation is a scaled version of covariance; note that the two parameters always have the same sign (positive, negative, or 0). When the sign is positive, the variables are said to be positively correlated; when the sign is negative, the variables are said to be negatively correlated; and when it is 0, the variables are said to be uncorrelated. Notice that the correlation between two random variables is often due only to the fact that both variables are correlated with the same third variable. As these terms suggest, covariance and correlation measure a certain kind behavior in both variables. Correlation is very similar to the derivative of a function that you may have studies in high school. Coefficient of Determination: The square of correlation coefficient r 2 indicates the proportion of the variation in one variable that can be associated with the variance in the other variable. The three typical possibilities are depicted in the following figure: The proportion of shared variance by two variables for the different values of the coefficient of determination: r 2 = 0, r 2 = 1, and r 2 = 0.25, as shown by the shaded areas in this figure. Properties: The following exercises give some basic properties of expected values. The main tool that you will need is the fact that expected value is a linear operation. You might like to use this Applet in performing some numerical experimentation to: Show that E[X/Y] ¹ E(X)/E(Y). Show that E[X &#180; Y] ¹ E(X) &#180; E(Y). Show that [E(X &#180; Y) 2 ] £ E(X 2 ) &#180; E(Y 2 ). Show that [E(X/Y) n ] ³ E(X n )/E(Y n ), for any n. Show that Cov(X, Y) = E(XY) - E(X)E(Y). Show that Cov(X, Y) = Cov(Y, X). Show that Cov(X, X) = V(X). Show that: If X and Y are independent random variables, then Var(XY) = 2 V(X) &#180; V(Y) + V(X)(E(Y)) 2 + V(Y)(E(X)) 2 . Pearson, Spearman, and Point-Biserial Correlations There are measures that describe the degree to which two variables are linearly related. For the majority of these measures, the correlation is expressed as a coefficient that ranges from 1.00 to -1.00. A value of 1 is indicating a perfect linear relationship, such that knowing the value of one variable will allow perfect prediction of the value of the related value. A value of 0 is indicating no predictability by a linear model. With negative values indicating that, when the value of one variable is higher than average, the other is lower than average (and vice versa); and positive values indicating that, when the value of one variable is high, so is the other (and vice versa). Correlation is similar to the derivative you have learned in calculus (a deterministic course). The Pearson's product correlation is an index of the linear relationship between two variables. Formulas and Notations: = S x i / n. This is just the mean of the x values. = S y i / n. This is just the mean of the y values. S xx = S (x i - ) 2 = S x i 2 - [ S x i ] 2 / n. S yy = S (y i - ) 2 = S y i 2 - [ S y i ] 2 / n. S xx = S (x i - )(y i - ) = S (x i y i ) - [ S x i . S y i ] / n. The Pearson's correlation is r = S xy / (S xx &#180; S yy ) 0.5 A positive relationship indicates that if an individual value of x is above the mean of x's, then this individual x is likely to have a y value that is above the mean of y's, and vice versa. A negative relationship would be an x score above the mean of x and a y score below the mean of y. It is a measure of the relationship between variables and an index of the proportion of individual differences in one variable that can be associated with the individual differences in another variable. Notice that, the correlation coefficient is the mean of the cross-products of scores. Therefore, if you have three values for r of .40, .60, and .80, you cannot say that the difference between r = .40 and r = .60 is the same as the difference between r =.60 and r = .80, or that r = .80 is twice as large as r = .40 because the scale of values for the correlation coefficient is not interval or ratio, but ordinal. Therefore, all you can say is that, for example, a correlation coefficient of +.80 indicates a high positive linear relationship and a correlation coefficient of +.40 indicates a some what lower positive linear relationship. The square of the correlation coefficient equals the proportion of the total variance in Y that can be associated with the variance in x. It can tell us how much of the total variance of one variable can be associated with the variance of another variable. Note that a correlation coefficient is done on linear correlation. If the data forms a parabola, then a linear correlation of x and y will produce an r-value equal to zero. So one must be careful and look at data. The standard statistics for hypothesis testing: H 0 : r = r 0 , is the Fisher's normal transformation: z = 0.5[Ln(1+r) - Ln(1-r)], with mean m = 0.5[Ln(1+ r 0 ) - Ln(1- r 0 )], and standard deviation s = (n-3) -½ . Having constructed a desirable confidence interval, say [a, b], based on statistic Z, it has to be transformed back to the original scale. That is, the confidence interval is: (e 2a -1)/ (e 2a +1), (e 2b -1)/ (e 2b +1). Provided | r 0 | &#185; 1, and | r 0 | &#185; 1, and n is greater than 3. Alternatively, {1+ r - (1-r) exp[2z a /(n-3) ½ ]} / {1+ r + (1-r) exp[2z a /(n-3) ½ ]} , and {1+ r - (1-r) exp[-2z a /(n-3) ½ ]} / {1+ r + (1-r) exp[-2z a /(n-3) ½ ]} You might like to use this calculator for your needed computation. You may perform Testing the Population Correlation Coefficient . Spearman rank-order correlation is used as a non-parametric version of Pearson's. It is expressed as: r = 1 - (6 S d 2 ) / [n(n 2 - 1)], where d is the difference rank between each X and Y pair. Spearman correlation coefficient can be algebraically derived from the Pearson correlation formula by making use of sums of series. Pearson contains expressions for S X(i), S Y(i), S X(i) 2 , and S Y(i) 2 . In the Spearman case, the X(i)'s and Y(i)' are ranks, and so the sums of the ranks, and the sums of the ranks squared, are entirely determined by the number of cases (without any ties). S i = (n+1)n/2, S i 2 = n(n+1)(2n+1)/6. The Spearman formula then is equal to: [12P - 3n(n+1) 2 ] / [n(n 2 - 1)], where P is the sum of the product of each pair of ranks X(i)Y(i). This reduces to: r = 1 - (6 S d 2 ) / [n(n 2 - 1)], where d is the difference rank between each x(i) and y(i) pair. An important consequence of this is that if you enter ranks into a Pearson formula, you get precisely the same numerical value as that obtained by entering the ranks into the Spearman formula. This comes as a bit of a shock to those who like to adopt simplistic slogans, such as "Pearson is for interval data, Spearman is for ranked data". Spearman doesn't work too well if there are many tied ranks. That's because the formula for calculating the sums of squared ranks no longer holds true. If one has many tied ranks, use the Pearson formula. One may use this measure as a decision-making tool: Value of | r | Interpretation 0.00 - 0.40 Poor 0.41 - 0.75 Fair 0.76 - 0.85 Good 0.86 - 1.00 Excellent This interpretation is widely accepted, and many scientific journals routinely publish papers using this interpretation for the estimated result, and even for the test of hypothesis. Point-Biserial Correlation is used when one random variable is binary (0, 1) and the other is a continuous random variable; the strength of relationship is measured by the point-biserial correlation: r = (X 1 - X 0 )[pq/S 2 ] ½ Where X 1 and X 0 are the means of scores having 1, and 0 values, and p and q are their proportions, respectively. S 2 is the sample variance of the continuous random variable. This is a simplified version of the Pearson correlation for the case when one of the two random variables is a (0, 1) Nominal random variable. Note also that r has the shift-invariant property for any positive scale. That is ax + c, and by + d, have same r as x and y, for any positive a and b. Correlation, and Level of Significance It is intuitive that with very few data points, a high correlation may not be statistically significant. You may see statements such as, "correlation is significant between x and y at the a = .005 level" and "correlation is significant at the a = .05 level." The question is: how do you determine these numbers? Using the simple correlation r, the formula for F-statistic is: F= (n-2) r 2 / (1-r 2 ), where n is at least 2. As you see, F statistic is monotonic function with respect to both: r 2 , and the sample size n. Notice that the test for the statistical significance of a correlation coefficient requires that the two variables be distributed as a bivariate normal. Independence vs. Correlated In the sense that it is used in statistics; i.e., as an assumption in applying a statistical test; a random sample from the entire population provides a set of random variables X1,...., Xn, that are identically distributed and mutually independent. Mutually independent is stronger than pairwise independence. The random variables are mutually independent if their joint distribution is equal to the product of their marginal distributions. In the case of joint normality, independence is equivalent to zero correlation, but not in general. Independence will imply zero correlation but not conversely. Not that not all random variables have a first moment, let alone a second moment, and hence there may not be a correlation coefficient. However; if the correlation coefficient of two random variables is not zero then the random variables are not independent. How to Compare Two Correlation Coefficients? Given that two populations have normal distributions, we wish to test for the following null hypothesis regarding the equality of correlation coefficients: H o : r 1 = r 2 , based on two observed correlation coefficients r 1 , and r 2 , obtained from two random sample of size n 1 and n 2 , respectively, provided | r 1 | &#185; 1, and | r 2 | &#185; 1, and n 1 , n 2 both are greater than 3. Under the null hypothesis and normality condition , the test statistic is: Z = (z 1 - z 2 ) / [ 1/(n 1 -3) + 1/(n 2 -3) ] ½ where: z 1 = 0.5 Ln [ (1+r 1 )/(1-r 1 ) ], z 2 = 0.5 Ln [ (1+r 2 )/(1-r 2 ) ], and n 1 = sample size associated with r 1 , and n 2 =sample size associated with r 2 . The distribution of the Z-statistic is the standard Normal(0,1); therefore, you may reject H 0 if |Z| &gt; 1.96 at the 95% confidence level. An Application: Suppose r 1 = 0.47, r 2 = 0.63 are obtained from two independent random samples of size n 1 =103, and n 2 = 103, respectively. Therefore, the z 1 = 0.510, and z 2 = 0.741, with Z-statistics: Z = (0.510 - 0.7)/ [1/(103-3) + 1/(103-3)] ½ = -1.63 This result is not within the rejection region of the two-tails critical values at a = 0.05, therefore is not significant. Therefore, there is not sufficient evidence to reject the null hypothesis that the two correlation coefficients are equal Clearly, this test can be modified and applied for test of hypothesis regarding population correlation r based on observed r obtained from a random sample of size n: Z = (z r - z r ) / [1/(n-3) ] ½ , provided | r | &#185; 1, and | r | &#185; 1, and n is greater than 3. Testing the Equality of Two Dependent Correlations: In testing the hypothesis of no difference between two population correlation coefficients: H 0 : r (X, Y) = r (X, Z) Against the alternative: H a : r (X, Y) &#185; r (X, Z) with a common covariare X, one may use the following test statistics: t = { (r xy - r xz ) [ (n-3)(1 + r yz )] ½ ] } / {2(1-r xy 2 - r xz 2 - r yz 2 + 2r xy r xz r yz )} ½ , with n - 3 degree of freedom, where n is the tripled-ordered sample size, provided all absolute value of r's are not equal to 1. Numerical example: Suppose n = 87, r xy = 0.631, r xz = 0.428, and r yz = 0.683, then t-statistic is equal to 3.014, with p-value equal to 0.002, indicating a strong evidence against the null hypothesis. Adjusted R 2 : In modeling selection process based of R 2 values, it is often necessary and meaningful to adjust the R 2 's for their degrees of freedom. Each Adjusted R 2 is calculated by: 1 - [(n - i)(1 - R 2 )] / (n - p), where i is equal to 1 if there is an intercept and 0 otherwise; n is the number of observations used to fit the model; and p is the number of parameters in the model. You might like to use the Testing the Population Correlation Coefficient JavaScript in performing some numerical experimentation for validating and a deeper understanding of the concepts. Planning, Development, and Maintenance of a Linear Model A. Planning: Define the problem; select response; suggest variables. Are the proposed variables fundamental to the problem, and are they variables? Are they measurable/countable? Can one get a complete set of observations at the same time? Ordinary regression analysis does not assume that the independent variables are measured without error. However, they are conditioned on whatever errors happened to be present in the independent data set. Is the problem potentially solvable? Find correlation matrix and first regression runs (for a subset of data). Find the basic statistics, correlation matrix. How difficult is the problem? Compute the Variance Inflation Factor: VIF = 1/(1 -r ij ), for all i, j. For moderate VIF's, say between 2 and 8, you might be able to come-up with a good' model. Inspect r ij 's; one or two must be large. If all are small, perhaps the ranges of the X variables are too small. Establish goal; prepare budget and time table. a. The final equation should have Adjusted R 2 = 0.8 (say). b. Coefficient of Variation of say; less than 0.10 c. Number of predictors should not exceed p (say, 3), (for example for p = 3, we need at least 30 points). Even if all the usual assumptions for a regression model are satisfied, over-fitting can ruin a model's usefulness. The widely used approach is the data reduction method to deal with the cases where the number of potential predictors is large in comparison with the number of observations. d. All estimated coefficients must be significant at m = 0.05 (say). e. No pattern in the residuals Are goals and budget acceptable? B. Development of the Model: Collect date; check the quality of date; plot; try models; check the regression conditions. Consult experts for criticism. Plot new variable and examine same fitted model. Also transformed Predictor Variable may be used. Are goals met? Have you found "the best" model? C. Validation and Maintenance of the Model: Are parameters stable over the sample space? Is there a lack of fit? Are the coefficients reasonable? Are any obvious variables missing? Is the equation usable for control or for prediction? Maintenance of the Model. Need to have control chart to check the model periodically by statistical techniques. You might like to use Regression Analysis with Diagnostic Tools in performing regression analysis. Conditions and the Check-list for Linear Models Almost all models of reality, including regression models, have assumptions that must be verified in order that the model has power to test hypotheses and for it to be able to predict accurately. The following is the list of basic assumptions (i.e., conditions) and the tools to check these necessary conditions. Any undetected outliers may have major impact on the regression model. Outliers are a few observations that are not well fitted by the "best" available model. In such case one, must first investigate the source of data, if there is no doubt about the accuracy or veracity of the observation, then it should be removed and the model should be refitted. You might like to use the Determination of the Outliers JavaScript to perform some numerical experimentation for validating and for a deeper understanding of the concepts The dependent variable Y is a linear function of the independent variable X. This can be checked by carefully examining all the points in the scatter diagram , and see if it is possible to bound them all within two parallel lines. You may also use the Detective Testing for Trend to check this condition. The distribution of the residual must be normal. You may check this condition by using the Lilliefors Test for Normality . The residuals should have a mean equal to zero, and a constant standard deviation (i.e., homoskedastic condition). You may check this condition by dividing the residuals data into two or more groups; this approach is known as the Goldfeld-Quandt test. You may use the Stationary Testing Process to check this condition. The residuals constitute a set of random variables. You may use the Test for Randomness and Test for Randomness of Fluctuations to check this condition . Durbin-Watson (D-W) statistic quantifies the serial correlation of least-squares errors in its original form. D-W statistic is defined by: D-W statistic = S 2 n (e j - e j-1 ) 2 / S 1 n e j 2 , where e j is the j th error. D-W takes values within [0, 4]. For no serial correlation, a value close to 2 is expected. With positive serial correlation, adjacent deviates tend to have the same sign, therefore D-W becomes less than 2; whereas, with negative serial correlation, alternating signs of error, D-W takes values larger than 2. For a least-squares fit where the value of D-W is significantly different from 2, the estimates of the variances and covariances of the parameters (i.e., coefficients) can be in error, being either too large or too small. The serial correlation of the deviates arise also time series analysis and forecasting. You may use the Measuring for Accuracy JavaScript to check this condition. The "good" regression equation candidate is further analyzed using a plot of the residuals versus the independent variable(s). If any patterns are seen in the graph; e.g., an indication of non-constant variance; then there is a need for data transformation. The following are the widely used transformations: X' = 1/X, for non-zero X. X' = Ln (X), for positive X. X' = Ln(X), Y' = Ln (Y), for positive X, and Y. Y' = Ln (Y), for positive Y. Y' = Ln (Y) - Ln (1-Y), for Y positive, less than one. Y' = Ln [Y/(100-Y)], known as the logit transformation , which is useful for the S-shape functions. Taking square root of a Poisson random variable , the transformed variable is more symmetric. This is a useful transformation in regression analysis with Poisson observations. It also stabilizes the residual variation. Box-Cox Transformations: The Box-Cox transformation, below, can be applied to a regressor, a combination of regressors, and/or to the dependent variable (y) in a regression. The objective of doing so is usually to make the residuals of the regression more homoskedastic (ie., independently and identically distributed) and closer to a normal distribution: (y l - 1) / l for a constant l not equal to zero, and log(y) for l = 0. You might like to use the Regression Analysis with Diagnostic Tools JavaScript to check your computations, and to perform some numerical experimentation for a deeper understanding of the concepts. Analysis of Covariance: Comparing the Slopes Consider the following two samples of before-and-after independent treatments. Values of Covariate X and a Dependent Variable Y Treatment-I Treatment-II X Y X Y 5 11 2 1 3 9 6 7 1 5 4 3 4 8 7 8 6 12 3 2 We wish to test the following test of hypothesis on the two means of the dependent variable Y1, and Y2: H 0 : The difference between the two means is about a given value M. H a : The difference between the two means is quite different than it is claimed. Since we are dealing with dependent variables, it's natural to investigate the linear regression coefficients of the two samples; namely, the slopes and the intercepts. Suppose we are interested in testing the equality of two slopes. In other words, we wish to determine if two given lines are statistically parallel. Let m 1 represent the regression coefficient for explanatory variable X 1 in sample 1 with size n 1 . Let m 2 represent the regression coefficient for X 2 in sample 2 with size n 2 . The difference between the two estimated slopes has the following variance: V= Var [m 1 - m 2 ] = {S xx1 &#180; S xx2 [(n 1 -2)S res1 2 + (n 2 -2)S res2 2 ] /[(n 1 + n 2 - 4)(S xx1 + S xx2 ]. Then, the quantity: (m 1 - m 2 ) / V ½ has a t-distribution with d.f. = n1 + n2 - 4. This test and its generalization in comparing more than two slopes are called the Analysis of Covariance (ANOCOV). The ANOCOV test is the same as in the ANOVA test; however there is an additional variable called covariate. ANOCOV enables us to conduct and to extend the before-and-after test for two different populations. The process is as follows: Find a linear model for (X 1 , Y 1 ) = (before 1 , after 1 ), and one for (X 2 , Y 2 ) = (before 2 , after 2 ) that fit best. Perform the test of the hypothesis m 1 = m 2 . If the test result indicates that the slopes are almost equal, then compute the common slope of the two parallel regression lines: Slope par = (m 1 S xx1 + m 2 S xx2 ) / (S xx1 + S xx2 ). The variance of the residuals is: S res 2 = [S yy1 + S yy2 - (S xx1 + S xx2 ) Slope par ] / ( n 1 + n 1 -3). Now, perform the test for the difference between the two the intercepts, which is the vertical difference between the two parallel lines: Intercepts' difference = 1 - 2 - ( 1 - 2 ) Slope par . The test statistic is: (Intercepts' difference) / {S res [1/n 1 + 1/n 2 + ( 1 - 2 ) 2 /(S xx1 + S xx2 )] ½ }, which has a t-distribution with parameter d.f. = n 1 + n 1 -3. Depending on the outcome of the last test, one may reject the null hypothesis. For our numerical example, using the Analysis of Covariance JavaScripts, we obtained the following statistics: Slope 1 = 1.3513514; its standard error = .2587641 Slope 2 = 1.4883721; its standard error = 1.0793906 These indicate that there is no evidence against equality of the slopes. Now, we may test for any differences in the intercepts. Suppose we wish to test the null hypothesis that the vertical distance between the two parallel lines is about 4 units. Using the second function in the Analysis of Covariance JavaScripts, we obtained the statistics: Common Slope = 1.425, Intercept =5.655, providing a moderate evidence against the null hypothesis. Further Reading: Wall F., Statistical Data Analysis Handbook , by McGraw-Hill, New York, 1986. Residential Properties Appraisal Application Estimating the market value of large numbers of residential properties is of interest to a number of socio-economic stakeholders, such as mortgage and insurance companies, banks and real-estate agencies, and investment property companies, etc. It is both a science and an art. It is a science, because it is based on formal, rigorous and proven methods. It is an art because interaction with socio-economic stakeholders and the methods used give rise to all sorts of tradeoffs and compromises that assessors and their organizations must take into account when making decisions on the basis of their experience and skills. The market value assessment of a set of selected houses involves performing an assessment by a few individual appraisers for each property and then computing an average obtained from the few individuals. Individual appraisal refers to the process of estimating the exchange value of a house on the basis of a direct comparison between its profiles and the profiles of a set of other comparable properties sold on acceptable conditions. The profile of a property consists of all the relevant attributes of each house, such as the location, size, gross living space, age, one-story, two-story or more, garage, swimming pool, basement, etc. Data on prices and characteristics of individual houses are available; e.g., from the U.S Bureau of the Census. Often regression analysis is used to determine what characteristics influence the price of the houses. Thus it is important to correct the subjective elements in the appraisal value before carrying out the regression analysis. Coefficients that are not significantly different from zero as indicated by insignificant t-statistics at a 5% level are dropped from the regression model. There are several practical questions to be answered before the actual data collection can take place. The first step is to use statistical techniques, such as geographic clustering, to define homogeneous groupings of houses within an urban area. How many houses should we look at? Ideally, one would collect information on as many houses as time and money allow. It is these practical considerations that make statistics so useful. Hardly anyone could spend the time, money, and effort needed to look at every house for sale. It is unrealistic to obtain information on every house of interest, or in statistical terms, on every item in the population. Thus, we can look only at a sample of houses -- a subset of the population -- and hope that this sample will give us reasonably accurate information about the population. Let us say we can afford to look at 16 houses. We would probably choose to select a simple random sample-that is, a sample in which, roughly speaking, every house in the population has equal chance of being included. Then we would expect to get a reasonably representative sample of houses throughout this selected size range, reflecting prices for the whole neighborhood. This sample should give us some information about all houses of all sizes within this range, since a simple random sample tends to select as many larger houses as smaller houses, and as many expensive as less expensive ones. Suppose that the 16 houses in our random sample have the sizes and prices shown in the following Table. If 160 houses are randomly selected, variables Y, X1, and X2 are random variables. We have no control over them and cannot know what specific values will be selected. It is chance only that determines them. - Sizes, Ages, and Prices of Twenty Houses - X1 = Size X2 = Age Y = Price X1 = Size X2 = Age Y = Price 1.8 30 32 2.3 30 44 1.0 33 24 1.4 17 27 1.7 25 27 3.3 16 50 1.2 12 25 2.2 22 37 2.8 12 47 1.5 29 28 1.7 1 30 1.1 29 20 2.5 12 43 2.0 25 38 3.6 28 52 2.6 2 45 What can we tell about the relationship between size and price from our sample? Reading the data from the above table row-wise, and entering them in the Regression Analysis with Diagnostic Tools JavaScript, we found the following simple regression model: Price = 9.253 + 12.873(Size) Now consider the problem of estimating the price (Y) of a house from knowing its size (X1) and also its age (X2). The sizes and prices will be the same as in the simple regression problem. What we have done is add ages of houses to the existing data. Note carefully that in real life, one would not first go out and collect data on sizes and prices and then analyze the simple regression problem. Rather, one would collect all data, which might be pertinent on all twenty houses at the outset. Then the analysis performed would throw out predictors which turn out not to be needed. The objectives in a multiple regression problem are essentially the same as for a simple regression. While the objectives remain the same, the more predictors we have the calculations and interpretations become more complicated. For large data set one may use the multiple regression module of any statistical package such as SAS and SPSS . Using the Multiple Linear Regression JavaScript, for our numerical example with X1 = Size, X2 = Age, and Y = Price, we obtain the following statistical model: Price = 9.959 + 12.800(Size) - 0.027(Age) The regression results suggest that, on average, as the Size of house increases the Price increases. However, the coefficient of the variable Age is significantly small with negative value indicating an inverse relationship. Older houses tend to cost less than newer houses. Moreover, the correlation between Price and Age is -0.236. This result indicates that only 6% of variation in price can be accounted by the different in ages of the houses. This result supports our suspicion that the Age is not a significant predictor of price. Therefore, the simple regression: Price = 9.253 + 12.873(Size) Now, the question is: Is this model is good enough to satisfy the usual conditions of the regression analysis. The following is the list of basic assumptions (i.e., conditions) and the tools to check these necessary conditions. Any undetected outliers may have major impact on the regression model. Using the Determination of the Outliers JavaScript we found that there is no outlier in the above data set. The dependent variable Price is a linear function of the independent variable Size. By carefully examining the scatter diagram we found that the linearity condition is satisfied. The distribution of the residual must be normal. Reading the data from the above table row-wise, and entering them in the Regression Analysis with Diagnostic Tools JavaScript, we found that the normality condition is also satisfied. The residuals should have a mean equal to zero, and a constant standard deviation (i.e., homoskedastic condition). By the Regression Analysis with Diagnostic Tools JavaScript, the results are satisfactory. The residuals constitute a set of random variables. The persistent non-randomness in the residuals violates the best linear unbiased estimator condition. However, since the numerical statistics corresponding to the residuals obtained by using Regression Analysis with Diagnostic Tools JavaScript, are not significant, therefore our ordinary least square regression is adequate for our analysis. Durbin-Watson (D-W) statistic quantifies the serial correlation of least-squares errors in its original form. D-W statistic for this model is 1.995, which is good enough in rejecting any serial correlation. More Useful Statistics for the Model: The standard errors for the Slope and the Intercept are0.881, and 1.916, respectively, which are small enough. The F-statistic is 213.599, which is large enough indicating that the model is good enough overall for prediction purposes. Notice that since the above analysis is performed on a specific set of data, as always, one must be careful in generalizing its findings. Further Reading: Lovell, R., and French, N., Estimated realization price: what do the banks want and what can be realistically provided? Journal of property finance , 6, 7-16, 1995. Newsome, B.A. and Zeitz, J., 1992. Adjusting comparable sales using multiple regression analysis-the need for segmentation, The Appraisal Journal , 8, 129-135. Introduction to Integrating Statistical Concepts Statistical thinking for decision-making requires a deeper understanding than merely memorizing each isolated technique . Understanding involves ever expansion of neural network by means of correct connectivity between concepts. The aim of this chapter is to look closely at some of the concepts and techniques that we have learned up to now in a unifying theme. The following case studies, improve your statistical thinking to see the wholeness and manifoldness of statistical tools . As you will see, although one would hope that all tests give the same results this is not always the case. It all depends on how informative the data are and to what extend they have been condensed before presenting them to you for analysis (while becoming a good statistician). The following sections are illustrations in examining how much useful information they provide and how they may result in opposite conclusions, if one is not careful enough. Hypothesis Testing with Confidence One of the main advantages of constructing a confidence interval (CI) is to provide a degree of confidence for the point estimate for the population parameter. Moreover, one may utilize CI for the test of hypothesis purposes. Suppose you wish to test the following general test of hypothesis: H 0 : The population parameter is almost equal to a given claimed value, against the alternative: H a : The population parameter is not even close to the claimed value. The process of executing the above test of hypothesis at a level of significance using CI is as follows: Ignore the claimed value in the null hypothesis, for the time being. Construct a 100(1- a )% confidence interval based on the available data. If the constructed CI does not contain the claimed value, then there is enough evidence to reject the null hypothesis; otherwise, there is no reason to reject the null hypothesis. You might like to use the Hypothesis Testing with Confidence JavaScript applet to perform some numerical experimentation for validating the above assertions and for a deeper understanding. Regression Analysis, ANOVA, and Chi-square Test There are close relationships among linear regression, analysis of variance and the Chi-square test. To illustrate the relationship, consider the following application: Relationship between age and income in a given neighborhood: A random survey sampling of size 33 individuals in a neighborhood revealed the following pairs of data. For each pair age is in years and the indicated income is in thousands of dollars: - Relation between Age and Income( $ 1000) - Age Income Age Income Age Income 20 15 42 19 61 13 22 13 47 17 62 14 23 17 53 13 65 9 28 19 55 18 67 7 35 15 41 21 72 7 24 21 53 39 65 22 26 26 57 28 65 24 29 27 58 22 69 27 39 31 58 29 71 22 31 16 46 27 69 9 37 19 44 35 62 21 Constructing a linear regression gives us: Income = 22.88 - 0.05834 (Age) This suggests a negative relationship; as people get older, they have lower income, on average. Although slope is small, it cannot be considered as zero, since the t-statistic for it is -0.70, which is significant. Now suppose you have only the following secondary data , where the original data have been condensed: - Relation between Age and Income( $ 1000) - Age ( 29 - 39 ) Age ( 40 - 59 ) Age ( 60 &amp; Over ) 15 19 13 13 17 14 17 13 9 21 21 7 15 39 21 26 28 24 27 22 27 31 26 22 16 27 9 19 35 22 19 18 7 One may use ANOVA in testing that there is no relationship among age and income. Performing the analysis provides the F-statistic equal to 3.87 which is quite significant; i.e., rejecting the hypothesis of no difference in population average income for the three age groups. Now, suppose more condensed secondary data are provided as in the following table: Relation between Age and Income( $ 1000): Age Income 20-39 40-59 60 and over Up to $ 20,000 7 4 6 $ 20,000 and over 4 7 5 One may use the Chi-square test for the null hypothesis that age and income are unrelated. The Chi-square statistic is 1.70, which is not significant; therefore there is no reason to believe income and age are related! But of course, these data are over-condensed, because when all data in the sample were used, there was an observable relationship. Regression Analysis, ANOVA, T-test, and Coefficient of Determination There are very direct relationships among linear regression, analysis of variance, t-test and the coefficient of determination. The following small data set is for illustrating the connections among the above statistical procedures, and therefore relationships among statistical tables: X1 4 5 4 6 7 7 8 9 9 11 X2 8 6 8 10 10 11 13 14 14 16 Suppose we apply the t-test . The statistic is t = 3.207, with d.f. = 18. The p-value is 0.003 indicating a very strong evidence against the null hypothesis. Now, by introducing a dummy variable x with two values, say 0 and 1, representing the two data sets, respectively, we are able to apply regression analysis : x 0 0 0 0 0 0 0 0 0 0 y 4 5 4 6 7 7 8 9 9 11 x 1 1 1 1 1 1 1 1 1 1 y 8 6 8 10 10 11 13 14 14 16 Among other statistics, we obtain a large slope = m = 4 &#185; 0, indicating the rejection of the null hypothesis. Notice that, the t-statistic for the slope is: t-statistic = slope/(its standard error) = 4/ 1.2472191 = 3.207, which is the t-statistic we obtained from the t-test. In general, the square of t-statistic of the slope is the F-statistic in the ANOVA table; i.e., t m 2 = F-statistic Moreover, the coefficient of determination r 2 = 0.36, which is always obtainable from the t-test, as follows: r 2 = t 2 / (t 2 + d.f.). For our numerical example, the r 2 is (3.207) 2 / [(3.207) 2 + 18] = 0.36, as expected. Now, applying ANOVA on the two sets of data, we obtain the F-statistic = 10.285, with d.f. 1 = 1, and d.f. 2 = 18. The F-statistic is not large enough; therefore, one must reject the null hypothesis. Note that, in general, F a , (1, n) = t 2 a/2 , n . For our numerical example, F = t 2 = (3.207) 2 = 10.285, as expected. As expected, by just looking at the data, all three tests indicate strongly that the means of the two sets are quite different. Relationships among Distributions and Unification of Statistical Tables Particular attention must be paid to a first course in statistics. When I first began studying statistics, it bothered me that there were different tables for different tests. It took me a while to learn that this is not as haphazard as it appeared. Binomial, Normal, Chi-square, t, and F distributions that you will learn are actually closely connected. A problem with elementary statistical textbooks is that they not only don't provide information of this kind, to permit a useful understanding of the principles involved, but they usually don't provide these conceptual links. If you want to understand connections between statistical concepts, then you should practice making these connections. Learning by doing statistics lends itself to active rather than passive learning. Statistics is a highly interrelated set of concepts, and to be successful at it, you must learn to make these links conscious in your mind. Students often ask: Why T- table values with d.f. = 1 are much larger compared with other d.f. values? Some tables are limited. What should I do when the sample size is too large?, How can I become familiar with tables and their differences. Is there any type of integration among tables? Is there any connection between test of hypotheses and confidence interval under different scenarios? For example, testing with respect to one, two, more than two populations, and so on. The following Figure demonstrates useful relationships among distributions and a unification of statistical tables: A Unification of Common Statistical Tables Click on the image to enlarge it and THEN print it For example, the following are some nice connections between major tables: Standard normal z and F-statistics: F = z 2 , where F has (d.f. 1 = 1, and d.f. 2 is the largest available in the F-table) T- statistic and F-statistic: F = t 2 , where F has (d.f. 1 = 1, and d.f. 2 = d.f. of the t-table) Chi-square and F-statistics: F = Chi-square/d.f. 1 , where F has (d.f. 1 = d.f. of the Chi-square-table, and d.f. 2 is the largest available in the F-table) T-statistic and Chi-square: (Chi-square) ½ = t, where Chi-square has d.f.=1, and t has d.f. = &#165; . Standard normal z and T-statistic: z = t, where t has d.f. = &#165; . Standard normal z and Chi-square: (2 Chi-square) ½ - (2d.f.-1) ½ = z, where d.f. is the largest available in the Chi-square table). Standard normal z, Chi-square, and T- statistic: z/[Chi-aquare/n) ½ = t with d.f. = n. F-statistics and its Inverse: F a (n1, n2) = 1/F 1-a (n2, n1), therefore it is only necessary to tabulate, say the upper tail probabilities. Correlation coefficient r and T-statistic: t = [r(n-2) ½ ]/[1 - r 2 ] ½ . Transformation of Some Inferential Statistics to the Standard normal Z: For the t(df): Z = {df &#180; Ln[1 + (t 2 /df)]} ½ &#180; {1 - [1/(2df)]} ½ . For the F(1,df): Z = {df &#180; Ln[1 + (F/df)]} ½ &#180; {1 - [1/(2df)]} ½ , where Ln is the natural logarithm. Visit also the Relationships among Common Distributions . You may like using the statistical tables at the back of your book and/or P-values JavaScript in performing some numerical experimentation for validating the above relationships for a deeper understanding of the concepts. You might need to use a scientific calculator , too. Further Reading: Kagan. A., What students can learn from tables of basic distributions, Int. Journal of Mathematical Education in Science and Technology , 30(6), 1999. Introduction to Visualization of Statistics Most of statistical data processing involves algebraic operations on the dataset. However, if the dataset contains more than 3 numbers, it is not possible to visualize it by geometric representation, mainly due to human sensory limitation. Geometry has a much longer history than algebra. Ancient Greeks applied geometry to measure land , and developed the geo-metric models. The analytic-geometry is to find equivalency between algebra and geometry . The aim is a better understanding by visualization in 2-or-3 dimensional space, and to generalize the ideas for higher dimensions by analytic thinking. Without the loss of generality, and conserving space, the following presentation is in the context of small sample size, allowing us to see statistics in 1, or 2-dimensional space. The Mean and The Median Suppose that four people want to get together to play poker. They live on 1 st Street, 3 rd Street, 7 th Street, and 15 th Street. They want to select a house that involves the minimum amount of driving for all parties concerned. Let's suppose that they decide to minimize the absolute amount of driving. If they met at 1 st Street, the amount of driving would be 0 + 2 + 6 + 14 = 22 blocks. If they met at 3 rd Street, the amount of driving would be 2 + 0+ 4 + 12 = 18 blocks. If they met at 7 th Street, 6 + 4 + 0 + 8 = 18 blocks. Finally, at 15 th Street, 14 + 12 + 8 + 0 = 34 blocks. So the two houses that would minimize the amount of driving would be 3 rd or 7 th Street. Actually, if they wanted a neutral site, any place on 4 th , 5 th , or 6 th Street would also work. Note that any value between 3 and 7 could be defined as the median of 1, 3, 7, and 15. So the median is the value that minimizes the absolute distance to the data points. Now, the person at 15 th is upset at always having to do more driving. So the group agrees to consider a different rule. In deciding to minimize the square of the distance driving, we are using the least square principle. By squaring, we give more weight to a single very long commute than to a bunch of shorter commutes. With this rule, the 7 th Street house (36 + 16 + 0 + 64 = 116 square blocks) is preferred to the 3 rd Street house (4 + 0 + 16 + 144 = 164 square blocks). If you consider any location, and not just the houses themselves, then 9 th Street is the location that minimizes the square of the distances driven. Find the value of x that minimizes: (1 - x) 2 + (3 - x) 2 +(7 - x) 2 + (15 - x) 2 . The value that minimizes the sum of squared values is 6.5, which is also equal to the arithmetic mean of 1, 3, 7, and 15. With calculus, it's easy to show that this holds in general. Consider a small sample of scores with an even number of cases; for example, 1, 2, 4, 7, 10, and 12. The median is 5.5, the midpoint of the interval between the scores of 4 and 7. As we discussed above, it is true that the median is a point around which the sum of absolute deviations is minimized. In this example the sum of absolute deviations is 22. However, it is not a unique point . Any point in the 4 to 7 region will have the same value of 22 for the sum of the absolute deviations. Indeed, medians are tricky. The 50% above -- 50% below is not quite correct. For example, 1, 1, 1, 1, 1, 1, 8 has no median. The convention says that, the median is 1; however, about 14% of the data lie strictly above it; 100% of the data are greater than or equal to the median. We will make use of this idea in regression analysis. In an analogous argument, the regression line is a unique line, which minimizes the sum of the squared deviations from it. There is no unique line that minimizes the sum of the absolute deviations from it. Arithmetic and Geometric Means Arithmetic Mean: Suppose you have two data points x and y, on real number- line axis: The arithmetic mean (a) is a point such that the following vectorial relation holds: ox - oa = oa - oy. Geometric Mean: Suppose you have two positive data points x and y, on the above real number- line axis, then the Geometric Mean (g) of these numbers is a point g such that |ox| / |og| = |og| / |oy|, where |ox| means the length of line segment ox, for example. Variance, Covariance, and Correlation Coefficient Consider a data set containing n = 2 observations (5, 1). Upon centralizing the data, one obtains the vector V1 = (5-3 = 2, 1-3 = -2), as shown in the following n = 2 dimensional coordinate system: Notice that the vector V1 length is: |V1| = [(2) 2 + (-2) 2 ] ½ = 8 ½ The variance of V1 is: Var(V1) = S X i 2 / n = |V1| 2 /n = 4 The standard deviation is: |OS1| = |V1| / n ½ = 8 ½ / 2 ½ = 2. Now, consider a second observation (2, 4). Similarly, it can be represented by vector V2 = (-1, 1). The covariance is, Cov (V1, V2) = the dot product / n = [(2)(-1) + (-2)(1)]/2 = -4/2 = -2 Therefore: n Cov (V1, V2) = the dot product of the two vectors V1, and V2 Notice that the dot-product is multiplication of the two lengths times the cosine of the angle between the two vectors. Therefore, Cov (V1, V2) = |OS1| &#180; |OS2| &#180; Cos (V1, V2) = (2) (1) Cos(180 ° ) = -2 The correlation coefficient is therefore: r = Cos (V1, V2) This is possibly the simplest proof that the correlation coefficient is always bounded by the interval [-1, 1]. The correlation coefficient for our numerical example is Cos (V1, V2) = Cos(180 ° ) = -1, as expected from the above figure. The distance between the two-point data sets V1, and V2 is also a dot-product: |V1 - V2| = (V1-V2) . (V1-V2) = |V1| 2 + |V2| 2 - 2 |V1| &#180; |V2 | = n[Var(V1) + VarV2 - 2Cov(V1, V2)] Now, construct a matrix whose columns are the coordinates of the two vectors V1 and V2, respectively. Multiplying the transpose of this matrix by itself provides a new symmetric matrix containing n times the variance of V1 and variance of V2 as its main diagonal elements (i.e., 8, 2), and n times Cov (V1, V2) as its off diagonal element (i.e., -4). You might like to use a graph paper , and a scientific calculator to check the results of these numerical examples and to perform some additional numerical experimentation for a deeper understanding of the concepts. Further Reading: Wickens T., The Geometry of Multivariate Statistics , Erlbaum Pub., 1995. Index Numbers with Applications When facing a lack of a unit of measure, we often use indicators as surrogates for direct measurement. For example, the height of a column of mercury is a familiar indicator of temperature. No one presumes that the height of mercury column constitutes temperature in quite the same sense that length constitutes the number of centimeters from end to end. However, the height of a column of mercury is a dependable correlate of temperature and thus serves as a useful measure of it. Therefore, and indicator is an accessible and dependable correlate of a dimension of interest; that correlate is used as a measure of that dimension because direct measurement of the dimension is not possible or practical . In like manner index numbers serve as surrogate for actual data. The primary purposes of an index number are to provide a value useful for comparing magnitudes of aggregates of related variables to each other, and to measure the changes in these magnitudes over time. Consequently, many different index numbers have been developed for special use. There are a number of particularly well-known ones, some of which are announced on public media every day. Government agencies often report time series data in the form of index numbers. For example, the consumer price index is an important economic indicator. Therefore, it is useful to understand how index numbers are constructed and how to interpret them. These index numbers are developed usually starting with base 100 that indicates a change in magnitude relative to its value at a specified point in time. For example, in determining the cost of living, the Bureau of Labor Statistics (BLS) first identifies a "market basket" of goods and services the typical consumer buys. Annually, the BLS surveys consumers to determine what they buy and the overall cost of the goods and services they buy: What, where, and how much. The Consumer Price Index (CPI) is used to monitor changes in the cost of living (i.e. the selected market basket) over time. When the CPI rises, the typical family has to spend more dollars to maintain the same standard of living. The goal of the CPI is to measure changes in the cost of living. It reports the movement of prices, not in dollar amounts, but with an index number. The Geometric Mean The Geometric Means are used extensively by the U.S. Bureau of Labor Statistics, "Geomeans" as they call them, in the computation of the U.S. Consumer Price Index. The geomeans are also used in price indexes Ratio Index Numbers The following provides the computational procedures with applications for some Index numbers, including the Ratio Index, and Composite Index numbers. Suppose we are interested in the labor utilization of two manufacturing plants A and B with the unit outputs and man/hours, as shown in the following table, together with the national standard over the last three months: Plant Type - A Plant Type - B Months Unit Output Man Hours Unit Output Man Hours 1 0283 200000 11315 680000 2 0760 300000 12470 720000 3 1195 530000 13395 750000 Standard 4000 600000 16000 800000 The labor utilization for the Plant A in the first month is: L A,1 = [(200000/283)] / [(600000/4000)] = 4.69 Similarly, L B,3 = 53.59/50 = 1.07. Upon computing the labor utilization for both plants for each month, one can present the results by graphing the labor utilization over time for comparative studies. Composite Index Numbers Consider the total labor, and material cost for two consecutive years for an industrial plant, as shown in the following table: Year 2000 Year 2001 Unit Needed Unit Cost Total Unit Cost Total Labor 20 10 200 11 220 Almunium 02 100 200 110 220 Electricity 02 50 100 60 120 Total 500 560 From the information given in the above table, the index for the two consecutive years are 500/500 = 1, and 560/500 = 1.12, respectively. Further Readings: Watson C., P. Billingsley, D. Croft, and D. Huntsberger, Statistics for Management and Economics , Allyn &amp; Bacon, Inc., 1993. Variation Index as a Quality Indicator A commonly used index of variation measure and comparison for nominal and ordinal data is called the index of dispersion: D = k (N 2 - S f i 2 )/[N 2 (k-1)] where k is the number of categories, f i is the number of ratings in each category, and N is the total number of rating. D is a number between zero and 1 depending if all ratings fall into one category, or if ratings were equally divided among the k categories. An Application: Consider the following data with N = 100 participants, k = 5 categories, f 1 = 25, f 2 = 42, and so on. Category Frequency A 25 B 42 C 8 D 13 E 12 Therefore the dispersion index is: D = 5 (100 2 - 2766)/[100 2 (4)] = 0.904, indicating a good spread of scores across the categories. Labor Force Unemployment Index Is a given city an economically depressed area? The degree of unemployment among labor (L) force is considered to be a proper indicator of economic depression. To construct the unemployment index, each person is classified both with respect to membership in the labor force and the degree of unemployment in fractional value, ranging from 0 to 1. The fraction that indicates the portion of labor that is idle is: L = S [U i P i ] / S P i , the sums are over all i = 1, 2,, n. where P i is the proportion of a full workweek for each resident of the area held or sought employment and n is the total number of residents in the area. U i is the proportion of P i for which each resident of the area unemployed. For example, a person seeking two days of work per week (5 days) and employed for only one-half day would be identified with P i = 2/5 = 0.4, and U i = 1.5/2 = 0.75. The resulting multiplication U i P i = 0.3 would be the portion of a full workweek for which the person was unemployed. Now the question is What value of L constitutes an economic depressed area. The answer belongs to the decision-maker to decide. Seasonal Index and Deseasonalizing Data Seasonal index represents the extent of seasonal influence for a particular segment of the year. The calculation involves a comparison of the expected values of that period to the grand mean. We need to get an estimate of the seasonal index for each month, or other periods such as quarter, week, etc, depending on the data availability. Seasonality is a pattern that repeats for each period. For example annual seasonal pattern has a cycle that is 12 periods long, if the periods are months, or 4 periods long if the periods are quartets. A seasonal index is how much the average for that particular period tends to be above (or below) the grand average. Therefore, to get an accurate estimate for it, we compute the average of the first period of the cycle, and the second period, etc, and divide each by the overall average. The formula for computing seasonal factors is: S i = D i /D, where: S i = the seasonal index for i th period, D i = the average values of i th period, D = grand avrage, i = the i th seasonal period of the cycle. A seasonal index of 1.00 for a particular month indicates that the expected value of that month is 1/12 of the overall average. A seasonal index of 1.25 indicates that the expected value for that month is 25% greater than 1/12 of the overall average. A seasonal index of 80 indicates that the expected value for that month is 20% less than 1/12 of the overall average. Deseasonalizing Process: Deseasonalizing the data, also called Seasonal Adjustment is the process of removing recurrent and periodic variations over a short time frame (e.g., weeks, quarters, months). Therefore, season variations are regularly repeating movements in series values that can be tied to recurring events. The Deseasonalized data is obtained by simply dividing each time series observation by the corresponding seasonal index. Almost all time series published by the government are already deseasonalized using the seasonal index to unmasking the underlying trends in the data, which could have been caused by the seasonality factor. A Numerical Application: The following table provides monthly sales ( $ 000) at a college bookstore. M T Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec Total 1 196 188 192 164 140 120 112 140 160 168 192 200 1972 2 200 188 192 164 140 122 132 144 176 168 196 194 2016 3 196 212 202 180 150 140 156 144 164 186 200 230 2160 4 242 240 196 220 200 192 176 184 204 228 250 260 2592 Mean: 208.6 207.0 192.6 182.0 157.6 143.6 144.0 153.0 177.6 187.6 209.6 221.0 2185 Index: 1.14 1.14 1.06 1.00 0.87 0.79 0.79 0.84 0.97 1.03 1.15 1.22 12 The sales show a seasonal pattern, with the greatest number when the college is in session and decrease during the summer months. For example, for January the index is: S(Jan) = D(Jan)/D =208.6/181.84 = 1.14, where D(Jan) is the mean of all four January month, and D is the grand mean of all past four years sales. You might like to use the Seasonal Index JavaScript to check your hand computation. As always you must first use Plot of the Time Series as a tool for the initial characterization process. For testing seasonality based on seasonal index, you may like to use Test for Seasonality JavaScript. For modeling the time series having both the seasonality and trend components, visit the Business Forecasting site. Statistical Technique and Index Numbers One must be careful in applying or generalizing any statistical technique to the index numbers . For example, the correlation of rates raises the potential problem. Specifically, let X, Y, and X be three independent variables, so that pair-wise correlations are zero; however, the ratios X/Y, and Z/Y will be correlated due to the common denominator. Let I = X 1 /X 2 where X 1 , and X 2 are dependent variables with correlation r , having mean and coefficient of variation m 1 , c 1 and m 2 , c 2 , respectively; then, Mean of I = m 1 (1-r &#180; c 1 &#180; c 2 + c 2 2 )/m 2 , Standard Deviation of I = m 1 (c 1 2 - 2 r &#180; c 1 &#180; c 2 + c 2 2 ) ½ /m 2 A Classification of the JavaScript by Application Areas This section is a part of the JavaScript E-labs learning technologies for decision making. The following is a classification of statistical JavaScript by their application areas: MENU 1. Summarizing Data Bivariate Sampling Statistics Detective Testing for Trend & Autocrrelation Descriptive Statistics Determination of the Outliers Empirical Distribution Function Histogram Residuals Random Fluctuations Testing Seasonal Index The Three Means 2. Computational probability Comparing Two Random Variables Markov Chains Calculator Multinomial Distributions P-values for the Popular Distributions 3. Requirements for most tests & estimations Removal of the Outliers Sample Size Determination Subjectivity in Hypothesis Testing Test for Homogeneity of Population Test for Normality Test for Randomness 4. One population & one variable Binomial Exact Confidence Intervals Compatibility of Multi-Counts Goodness-of-Fit for Discrete Variables Revising the Mean and the Variance Testing the Exponential Distribution Testing the Mean Testing the Medians Testing the Percentage Testing the Poisson Process Testing the Variance Test for Uniform Distribution 5. One population & two or more variables The Before-and-After Test for Means and Variances The Before-and-After Test for Proportions Chi-square Test for Crosstable Relationship Multiple Regressions Polynomial Regressions Quadratic Regression Simple Regression with Diagnostic Tools Testing the Population Correlation Coefficient 6. Two or three populations & one variable ANOVA for Dependent Populations ANOVA: Testing Equality of the Means K-S Test for Equality of Two Populations Two Populations Testing Means & Variances 7. Several populations & one or more variables Analysis of Covariance ANOVA for Condensed Data Sets Compatibility of Multi-Counts Equality of Multi-variances: The Bartlett's Test Identical Populations Test for Crosstable Data Subjective Assessment of Estimates Testing the Proportions Testing Several Correlation Coefficients Two-Way ANOVA Test Two-Way ANOVA with Replications A selection of: | Academic Info | Association of American Colleges and Universities | BUBL Catalogue | Business (Indexes) | Business and Economics (Biz/ed) | Business &amp; Finance | Business &amp; Industrial | Business Nation | Chance | Education World | Educypedia | Economics LTSN | | Epidemiology and Biostatistics | Emerging Technologies | Estadística | Federation of Paralegal Associations | Financial and Economic Links | IFORS | Institute of Statistical Sciences | International Business | Marine Institute | Management | Management Sources | Mathematics and Statistics | MathForum | Maths, Stats &amp; OR Network | McGraw-Hill | Merlot | NEEDS | NetFirst | NRICH | Open Encyclopedia | Physician's Quiklinks | Ressources en Statistique | Science Gateway | Search Engines Directory: | AltaVista | AOL | Excite | HotBot | Looksmart | Lycos | MSN | Netscape | OpenDirectory | Scientopica | Webcrawler | Yahoo | | Scout Report | Small Business | Social Science | Statistical Data | Statistical Societies | Statistics on the Web | SurfStat | Wall Street | Virtual Learning | Virtual Library | WebEc | World Lecture Hall | Additional useful sites may be found by clicking on the following search engine: AllTheWeb The Copyright Statement: The fair use, according to the 1996 Fair Use Guidelines for Educational Multimedia , of materials presented on this Web site is permitted for non-commercial and classroom purposes only. This site may be mirrored intact (including these notices), on any server with public access. Kindly e-mail me your comments, suggestions, and concerns. Thank you. Professor Hossein Arsham This site was launched on 1/18/1994, and its intellectual materials have been thoroughly revised on a yearly basis. The current version is the 9 th Edition. All external links are checked once a month. Back to Dr. Arsham's Home Page EOF: © 1994-2004 
	</PLAINTEXT>
	<CONTENT>
-->
<HTML><HEAD><TITLE>Dr. Arsham's Statistics Site</TITLE>
<META content="text/html; charset=windows-1252" http-equiv=Content-Type>
<META content=1/18/1994 name=DATE>
<META NAME="Dr. Hossein Arsham" CONTENT="Lecture Notes">
<META content="MSHTML 5.00.2614.3500" name=GENERATOR></HEAD>
<BODY aLink=#551a8b bgColor=#ffffff link=#0063a4 text=#000000 vLink=#990000>
<BLOCKQUOTE>
  <META 
  content="learn statistics,  what is statistics,  statistiques commerciales, geschäft statistiken, statistiche d' impresa, statistics de negócio, estadística de negocio,  accuracy in statistics, precision in statistics,   statistics with confidence,  prediction interval, history of statistics, coefficient of variation, what is central limit theorem?,  what is a sampling distribution?,  what is a linear least squares model?,  Kolmogorov-Smirnov test" 
  name=keywords>
  <META 
  content="A Web site designed to increase the extent to which statistical thinking is embedded in management thinking for decision making under uncertainties. The main thrust of the site is to explain various topics in statistical analysis such as the linear model, hypothesis testing, and central limit theorem" 
  name=description>
<CENTER><FONT color=#dc143c size=5><B>Statistical Thinking for<br> Managerial Decision Making</B></FONT></CENTER>
  <p>
<br>
<p>
  <CENTER>
  <P><A href="http://www.businessandlaw.vu.edu.au/beo5682busstat" 
  target=new><B><FONT color=#dc143c>Asia-Pacific Mirror Site</FONT></B></A> 
  <BR><A href="http://www.mirror.ac.uk/sites/ubmail.ubalt.edu/~harsham/Business-stat/opre504.htm" 
  target=new><FONT color=#dc143c><B>Europe Mirror Site</B></FONT></A> 
  <BR><A 
  href="http://163.121.24.109/decision_making_tools/opre504.htm" 
  target=new><B><FONT color=#dc143c>Middle East Mirror Site</FONT></B></A> 
  <BR><A 
  href="http://www.universidadabierta.edu.mx/SerEst/MAP/METODOS%20CUANTITATIVOS/Business%20Statistics.htm" 
  target=new><B><FONT color=#dc143c>South America Mirror Site</FONT></B></A>
<BR><A 
  href="http://www.mirror.ac.uk/sites/ubmail.ubalt.edu/~harsham/Business-stat/opre504.htm" 
  target=new><FONT color=#dc143c><B>UK Mirror Site</B></FONT></A> <BR><A href="http://ubmail.ubalt.edu/~harsham/#rrinstr" 
  target=new><B><FONT color=#dc143c>USA Site</B></FONT></A> 
  <P>
<p>
</CENTER>
  <P>
  <P>
  <DD>This Web site is a course in statistics appreciation; i.e., acquiring a   feeling for the statistical way of thinking. It is an introductory course in   statistics that is designed to provide you with the basic concepts and methods of statistical analysis for decision making under uncertainties. Materials in  this Web site are tailored to meet your needs in making good decisions by   fostering statistical thinking. The cardinal objective for this Web site is to   increase the extent to which statistical thinking is merged with managerial   thinking for decision making under uncertainty.
<DIV align=right><CITE><A href="http://ubmail.ubalt.edu/~harsham/index.html" 
  target=new>Professor Hossein Arsham</A>&nbsp; &nbsp;</CITE></DIV>
<P>
<HR>
<FONT color=#dc143c size=4><B>&nbsp; &nbsp;MENU</B></FONT> 
<p>
<LI><A href="#rrstatthink">Chapter 1: &nbsp;&nbsp;Towards Statistical Thinking for Decision Making</A> 
<LI><A href="#rrTopiinbuSt">Chapter 2: &nbsp;&nbsp;Descriptive Sampling Data Analysis</A>
<LI><A href="#rrprobinInf">Chapter 3: &nbsp;&nbsp;Probability for Statistical Inference and Modeling</A> 
<LI><A href="#rInferCond">Chapter 4: &nbsp;&nbsp;Necessary Conditions for Statistical Decision Making</A>
<LI><A href="#rEstQual">Chapter 5: &nbsp;&nbsp;Estimators and Their Qualities</A>
<LI><A href="#rRejeClaim">Chapter 6: &nbsp;&nbsp;Hypothesis Testing: Rejecting a Claim</A>
<LI><A href="#rtestaverperce">Chapter 7: &nbsp;&nbsp;Hypotheses Testing for Means and Proportions</A>
<LI><A href="#rtestAvovaKS">Chapter 8: &nbsp;&nbsp;Tests for Statistical Equality of Two or More Populations</A>
<LI><A href="#rtestothercl">Chapter 9: &nbsp;&nbsp;Applications of the Chi-square Statistic</A>
<LI><A href="#rregmodel">Chapter 10: &nbsp;Regression Modeling and Analysis</A>
<LI><A href="#runifTech">Chapter 11: &nbsp;Unified Views of Statistical Decision Technologies</A>
<li><A   href="#riAnalVisualStat">Chapter 12: &nbsp;Visualization of Statistics</A>
<LI><A href="#rapplIndexnu">Chapter 13: &nbsp;Index Numbers with Applications</A>
<br><p>
<FONT color=#dc143c size=4><B>&nbsp; &nbsp;Companion Sites:</B></FONT> 
<p>
<p>

<LI><A href="http://ubmail.ubalt.edu/~harsham/zero/scientificCal.htm"   target="new">JavaScript E-labs Learning Objects</A> ,  &nbsp;<a href="http://www.mirror.ac.uk/sites/ubmail.ubalt.edu/~harsham/Business-stat/otherapplets/scientificCal.htm" target="new"><font color="#DC143C">Europe Mirror Site Collection</font></a>.
<li><a href="http://ubmail.ubalt.edu/~harsham/excel/excel.htm"  target="new">Excel For Introductory Statistical Analysis</a>,  &nbsp;<a href="http://aftnn.org/blanket/http:/ubmail.ubalt.edu/~harsham/excel/index.htm" target="new"><font  color="#DC143C">Europe Mirror Site</a></font>.

<li><a HREF="WHY.DOC" target="new" >Frequently Asked Questions: A Statistical Why? List (Word.Doc)</a>
<LI><A href="http://ubmail.ubalt.edu/~harsham/stat-data/opre330.htm"  target=new>Statistical Data Analysis</A>,&nbsp;<A href="http://www.da-tang.net/yzp-back/Inferring%20from%20Data.htm" target=new><FONT color=#dc143c>Asia-Pacific Mirror Site</FONT></A>,&nbsp;<A href="http://149.170.199.144/rd/arsham/opre330.htm"  target=new><FONT color=#dc143c>Europe Mirror Site</FONT></A>. 
<LI><A href="http://ubmail.ubalt.edu/~harsham/stat-data/opre330Forecast.htm"  target=new>Time Series Analysis and Business Forecasting</A>,&nbsp;<A   href="http://aftnn.org/blanket/http:/ubmail.ubalt.edu/~harsham/stat-data/index.htm" target=new><FONT color=#dc143c size=3>Europe Mirror Site</FONT></A>. 
<LI><A href="http://ubmail.ubalt.edu/~harsham/stat-data/opre330SPSSSAS.htm"  target=new>Computers and Computational Statistics</A>,&nbsp;<A  href="http://aftnn.org/blanket/http:/ubmail.ubalt.edu/~harsham/webstat/index.htm"  target=new><FONT color=#dc143c size=3>Europe Mirror Site</FONT></A>. 
<LI><A  href="http://ubmail.ubalt.edu/~harsham/stat-data/opre330Surveys.htm"  target=new>Questionnaire Design and Surveys Sampling</A>,&nbsp;<A     href="http://aftnn.org/blanket/http:/ubmail.ubalt.edu/~harsham/edit1/index.htm" target=new><FONT color=#dc143c size=3>Europe Mirror Site</FONT></A>. 
<LI><A href="http://ubmail.ubalt.edu/~harsham/opre640a/partIX.htm"  target=new>Probabilistic Modeling</A>, &nbsp;<a href="http://aftnn.org/blanket/http:/ubmail.ubalt.edu/~harsham/opre640a/index.htm" target="new"><font  color="#DC143C">Europe Mirror Site</a></font>, &nbsp;<A  href="http://ubmail.ubalt.edu/~harsham/opre640S/SpanishP.htm" target=new><FONT color=#dc143c>Versión en Español</FONT></A>. 
<LI><A href="http://ubmail.ubalt.edu/~harsham/simulation/sim.htm" 
    target=new>Systems Simulation</A>,   &nbsp; <a href="http://aftnn.org/blanket/http:/ubmail.ubalt.edu/~harsham/simulation/index.htm" target="new"><font  color="#DC143C">Europe Mirror Site</a></font>, &nbsp;<A     href="http://www.cs.sun.ac.za/~lynette/simulation/harsham/sim.html" target=new><FONT color=#dc143c>South Africa Mirror Site</FONT></A>. 
<LI><A href="http://ubmail.ubalt.edu/~harsham/statistics/REFSTAT.HTM" 
    target=new>Probability and Statistics Resources</A>,  &nbsp; <a href="http://aftnn.org/blanket/http:/ubmail.ubalt.edu/~harsham/statistics/INDEX.HTM" target="new"><font   color="#DC143C">Europe Mirror Site</a></font>.
  <P><FONT face="Bookman Old Style" size=-1><FONT color=#dc143c><B>To search the site</B></FONT>, try <U>E</U>dit | <U>F</U>ind in page [Ctrl + f]. Enter a word or phrase in the dialogue box, e.g. "<SAMP>parameter" </SAMP>or "<SAMP>probability"</SAMP>. If the first appearance of the word/phrase is not   what you are looking for, try <U><SAMP>F</SAMP></U><SAMP>ind Next</SAMP>.</FONT> 
<P>
<HR>
</BLOCKQUOTE>
<P><A name=rrstatthink></A><FONT color=#dc143c size=4>
<ol>
<li><H4>Towards Statistical Thinking for Decision Making</H4></FONT>
  <ol>
<LI><A href="#rintroduction" target=new>Introduction</A> 
<LI><A href="#rbosim" target=new>The Birth of Probability and Statistics</A> 
<LI><A href="#rstatdecisionmake"  target=new>Statistical Modeling for Decision-Making under  Uncertainties</A> 
<LI><A  href="#rrstatdecproce"  target=new>Statistical Decision-Making Process</A> 
<LI><A href="#rwhatbussts"   target=new>What is Business Statistics?</A> 
<LI><A href="#rcommTemin">Common Statistical Terminology with Applications</A> 
</ol>
<CENTER>
<P><IMG height=2 src="sep1.gif" width=250></CENTER><P>
  <A name=rrTopiinbuSt></A><FONT color=#dc143c 
  size=4>
<li><H4>Descriptive Sampling Data Analysis</H4></FONT>
<ol>

<li><A  href="#rgl" target=new>Greek Letters Commonly Used in Statistics</A>
<li><A href="#rlom"  target=new>Type of Data and Levels of Measurement</A>
<li><A href="#rwhyrssm" target=new>Why Statistical Sampling?</A> 
<li><A href="#rssm"  target=new>Sampling Methods</A> 
<LI><A  href="#rmeanmodemed"  target=new>Representative of a Sample: Measures of Central Tendency</A> 
<LI><A href="#rselecting" target=new>Selecting Among the Mean, Median, and Mode</A> 
<LI><A href="#rspecialmean" target=new>Specialized Averages: The Geometric & Harmonic Means</A> 
<LI><A href="#rncih" target=new>Histogramming: Checking for Homogeneity of Population</A> 
<LI><A href="#rbplot"  target=new>How to Construct a BoxPlot</A> 
<LI><A  href="#rvarstanran"  target=new>Measuring the Quality of a Sample</A> 
<LI><A href="#rselectingdisp" target=new>Selecting Among the Measures of Dispersion</A> 
<LI><A  href="#rskewKur"  target=new>Shape of a Distribution Function: The Skewness-Kurtosis Chart</A> 
<LI><A  href="#rexamdisc" target=new>A Numerical Example &amp; Discussions</A>
<LI><A  href="#rdensityDist" target=new>The Two Statistical Representations of a Population</a>
<LI><A  href="#rcdffunc" target=new>Empirical (i.e., observed) Cumulative Distribution Function</a>
</ol>
<CENTER><P><IMG height=2 src="sep1.gif" width=250></CENTER><P>
<A name=rrprobinInf target=new></A><FONT color=#dc143c 
  size=4>

<li><H4>Probability for Statistical Inference and Modeling</H4></FONT>
<ol>
<LI><A  href="#rintropclao"  target=new>Introduction</A> 
<LI><A href="#rpclao"  target=new>Probability, Chance, Likelihood, and Odds</A> 
<LI><A href="#rhowprob"  target=new>How to Assign Probabilities</A> 
<LI><A href="#rlawofProb" target=new>General Laws of Probability</A> 
<LI><A href="#rmuexindev" target=new>Mutually Exclusive versus Independent Events</A> 
<LI><A href="#rwissoimpabNor"  target=new>What Is so Important About the Normal Distributions?</A> 
<LI><A href="#rwsd"  target=new>What Is a Sampling Distribution?</A> 
<LI><A href="#rwclt"  target=new>What Is The Central Limit Theorem?</A> 
<LI><A href="#rWhatIsDofF"  target=new>What Is "Degrees of Freedom"?</A> 
<LI><A  href="#rappTable" target=new>Applications of and Conditions for Using Statistical Tables</A>
<UL type=Disc>
<LI><A href="#rBeta"  target=new>Beta Density Function</A> 
<LI><A  href="#rBinomial"  target=new>Binomial Probability Function</A> 
<LI><A href="#rtChiSquare"  target=new>Chi-square Density Function</A> 
<LI><A href="#rExponential" target=new>Exponential Density Function</A> 
<LI><A href="#rFisherF" target=new>F-Density Function</A> 
<LI><A href="#rGamma" target=new>Gamma Density Function</A> 
<LI><A href="#rLog-normal" target=new>Log-normal Density Function</A> 
<LI><A href="#rmultinomial"  target=new>Multinomial Probability Function</A> 
<LI><A href="#rnd"  target=new>Normal Density Function</A> 
<LI><A  href="#rPoisson"  target=new>Poisson Probability Function</A> 
<LI><A href="#rtdistributions"  target=new>Student T-Density Function</A> 
<LI><A href="#rTriangular"  target=new>Triangular Density Function</A> 
<LI><A href="#rUniform" target=new>Uniform Density Function</A></UL>
</ol>
<CENTER><P><IMG height=2 src="sep1.gif" width=250></CENTER><P>
<A name=rInferCond> </A>
<FONT color=#dc143c 
  size=4>
<li><H4>Necessary Conditions for Statistical Decision Making</H4></FONT>
<ol>
<li><a href="#rConditionTest" target=new>Introduction</A>
<LI><a href="#routlier" target="new">Measure of Surprise for Outlier Detection</a>
<LI><A href="#rhomoPop"  target=new>Homogeneous Population (Don't mix apples and oranges)</A> 
<LI><A href="#rrunstest"  target=new>Test for Randomness</A> 
<LI><A  href="#rTestNormal"  target=new>Test for Normality</A> 
</ol>
<CENTER><P><IMG height=2 src="sep1.gif" width=250></CENTER><P>
<A name=rEstQual></A><FONT color=#dc143c 
  size=4>
</a>
<li><H4>Estimators and Their Qualities</H4></FONT>
<ol>
<li><A href="#rqualestiunbsuff"  target=new>Introduction</A>
<li><A href="#rgoodquali" target=new>Qualities of a Good Estimator</a>
<LI><A  href="#rwci"  target=new>Statistics with Confidence</A> 
<LI><A href="#rmarginerror" target=new>What Is the Margin of Error?</A> 
<LI><A  href="#rrst"  target=new>Bias Reduction Techniques: Bootstrapping and Jackknifing</A> 
<LI><A href="#rpredictsamplemean" target=new>Prediction Intervals</A> 
<LI><A href="#rwstanderrors"  target=new>What Is a Standard Error?</A> 
<LI><A href="#rssss"  target=new>Sample Size Determination</A>
<LI><A href="#rrevisemv" target=new>Revising the Expected Value and the Variance</a>
<LI><A href="#rsubsevest" target=new>Subjective Assessment of Several Estimates</a>
</ol>
<CENTER><P><IMG height=2 src="sep1.gif" width=250></CENTER><P>
<A name=rRejeClaim></A><FONT color=#dc143c 
  size=4>
</a>
<li><H4>Hypothesis Testing: Rejecting a Claim</H4></FONT>
<ol>
<li><A href="#rht" target=new>Introduction</A>
<li><A href="#rrstatdecision" target=new>Managing the Producer's or the Consumer's Risk</a>
<LI><A  href="#rclassicalTest"  target=new>Classical Approach to Testing  Hypotheses</A> 
<LI><A  href="#rmip" target=new>The Meaning and Interpretation of P-values (what the data say)</A> 
<LI><A href="#rcombinpandalpfa"  target=new>Blending the Classical and the P-value Based Approaches in Test of Hypotheses</A> 
<LI><A href="#rbm" target=new>Bonferroni Method for Multiple P-Values Procedure</A> 
<LI><A  href="#rpowt"  target=new>Power of a Test and the Size Effect</A> 
<LI><A  href="#rwunp" target=new>Parametric vs. Non-Parametric vs. Distribution-free Tests</A> 
</ol>
<CENTER><P><IMG height=2 src="sep1.gif" width=250></CENTER><P>
<A name=rtestaverperce> </A><FONT color=#dc143c 
  size=4>
</a>
<li><H4>Hypotheses Testing for Means and Proportions</H4></FONT>
<ol>
<li><A href="#rTwoIndIntroTest"  target=new>Introduction</A>
<LI><A  href="#rsinglepoputest"  target=new>Single Population t-Test</A> 
<LI><A  href="#rTwoIndTest"  target=new>Two Independent Populations</A> 
<LI><A  href="#rwwspve"  target=new>When Should We Pool Variance Estimates?</A>
<LI><A  href="#rMorethanwononpar"  target=new>Non-parametric Multiple Comparison Procedures</A> 
<LI><A href="#rTwoDepdTest"  target=new>The Before-and-After Test</A>
<LI><A  href="#rANOVACond"   target=new>ANOVA for Normal but Condensed Data Sets</A> 
<LI><A  href="#rMorethantwodep"  target=new>ANOVA for Dependent Populations</a> 
</ol>
<CENTER><P><IMG height=2 src="sep1.gif" width=250></CENTER><P>
<A name=rtestAvovaKS> </A><FONT color=#dc143c 
  size=4>
</a>
<li><H4>Tests for Statistical Equality of Two or More Populations</H4></FONT>
<ol>
<li><A href="#requalPopulasTest"  target=new>Introduction</A>
<LI><A  href="#requalPopus"   target=new>Equality of Two Normal Populations</A> 
<LI><A  href="#rshiftPopulasTest"   target=new>Testing a Shift in Normal Populations</A> 
<LI><A  href="#rANOVA"   target=new>Analysis of Variance (ANOVA)</A> 
<LI><A href="#rHomPropor">Equality of Proportions in Several Populations</A> 
<LI><A  href="#rkstest" target=new>Distribution-free Equality of Two Populations</A> 
<LI><A  href="#rustatistic" target=new>Comparison of Two Random Variables</A>
</ol>
<CENTER><P><IMG height=2 src="sep1.gif" width=250></CENTER><P>
<A name=rtestothercl></A><FONT color=#dc143c 
  size=4>
</a>
<li><H4>Applications of the Chi-square Statistic</H4></FONT>
<ol>
<li><A href="#rcst" target=new>Introduction</A>
<LI><A  href="#rCrossTab"  target=new>Test for Crosstable Relationship</A> 
<LI><A href="#rHomCrossTab"  target=new>Identical Populations Test for Crosstable Data</A> 
<LI><A  href="#rHomPropor" target=new>Test for Equality of Several Population Proportions</A> 
<LI><A  href="#rEualMedian" target=new>Test for Equality of Several Population Medians</A> 
<LI><A  href="#rgoodnessofforDrv"  target=new>Goodness-of-Fit Test for Probability Mass Functions</A> 
<LI><A  href="#rCompMultiCo"  target=new>Compatibility of Multi-Counts</A> 
<LI><A  href="#rchiconditions" target=new>Necessary Conditions in Applying the Above Tests</A> 
<LI><A href="#rchivariance"  target=new>Testing the Variance: Is the Quality that Good?</A> 
<LI><A  href="#rMultivariances"  target=new>Testing the Equality of Multi-Variances</A>
<LI><A  href="#rmulticorr" target=new>Correlation Coefficients Testing</a>
</ol>
<CENTER><P><IMG height=2 src="sep1.gif" width=250></CENTER><P>
<A name=rregmodel></A><FONT color=#dc143c 
  size=4>
</a>
<li><H4>Regression Modeling and Analysis</H4></FONT>
<ol>
<li><A href="#rwlls"  target=new>Introduction</A>
<LI><A  href="#rranaproces"  target=new>Regression Modeling Selection Process</A> 
<LI><A  href="#rcorrIationCovar"  target=new>Covariance and Correlation</A> 
<LI><A  href="#rppc"  target=new>Pearson, Spearman, and Point-biserial Correlations</A> 
<LI><A  href="#rcals"  target=new>Correlation, and Level of Significance</A> 
<LI><A  href="#rcorrInd"  target=new>Independence vs. Correlated</A> 
<LI><A   href="#rhccc"  target=new>How to Compare Two Correlation Coefficients</A> 
<LI><A  href="#rregplandevmain"  target=new>Planning, Development, and Maintenance of a Model</A>
<LI><A  href="#rregconditions"  target=new>Conditions and the Check-list for Linear Models</A> 
<li><a href="#rconariance" target="new">Analysis of Covariance: Comparing the Slopes</a>
<li><a href="#rApplyRegress" target="new">Residential Properties Appraisal Application</a>
</ol>
<CENTER><P><IMG height=2 src="sep1.gif" width=250></CENTER><P>
<A name=runifTech></A><FONT color=#dc143c 
  size=4>
</a>
<li><H4>Unified Views of Statistical Decision Technologies</H4></FONT>
<ol>
<li><A href="#runfyviewthests" target=new>Introduction</a>
<li><A href="#rhyptestconf" target=new>Hypothesis Testing with Confidence</a>
<li><A href="#rreganovachi" target=new>Regression Analysis, ANOVA, and Chi-square Test</a>
<li><A href="#rreganovattes" target=new>Regression Analysis, ANOVA, T-test, and Coefficient of Determination</a>
<li><A href="#rradast"  target=new>Relationships among Distributions and Unification of Statistical 
Tables</A> 
</ol>
<CENTER><P><IMG height=2 src="sep1.gif" width=250></CENTER><P>
<A name=riAnalVisualStat></A><FONT color=#dc143c 
  size=4>
</a>
<li><H4>Visualization of Statistics: Analytic-Geometry & Statistics</H4></FONT>
<ol>
<li><A href="#rAnalVisualStat" target=new>Introduction</a>
<li><A href="#rneanmedian" target=new>Mean and Median</a>
<li><A href="#rartandgeo" target=new>Geometric Mean</a>
<li><A href="#rvarcovcoef" target=new>Variance, Covariance, and Correlation Coefficient </a>
</ol>
<CENTER><P><IMG height=2 src="sep1.gif" width=250></CENTER><P>
<A name=rapplIndexnu></A><FONT color=#dc143c 
  size=4>
</a>
<li><H4>Index Numbers with Applications</H4></FONT>
<ol>
<li><A  href="#rindexnum" target=new>Introduction</a>
<li><A  href="#rgeomean"  target=new>The Geometric Mean</a>
<li><A  href="#rratioindex" target=new>Ratio Indexes</a>
<li><A  href="#rcompindex" target=new>Composite Index Numbers </a>
<li><A  href="#rvarinqual" target=new>Variation Index as a Quality Indicator</a>
<li><A  href="#rlaborfoun" target=new>Labor Force Unemployment Index</a>
<li><A  href="#rseasonindex" target=new>Seasonal Index and Deseasonalizing Data</a>
<li><A  href="#rstatindex" target=new>Statistical Technique and Index Numbers</a>
</ol>
<P>
</ol>

<p>
<BLOCKQUOTE>
<A name=rintroduction></A>
<HR>
<H4><FONT color=#dc143c>Introduction to Statistical Thinking for Decision Making</FONT></H4>
<P>This site builds up the basic ideas of business statistics systematically and correctly. It is a combination of lectures and computer-based practice, joining theory firmly with practice. It introduces techniques for summarizing and presenting data, estimation, confidence intervals and hypothesis testing. The presentation focuses more on understanding of key concepts and statistical thinking, and less on formulas and calculations, which can now be done on small computers through user-friendly  <A href="otherapplets/Descriptive.htm" 
  target=new>Statistical JavaScript Applets</A>, etc.   <P>Today's good decisions are driven by data. In all aspects of our lives, and importantly in the business context, an amazing diversity of data is available for inspection and analytical insight.  Business managers and professionals are increasingly <FONT color=#dc143c>required to justify decisions on the basis of data</font>.  They need statistical model-based decision support systems.   
<p>
Statistical skills enable them to intelligently collect, analyze and interpret data relevant to their decision-making. Statistical concepts and statistical thinking enable them to: 
  <UL>
    <LI>solve problems in a diversity of contexts.<LI>add substance to decisions.  </LI> <LI>reduce guesswork.  </LI></UL>
  <P></P>  <DD>
This Web site is a course in statistics appreciation; i.e., acquiring a feel for the statistical way of thinking.  <FONT color=#dc143c>It hopes to make sound statistical thinking understandable in business terms.</font>  An introductory course in statistics, it is designed to provide you with the basic concepts and methods of statistical analysis for processes and products. Materials in this Web site are tailored to help you make better decisions and to get you thinking statistically. A cardinal objective for this Web site is <FONT color=#dc143c>to embed statistical thinking into managers</font>, who must often decide with little information. 
  <P>In competitive environment,  business managers must design quality into products, and into 
  the processes of making the products. They must facilitate a process of  never-ending improvement at all stages of manufacturing and service. This is a  strategy that employs statistical methods, particularly <A 
  href="#rdesinexp">statistically designed experiments</A>, and produces processes that provide high yield and products that seldom fail. Moreover, it facilitates development of robust products that are insensitive to changes in the environment and internal  component variation. Carefully planned statistical studies remove hindrances to high quality and productivity at every stage of production. This saves time and money. It is well recognized that quality must be engineered into products as early as possible in the design process. One must know how to use carefully 
  planned, cost-effective <A href="#rstaexper">statistical experiments</A> to improve, optimize and make robust products and processes. <P><FONT color=#dc143c><B>Business Statistics is a science assisting you to make business decisions under uncertainties </B></FONT>based on some numerical and measurable scales. Decision making processes must be based on data, not on personal opinion nor on belief. <P><FONT color=#dc143c><B>The Devil is in the Deviations:</B></FONT> Variation is inevitable  in life! Every process, every measurement, every sample has variation. Managers need to understand variation for two key reasons. First,   so that they can lead others to apply statistical thinking in day-to-day  activities and secondly, to apply the concept for the purpose of continuous  improvement. This course will provide you with hands-on experience to promote  the use of statistical thinking and techniques to apply them to make educated  decisions, whenever you encounter variation in business data. You will learn techniques to intelligently assess and manage the risks inherent in decision-making. Therefore, remember that:   <P><FONT color=#dc143c><B>Just like weather, if you cannot control something,  you should learn how to measure and analyze it, in order to predict it, 
  effectively</B></FONT>.   <P>If you have taken statistics before, and have a feeling of inability to 
  grasp concepts, it may be largely due to your former non-statistician instructors  teaching statistics. Their deficiencies lead students to develop phobias for  <FONT color=#dc143c><B>the sweet science of statistics</B></FONT>. In this respect, Professor Herman Chernoff (1996) made the following remark: 
  <P>  <MENU><B>"Since everybody in the world thinks he can teach statistics even  though he does not know any, I shall put myself in the position of teaching  biology even though I do not know any"</B></MENU>
  <P>Inadequate statistical teaching during university education leads even after graduation, to one or a combination of the following scenarios:  <P></P></DD></BLOCKQUOTE>
<OL>
  <LI>In general, people do not like statistics and therefore they try to avoid it. 
  <LI>There is a pressure to produce scientific papers, however often confronted  with "I need something quick."   <LI>At many institutes in the world, there are only a few (mostly 1)  statisticians, if any at all. This means that these people are extremely busy.  As a result, they tend to advise simple and easy to apply techniques, or they will have to do it themselves. 
  <LI>Communication between a statistician and decision-maker can be difficult. One speaks in statistical jargon; the other understands the monetary or <A href="otherapplets/Utility.htm" 
  target=new>utilitarian</a> benefit of using the statistician's recommendations. </LI></OL>
<BLOCKQUOTE>
  <P>Plugging numbers into the formulas and crunching them have no value by themselves. You should continue to put effort into the concepts and concentrate on interpreting the results. 
  <P>Even when you solve a small size problem by hand, I would like you to use the available computer software and Web-based computation to do the dirty work for you. 
  <P>You must be able to read the logical secret in any formulas not memorize them. For example, in computing the variance, consider its formula. 
  Instead of memorizing, you should start with some why: <P>i. Why do we square the deviations from the mean. <BR>Because, if we add up all deviations, we get always zero value. So, to deal with this problem, we 
  square the deviations. Why not raise to the power of four (three will not work)? Squaring does the trick; why should we make life more complicated than  it is? Notice also that squaring also magnifies the deviations;  therefore it works to our advantage to measure the quality of the data. <P>ii. Why is there a summation notation in the formula.<BR>To add up the squared deviation of each data point to compute the total sum of squared deviations. <P>iii. Why do we divide the sum of squares by n-1. <BR>The amount of 
  deviation should reflect also how large the sample is; so we must bring in the sample size. That is, in general,  larger sample sizes have larger sum of square deviation from the mean.  Why n-1 not n? The reason for n-1 is that when you divide by n-1, the sample's variance provides an estimated variance 
  much closer to the <A href="#rPopulation">population</A> variance, than when you divide by n. You note that for large sample size n (say over 30), it really does not matter whether it is divided by n or n-1. The results are almost the same, and they are acceptable. The factor n-1 is what we consider as the "degrees of freedom". 
  <P>This example shows how to question statistical formulas, rather than memorizing them. In fact, when you try to understand the formulas, you do not need to remember them, they are part of your brain connectivity. <FONT color=#dc143c>Clear thinking is always more important than the ability to do arithmetic</FONT>. 
  <P>When you look at a statistical formula, the formula should talk to you, as when a musician looks at a piece of musical-notes, he/she hears the music.  <P><FONT color=#dc143c><B>computer-assisted learning:</B></FONT> The 
  computer-assisted learning provides you a "hands-on" experience which will enhance your understanding of the concepts and techniques covered in this site. 
  <P>
Java, once an esoteric programming language for animating Web pages, is now a full-fledged platform for building JavaScript E-labs' learning objects with useful applications. As you used to do experiments in physics labs to learn physics, computer-assisted learning enables you to use any online interactive tool available on the Internet to perform experiments. The purpose is the same;  i.e., to understand statistical concepts by using statistical applets which are entertaining and educating. <P>The appearance of computer software, JavaScript Applets, Statistical   Demonstration Applets, and Online Computation are the most important events in the process of teaching and learning concepts in model-based, statistical decision making courses. These e-lab  Technologies allow you to construct numerical examples to understand the concepts, and to find their significance for  yourself. 
  <P>
  <P>Unfortunately, most classroom courses are not learning systems. The way the instructors attempt to help their students acquire skills and knowledge has absolutely nothing to do with the way students actually learn. Many instructors rely on lectures and tests, and memorization. All too often, they rely on "telling." No one remembers much that's taught by telling, and what's told doesn't translate into usable skills. Certainly, we learn by doing, failing, and practicing until we do it right. The computer assisted learning serves this purpose. 
  <P><DD>A course in appreciation of statistical thinking gives business professionals an edge. Professionals with strong quantitative skills are in demand. This phenomenon will grow as the impetus for data-based decisions strengthens and the amount and availability of data increases. The statistical toolkit can be developed and enhanced at all stages of a career. Decision making process under uncertainty is largely based on application of statistics for probability assessment of uncontrollable events (or factors), as well as 
  risk assessment of your decision. <P>The main objective for this course is to learn statistical thinking; to 
  emphasize more on concepts, and less theory and fewer recipes, and finally to foster active learning using  the useful and interesting Web-sites. It is already a known fact that "Statistical thinking will one day be as 
  necessary for efficient citizenship as the ability to read and write." So, let's be ahead of our time. 
  <P><B>Further Readings:</B><BR><FONT face="Bookman Old Style" size=-2>Chernoff 
  H., A Conversation With Herman Chernoff, <I>Statistical Science</I>, Vol. 11, No. 4, 335-350, 1996.<BR>Churchman C., <I>The Design of Inquiring Systems</I>, Basic Books, New York, 1971. Early in the book he stated that knowledge could be considered as a collection of information, or as an activity, or as a 
  potential. He also noted that knowledge resides in the user and not in the collection.<BR>Rustagi M., <I>et al.</I> (eds.), <I>Recent Advances in Statistics: Papers in Honor of Herman Chernoff on His Sixtieth Birthday</I>, Academic Press, 1983.<BR></FONT>
  <P>
  <P><A name=rbosim></A>
  <HR>

  <H4><FONT color=#dc143c>The Birth of Probability and Statistics</FONT></H4>The  original idea of "statistics" was the collection of information about and for the "state". The word statistics derives directly, not from any classical Greek  or Latin roots, but from the Italian word for <FONT  color=#dc143c>state</FONT>. 
  <P>The birth of statistics occurred in mid-17<FONT size=+0><SUP>th</SUP></FONT> century. A commoner, named John Graunt, who was a native of London, began reviewing a weekly church publication issued by the 
  local parish clerk that listed the number of births, christenings, and deaths in each parish. These so called Bills of Mortality also listed the causes of death. Graunt who was a shopkeeper organized this data in the form  we call descriptive statistics, which was published as <I>Natural and Political  Observations Made upon the Bills of Mortality</I>. Shortly thereafter he was  elected as a member of Royal Society. Thus, statistics has to borrow some  concepts from sociology, such as the concept of <A href="#rPopulation">Population</A>.  It has been argued that since statistics usually involves the study of human  behavior, it cannot claim the precision of the physical sciences.   <P>Probability has much longer history. <FONT color=#dc143c>Probability</FONT> is derived from the verb <FONT color=#dc143c>to probe</FONT> meaning to "find out" what is not too easily accessible or understandable. The word "proof" has 
  the same origin that provides necessary details to understand what is claimed to be true. 
  <P>Probability originated from the study of games of chance and gambling during the 16<FONT><SUP>th</SUP></FONT> century. Probability theory was a branch of mathematics studied by Blaise Pascal and Pierre de Fermat in the seventeenth century. Currently in 21<FONT size=+0><SUP>st</SUP></FONT> century, probabilistic 
  modeling is used to control the flow of traffic through a highway system, a telephone interchange, or a computer processor; find the genetic makeup of individuals or populations; quality control; insurance; investment; and other sectors of business and industry. 
  <P>New and ever growing diverse fields of human activities are using statistics; however, it seems that this field itself remains obscure to the public. Professor Bradley Efron expressed this fact nicely: 
  <MENU>During the 20<FONT size=+0><SUP>th</SUP></FONT> Century statistical thinking and methodology have become the scientific framework for literally dozens of fields including education, agriculture, economics, biology, and medicine, and with increasing influence recently on the hard sciences such as astronomy, geology, and physics. In other words, we have grown from a small obscure field into a big obscure field.</MENU><P><B>Further Readings:</B><BR><FONT face="Bookman Old Style" size=-2>Daston 
  L., <I>Classical Probability in the Enlightenment</I>, Princeton University Press, 1988. <BR>The book points out that early Enlightenment thinkers could not face uncertainty. A mechanistic, deterministic machine, was the  Enlightenment view of the world.<BR>Gillies D., <I>Philosophical Theories of  Probability</I>, Routledge, 2000. Covers the classical, logical, subjective, frequency, and propensity views.<BR>Hacking I., <I>The Emergence of   Probability</I>, Cambridge University Press, London, 1975. A philosophical  study of early ideas about probability, induction and statistical inference.<BR>
Hald A., <I>A History of Probability and Statistics and Their Applications before 1750</I>, Wiley, 2003.<br>           Peters W., <I>Counting for Something: Statistical Principles and  Personalities</I>, Springer, New York, 1987. It teaches the principles of applied economic and social statistics in a historical context. Featured topics include public opinion polls, industrial quality control, factor analysis, Bayesian methods, program evaluation, non-parametric and robust methods, and exploratory data analysis.<BR>Porter T., <I>The Rise of  Statistical Thinking</I>, 1820-1900, Princeton University Press, 1986. The   author states that statistics has become known in the twentieth century as the  mathematical tool for analyzing experimental and observational data. Enshrined  by public policy as the only reliable basis for judgments as the efficacy of  medical procedures or the safety of chemicals, and adopted by <B>business</B> 
  for such uses as industrial quality control, it is evidently among the products of science whose influence on public and private life has been most pervasive. Statistical analysis has also come to be seen in many scientific   disciplines as indispensable for drawing reliable conclusions from empirical (i.e., observed) results. This new field of mathematics found so extensive a domain of  applications. <BR>Stigler S., <I>The History of Statistics: The Measurement of  Uncertainty Before 1900</I>, U. of Chicago Press, 1990. It covers the people, 
  ideas, and events underlying the birth and development of early statistics.<BR>Tankard J., <I>The Statistical Pioneers</I>, Schenkman Books, New York, 1984.<BR>This work provides the detailed lives and times of 
  theorists whose work continues to shape much of the modern statistics. </FONT>
  <P>
  <P><A name=rstatdecisionmake></A>
  <HR>
  <H4><FONT color=#dc143c>Statistical Modeling for Decision-Making under Uncertainties:<BR>From Data to the Instrumental Knowledge</FONT></H4>In this diverse world of ours, no two things are exactly the same. A statistician is  interested in both the <FONT color=#dc143c><B>differences</B></FONT> and the 
  <FONT color=#dc143c><B>similarities</B></FONT>; i.e., both departures and patterns.   <P>The actuarial tables published by insurance companies reflect their statistical analysis of the average life expectancy of men and women at any given age. From these numbers, the insurance companies then calculate the 
  appropriate premiums for a particular individual to purchase a given amount of insurance. 
  <P>Exploratory analysis of data makes use of numerical and graphical techniques to study patterns and departures from patterns. The widely used descriptive statistical techniques are: Frequency <A 
  href="#rdensityDist">Distribution</A>;  Histograms; Boxplot;  Scattergrams and Error Bar plots;  and diagnostic plots.     <P>In examining distribution of data, you should be able to detect important characteristics, such as shape, location, variability, and unusual values.   From careful observations of patterns in data, you can generate conjectures about relationships among variables. The notion of how one variable may be associated with another permeates almost all of statistics, from simple comparisons of proportions through linear regression. The difference between association and causation must accompany this conceptual development. <P>Data must be collected according to a well-developed plan if valid information on a conjecture is to be obtained. The plan must identify   important variables related to the conjecture, and specify how they are to be measured. From the data collection plan, a statistical model can be formulated from which <A href="#rstatInferentia">inferences</A> 
  can be drawn.   <P>As an example of <FONT color=#dc143c>statistical modeling with managerial 
  implications</FONT>, such as <FONT color=#dc143c>"what-if" analysis</FONT>,   consider regression analysis. Regression analysis is a powerful technique for studying relationship between dependent variables  (i.e., output, performance measure) and independent variables (i.e., inputs, factors, decision variables). Summarizing relationships among the variables by the most appropriate equation (i.e., modeling) allows us to predict or identify the most influential factors and study their impacts on the output for any changes 
  in their current values. <P>Frequently, for example the marketing managers are faced with the 
  question, What Sample Size Do I Need? This is an important and common statistical decision, which should be given due consideration, since an inadequate sample size invariably leads to wasted resources. The sample size determination section provides a practical solution to this <FONT color=#dc143c>risky decision</FONT>. 
  <P>Statistical models are currently used in various fields of business and science. However, the <FONT color=#dc143c>terminology differs from field to field</FONT>. For example, the fitting of models to data, called calibration, history matching, and data assimilation, are all synonymous with <A  href="#rparamerts">parameter</A>   estimation. <P>Your organization database contains a wealth of information, yet the  decision technology group members tap a fraction of it. Employees waste time scouring multiple sources for a database. The decision-makers are frustrated because they cannot get business-critical data exactly when they need it.   Therefore, <FONT color=#dc143c>too many decisions are based on guesswork, not facts</FONT>. Many opportunities are also missed, if they are even noticed at all. 
  <P>Knowledge is what we know. Information is the communication of knowledge. In every knowledge exchange, there is a sender and a receiver. The sender makes common what is private, does the informing, the communicating. Information can be classified as <B>explicit and tacit</B> forms. The explicit information can 
  be explained in structured form, while tacit information is inconsistent and fuzzy to explain. 
  <P>Data is known to be crude information and not knowledge by itself. The sequence from data to knowledge is: <FONT color=#dc143c><B>from Data to Information, from Information to Facts, and finally, from Facts to 
  Knowledge</B></FONT>. Data becomes information, when it becomes relevant to your decision problem. Information becomes fact, when the data can support it. Facts are what the data reveals. However the decisive instrumental knowledge is expressed together with some <FONT color=#dc143c>statistical degree of 
  confidence</FONT>.  <P>Fact becomes knowledge, when it is used in the successful completion of 
  a decision process. <FONT color=#dc143c>Knowledge needs wisdom.</font> Wisdom is the power to put our time and our knowledge to the proper use.  Once you have a massive amount of facts integrated as knowledge, then your mind will be superhuman in the same sense that mankind with writing is superhuman compared to mankind before writing. The following figure illustrates the statistical thinking process based on data in constructing statistical models for decision making under uncertainties.  <P>
  <CENTER><IMG alt="From Data to Knowledge"  src="data.gif"> </CENTER>
<p>
The above figure depicts the fact that as the exactness of a statistical model increases, the level of improvements in decision-making increases. That's why we need Business Statistics. Statistics arose from the need to place knowledge on a systematic evidence base. This required a study of the laws of probability, the development of measures of data properties and relationships, and so on. <p>Statistical inference aims at determining whether any statistical significance can be attached that results after due allowance is made for any random variation as a source of error. Intelligent and critical inferences cannot be made by those who do not understand the purpose, the conditions, and applicability of the various techniques for judging significance.
<p>The purpose of statistical thinking is to get acquainted with the statistical techniques, to be able to execute  procedures using available JavaScript Applets, and to be conscious of the conditions and limitations of various techniques. 
  <P><A name=rrstatdecproce></A>
  <HR>
  <H4><FONT color=#dc143c>Statistical Decision-Making Process</FONT></H4>
  <DD>Unlike the <A href="http://ubmail.ubalt.edu/~harsham/opre640a/partVIII.htm" 
  target=new>deterministic</A> decision-making process, such as <A  href="otherapplets/LPTools.htm" 
  target=new>linear optimization</a> by solving <a href="otherapplets/SysEq.htm" target="new">systems of equations</a> and 
 in <A href="otherapplets/ADuncertain.htm" 
  target=new>decision making  under pure uncertainty</a>, the variables are often more numerous and more difficult to measure and control. However, the steps are the same. They are:  </DD></BLOCKQUOTE>
<OL>
  <LI>Simplification   <LI>Building a decision model   <LI>Testing the model   <LI>Using the model to find the solution:   <UL>    <LI>It is a simplified representation of the actual situation     <LI>It need not be complete or exact in all respects     <LI>It concentrates on the most essential relationships and ignores the less 
    essential ones.     <LI>It is more easily understood than the empirical (i.e., observed) situation, and hence permits the problem to be  solved more readily with minimum time and effort.    </LI></UL>  <LI>It can be used again and again for similar problems or can be modified. </LI></OL>
<P>
<BLOCKQUOTE>Fortunately the probabilistic and statistical methods for analysis and decision making under uncertainty are more numerous and powerful today than ever before. The computer makes possible many practical applications. A few examples of <FONT color=#dc143c>business applications</FONT> are the 
  following: </BLOCKQUOTE>
<UL>  <LI>An auditor can use random sampling techniques to audit the accounts receivable for clients. 
  <LI>A plant manager can use statistical quality control techniques to assure the quality of his production with a minimum of testing or inspection.  <LI>A financial analyst may use regression and correlation to help understand the relationship of a financial ratio to a set of other variables in business.   <LI>A market researcher may use test of significace to accept or reject the hypotheses about a group of buyers to which the firm wishes to sell a particular product. <LI>A sales manager may use statistical techniques to forecast sales for the 
  coming year. </LI></UL><BLOCKQUOTE>
  <P><FONT color=#dc143c><B>Questions Concerning Statistical the Decision-Making Process:</B></FONT> 
  <P></P></BLOCKQUOTE><OL>  <LI><FONT color=#dc143c>Objectives or Hypotheses:</FONT> What are the objectives of the study or the questions to be answered? What is the population to which the investigators intend to refer their findings?   <P></P> <LI><FONT color=#dc143c>Statistical Design:</FONT> Is the study a planned experiment (i.e., <A  href="#rseconprim">primary data</A>), or an analysis of records ( i.e., <A 
  href="#rseconprim">secondary data</A>)? How is the sample to be selected? Are there possible sources of 
  selection, which would make the sample atypical or non-representative? If so,  what provision is to be made to deal with this bias? What is the nature of the control group, standard of comparison, or cost? Remember that <FONT color=#dc143c>statistical modeling means <B>reflections before actions</B>.</FONT> 
  <P></P>  <LI><FONT color=#dc143c>Observations:</FONT> Are there clear definition of   variables, including classifications, measurements (and/or counting), and the  outcomes? Is the method of classification or of measurement consistent for all  the subjects and relevant to Item No. 1.? Are there possible biased in    measurement (and/or counting) and, if so, what provisions must be made to deal   with them? Are the observations reliable and replicable (to defend your finding)?   <P></P>  <LI><FONT color=#dc143c>Analysis:</FONT> Are the data sufficient and worthy of statistical analysis? If so, are the necessary conditions of the methods of statistical analysis appropriate to the source and nature of the data? The  analysis must be correctly performed and interpreted.   <P></P>
  <LI><FONT color=#dc143c>Conclusions:</FONT> Which conclusions are justifiable  by the findings? Which are not? Are the conclusions relevant to the questions posed in Item No. 1?   <P></P> <LI><FONT color=#dc143c>Representation of Findings:</FONT> The finding must be represented clearly, objectively, in sufficient but non-technical terms and detail to enable the decision-maker (e.g., a manager) to understand and judge them for himself? Is the finding internally consistent; i.e., do the numbers  added up properly? Can the different representation be reconciled?  <P></P> <LI><FONT color=#dc143c>Managerial Summary:</FONT> When your findings and  recommendation(s) are not clearly put, or framed in an appropriate manner understandable by the decision maker, then the decision maker does not feel  convinced of the findings and  therefore will not implement any of the recommendations. You have wasted the time, money, etc. for nothing. </LI></OL><BLOCKQUOTE>  <P><B>Further  Readings:</B><BR><FONT face="Bookman Old Style" size=-2>Corfield  D., and J. Williamson, <I>Foundations of Bayesianism</I>, Kluwer Academic  Publishers, 2001. Contains Logic, Mathematics, Decision Theory, and Criticisms  of Bayesianism.<BR>Lapin L., <I>Statistics for Modern Business Decisions</I>,   Harcourt Brace Jovanovich, 1987.<BR>Pratt J., H. Raiffa, and R. Schlaifer,   <I>Introduction to Statistical Decision Theory</I>, The MIT Press,   1994.<BR></FONT>

  <P><A name=rwhatbussts></A>  <HR>
  <H4><FONT color=#dc143c>What is Business Statistics?</FONT></H4>The main objective of Business Statistics is to make <A 
  href="#rstatInferentia">inferences</A>  (e.g., prediction, making decisions) about certain characteristics of a <A   href="#rPopulation">population</A>   based on information contained in a random sample from the entire population.  The condition for <FONT color=#dc143c>randomness is essential to make sure the 
  sample is representative of the population</FONT>.
  <P>Business Statistics is the science of <FONT color=#dc143c>good' decision making in the face of uncertainty</FONT> and is used in many disciplines, such as financial analysis, econometrics, auditing, production and operations, and marketing research. It provides knowledge 
  and skills to interpret and use statistical techniques in a variety of business applications. A typical Business Statistics course is intended for business majors, and covers statistical study, descriptive statistics 
  (collection, description, analysis, and summary of data), probability, and the binomial and normal distributions, test of hypotheses and confidence intervals, linear regression, and correlation. 
  <P>Statistics is a science of making decisions with respect to the characteristics of a group of persons or objects on the basis of numerical information obtained from a randomly selected sample of the group. 
  Statisticians refer to this numerical observation as <FONT color=#dc143c>realization</FONT> of a random sample. However, notice that <FONT color=#dc143c>one cannot see a random sample.</FONT> A random sample is only a sample of a finite <FONT color=#dc143c>outcomes of a random process.</FONT> 
  <P>At the planning stage of a statistical investigation, the question of sample size (n) is critical. For example, sample size for sampling from a finite population of size N, is set at: N<FONT size=+0><SUP>½</SUP></FONT>+1, rounded up to the nearest integer. Clearly, a larger sample provides more relevant information, and as a result a more accurate estimation and better statistical judgement regarding test of hypotheses.  <P>
  <CENTER>    <P><a href="risk.gif"><IMG alt="What is statistics?" src="risk.gif" width="266" height="269" border="0"></a> <p>Activities Associated with the General Statistical Thinking	
<br>      <b> <font color="#dc143c">Click on the image to enlarge it and THEN print it</font></b> <P></CENTER>
  <P>The above figure illustrates the idea of <A   href="#rstatInferentia">statistical inference</A> from a random sample about the <A  href="#rPopulation">population</A>.  It also provides estimation for the <A 
  href="#rparamerts">population's  parameters</A>; namely the expected value µ<FONT size=+0><SUB>x</SUB></FONT>, the standard deviation, and the <A href="#rdensityDist">cumulative 
  distribution function (cdf)</A> F<FONT size=+0><SUB>x</SUB></FONT>, <FONT 
  face=symbol>s</FONT> and their corresponding sample statistics, mean <IMG   src="xbaru.gif">, sample standard deviation S<FONT  size=+0><SUB>x</SUB></FONT>, and <A href="#rcdffunc">empirical (i.e., observed) cumulative distribution function</A> (cdf), respectively. <P>The major task of statistics is to study the characteristics of populations whether these populations are people, objects, or collections of information. For two major reasons, it is often impossible to study an entire population:  <P> <DD>The process would be too expensive or too time-consuming.  <DD>The process would be destructive.  <P>In either case, we would resort to looking at a sample chosen from the  population and trying to infer information about the entire population by only examining the smaller sample. Very often the numbers which interest us most about the population are the mean <FONT face=symbol>m</FONT> and standard deviation <FONT face=symbol>s</FONT>. Any number -- like the mean or standard deviation -- which is calculated from an entire population, is called a <FONT  color=#000000>Parameter</FONT>. If the very same numbers are derived only from the data of a sample, then the resulting numbers are called <FONT color=#000000>Statistics</FONT>. Frequently, Greek letters represent <A   href="#rparamerts">parameters</A>   and Latin letters represent <A  href="#rstatisticterm">statistics</A> (as shown in the above Figure).   <P>Statistics is a tool that enables us to impose order on the disorganized  cacophony of the real world of modern society. The business world has grown both in size and competition. <FONT color=#000000>Corporate executive must take risk in business</font>, hence the need for business statistics.   <P>Business statistics has grown with the art of constructing charts and tables! It is a science of basing decisions on numerical data in the face of  uncertainty.  <P>Business statistics is a scientific approach to decision making under risk. In practicing business statistics, we search for an insight, not the solution. Our search is for the one solution that meets all the business's needs with  the lowest level of risk. Business statistics can take a normal business situation, and with the proper data gathering, analysis, and re-search for a solution, turn it into an opportunity. <P>While business statistics cannot replace the knowledge and experience of  the decision maker, it is a valuable tool that the manager can employ to  assist in the decision making process in order to reduce the inherent risk.  <P>Business Statistics provides justifiable answers to the following concerns  for every consumer and producer:   <OL> <LI>What is your or your customer's, <FONT color=#dc143c>Expectation</FONT> of the product/service you sell or that your customer buys? That is, what is a good estimate for <FONT face=symbol>m </FONT>?   <LI>Given the information about your, or your customer's, expectation, what is  the <FONT color=#dc143c>Quality</FONT> of the product/service you sell or that you customer buys. That is, what is a good estimate for <FONT face=symbol>s </FONT>?  <LI>Given the information about your or your customer's expectation, and the quality of the product/service you sell or you customer buy, how does the product/service  <FONT color=#dc143c>compare</FONT> with other existing similar types? That is, comparing several <FONT face=symbol>m </FONT>'s, and several <FONT face=symbol>s </FONT>'s . </LI></OL>  <P>  <P><A name=rcommTemin></A><HR>  <H4><FONT color=#dc143c>Common Statistical Terminology with  Applications</FONT></H4>  <DD>Like all profession, also statisticians have their own keywords and phrases to ease a precise communication. However, one must interpret the results of any  decision making in a language that is easy for the decision-maker to understand. Otherwise, he/she does not believe in what you recommend, and therefore does not go into the implementation phase. This lack of communication between statisticians and the managers is the major roadblock for using statistics. <A name=rPopulation></A><P><FONT color=#dc143c><b>Population:</b></FONT> A population is any entire collection of people, animals, plants or things on which we may collect data. It is the entire group of  interest, which we wish to describe or about which we wish to draw conclusions. In the above figure the life of the light bulbs manufactured say by GE, is the concerned population.

<A name=rQualQuany></A><P><FONT color=#dc143c><b>Qualitative and Quantitative Variables:</b></FONT> Any object or event, which can vary in successive observations either in quantity or quality is called a "variable."  Variables are  classified accordingly as quantitative or qualitative.  A qualitative variable,  unlike a quantitative variable does not vary in magnitude in successive observations. The values of quantitative and qualitative variables are called "Variates" and "Attributes", respectively.
 <P><FONT color=#dc143c><B>Variable:</B></FONT> A characteristic or phenomenon, which may take different values, such as weight, gender since they are different from individual to individual.
<P><FONT color=#dc143c><B>Randomness:</B></FONT> Randomness means unpredictability.  The fascinating fact about inferential statistics is that, although each random observation may not be predictable when taken alone, collectively they follow a predictable pattern called its distribution function.  For example, it is a fact that the distribution of a sample average follows a normal distribution for sample size over 30.   In other words, an extreme value of the sample mean is less likely than an extreme value of a few raw data.<P><FONT color=#dc143c><B>Sample:</B></FONT> A subset of a population or universe.<P><FONT color=#dc143c><B>An Experiment:</B></FONT> An experiment is a process  whose outcome is not known in advance with certainty. <A name=rstaexper></A>  <P><FONT color=#dc143c><B>Statistical Experiment:</B></FONT> An experiment in general is an operation in which one chooses the values of some variables  and measures the values of other variables, as in physics.  A statistical  experiment, in contrast is an operation in which one take a random sample from a population and infers the values of some variables. For example, in a survey, we "survey" i.e. "look at" the situation without aiming to change it, such as in a survey of political opinions. A random sample from the relevant population provides   information about the voting intentions.   <P>In order to make any generalization about a population, a random sample from the entire population; that is meant to be representative of the  population, is often studied. For each population, there are many possible samples. A sample statistic gives information about a corresponding <A  href="#rparamerts">population parameter</A>. For example, the sample mean for a set of data would give information about the overall population mean <FONT face=symbol>m </FONT>. <P>It is important that the investigator carefully and completely defines the population before collecting the sample, including a description of the   members to be included.  <P><B>Example:</B> The population for a study of infant health might be all  children born in the U.S.A. in the 1980's. The sample might be all babies born on 7<FONT size=+0><SUP>th</SUP></FONT> of May in any of the years.  <P>An experiment is any process or study which results in the collection of data, the outcome of which is unknown. In statistics, the term is usually restricted to situations in which the researcher has control over some of the conditions under which the experiment takes place. <P><B>Example:</B> Before introducing a new drug treatment to reduce high blood pressure, the manufacturer carries out an experiment to compare the effectiveness of the new drug with that of one currently prescribed. Newly diagnosed subjects are recruited from a group of local general practices. Half of them are chosen at random to receive the new drug, the remainder receives the present one. So, the researcher has control over the subjects recruited and the way in which they are allocated to treatment. <A name=rdesinexp></A><P><FONT color=#dc143c><B>Design of experiments</B></FONT> is a key tool for increasing the rate of acquiring new knowledge. Knowledge in turn can be used to gain competitive advantage, shorten the product development cycle, and produce new products and processes which will meet and exceed your customer's expectations. <A name=rseconprim></A> <P><FONT color=#dc143c><B>Primary data and Secondary data sets:</B></FONT> If the data are from a planned experiment relevant to the objective(s) of the statistical investigation, collected by the analyst, it is called a Primary Data set. However, if some condensed records are given to the analyst, it is called a Secondary Data set.    <P><A name=rrandomva></A>
  <P><FONT color=#dc143c><B>Random Variable:</B></FONT> A random variable is a real function (yes, it is called " variable", but in reality it is a  function) that assigns a numerical value to each simple event. For example, in sampling for quality control an item could be defective or non-defective, therefore, one may assign X=1, and X = 0 for a defective and non-defective item, respectively. You may assign any other two distinct real numbers, as you wish; however, non-negative integer random variables are easy to work with.  Random variables are needed since one cannot do arithmetic operations on words; the random variable enables us to compute statistics, such as average and variance. Any random variable has a distribution of probabilities associated with it.

<A name=rproby></A><P><FONT color=#dc143c><B>Probability:</B></FONT> Probability (i.e., probing for the unknown) is the tool used for anticipating what the distribution of data should look like under a given model. Random phenomena   are not haphazard: they display an order that emerges only in the long run and is described by a <A  href="#rdensityDist">distribution</A>. The mathematical description of variation is central to statistics. The   probability required for <A  href="#rstatInferentia">statistical  inference</A> is not primarily axiomatic or combinatorial, but is oriented  toward describing data distributions. <A name=rsampleunut></A>
  <P><FONT color=#dc143c><B>Sampling Unit:</B></FONT> A unit is a person, animal, plant or thing which is actually studied by a researcher; the basic objects upon which the study or experiment is executed. For example, a  person; a sample of soil; a pot of seedlings; a zip code area; a   doctor's practice. <A name=rparamerts></A>  <P><FONT color=#dc143c><B>Parameter:</B></FONT> A parameter is an unknown 
  value, and therefore it has to be estimated. Parameters are used to represent a certain population characteristic. For example, the population mean <FONT  face=symbol>m </FONT>is a parameter that is often used to indicate the average  value of a quantity.   <P>Within a population, a parameter is a fixed value that does not vary. Each  sample drawn from the population has its own value of any statistic that is   used to estimate this parameter. For example, the mean of the data in a sample  is used to give information about the overall mean <FONT face=symbol>m</FONT>in the population from which that sample was drawn. <A 
  name=rstatisticterm></A> <P><FONT color=#dc143c><B>Statistic:</B></FONT> A statistic is a quantity 
  that is calculated from a sample of data. It is used to give information about unknown values in the  corresponding population. For example, the average of   the data in a sample is used to give information about the overall average in  the population from which that sample was drawn.   <P>A statistic is a function of an observable random sample. It is therefore an observable <A 
  href="#rrandomva">random variable</A>. Notice that, while a statistic is a "function" of observations, 
  unfortunately, it is commonly called a random "variable" not a function.  <P>It is possible to draw more than one sample from the same population, and the value of a statistic will in general vary from sample to sample. For example, the average value in a sample is a statistic. The average values in more than one sample, drawn from the same population, will not necessarily be equal.  <P>Statistics are often assigned Roman letters (e.g. <IMG  src="xbaru.gif"> and s), whereas the equivalent unknown values in the population (parameters ) are assigned Greek letters (e.g., µ, <FONT face=symbol>s</FONT>).  <P>The word estimate means to esteem, that is giving a value to something. A statistical estimate is an indication of the value of an unknown quantity   based on observed data. <P>More formally, an estimate is the particular value of an estimator that is  obtained from a particular sample of data and used to indicate the value of a  parameter.   <P><B>Example:</B> Suppose the manager of a shop wanted to know <FONT  face=symbol>m </FONT>, the mean expenditure of customers in her shop in the last year. She could calculate the average expenditure of the hundreds (or perhaps thousands) of customers who bought goods in her shop; that is, the population mean <FONT face=symbol>m </FONT>. Instead she could use an estimate of this population mean <FONT face=symbol>m </FONT>by calculating the mean of a representative sample of customers. If this value were found to be $25, then $25 would be her estimate. <P>There are two broad subdivisions of statistics: Descriptive Statistics and Inferential Statistics as described below. <A name=rDescriptiveStatist></A><P><FONT color=#dc143c><B>Descriptive Statistics:</B></FONT> The numerical statistical data should be presented clearly, concisely, and in such a way  that the decision maker can quickly obtain the essential characteristics of  the data in order to incorporate them into decision process. <P>The principal descriptive quantity derived from sample data is the mean (<IMG src="xbaru.gif">), which is the arithmetic  average of the sample data. It serves as the most reliable single measure of   the value of a typical member of the sample. If the sample contains a few values that are so large or so small that they have an exaggerated effect on  the value of the mean, the sample is more accurately represented by the median -- the value where half the sample values fall below and half above. <P>The quantities most commonly used to measure the dispersion of the values about their mean are the variance s<FONT size=+0><SUP>2</SUP></FONT> and its square root , the standard deviation s. The variance is calculated by determining the mean, subtracting it from each of the sample values (yielding the deviation of the samples), and then averaging the squares of these deviations. The mean and standard deviation of the sample are used as estimates of the corresponding characteristics of the entire group from which the sample was drawn. They<I><B> do not</B></I>, in general, completely describe the distribution (F<FONT size=+0><SUB>x</SUB></FONT>) of values within either the sample or the parent group; indeed, different distributions may have the same mean and standard deviation. They do, however, provide a   complete description of the normal distribution, in which positive and negative deviations from the mean are equally common, and small deviations are much more common than large ones. For a normally distributed set of values, a graph showing the dependence of the frequency of the deviations upon their magnitudes is a bell-shaped curve. About 68 percent of the values will differ from the mean by less than the standard deviation, and almost 100 percent will differ by less than three times the standard deviation. <A 
  name=rInferentiaStatist></A> <P><FONT color=#dc143c><B>Inferential Statistics:</B></FONT> Inferential 
  statistics is concerned with making inferences from samples about the populations from which they have been drawn. In other words, if we find a difference between two samples, we would like to know, is this a "real" 
  difference (i.e., is it present in the population) or just a "chance" difference (i.e. it could just be the result of random sampling error). That's what tests of statistical significance are all about. Any inferred conclusion 
  from a sample data to the population from which the sample is drawn must be expressed in a probabilistic term. <FONT color=#dc143c>Probability is the language and a measuring tool for uncertainty in our statistical 
  conclusions.</FONT> 
<p>Inferential statistics could be used for explaining a phenomenon or checking for validity of a claim.  In these instances, inferential statistics is called <FONT color=#dc143c>Exploratory Data Analysis or Confirmatory Data Analysis</font>, respectively.<A name=rstatInferentia></A><P><FONT color=#dc143c><B>Statistical Inference:</B></FONT> Statistical inference refers to extending your knowledge obtained from a random sample from the entire population to the whole population. This is known in  mathematics as <FONT color=#dc143c>Inductive Reasoning</FONT>, that is, knowledge of the whole from a particular. Its main application is in hypotheses testing about a given population. Statistical inference guides the 
  selection of appropriate statistical models. Models and data interact in statistical work.  Inference from data can be thought of as the process of selecting a reasonable model, including a statement in probability language of how confident one can be about the selection. <A name=rnormalcond></A> <P><FONT color=#dc143c><B>Normal Distribution Condition:</B></FONT> The normal or Gaussian distribution is a continuous symmetric distribution that follows the familiar bell-shaped curve. One of its nice features is that, the mean and variance uniquely and independently determines the distribution. It has been noted empirically that many measurement variables have distributions that are at least approximately normal. Even when a distribution is  non-normal, the distribution of the mean of many independent observations from the same distribution becomes arbitrarily close to a normal distribution, as  the number of observations grows large. Many frequently used statistical tests   make the condition that the data come from a normal distribution.  <P><A name=restimateHypoth></A><P><FONT color=#dc143c><B>Estimation and Hypothesis Testing:</B></FONT>Inference in statistics are of two types. The first is <FONT  color=#dc143c>estimation</FONT>, which involves the determination, with a possible error due to sampling, of the unknown value of a population characteristic, such as the proportion having a specific attribute or the average value <FONT face=symbol>m</FONT> of some numerical measurement. To express the accuracy of the estimates of population characteristics, one must also compute the <FONT color=#dc143c>standard errors</FONT> of the estimates.  The second type of inference is <FONT color=#dc143c>hypothesis testing</FONT>. It involves the definitions of a <FONT color=#dc143c>hypothesis</FONT> as one set of possible population values and <FONT color=#dc143c>an alternative,</FONT> a different set. There are many statistical procedures for determining, on the basis of a sample, whether the true population characteristic belongs to the set of values in the hypothesis or the alternative.  <P><FONT color=#dc143c>Statistical inference</FONT> is grounded in probability, idealized concepts of the group under study, called the population, and the sample. The statistician may view the population as a set of balls from which the sample is selected at random, that is, in such a way that each ball has the same chance as every other one for inclusion in the  sample. <P>Notice that to be able to <A  href="#restimateHypoth">estimate</A>  the <A  href="#rparamerts">population parameters</A>, the sample size n must be greater than one. For example, with a sample size of one, the variation (s<SUP>2</SUP>) within the sample is 0/1 = 0. An estimate for the variation (<FONT face=symbol>s</FONT><FONT   size=+0><SUP>2</SUP></FONT>) within the population would be 0/0, which is  indeterminate quantity, meaning impossible.    <P>  

 <P><A name=rgl></A> <HR>    <H4><FONT color=#dc143c>Greek Letters Commonly Used as Statistical Notations</FONT></H4>We use Greek letters as scientific notations in statistics and other scientific fields  to honor the ancient Greek philosophers who invented science and scientific thinking. Before Socrates, in 6<FONT size=+0><SUP>th</SUP></FONT> 
    Century BC, Thales and Pythagoras, amomg others, applied geometrical concepts to arithmetic, and Socrates is the inventor of dialectic reasoning. The revival of scientific thinking (initiated by Newton's work) was valued and hence reappeared almost 2000 years later. <P><CENTER><TABLE border=1 borderColor=#000000 cellSpacing=0><CENTER><TBODY>
<TR><TD align=middle colSpan=11><B><FONT color=#dc143c>Greek Letters Commonly Used as Statistical Notations</FONT></B></TD><TR><TD>alpha</TD><TD>beta</TD><TD>ki-sqre</TD><TD>delta</TD> <TD>mu</TD><TD>nu</TD><TD>pi</TD><TD>rho</TD><TD>sigma</TD><TD>tau</TD><TD>theta</TD>
</TR></CENTER><TR><CENTER><TD><CENTER><FONT face=symbol>a</FONT></CENTER></TD>
<TD><CENTER><FONT face=symbol>b</FONT></CENTER></TD><TD><CENTER><FONT face=symbol>c </FONT><FONT size=+0><SUP>2</SUP></FONT></CENTER></TD><TD><CENTER><FONT face=symbol>d</FONT></CENTER></TD><TD><CENTER><FONT face=symbol>m</FONT></CENTER></TD>
<TD><CENTER><FONT face=symbol>n</FONT></CENTER></TD><TD><CENTER><FONT face=symbol>p</FONT>
</CENTER></TD><TD><CENTER><FONT face=symbol>r</FONT></CENTER></TD></CENTER><TD> 
<CENTER><FONT face=symbol>s</FONT></CENTER></TD><TD><CENTER><FONT face=symbol>t</FONT>
</CENTER></TD><TD><CENTER><FONT face=symbol>q</FONT></CENTER></TD></TR></TBODY>
</TABLE></CENTER><P><P><B>Note:</B> ki-square (ki-sqre, Chi-square), <FONT face=symbol>c</FONT>
<FONT size=+0><SUP>2</SUP>, is not the square of anything, its name implies Chi-square (read, ki-square). Ki does not exist in statistics.
<p>I'm glad that you're overcoming all the confusions that exist in learning statistics.  <P><A name=rlom></A><HR><H4><FONT color=#dc143c>Type of Data and Levels of Measurement</FONT></H4>Information can be collected in statistics using <a href="#rQualQuany">qualitative or quantitative</a> data. Qualitative data</FONT>, such as eye color of a group of individuals, is not computable by arithmetic relations. They are labels that advise in which category or class an individual, object, or process fall. They are called categorical variables. <P><FONT color=#dc143c>Quantitative data</FONT> sets consist of measures that take numerical values for which descriptions such as means and standard deviations are meaningful. They can be put into an order and further divided into two groups: discrete data or continuous data. <P><FONT color=#dc143c>Discrete data</FONT> are countable data and are collected by <B>counting</B>, for example, the number of defective items produced during a day's production. <P><FONT color=#dc143c>Continuous data</FONT> are collected by <B>measuring</B> and are expressed on a continuous scale. For example, measuring the height of a person. <P>Among the first activities in statistical analysis is to count or measure: Counting/measurement theory is concerned with the connection between data and reality. <FONT color=#dc143c>A set of data is a representation (i.e., a model) of the reality</FONT> based on numerical and measurable scales. Data are called "primary type" data if the analyst has been involved in collecting the data relevant to his/her investigation. Otherwise, it is called "secondary type" data. <P>Data come in the forms of <B>N</B>ominal, <B>O</B>rdinal, <B>I</B>nterval, and <B>R</B>atio (remember the French word NOIR for the color black).   Data can be either continuous or discrete.  <P><CENTER><TABLE width="70%"><TBODY><TR><TD></TD>
<TD align=middle colSpan=3><B>Levels of Measurements</B></TD></TR><TR><TD></TD><TD colSpan=3 vAlign=top>_________________________________________</TD></TR><TR><TD></TD><TD align=middle>Nominal</TD><TD align=middle>Ordinal</TD><TD align=middle>Interval/Ratio</TD></TR>
<TR><TD>Ranking?</TD><TD align=middle>no</TD><TD align=middle>yes</TD><TD align=middle>yes</TD> 
</TR><TR><TD>Numerical difference</TD><TD align=middle>no</TD><TD align=middle>no</TD><TD align=middle>yes</TD></TR></TBODY></TABLE></CENTER>    <P>Both the zero point and the units of measurement are arbitrary on the Interval scale. While the unit of measurement is arbitrary on the Ratio scale, its zero point is a natural attribute. The categorical variable is measured on an ordinal or nominal scale. 
<P>Counting/measurement theory is concerned with the connection between data and reality. Both statistical theory and counting/measurement theory are necessary to make inferences about reality. <P>Since statisticians live for precision, they prefer Interval/Ratio levels of measurement.
<p>
For a good business application of discrete random variables, visit <a  href="Matrix/Mat4.htm" target=new>Markov Chain  Calculator</a> and  <a  href="Matrix/Mat10.htm" target=new>Large Markov Chain Calculator</a>.
<P><A name=rwhyrssm></A><HR> <H4><FONT color=#dc143c>Why Statistical Sampling? </FONT></H4> Sampling is the selection of part of an aggregate or totality known as <A href="#rPopulation">population</A>, on the basis of which a decision concerning the population is made. <P>The following are the advantages and/or necessities for sampling in statistical decision making: <OL><LI><FONT color=#dc143c>Cost:</FONT> Cost is one of the main arguments in favor of sampling, because often a sample can furnish data of sufficient accuracy and at much lower cost than a census. <P></P><LI><FONT color=#dc143c>Accuracy:</FONT> Much better control over data collection errors is possible with sampling than with a census, because a sample is a smaller-scale undertaking. 
<P></P><LI><FONT color=#dc143c>Timeliness:</FONT> Another advantage of a sample over a census is that the sample produces information faster. This is important for timely decision making.  <P></P><LI><FONT color=#dc143c>Amount of Information:</FONT> More detailed information can be obtained from a sample survey than from a census, because it take less time, is less costly, and allows us to take more care in the  data 
processing stage. <P></P><LI><FONT color=#dc143c>Destructive Tests:</FONT> When a test involves the 
 destruction of an item under study, sampling must be used. Statistical sampling determination can be used to find the optimal sample size within an acceptable cost. </LI></OL><P><B>Further Reading:</B><BR>
<FONT face="Bookman Old Style" size=-2>Thompson S., <I>Sampling</I>, Wiley,  2002. </FONT><p>

 <P><A name=rssm></A><HR><H4><FONT color=#dc143c>Sampling Methods</FONT></H4>From the food you eat to the television you watch, from political elections to school board actions, much of your life is regulated by the results of sample surveys. <P>A sample is a group of units selected from a larger group (the population). By studying the sample, one hopes to draw valid conclusions about the larger group. <P>A sample is generally selected for study because the population is too large to study in its entirety. The sample should be representative of the general population. This is often best achieved by random sampling. Also, before collecting the sample, it is important that one carefully and completely defines the population, including a description of the members to be included. <P>A common problem in business statistical decision-making arises when we need information about a collection called a population but find that the cost of obtaining the information is prohibitive. For instance, suppose we need to know the average shelf life of current inventory. If the inventory is large, the cost of checking records for each item might be high enough to cancel the benefit of having the information. On the other hand, a hunch about the average shelf life might not be good enough for <FONT color=#dc143c>decision-making</font> purposes. This means we must arrive at a compromise that involves selecting a small number of items and calculating an average shelf life as an estimate of the average shelf life of all items in inventory. This is a compromise, since the measurements for a sample from the inventory will produce only an estimate of the value we want, but at substantial savings. What we would like to know is how "good" the estimate is and how much more will it cost to make it "better". Information of this type is intimately related to <FONT color=#dc143c>sampling techniques</FONT>. This section provides a short discussion on the common methods of business statistical sampling. <P><FONT color=#dc143c><B>Cluster sampling</B></FONT> can be used whenever the population is homogeneous but can be partitioned. In many applications the partitioning is a result of physical distance. For instance, in the insurance industry, there are small "clusters" of employees in field offices scattered about the country. In such a case, a random sampling of employee work habits might not required travel to many of the "clusters" or field offices in order to get the data. Totally sampling each one of a small number of clusters chosen at random can eliminate much of the cost associated with the data requirements of management. <P><FONT color=#dc143c><B>Stratified sampling</B></FONT> can be used whenever the population can be partitioned into smaller sub-populations, each of which is homogeneous according to the particular characteristic of interest. If there are k sub-populations and we let N<FONT size=+0><SUB>i</SUB></FONT> denote the size of sub-population i, let N denote the overall population size,  and let n denote the sample size, then we select a stratified sample whenever  we choose: <P> 
<CENTER>n<FONT size=+0><SUB>i</SUB></FONT> = n(N<FONT size=+0><SUB>i</SUB></FONT>/N) 
</CENTER><P><CENTER></CENTER>items at random from sub-population i, i = 1, 2, . . . . , k. 
<P>The estimates is: <P><IMG src="xbaru.gif"><FONT size=+0><SUB>s</SUB></FONT> = <FONT face=symbol>S</FONT> W<FONT  size=+0><SUB>t</SUB></FONT>. <IMG  src="xbaru.gif"><FONT size=+0><SUB>t</SUB></FONT>, over t = 1, 2, ..L (strata), and <IMG  src="xbaru.gif"><FONT size=+0><SUB>t</SUB></FONT> is <FONT face=symbol>S</FONT>X<FONT size=+0><SUB>it</SUB></FONT>/n<FONT size=+0><SUB>t</SUB></FONT>. <P>Its variance is: 
<P><CENTER><FONT face=symbol>S</FONT>W<FONT size=+0><SUP>2</SUP></FONT><FONT 
  size=+0><SUB>t</SUB></FONT> /(N<FONT size=+0><SUB>t</SUB></FONT>-n<FONT 
  size=+0><SUB>t</SUB></FONT>)S<FONT size=+0><SUP>2</SUP></FONT><FONT 
  size=+0><SUB>t</SUB></FONT>/[n<FONT size=+0><SUB>t</SUB></FONT>(N<FONT 
  size=+0><SUB>t</SUB></FONT>-1)] </CENTER><P>Population total T is estimated by N. <IMG 
  src="xbaru.gif"><FONT size=+0><SUB>s</SUB></FONT>;  its variance is <P><CENTER><FONT face=symbol>S</FONT>N<FONT size=+0><SUP>2</SUP></FONT><FONT  size=+0><SUB>t</SUB></FONT>(N<FONT size=+0><SUB>t</SUB></FONT>-n<FONT 
  size=+0><SUB>t</SUB></FONT>)S<FONT size=+0><SUP>2</SUP></FONT><FONT 
  size=+0><SUB>t</SUB></FONT>/[n<FONT size=+0><SUB>t</SUB></FONT>(N<FONT 
  size=+0><SUB>t</SUB></FONT>-1)]. </CENTER><P><P><FONT color=#dc143c><B>Random sampling</B></FONT> is probably the most popular sampling method used in decision making today. Many decisions are made, for instance, by choosing a number out of a hat or a numbered bead from a barrel, and both of these methods are attempts to achieve a random choice from a set of items. But true random sampling must be achieved with the aid of a computer or a random number table whose values are generated 
by computer random number generators. <P>A random sampling of size n is drawn from a population size N.  The unbiased estimate for  variance of <IMG src="xbaru.gif"> is:
<p>
<center>
 Var(<IMG   src="xbaru.gif">) = S<FONT 
  size=+0><SUP>2</SUP></FONT>(1-n/N)/n, 
<p>
</center>
where n/N is the sampling fraction. For sampling fraction less than 10% the finite population correction factor (N-n)/(N-1) is almost 1. 
    <P>The total T is estimated by N <font face=symbol>&#180;</font>	 <IMG  src="xbaru.gif">, its variance is N<FONT   size=+0><SUP>2</SUP></FONT>Var(<IMG   src="xbaru.gif">).  <P>For 0, 1, (binary) type variables, variation in estimated proportion p  is:  <P><CENTER>S<FONT size=+0><SUP>2</SUP></FONT> = p(1-p) <font face=symbol>&#180;</font> (1-n/N)/(n-1).
</CENTER><P>For ratio r = <FONT face=symbol>S</FONT>x<FONT  size=+0><SUB>i</SUB></FONT>/<FONT face=symbol>S</FONT>y<FONT size=+0><SUB>i</SUB></FONT>=<IMG src="xbaru.gif"> / <IMG src="ybar.gif">, the variation for r is: <P><CENTER>[(N-n)(r<FONT size=+0><SUP>2</SUP></FONT>S<FONT 
  size=+0><SUP>2</SUP></FONT><FONT size=+0><SUB>x</SUB></FONT> + S<FONT 
  size=+0><SUP>2</SUP></FONT><FONT size=+0><SUB>y</SUB></FONT> -2 r Cov(x, y)]/[n(N-1)<IMG src="xbaru.gif"><FONT size=+0><SUP>2</SUP></FONT>]. </CENTER><P>Determination of sample sizes (n) with regard to binary data: Smallest integer greater than or equal to:
 <P>
<center>[t<FONT size=+0><SUP>2</SUP></FONT> N p(1-p)] / [t<FONT  size=+0><SUP>2</SUP></FONT> p(1-p) + <FONT face=symbol>a</FONT><FONT 
  size=+0><SUP>2</SUP></FONT> (N-1)],
<P>
</center>with N being the size of the total number of cases, n being the sample size, <FONT face=symbol>a</FONT> the expected error, t being the value taken  from the t-distribution corresponding to a certain confidence interval, and p being the probability of an event. <P><FONT color=#dc143c><B>Cross-Sectional Sampling:</B></FONT>Cross-Sectional study the observation of a defined population at a single point in time or time interval. Exposure and outcome are determined simultaneously. 
<p>
<font  color="#DC143C"><b>What is a statistical instrument?</b></font>  A statistical instrument is any process
that aim at describing a phenomena by using any instrument or device, however the results may be used as a control tool.  Examples of statistical instruments are questionnaire and surveys sampling.
<p>
<font  color="#DC143C"><b>What is grab sampling technique?</b></font> The grab sampling technique is to take a relatively small sample over a very short period of time, the result obtained are usually instantaneous.  However, the <B>Passive Sampling</B> is a technique where a sampling device is used for an extended time under similar conditions. Depending on the desirable statistical investigation, the passive sampling may
be a useful alternative or even more appropriate than grab sampling.  However, a passive sampling technique needs to be developed and tested in the field.
   <P><B>Further Reading:</B><BR><FONT face="Bookman Old Style" size=-2>Thompson S., <I>Sampling</I>, Wiley, 2002. </FONT> <P><A name=rvsd><HR></A><H4><FONT color=#dc143c>Statistical Summaries</FONT></H4><A name=rmeanmodemed></A> <H4><FONT color=#dc143c>Representative of a Sample: Measures of Central Tendency Summaries</FONT></H4>How do you describe the "average" or "typical" piece of information in a set of data? Different procedures are used to summarize the most representative information depending of the type of question asked and the nature of the data being summarized.  <P>Measures of location give information about the <B><FONT  color=#dc143c>location</FONT></B> of the central tendency within a group of numbers. The measures of location presented in this unit for ungrouped (raw) data are the mean, the median, and the mode.  <P><FONT color=#dc143c><B>Mean:</B></FONT> The arithmetic mean (or the average, simple mean) is computed by summing all numbers in an array of numbers (x<FONT size=+0><SUB>i</SUB></FONT>) and then dividing by the number of observations (n) in the array. <P><CENTER>Mean = <IMG src="xbaru.gif"> = <FONT 
  face=symbol>S</FONT> X<FONT size=+0><SUB>i</SUB></FONT> /n, &nbsp; &nbsp; the sum is over all i's. 
</CENTER><P>The mean uses all of the observations, and each observation affects the mean. Even though the mean is sensitive to extreme values; i.e., extremely large or small data can cause the mean to be pulled toward the extreme data; it is still the most widely used measure of location. This is due to the fact that the mean has valuable mathematical properties that make it convenient for use with inferential statistical analysis. For example, the sum of the deviations of the numbers in a set of data from the mean is zero, and the sum of the squared deviations of the numbers in a set of data from the mean is the minimum value. <P> You might like to use <A  href="otherapplets/Descriptive.htm"   target=new>Descriptive Statistics</A>  Applet to compute the mean. <P><FONT color=#dc143c><B>Weighted Mean:</B></FONT> In some cases, the data in the sample or population should not be weighted equally, rather each value should be weighted according to its importance. <P><FONT color=#dc143c><B>Median:</B></FONT> The median is the middle value in an <B>ordered </B>array of observations. If there is an even number of observations in the array, the median is the <B>average</B> of the two middle numbers. If there is an odd number of data in the array, the median is the  <B>middle</B> number. <P>The median is often used to summarize the distribution of an outcome. If the distribution is <A href="#rskewKur">skewed</A>, the median and the interquartile range (IQR) may be better than other measures to indicate where the observed data are concentrated. <P>Generally, the median provides a better measure of location than the mean when there are some extremely large or small observations; i.e., when the data are skewed to the right or to the left. For this reason, median income is used as the measure of location for the U.S. household income. Note that if the median is <B>less than</B> the mean, the data set is skewed to the right. If the median is <B>greater than</B> the mean, the data set is skewed to the left.  For normal population, the sample median is distributed normally with <FONT face=symbol>m</FONT> = the mean, and <FONT color=#dc143c>standard error of the median</FONT> (<FONT  face=symbol>p</FONT>/2)<FONT><SUP>½</SUP></FONT> times standard error of the mean. <p>The mean has two distinct advantages over the median. It is more stable, and one can compute the mean based of two samples by combining the two means.<P><FONT color=#dc143c><B>Mode:</B></FONT> The mode is the most frequently occurring value in a set of observations. Why use the mode? The classic example is the shirt/shoe manufacturer who wants to decide what sizes to  introduce. Data may have two modes. In this case, we say the data are <B>bimodal</B>, and sets of observations with more than two modes are referred to as <B>multimodal</B>.  Note that the mode is not a helpful measure of location, because there can be more than one mode or even no mode. <p> When the mean and the median are known, it is possible to estimate the mode for the unimodal distribution using the other two averages as follows:<p><center>Mode <FONT FACE="Symbol">&#187;</FONT> 3(median) - 2(mean)<p></center>This estimate is applicable to both grouped and ungrouped data sets.<P>Whenever, more than one mode exist, then the population from which the  sample came is a <FONT color=#dc143c>mixture</FONT> of more than one population. However, notice that a <A  href="#rUniform">Uniform </A>distribution has uncountable number of modes having equal density value; therefore it is considered as a homogeneous population. 
<p>
Almost all standard statistical analyses are conditioned on the assumption that the population is homogeneous.<P>Notice that Excel has  very limited statistical capability. For example, it displays <FONT color=#dc143c>only one mode</FONT>, the first one. Unfortunately, this is very misleading. However, you may find out if  there are others by inspection only, as follow: Create a frequency distribution, invoke the menu sequence: Tools, Data analysis, Frequency and follow instructions on the screen. You will see the frequency distribution and then find the  mode visually. Unfortunately, Excel does not draw a Stem and Leaf diagram.  All commercial off-the-shelf software, such as <a href="http://ubmail.ubalt.edu/~harsham/stat-data/opre330SPSSSAS.htm" target ="new"> SAS and  SPSS</a>, display a Stem and Leaf diagram, which is a frequency distribution of a given data set. <P><A name=rselecting></A><H4><FONT color=#dc143c>Selecting Among the Mode, Median, and Mean</FONT></H4>It is a common mistake to specify the wrong index for central tenancy. <CENTER> <IMG src="chart.gif"></CENTER><p>The first consideration is the type of data, if the variable is categorical,  the mode is the single measure that best describes that data. <P>The second consideration in selecting the index is to ask whether the total  of all observations is of any interest. If the answer is yes, then the mean  is the proper index of central tendency. <P>If the total is of no interest, then depending on whether the histogram is symmetric or <A  href="#rskewKur">skewed</A> one must use either mean or median, respectively. <P>In all cases the histogram must be unimodal. However, notice that, e.g.,  a <A  href="#rUniform">Uniform</A> distribution has uncountable number of modes having equal density value; therefore it is considered as a homogeneous population. <P>Notice also that: <CENTER>|Mean - Median| <FONT face=symbol>£</FONT><FONT face=symbol>s</FONT> 
</CENTER><P>The main characteristics of these three statistics are tabulated below: <P></P>    </font></font></font></DD></BLOCKQUOTE><FONT size=+0><FONT size=+0><FONT size=+0> 
<P> <CENTER><TABLE border=3><FONT color=#dc143c size=3><B>The Main Characteristics of the Mode, the Median, and the Mean</B></FONT> <TBODY> <TR><TH>Fact No.<TH>The Mode<TH>The Median<TH>The Mean  <TR> <TD>1<TD>It is the most frequent value in the distribution; it is the point of greatest density. <TD>It is the value of the middle point of the array (not midpoint of range), such that half the item are above and half below it.  <TD>It is the value in a given aggregate which would obtain if all the values  were equal. <TR> <TD>2 
      <TD>The value of the mode is established by the predominant frequency, not by the value in the distribution.  <TD>The value of the media is fixed by its position in the array and doesn't reflect the individual value.  <TD>The sum of deviations on either side of the mean are equal; hence, the algebraic sum of the deviation is equal zero.  <TR> <TD>3 <TD>It is the most probable value, hence the most typical. <TD>The aggregate distance between the median point and all the value in the array is less than from any other point. 
      <TD>It reflect the magnitude of every value. <TR> <TD>4 <TD>A distribution may have 2 or more modes. On the other hand, there is no mode in a rectangular distribution. <TD>Each array has one and only one median. 
      <TD>An array has one and only one mean. <TR> <TD>5 <TD>The mode does nott reflect the degree of modality. <TD>It cannot be manipulated algebraically: medians of subgroups cannot be weighted and combined.  <TD>Means may be manipulated algebraically: means of subgroups may be combined 
  when properly weighted.  <TR> <TD>6 <TD>It cannot be manipulated algebraically: modes of subgroups cannot be  combined.  <TD>It is stable in that grouping procedures do not affect it appreciably.  <TD>It may be calculated even when individual values are unknown, provided the sum of the values and the sample size n are known. <TR> <TD>7 <TD>It is unstable that it is influenced by grouping procedures. <TD>Value must be ordered, and may be grouped, for computation. <TD>Values need not be ordered or grouped for this calculation. <TR> <TD>8 <TD>Values must be ordered and group for its computation. <TD>It can be compute when ends are open  <TD>It cannot be calculated from a frequency table when ends are open. <TR> 
      <TD>9  <TD>It can be calculated when table ends are open.  <TD>It is not applicable to <a href="#rQualQuany">qualitative</a> data. 
      <TD>It is stable in that grouping procedures do not seriously affected it. <TR></TR></TBODY>
  </TABLE></CENTER><P> <BLOCKQUOTE>
<p>
The <A 
  href="otherapplets/Descriptive.htm"  target=new>Descriptive Statistics </A> JavaScript provides a complete set of information about all statistics that you ever need.
You might like to use it to perform some numerical experimentation for validating the above assertions for a deeper understanding.
<p>
 <P><A name=rspecialmean></A> <HR><H4><FONT color=#dc143c>Specialized Averages: The Geometric & Harmonic Means</FONT></H4>

<FONT color=#dc143c><b>The Geometric Mean:</b></font> The geometric mean (G) of n non-negative numerical values is the n<FONT><SUP>th</SUP></FONT> root of the product of the n values. <p>
If some values are very large in magnitude and others are small, then the geometric mean is a better representative of the data than the simple average.  In a "geometric series", the most meaningful average is the geometric mean (G). The arithmetic mean is very biased toward the larger numbers in the series.
<p>
<FONT color=#dc143c><b>An Application:</b></font>  Suppose sales of a certain item increase to 110% in the first year and to 150% of that in the second year.  For simplicity, assume you sold 100 items initially.  Then the number sold in the first year is 110 and the number sold in the second is 150% x 110 = 165.  The arithmetic average of 110% and 150% is 130% so that we would incorrectly estimate that the number sold in the first year is 130 and the number in the second year is 169.  The geometric mean of 110% and 150% is G = (1.65)<FONT><SUP>1/2</SUP></FONT> so that we would correctly estimate that we would sell 100 (G)<FONT><SUP>2</SUP></FONT> = 165 items in the second year.<p><FONT color=#dc143c><b>The Harmonic Mean:</b></font>The harmonic mean (H) is another specialized average, which is useful in averaging variables expressed as rate per unit of time, such as mileage per hour, number of units produced per day.  The harmonic mean (H) of n non-zero numerical values x(i) is:  H = n/[<FONT face=symbol>S</FONT> (1/x(i)].
<p>
<FONT color=#dc143c><b>An Application:</b></font>  Suppose 4 machines in a machine shop are used to produce the same part.  However, each of the four machines takes 2.5, 2.0, 1.5, and 6.0 minutes to make one part, respectively. What is the average rate of speed?
<p>
The harmonic means is:  H = 4/[(1/2.5) + (1/2.0) + 1/(1.5) + (1/6.0)] = 2.31 minutes.
<p>
If all machines working for one hour, how many parts will be produced?  Since four machines running for one hour represent 240 minutes of operating time, then: 240 / 2.31 = 104 parts will be produced. 
<p>
<FONT color=#dc143c><b>The Order Among the Three Means:</b></font> If all the three means exist, then the Arithmetic Mean is never less than the other two, moreover, the Harmonic Mean is never larger than the other two.
<p>
You might like to use  <a href="otherapplets/ThreeMeans.htm" target= "new">The Other Means</a> JavaScript applet in performing some numerical experimentation for validating the above assertions for a deeper understanding.  
<p>
<B> Further Reading:</B><br>
<FONT SIZE="-2" FACE="Bookman Old Style">
Langley R., <I>Practical Statistics Simply Explained</I>, 1970, Dover Press.
</font>
<p>
 <P><A name=rncih></A> <HR><H4><FONT color=#dc143c>Histogramming: Checking for Homogeneity of Population</FONT></H4>
  A histogram is a graphical presentation of an estimate for the density (for continuous <A 
  href="#rrandomva">random variables</A>) or probability mass function (for discrete random variables) 
  of the population.  <P>The geometric feature of histogram enables us to find out useful information about the data, such as:  <OL><LI>The location of the "center" of the data. <LI>The degree of dispersion. <LI>The extend to which its is skewed, that is, it does not fall off systemically on both side of its peak. <LI>The degree of peakedness. How steeply it rises and falls. </LI></OL><P>The mode is the most frequently occurring value in a set of observations. Data may have two modes. In this case, we say the data are <B>bimodal</B>, and sets of observations with more than two modes are referred to as <B>multimodal</B>. Whenever, more than one mode exist, then the population from which the sample came is a <FONT color=#dc143c>mixture</FONT> of more than one population.  Almost all standard statistical analyses are conditioned on the assumption that the population is homogeneous, meaning that its density (for continuous random variables) or probability mass function (for discrete random variables) is unimodal. However, notice that, e.g.,  a <A  href="#rUniform">Uniform</A> distribution has uncountable number of modes having equal density value; therefore it is considered as a homogeneous population.
<p>
To check the unimodality of sampling data, one may use the histogramming  process. <P><FONT color=#dc143c>Number of Class Intervals in a Histogram:</FONT> Before 
 we can construct our frequency distribution we must determine how many classes  we should use. This is purely arbitrary, but too few classes or too many classes  will not provide as clear a picture as can be obtained with some more nearly optimum number. An empirical (i.e., observed) relationship, known as Sturge's rule, may be used as a useful guide to determine the optimal number of classes (k) is given by <P> <CENTER>k = the smallest integer greater than or equal to 1 + 3.332 Log(n) </CENTER><P>where k is the number of classes, Log is in base 10, and n is the total number of the numerical values which comprise the data set.  <P>Therefore, class width is:  <P> <CENTER>(highest value - lowest value) / (1 + 3.332 Logn)</CENTER><P>where n is the total number of items in the data set. <P>The following JavaScript produces a histogram based on this rule:<BR><A 
  href="histograming/topframe.html"   target="new">Test for Homogeneity of a Population</A>. <P>To have an "optimum" you need some measure of quality -- presumably in this case, the "best" way to display whatever information is available in the data.  The sample size contributes to this; so the usual guidelines are to use between 5 and 15 classes, with more classes, if you have a larger sample. You should take into account a preference for tidy class widths, preferably a multiple of 5 or 10, because this makes it easier to understand.  <P>Beyond this it becomes a matter of judgement. Try out a range of class widths,  and choose the one that works best. This assumes you have a computer and can generate alternative histograms fairly readily. <P>There are often management issues that come into play as well. For example,  if your data is to be compared to similar data -- such as prior studies, or from other countries -- you are restricted to the intervals used therein. <P>If the histogram is very skewed, then unequal classes should be considered.  Use narrow classes where the class frequencies are high, wide classes where they are low. 
  <P>The following approaches are common:  <P>Let n be the sample size, then the number of class intervals could be <P><CENTER>Min {n<FONT size=+0><SUP>½</SUP></FONT>, 10 Log(n) }.</CENTER>
  <P>The Log is the logarithm in base 10. Thus for 200 observations you would use 14 intervals but for 2000 you would use 33. <P><BR><STRONG>Alternatively</STRONG>, <P> <OL><LI>Find the range (highest value - lowest value). <LI>Divide the range by a reasonable interval size: 2, 3, 5, 10 or a multiple of 10.  <LI>Aim for no fewer than 5 intervals and no more than 15. </LI> </OL><P>One of the main applications of histogramming is to <A  href="histograming/topframe.html"   target="new">Test for Homogeneity of a Population</A>. The unimodality of the histogram is a necessary condition for the homogeneity of population to make any statistical analysis meaningful. However, notice that, e.g.,  a <A  href="#rUniform">Uniform</A> distribution has uncountable number of modes having equal density value; therefore it is considered as a homogeneous population.<P><B>Further Reading:</B><BR>
    <FONT face="Bookman Old Style" size=-2>Efron B., and R. Tibshirani, <I>An Introduction to the Bootstrap</I>, Chapman &amp; Hall (now the CRC Press), 1994. Contains a tedious test for multimodality that is based on the Gaussian kernel density estimates and then test for multimodality by using the window-size approach. </FONT> <P><A name=rbplot></A> <HR><H4><FONT color=#dc143c>How to Construct a BoxPlot</FONT></H4>A <FONT  color=#dc143c>BoxPlot</FONT> is a graphical display that has many characteristics.  It includes the presence of possible <A  href="#routlier">outliers</a>. It illustrates the range of data. It shows a measure of dispersion such as the upper quartile, lower quartile and interquartile range (IQR) of the data set as well as the median as a measure of central location, which is useful for comparing sets of data. It also gives an indication of the symmetry or <A  href="#rskewKur">skewness</A> of the distribution. The main reason for the popularity of boxplots is that they offer much of information in a compact way.  <P> <CENTER><IMG src="Boxplot.gif"> </CENTER><P><FONT color=#dc143c><B>Steps to Construct a BoxPlot:</B></FONT> <P> <OL><LI>Horizontal lines are drawn at the smallest observation (A), lower quartile. And another  from the upper quartile (D), and the largest observation (E).  Vertical lines to produce the box join these horizontal lines at points (B, and D).  <P></P><LI>A vertical line is drawn at the median point (C), as shown on the above Figure. </LI>
  </OL> 
<p>
<P>For a deeper understanding, you may like using <A  href="http://search.officeupdate.microsoft.com/TemplateGallery/ct146.asp"   target=new>graph paper</A>, and  <A   href="otherapplets/Descriptive.htm"  target=new>Descriptive Sampling Statistics</A> Applet in constructing the BoxPlots for some sets of data; e.g.,  from your textbook. 
<P><A name=rvarstanran></A> <H4><FONT color=#dc143c>Measuring the Quality of a Sample </FONT></H4> Average by itself is not a good indication of quality. You need to know the 
  variance to make any educated assessment. We are reminded of the dilemma of the six-foot tall statistician who drowned in a stream that had an average depth of three feet. <P>Statistical measures are often used for describing the nature and extent of differences among the information in the distribution. A measure of variability is generally reported together with a measure of central tendency. <P>Statistical measures of variation are numerical values that indicate the <B>variability</B> inherent in a set of data measurements. Note that a small 
 value for a measure of dispersion indicates that the data are concentrated around the mean; therefore, the mean is a good representative of the data set. On the other hand, a large measure of dispersion indicates that the mean is not a good representative of the data set. Also, measures of dispersion can be used when we want to compare the distributions of two or more sets of data. <I>Quality of a data set is measured by its variability: Larger variability indicates lower quality.</I> That is why high variation makes the manager very worried. <FONT color=#dc143c>Your job, as a statistician, is to measure the variation</FONT>, and if it is too high and unacceptable, then it is the job of the technical staff, such as engineers, to fix the process. <P>Decision situations with complete lack of knowledge, known as the <FONT color=#dc143c><B>flat uncertainty</B></FONT>, have the largest risk. For simplicity, consider the case when there are only two outcomes, one with probability of p. Then, the variation in the outcomes  is p(1-p). This variation is the largest if we set p = 50%. That is, equal chance for each outcome. In such a case, the quality of information is at  its lowest level. <P>Remember, <FONT color=#dc143c><B>quality of information and variation are inversely related</B></FONT>. The larger the variation in the data, the lower  the quality of the data (i.e., information): <FONT color=#dc143c><B>the Devil is in the Deviations.</B></FONT> <P>The four most common measures of variation are the <FONT   color=#dc143c><B>range</B></FONT>, <FONT color=#dc143c><B>variance</B></FONT>, <FONT color=#dc143c><B>standard deviation</B></FONT>, and <FONT  color=#dc143c><B>coefficient of variation</B></FONT>.<P><B><FONT  color=#dc143c>Range:</FONT></B> The range of a set of observations is the absolute value of the difference between the largest and smallest values in the data set. It measures the size of the smallest contiguous interval  of real numbers that encompasses all of the data values. It is not useful when extreme values are present. It is based solely on two values, not on the entire data set. In addition, it cannot be defined for open-ended distributions such as Normal distribution. <P>Notice that, when dealing with <FONT color=#dc143c>discrete random observations</FONT>, some authors define the range as:<br> Range = Largest value - Smallest value +  1. 
  <P>A normal distribution does not have a range. A student said, "since the tails of a normal density function never touch the x-axis and since for an observation to contribute to forming such a curve, very large positive  and negative values must exist" Yet such remote values are always possible, but increasingly improbable. This encapsulates the asymptotic behavior of normal density very well.  Therefore, in spite of this behavior, it is useful and applicable to a wide range of decision-making situations. <P><FONT color=#dc143c><B>Quartiles:</B></FONT> When we order the data, for example in ascending order, we may divide the data into quarters, Q1Q4, known as quartiles. The first Quartile (Q1) is that value where 25%  of the values are smaller and 75% are larger. The second Quartile (Q2) is that value where 50% of the values are smaller and 50% are larger. The third Quartile (Q3) is that value where 75% of the values are smaller and 25% are larger.  <P><FONT color=#dc143c><B>Percentiles:</B></FONT> Percentiles have a similar concept and therefore, are related; e.g., the  25<FONT><SUP>th</SUP></FONT> percentile corresponds to the first quartile Q1, etc. The advantage of percentiles is that they may be subdivided into 100 parts. The percentiles and quartiles are most conveniently read from a cumulative distribution function.  
 <P><FONT color=#dc143c><B>Interquartiles Range:</B></FONT> The interquartile range (IQR) describes the extent for which the middle 50% of the observations scattered or dispersed. It is the distance between the first and the third quartiles:  <P> <CENTER>IQR = Q3 - Q1, </CENTER>
  <P>which is twice the <FONT color=#dc143c>Quartile Deviation</FONT>. For data that are <A 
  href="#rskewKur">skewed</A>, the <B>relative dispersion</b>, similar to the coefficient of variation (C.V.)</B> is given (provided the 
    denominator is not zero) by the <FONT color=#dc143c>Coefficient of Quartile  Variation</FONT>: 
  <P> <CENTER>CQV = (Q3-Q1) / (Q3 + Q1). </CENTER> <P>Note that almost all statistics that  we have covered up to now can be obtained and understood deeply by <FONT color=#dc143c>graphical method</FONT> using <A 
  href="otherapplets/ECDF.htm"  target=new>Empirical (i.e., observed) Cumulative Distribution Function (ECDF)</A> JavaScript. However, the numerical <A 
  href="otherapplets/Descriptive.htm"  target=new>Descriptive Statistics </A>applet provides a complete set of information about all statistics that you ever need. 
<p>
<FONT color=#dc143c><b>The Duality between the ECDF and the Histogram:</b></font> Notice that the empirical (i.e., observed) cumulative distribution function (<A  href="#rcdffunc">ECDF</a>) indicates by its height at a particular pointthat  is numerically equal to the area in the corresponding histogram to the left of that point.  Therefore, either or both could be used depending on the intended applications.
<P><FONT color=#dc143c><STRONG>Mean Absolute Deviation (MAD):</STRONG></FONT>
A simple measure of variability is the mean absolute deviation:
<P><CENTER>MAD = <FONT face=symbol>S</FONT> |(x<FONT 
  size=+0><SUB>i</SUB></FONT> - <IMG src="xbaru.gif"> )| / n. </CENTER>
<p>
 The mean absolute deviation is widely used as a performance measure to assess the quality of the modeling,  such <A   href="otherapplets/ForecaSmo.htm"  target=new>forecasting techniques</a>.  However, MAD does not lend itself to further use in making inference;  moreover, even in the error analysis studies, the variance is preferred since variances of independent (i.e., uncorrelated) errors are additive; however MAD does not have such a nice feature.
<p>
The MAD is a simple measure of variability, which unlike range and quartile deviation, takes every item into account, and it is simpler and less affected by extreme deviations.  It is therefore often <FONT color=#dc143c>used in small samples that include extreme values</font>.
<p>The mean absolute deviation theoretically should be measured from the median, since it is at its minimum;  however, it is more convenient to measure the deviations from the mean.   <p>As a numerical example, consider the price (in $) of same item at 5 different stores:  $4.75, $5.00, $4.65, $6.10, and $6.30.  The mean absolute deviation from the mean is $0.67, while from the median is $0.60, which is a better representative of deviation among the prices.
<p>
 <P><FONT color=#dc143c><STRONG>Variance:</STRONG></FONT> An important measure  of variability is variance. Variance is the <B>average</B> of the <B>squared deviations </B>of each observation in the set from the arithmetic mean of all of the observations. 
<P><CENTER>Variance = <FONT face=symbol>S</FONT> (x<FONT 
  size=+0><SUB>i</SUB></FONT> - <IMG src="xbaru.gif"> ) <FONT size=+0><SUP>2</SUP></FONT> 
  / (n - 1), &nbsp; &nbsp; where n is at least 2. </CENTER><P>The variance is a measure of spread or dispersion among values in a data set. Therefore, <FONT color=#dc143c>the greater the variance, the lower the quality</FONT>.   <P>The variance is <B>not expressed in the same units as the observations.</B>  In other words, the variance is hard to understand because the deviations from the mean are squared, making it too large for logical explanation. This  problem can be solved by working with the <B>square root</B> of the variance,  which is called the <B>standard deviation.</B> <P><FONT color=#dc143c><B>Standard Deviation:</B></FONT> Both variance and standard deviation provide the same information; <B><FONT color=#dc143c>one can always be obtained from the other</FONT></B>. In other words, the process of computing a standard deviation always involves computing a variance. Since standard deviation is the square root of the variance, it is always expressed in the 
    <B>same units</B> as the raw data: <P><CENTER>Standard Deviation = S = (Variance) <FONT size=+0><SUP>½</SUP></FONT></CENTER><P>For large data sets (say, more than 30), approximately 68% of the data are contained within one standard deviation of the mean, 95% fall within two standard deviations.  97.7% (or almost 100% ) of the data are contained within within three standard deviations (S) from the mean. <P>You may use <A 
  href="otherapplets/Descriptive.htm"   target=new>Descriptive Statistics</A> Applet to compute the mean, and standard deviation.  
<P><FONT color=#dc143c><B>The Mean Square Error (MSE)</B></FONT> of an estimate is the variance of the estimate plus the square of its bias; therefore, if an estimate is unbiased, then its MSE is equal to its variance, as it is the case in the ANOVA table. 
  <P><FONT color=#dc143c><B>Coefficient of Variation</B></FONT>: Coefficient of Variation (CV) is the <I>absolute relative deviation </I>with respect to size <IMG src="xbaru.gif">, provided <IMG   src="xbaru.gif"> is not zero, expressed in percentage: <P> <CENTER>CV =100 |S/<IMG src="xbaru.gif">| % </CENTER>
  <P>CV is independent of the unit of measurement. In estimation of a parameter, when its CV is less than 10%, the estimate is assumed acceptable. The inverse of CV; namely, 1/CV is called the <B>Signal-to-noise Ratio</B>. <P>The coefficient of variation is used to represent the relationship of the standard deviation to the mean, telling how representative the mean is of the numbers from which it came. It expresses the standard deviation as a percentage of the mean; i.e., it reflects the variation in a distribution relative to the mean. However, confidence intervals for the coefficient of variation are rarely reported. One of the reasons is that the exact confidence interval for the coefficient of variation is computationally tedious.
<p>
Note that, for a skewed or grouped data set, the <FONT color=#dc143c><STRONG>coefficient of quartile</STRONG></FONT> variation:
<p>
<center>
V<FONT><SUB>Q</SUB></FONT> = 100(Q<FONT><SUB>3</SUB></FONT> - Q<FONT><SUB>1</SUB></FONT>)/(Q<FONT><SUB>3</SUB></FONT> + Q<FONT><SUB>1</SUB></FONT>)%
<p>
</center>
is more useful than the CV.
<p> <P>You may use <A 
  href="otherapplets/Descriptive.htm" 
  target=new>Descriptive Statistics</A> Applet to compute the mean, standard deviation and the coefficient of variation.<p> 
<P><FONT color=#dc143c><B>Variation Ratio for Qualitative Data:</B></FONT> Since the mode is the most frequently used measure of central tendency for qualitative variables, variability is measured with reference to the mode. The statistic that describes the variability of quantitative data is the Variation Ratio (VR):
 <P><CENTER>VR = 1 -  f<FONT><SUB>m</SUB></FONT>/n,</CENTER><P>
where f<FONT><SUB>m</SUB></FONT> is the frequency of the mode, and n is the total number of scores in the distribution.

 <P><FONT color=#dc143c><STRONG>Z Score:</STRONG></FONT> how many standard deviations a given point (i.e., observation) is above or below the mean. In other words, a <FONT  color=#dc143c><B>Z</B></FONT> score represents the number of standard deviations that an observation (x) is <I>above or below</I> the mean. <I>The larger the Z value, the further away a value will be from the mean</I>. Note that values beyond three standard deviations are very unlikely. Note that if a Z score is negative, the observation (x) is below the mean. If the Z score is positive, the observation (x) is above the mean. The Z score is found as: <P> 
    <CENTER>Z = (x - <IMG src="xbaru.gif">) / standard deviation of X </CENTER><P>The Z score is a measure of the number of standard deviations that an observation is above or below the mean. Since the standard deviation is never negative, a positive Z score indicates that the observation is above the mean, a negative 
    Z score indicates that the observation is below the mean. Note that Z is a dimensionless value, and therefore is a useful measure by which to compare data values from two different populations, even those measured by different units. <P><FONT color=#dc143c><STRONG>Z-Transformation:</STRONG></FONT> Applying the 
    formula z = (X - <FONT face=symbol>m</FONT>) / <FONT face=symbol>s </FONT>will always produce a transformed variable with a mean of zero and a standard deviation of one. However, the shape of the distribution will not be affected by the  transformation. If X is not normal, then the transformed distribution will 
    not be normal either.  <p>
One of the nice features of the z-transformation is that the resulting distribution of the transformed data has an <B>identical shape</B> but with mean zero, and standard deviation equal to 1. <p>One can generalize this data transformation to have any desirable mean and standard deviation other than 0 and 1, respectively.    Suppose we wish the transformed data to have the mean and standard deviation of M and D, respectively. For example, in the SAT Scores, they are set at M = 500, and D=100.  The following transformation should be applied:<p><center>Z = (standard Z) <font face=symbol>&#180;</font> D + M</center> <p>Suppose you have two data sets with very different scales (e.g., one has very low values, another very high values). If you wish to compare these two data sets, due to differences in scales, the statistics that you generate are not comparable. It is a good idea to use the Z-transformation of both original data sets and then make any comparison. <P>You have heard the terms <FONT color=#dc143c>z value, z test, z transformation, 
    and z score</FONT>. Do all of these terms mean the same thing? Certainly not: <P>The <FONT color=#dc143c>z value</FONT>  refers to the critical value (a point on the horizontal axes) of the Normal (0, 1) density function, for a given area to the left of that z-value. <P>The <FONT color=#dc143c>z test</FONT>   refers  to the procedures for testing the equality of mean (s) of one (or two) population(s). <P> The <FONT color=#dc143c>z score</FONT> of a given observation x, in a sample of size n, is simply (x - average of the sample) divided by the standard deviation of the sample. One must be careful not to mistake  z scores for the Standard Scores. <P>The <FONT color=#dc143c>z transformation</FONT> of a set of observations of size n is simply (each observation - average of all observations) divided by the standard deviation among all observations. The aim is to produce a  transformed data set with a mean of zero and a standard deviation of one.  This makes the transformed set dimensionless and manageable with respect to its magnitudes. It is  used also in comparing several data sets that have been measured using different scales of measurements. <P><A 
  href="http://www-history.mcs.st-and.ac.uk/~history/Mathematicians/Pearson.html"   target=new>Pearson</A> coined the term "standard deviation" sometime near 1900.   The idea of using squared deviations goes back to <A  href="http://www-history.mcs.st-and.ac.uk/~history/Mathematicians/Laplace.html"    target=new>Laplace</A> in the early 1800's. <P>Finally, notice again, that the transforming raw scores to z scores do NOT normalize the data.  <P><FONT color=#dc143c><B>Computation of Descriptive Statistics for Grouped Data:</B></FONT> One of the most common ways to describe a single variable is with a frequency distribution. A histogram is a graphical presentation of an estimate for the frequency distribution of the population. Depending upon the particular variable, all of the data values may be represented, or you may group the values into categories first (e.g., by age). It would usually not be sensible to determine the frequencies for each value. Rather, the values are grouped into ranges, and the frequency is then determined.). Frequency distributions can be depicted in two ways: as a table or as a graph that is often referred to as a histogram or bar chart. The bar chart is often used to show the relationship between two categorical variables. 
  <P>Grouped data is derived from raw data, and it consists of frequencies (counts of raw values) tabulated with the classes in which they occur. The Class Limits represent the largest (Upper) and lowest (Lower) values which the class will contain. The formulas for the descriptive statistic becomes much simpler for the grouped data, as shown below for Mean, Variance, Standard Deviation, respectively, where (f) is for the frequency  of each class, and n is the total frequency:   <P> 
  <CENTER><IMG src="Meangroup.gif"> <P><IMG src="VarianceGroup.gif"> <P><IMG src="StanDevGroup.gif"> 
  </CENTER><P><P><A name="rselectingdisp"></A><HR><H4><FONT color=#dc143c>Selecting Among the Quartile Deviation, Mean Absolute Deviation, and Standard Deviation</FONT></H4>
A general guideline for selecting a suitable statistic in describing the dispersion in a population includes consideration of the following factors:
<p>
<ol>
<li>The concept of dispersion required by the problem. Is a single pair of values adequate, such as the two extremes or the two quartiles (range or Q)?  
<p>
<li>The type of data available. If they are few in numbers, or contain extreme value, avoid the standard deviation. If they are generally skewed, avoid the mean absolute deviation as well. If they have a gap around the quartile, the quartile deviation should be avoided.
<p>
<li>The peculiarity of the dispersion measures themselves. These are summarized under "The Main Characteristics of the Quartile Deviation, the Mean Absolute Deviation, and the Standard deviation" below.
</ol>

 <P></P>    </font></font></font></DD></BLOCKQUOTE><FONT size=+0><FONT size=+0><FONT size=+0> 
<P> <CENTER><TABLE border=3><FONT color=#dc143c size=3><B>The Main Characteristics of the Quartile Deviation, the Mean Absolute Deviation, and the Standard Deviation</B></FONT> <TBODY> <TR><TH>Fact No.<TH>The Quartile Deviation<TH>The Mean Absolute Deviation<TH>The Standard Deviation<TR> <TD>1<TD>The quartile deviation is also easy to calculate and to understand. However, it is unreliable if there are gaps in the data around the quartiles.<TD>The mean absolute deviation has the advantage of giving equal weight to the deviation of every value form the mean or median.<TD>The standard deviation is usually more useful and better adapted to further analysis than the mean absolute deviation.<TR> <TD>2<TD>It depends on only 2 values, which include the middle half of the items. <TD>Therefore, it is a more sensitive measure of dispersion than those described above and ordinarily has a smaller sampling error. <TD> It is more reliable as an estimator of the population dispersion than other measures, provided the distribution is normal.<TR> <TD>3 <TD>It is usually superior to the range as a rough measure of dispersion. <TD>It is also easier to compute and to understand and is less affected by extreme values than the standard deviation.   <TD>It is the most widely used measure of dispersion and the easiest to handle algebraically. <TR> <TD>4 <TD>It may be determined in an open-end distribution, or one in which the data may be ranked but not measured quantitatively.<TD>Unfortunately, it is difficult to handle algebraically, since minus signs must be ignored in its computation. <TD>Compared with the others, it is harder to compute and more difficult to understand.<TR> <TD>5 <TD>It also useful in badly skewed distributions or those in which other measures of dispersion would be warped by extreme values. <TD>Its main application is in modeling accuracy for comparative forecasting techniques. <TD>It is generally affected by extreme values that may be due to skewness of data<TR></TR></TBODY>  </TABLE></CENTER><P> <BLOCKQUOTE> <P>
<p>You might like to use the <a href="otherapplets/Descriptive.htm" target ="new">Descriptive Sampling Statistics</a> JavaScript applet in performing some numerical experimentation for validating the above assertions for a deeper understanding. 
<p>
<P><A name=rskewKur></A><HR><H4><FONT color=#dc143c>Shape of a Distribution Function:<BR> The Skewness-Kurtosis Chart</FONT></H4>The pair of statistical measures, skewness and kurtosis, are measuring tools, which is used in selecting a distribution(s) to fit your data. To make an inference with respect to the population distribution, you may first compute skewness and kurtosis from your random sample from the entire population. Then, locating a point with these coordinates on the widely used <A href="SkewKurchart.pdf"  target=new>skewness-kurtosis chart </A>,  guess a couple of possible distributions to fit your data. Finally, you might use the goodness-of-fit test to rigorously come up with the best candidate fitting your data. Removing <A  href="#routlier">outliers</a> improves  the accuracy of both skewness and kurtosis.  <P><FONT color=#dc143c><STRONG>Skewness:</STRONG></FONT> Skewness is a measure of the degree to which the sample population deviates from symmetry with the mean at the center. 
  <P><CENTER>Skewness = <FONT face=symbol>S</FONT> (x<FONT  size=+0><SUB>i</SUB></FONT> - <IMG src="xbaru.gif"> ) <FONT size=+0><SUP>3</SUP></FONT>  / [ (n - 1) S <FONT  size=+0><SUP>3</SUP></FONT> ],&nbsp; &nbsp; n is at least 2. </CENTER><P>Skewness will take on a value of zero when the distribution is a symmetrical curve. A positive value indicates the observations are clustered more to the left of the mean with most of the extreme values to the right of the mean. A negative skewness indicates clustering to the right. In this case we have:  Mean <FONT face=symbol>£</FONT> Median <FONT face=symbol>£</FONT> Mode. The 
    reverse order holds for the observations with positive skewness.   <P><FONT color=#dc143c><STRONG>Kurtosis:</STRONG></FONT> Kurtosis is a measure of the relative peakedness of the curve defined by the distribution of the  observations.  <P><CENTER>Kurtosis = <FONT face=symbol>S</FONT> (x<FONT   size=+0><SUB>i</SUB></FONT> - <IMG src="xbaru.gif"> ) <FONT size=+0><SUP>4</SUP></FONT> 
      / [ (n - 1) S <FONT  size=+0><SUP>4</SUP></FONT> ], &nbsp; &nbsp; n is at least 2. </CENTER>
  <P>Standard normal distribution has kurtosis of +3. A kurtosis larger than 3 indicates the distribution is more peaked than the standard normal distribution. <P>  <CENTER>Coefficient of Excess Kurtosis = Kurtosis - 3.
    </CENTER><P>A value of less than 3 for kurtosis indicates that the distribution is flatter than the standard normal distribution. <P>It can be shown that,  <P><CENTER>Kurtosis - Skewness <FONT size=+0><SUP>2</SUP></FONT>&nbsp;is greater than or equal to 1, and<BR>&nbsp; &nbsp;&nbsp;Kurtosis&nbsp;is less than or equal to the sample size n .  </CENTER><P>These inequalities hold for any probability distribution having finite skewness and kurtosis. <P>In the <B>Skewness-Kurtosis Chart</B>, you notice two useful families of distributions, namely the beta and gamma families. <P><A name=rBeta></A>   <P><FONT color=#dc143c><B>The Beta-Type Density Function:</B></FONT> Since the beta density has both a shape and a scale parameter, it describes many random phenomena provided the <A   href="#rrandomva">random variable</A> is between [0, 1]. For example, when both parameters are integer 
    with random variables the result is the binomial Probability function. <P><B>Applications:</B> A basic distribution of statistics for variables bounded at both sides; for example x between [0, 1]. The beta density  is useful for both theoretical and  applied problems in many areas. Examples include distribution of proportion of population located between lowest and highest value in sample; distribution of daily per cent yield in a manufacturing process; description of elapsed  times to task completion (PERT). There is also a relationship between the  Beta and Normal distributions. The conventional calculation is that given a PERT Beta with highest value as b, lowest as a, and most likely as m, the equivalent normal distribution has a mean and mode of (a + 4M + b)/6 and a standard deviation of (b - a)/6.  <P><B>Comments:</B> Uniform, right triangular, and parabolic distributions are special cases. To generate beta, generate two random values from a gamma, g<FONT size=+0><SUB>1</SUB></FONT>,  g<FONT size=+0><SUB>2</SUB></FONT>. The ratio g<FONT size=+0><SUB>1</SUB></FONT>/(g<FONT size=+0><SUB>1</SUB></FONT> +g<FONT size=+0><SUB>2</SUB></FONT>) is distributed like a beta distribution.     The beta distribution can also be thought of as the distribution of X1 given     (X1+X2), when X1 and X2 are independent gamma random variables. 
<P><A name=rGamma></A> <P><FONT color=#dc143c><B>Gamma-Type Density Function:</B></FONT> Some <A  href="#rrandomva">random variables</A> are always non-negative. The density function associated with these random variables often is adequately modeled as the gamma density function. The Gamma-Type Density Function has both a shape and a scale parameter. With both the shape and scale parameters equal to 1, the result is the exponential density function. Chi-square is also a special case of gamma density function with shape parameter equal to 2. <P><B>Applications:</B> A basic distribution of statistics for variables bounded at one side ; for example x greater than or equal to zero.  The gamma density gives distribution of time required for exactly k independent events to occur, assuming events take place at a constant rate. Used frequently in queuing theory, reliability, and other industrial applications. Examples include distribution of time between re-calibrations of instrument that needs re-calibration after k uses; time between inventory restocking, time to failure for a system with standby components. <P><B>Comments:</B> Erlangian, Exponential, and Chi-square distributions are special cases. The negative binomial is an analog to gamma distribution with discrete  <A   href="#rrandomva">random variable</A>. <P>What is the distribution of the product of sample observations  
from the uniform (0, 1) random? Like  many problems with products, this becomes a familiar problem when turned into  a problem about sums. If X is uniform (for simplicity of notation make it U(0,1)), Y=-log(X) is exponentially distributed, so the log of the product  of X1, X2, ... Xn is the sum of Y1, Y2, ... Yn which has a gamma (scaled Chi-square)  distribution. Thus, it is a gamma density with shape parameter n and scale  1. 
  <P><A name=rLog-normal></A> <P><FONT color=#dc143c><B>The Log-normal Density Function:</B></FONT> Permits representation of  a <A  href="#rrandomva">random variable</A> whose logarithm follows a normal distribution. The ratio of two log-normally random variables is also log-normal. <P>Applications: Model for a process arising from many small multiplicative  errors. Appropriate when the value of an observed variable is a random proportion  of the previously observed value. <p><B>Applications:</B> Examples include distribution of sizes from a breakage process; distribution of income size, inheritances and bank deposits; distribution of various biological phenomena; life distribution of some transistor types. 
<p>
The lognormal distribution is widely used in situations where values are positively skewed (where the distribution has a long right tail; negatively skewed distributions have a long left tail; a normal distribution has no skewness). Examples of data that "fit" a lognormal distribution include financial security valuations or real estate property valuations. Financial analysts have observed that the stock prices are usually positively skewed, rather than normally (symmetrically) distributed. Stock prices exhibit this trend because the stock price cannot fall below the lower limit of zero but may increase to any price without limit. Similarly, healthcare costs illustrate positive skewness since unit costs cannot be negative. For example, there can't be negative cost for services in a capitation contract. This distribution accurately describes most healthcare data.
<P>In the case where the data are log-normally distributed, the <A href="#rspecialmean">Geometric Mean</A> acts as a better data descriptor than the mean. The more closely the data  follow a log-normal distribution, the closer the geometric mean is to the median, since the log re-expression produces a symmetrical distribution.  <P> <P><B>Further Reading:</B><BR><FONT face="Bookman Old Style" size=-2>Snell J., <I>Introduction to Probability</I>,  Random House, 1987. Read section 4.2 for a link between beta and F distributions  (with the advantage that tables are easy to find).<BR> Tabachnick B., and L. Fidell, <I>Using Multivariate Statistics</I>, HarperCollins,  1996. Has a good discussion on applications and significance tests for skewness  and kurtosis.<BR></FONT><BR><P><A name=rexamdisc></A><HR>
  <H4><FONT color=#dc143c>Numerical Example and Discussions</FONT></H4><FONT 
  color=#dc143c><B>A Numerical Example:</B></FONT> Given the following, small (n = 4) data set, compute the descriptive statistics: x<FONT  size=+0><SUB>1</SUB></FONT> = 1, x<FONT size=+0><SUB>2</SUB></FONT> = 2, x<FONT size=+0><SUB>3</SUB></FONT> = 3, and x<FONT size=+0><SUB>4</SUB></FONT> = 6. 
  <P><CENTER><TABLE border=1 borderColor=#dddddd cellSpacing=0><TBODY><TR><TD> <CENTER>i          </CENTER></TD><TD><CENTER>x<FONT size=+0><SUB>i</SUB></FONT></CENTER></TD><TD>           <CENTER>( x<SUB>i</SUB>- <IMG src="xbaru.gif"> )</CENTER></TD><TD><CENTER>( x<FONT size=+0><SUB>i</SUB></FONT> - <IMG         src="xbaru.gif"> ) <FONT  size=+0><SUP>2</SUP></FONT>
</CENTER></TD><TD>  <CENTER> ( x<SUB>i</SUB> - <IMG src="xbaru.gif"> ) <FONT size=+0><SUP>3</SUP></FONT> </CENTER></TD><TD><CENTER> ( x<SUB>i</SUB> - <IMG src="xbaru.gif"> )<FONT size=+0><SUP>4</SUP></FONT> </CENTER>  </TD> </TR><TR> <TD><CENTER>1</CENTER></TD>        <TD><CENTER>1</CENTER></TD><TD><CENTER> -2 </CENTER> </TD> <TD><CENTER>4</CENTER></TD>        <TD><CENTER> -8 </CENTER></TD><TD><CENTER>16</CENTER></TD></TR><TR><TD><CENTER>2</CENTER>   </TD><TD>  <CENTER>2</CENTER></TD><TD><CENTER> -1 </CENTER>  </TD> <TD> <CENTER>1</CENTER>      </TD><TD>  <CENTER> -1</CENTER></TD><TD> <CENTER>1</CENTER></TD></TR><TR><TD><CENTER> 3          </CENTER> </TD><TD> <CENTER> 3 </CENTER>  </TD><TD> <CENTER> 0 </CENTER>  </TD> <TD>       <CENTER>  0</CENTER> </TD> <TD> <CENTER> 0 </CENTER> </TD> <TD> <CENTER> 0</CENTER>  </TD>      </TR> <TR> <TD> <CENTER> 4 </CENTER></TD><TD> <CENTER> 6 </CENTER> </TD> <TD>  <CENTER> 3  </CENTER></TD><TD><CENTER>  9</CENTER></TD>  <TD> <CENTER> 27 </CENTER></TD> <TD> 
<CENTER> 81</CENTER></TD></TR><TR><TD><CENTER>Sum</CENTER> </TD> <TD> <CENTER>12 </CENTER> </TD><TD><CENTER>0</CENTER></TD><TD><CENTER>14 </CENTER> </TD> <TD> <CENTER>18 </CENTER>        </TD><TD> <CENTER> 98</CENTER></TD></TR></TBODY></TABLE></CENTER>  <P>The mean <IMG src="xbaru.gif"> is 12 / 4 = 3; the variance is s<FONT size=+0><SUP>2</SUP></FONT>     = 14 / 3 = 4.67;  the standard deviation is s = (14/3) <FONT size=+0><SUP>0.5</SUP></FONT>     = 2.16; the skewness is 18 / [3 (2.16) <FONT size=+0><SUP>3</SUP></FONT> ]     = 0.5952, and finally, the kurtosis is 98 / [3 (2.16) <FONT size=+0><SUP>4</SUP></FONT>] = 1.5.  <P>You might like to use <A   href="otherapplets/Descriptive.htm"   target=new>Descriptive Statistics</A>   Applet to check your hand computation.   <P><FONT color=#dc143c><B>A Short Discussion on the Descriptive Statistic:</B></FONT>   <P>Deviations about the mean <FONT face=symbol>m </FONT>of a distribution is  the basis for most of the statistical tests we will learn. Since we are measuring     how much a set of scores is dispersed about the mean <FONT   face=symbol>m </FONT>, we are measuring <B>variability</B>. We can calculate     the deviations about the mean <FONT face=symbol>m </FONT>and express it as     variance <FONT face=symbol>s</FONT><FONT size=+0><SUP>2</SUP></FONT> or standard     deviation <FONT face=symbol>s</FONT>.<B> It is very important to have a firm     grasp of this concept because it will be a central concept throughout your statistics course</B>.  <P>Both variance <FONT face=symbol>s</FONT><FONT size=+0><SUP>2</SUP></FONT>  and standard deviation <FONT face=symbol>s</FONT> measure variability within a distribution. Standard deviation <FONT face=symbol>s</FONT> is a number  that indicates how much on average each of the values in the distribution 
    deviates from the mean <FONT face=symbol>m </FONT>(or center) of the distribution. Keep in mind that variance <FONT face=symbol>s</FONT><FONT  size=+0><SUP>2</SUP></FONT> measures the same thing as standard deviation <FONT face=symbol>s</FONT>  (dispersion of scores in a distribution). Variance <FONT face=symbol>s</FONT><FONT size=+0><SUP>2</SUP></FONT>, however, is the average squared deviations about the mean <FONT face=symbol>m </FONT>. Thus, variance <FONT face=symbol>s</FONT><FONT size=+0><SUP>2</SUP></FONT>  is the square of the standard deviation <FONT face=symbol>s</FONT>. 
  <P>The expected value and the variance of the statistic  <IMG  src="xbaru.gif"> are <FONT face=symbol>m</FONT> and <FONT face=symbol>s</FONT><FONT size=+0><SUP>2</SUP></FONT>/n,  respectively.   <P>The expected value and variance of statistic S<FONT size=+0><SUP>2</SUP></FONT> are <FONT   face=symbol>s</FONT><FONT size=+0><SUP>2</SUP></FONT> and 2<FONT   face=symbol>s</FONT><FONT size=+0><SUP>4</SUP></FONT> / (n-1), respectively.  <P><IMG src="xbaru.gif"> and S<FONT  size=+0><SUP>2</SUP></FONT> are the best estimators for <FONT face=symbol>m </FONT>and <FONT face=symbol>s</FONT><FONT size=+0><SUP>2</SUP></FONT>. They are Unbiased (you may update your estimate); Efficient (they have the smallest  variation among other estimators); Consistent (increasing sample size provides a better estimate); and Sufficient (you do not need to have the whole data  set; what you need are <FONT face=symbol>S</FONT>x<FONT  size=+0><SUB>i</SUB></FONT> and <FONT face=symbol>S</FONT>x<FONT 
  size=+0><SUB>i</SUB></FONT><FONT size=+0><SUP>2</SUP></FONT> for estimations). Note also that the above variance S<FONT size=+0><SUP>2</SUP></FONT> is justified only in the case where the population distribution tends to be normal, otherwise one may use bootstrapping techniques. <P>In general, it is believed that the pattern of mode, median, and mean go from lower to higher in positive skewed data sets, and just the opposite pattern in negative skewed data sets. However; for example, in the following 23 numbers,  mean = 2.87, median = 3, but the data is positively skewed:  <P>  <CENTER> 4, 2, 7, 6, 4, 3, 5, 3, 1, 3, 1, 2, 4, 3, 1, 2, 1, 1, 5, 2, 2, 3, 1 </CENTER><P>and, the following 10 numbers have mean = median = mode = 4, but the data set is  left skewed: <P> <CENTER> 1, 2, 3, 4, 4, 4, 5, 5, 6, 6.</CENTER><P>Note also,  that most commercial software do not correctly compute <A  href="#rskewKur">skewness and kurtosis</A>. There is no easy way to determine confidence intervals about     a computed skewness or kurtosis value from a small to medium sample. The literature     gives tables based on asymptotic methods for sample sets larger than 100 for     normal distributions only.   <P>You may have noticed that using the above numerical example on some computer     packages such as SPSS, the skewness and the kurtosis are different from what     we have computed. For example, the SPSS output for the skewness is 1.190.     However, for large a sample size n, the results are identical.   <P><B>Reference and Further Readings:</B><BR> <FONT face="Bookman Old Style" 
  size=-2>David H., Early sample measures of variability, <I>Statistical Science</I>,  Vol. 13, 1998, 368-377. This article provides a good historical account of  statistical measures.<BR>  Groeneveld R., A class of quantile measures for kurtosis, <I>The American Statistician</I>, 325, Nov. 1998.<BR>  Lehmann E., <I>Testing Statistical Hypotheses</I>, 1996, Wiley. Exact confidence interval for the coefficient of variation is computationally tedious as shown in this  book.<BR>    </FONT>   <P> 

<P><A name=rdensityDist></A> 
  <HR>
  <H4><FONT color=#dc143c>The Two Statistical Representations of a Population</FONT></H4>
 The following figure depicts a typical relationship between the cumulative distribution function (cdf) and the density (for continuous <A  href="#rrandomva">random variables</A>),    <P> <CENTER><IMG alt="The Two representations of a population"   src="CdfAndPdf.gif"> </CENTER>  <P>All characteristics of the population are well described by either of these two functions. The figure also illustrates their applications in determining the (lower) percentile measures denoted by <B>P</B>:<P><CENTER><B>P</B> = P[ X <FONT face=symbol>£</FONT> x] = Probability that the random variable<BR>X is less than or equal to a given number x,</CENTER>  <P>among other useful information. Notice that the probability P is the area  under the density function curve, while numerically equal to the height of cdf curve at point x.   <P>Both functions can be estimated by <I>smoothing the empirical (i.e., observed) cumulative step-function</I>, and <I>smoothing the histogram</I> constructed from a  random sample.
<P><A name=rcdffunc></A> 
  <HR>
  <H4><FONT color=#dc143c>Empirical (i.e., observed) Cumulative Distribution Function</FONT></H4>
<dd> The empirical cumulative distribution function (ECDF),  also  known as <FONT color=#dc143c><B>Ogive</B></FONT> (pronounced o-jive), is used to graph cumulative frequency. <P>The ogive is the estimator for the population's cumulative distribution function, which contains all the characteristic of the population.  The empirical distribution is a staircase function with the location of the drops randomly placed. The size of the each stair at each  point depends on the frequency of that point value, and it is equal to the <FONT color=#dc143c><B>frequency/n</B></FONT> where n is the sample size. The sample size is the sum of all frequencies. 
<P>Note that almost all statistics we have covered up to now can be obtained and understood more deeply  by <FONT color=#dc143c>graph paper</FONT> using <A 
  href="otherapplets/ECDF.htm"  target=new>Empirical Distribution Function</A> JavaScript. You may like using this JavaScript in performing some numerical experimentation for a deeper understanding.
<p>
Other widely used decision model based upon empirical
 cumulative distribution function (ECDF) as a measuring tool and decision procedure are the <a href="otherapplets/ABClass.htm" target="new">ABC Inventory Classification</a>, <a href="otherapplets/Newsboy.htm" target="new">Single-period Inventory Analysis (The Newsboy Model)</a>, and determination of the <a href="otherapplets/Replacement.htm" target="new">Best Time to Replace Equipment</a>. For other inventory decisions, visit  the <a href="otherapplets/Inventory.htm" target = "new">Inventory Control Models</a> site.


<p>
<P><A name=rintropclao></A><HR><H4><FONT color=#dc143c>Introduction</FONT></H4> <B>Modeling of a Data Set:</B> Families of parametric distribution models are widely used to <FONT  color=#dc143c>summarize a huge data set</FONT>, to obtain predictions,  assess goodness of fit, to estimate functions of the data not easily derived directly, or to render manageable random effects. The trustworthiness of the results obtained depends on the generality of the distribution family employed. 
  <P><B>Inductive Inference:</B> This extension of our knowledge from a particular random sample to the population is called inductive inference. The main function of business statistics is the provision of techniques for making inductive <A  href="#rstatInferentia">inference</A> and for measuring the degree of uncertainty of such inference. Uncertainty is measured in terms of <A  href="#rproby">probability</A> statements, and that is the reason we need to <FONT color=#dc143c>learn the language of uncertainty and its measuring tool called probability.</FONT><P>In contrast to the inductive inference, mathematics often uses deductive inference to prove theorems, while in empirical science, such as statistics, inductive inference is used to find new knowledge or to extend our knowledge. <P><B>Further Readings</B>:<BR><FONT face="Bookman Old Style" size=-2>Brown B., F. Spears, and L. Levy, The log F: A distribution for all seasons, <I>Computational Statistics</I>, 17(1), 47-58, 2002.<BR></FONT><P><A name=rpclao></A> <HR><H4><FONT color=#dc143c>Probability, Chance, Likelihood, and Odds</FONT></H4>The concept of probability occupies an important place in the decision-making process under uncertainty, whether the problem is one faced in business, in government, in the social sciences, or just in one's own everyday personal life. In very few decision-making situations is perfect information -- all the needed facts -- available. <FONT color=#dc143c>Most decisions are made in the face of uncertainty.</FONT> Probability enters into the process by playing the role of a substitute for certainty - a substitute for complete knowledge. <P><FONT color=#dc143c>Probability</FONT> is especially significant in the area of <A  href="#rstatInferentia">statistical inference</A>. Here the statistician's prime concern lies in drawing conclusions or making <A  href="#rInferentiaStatist">inferences</A> from <A   href="#rstaexper">experiments</A>  which involve uncertainties. The concepts of probability make it possible for  the statistician to generalize from the known (sample) to the unknown (population) and to place a high degree of confidence in these generalizations. Therefore, <FONT color=#dc143c>Probability is one of the most 
  important tools of statistical inference</FONT>. <P>Probability has an exact technical meaning -- well, in fact it has several, and there is still debate as to which term ought to be used. However, for most  events for which probability is easily computed; e.g., rolling of a die, the probability of getting a four [::], almost all agree on the actual value (1/6), if not the philosophical interpretation. A probability is always a number between 0 and 1. Zero is not "quite" the same thing as impossibility.  It is possible that "if" a coin were flipped infinitely many times, it would never show "tails", but the probability of an infinite run of heads is 0. One is not "quite" the same thing as certainty but close enough.  <P>The word "chance" or "chances" is often used as an approximate synonym of  "probability", either for variety or to save syllables. It would be better practice to leave "chance" for informal use, and say "probability" if that is what is meant.  One occasionally sees "likely" and "likelihood"; however, these terms are used casually as synonyms for "probable" and "probability".   <P><FONT color=#dc143c>Odds</FONT> is a probabilistic concept related to probability. It is the ratio of the probability (p) of an event to the probability (1-p) that it does not happen: p/(1-p). It is often expressed as a 
  ratio, often of whole numbers; e.g., "odds" of 1 to 5 in the die example above, but for technical purposes the division may be carried out to yield a positive real number (here 0.2). Odds are a ratio of nonevents to events. If the event rate for a disease is 0.1 (10 per cent), its nonevent rate is 0.9 and therefore its odds are 9:1.  
  <P>Another way to compare probabilities and odds is using "part-whole thinking" with a binary (dichotomous) split in a group. A probability is often a ratio of a part to a whole; e.g., the ratio of the part [those who survived 
  5 years after being diagnosed with a disease] to the whole [those who were diagnosed with the disease]. Odds are often a ratio of a part to a part; e.g., the odds against dying are the ratio of the part that succeeded [those who survived 5 years after being diagnosed with a disease] to the part that  'failed' [those who did not survive 5 years after being diagnosed with a  disease].   <P>Aside from their value in betting, odds allow one to specify a small   probability (near zero) or a large probability (near one) using large whole  numbers (1,000 to 1 or a million to one). Odds magnify small probabilities (or  large probabilities) so as to make the relative differences visible. Consider two probabilities: 0.01 and 0.005. They are both small. An untrained observer might not realize that one is twice as much as the other. But if expressed as odds (99 to 1 versus 199 to 1) it may be easier to compare the two situations by focusing on large whole numbers (199 versus 99) rather than on small ratios or fractions. 
 <P><A name=rhowprob></A><HR><H4><FONT color=#dc143c>How to Assign Probabilities?</FONT></H4>  Probability is an instrument to measure the likelihood of the occurrence of an event. There are five major approaches of assigning probability: Classical Approach, Relative Frequency Approach, Subjective Approach, Anchoring, and the Delphi Technique: </BLOCKQUOTE><OL><LI><FONT color=#dc143c>Classical Approach</FONT>: Classical probability is predicated on the condition that the outcomes of an experiment are equally likely to happen. The classical probability utilizes the idea that the lack of knowledge implies that all possibilities are equally likely. The classical probability is applied when the events have the same chance of occurring (called equally likely events), and the sets of events are mutually exclusive and collectively exhaustive. The classical probability is defined as: <P><CENTER>P(X) = Number of favorable outcomes / Total number of possible outcomes</CENTER><P>  <LI><FONT color=#dc143c>Relative Frequency Approach</FONT>: Relative probability is based on accumulated historical or experimental data. Frequency-based probability is defined as: <P><CENTER>P(X) = Number of times an event occurred / Total number of  opportunities for the event to occur.</CENTER> <P>Note that relative probability is based on the ideas that what has happened 
  in the past will hold. <p><LI><FONT color=#dc143c>Subjective Approach</FONT>: The subjective probability is based on personal judgment and experience. For example, medical doctors sometimes assign subjective probability to the length of life expectancy for a person who has cancer.  <P></P><LI><FONT color=#dc143c>Anchoring:</FONT> is the practice of assigning a value obtained from a prior experience and adjusting the value in consideration of current expectations or circumstances <LI><FONT color=#dc143c>The Delphi Technique:</FONT> It consists of a series of questionnaires. Each series is one "round". The responses from the first "round" are gathered and become the basis for the questions and feedback of the second "round". The process is usually repeated for a predetermined number of "rounds" or until the responses are such that a pattern is observed. This process allows expert opinion to be circulated to all members of the group and eliminates the bandwagon effect of majority opinion. <P>Delphi Analysis is used in decision making processes, in particular in forecasting. Several "experts" sit together and try to compromise on something upon which they cannot agree. </P></LI></OL><BLOCKQUOTE> <P><B>Further Reading:</B><BR><FONT face="Bookman Old Style" size=-2>Delbecq, A., <I>Group Techniques for Program Planning</I>, Scott Foresman, 1975. </FONT><P><A name=rlawofProb></A><HR><H4><FONT color=#dc143c>General Laws of Probability</FONT></H4></BLOCKQUOTE><OL><LI><FONT color=#dc143c>General Law of Addition:</FONT> When two or more events will happen at the same time, and the events <B>are not</B> mutually exclusive, then: <P>  <CENTER>P (X or Y) = P (X) + P (Y) - P (X and Y)</CENTER> <P>Notice that, the equation P (X or Y) = P (X) + P (Y) - P (X and Y),   contains especial events: An event (X and Y) which is the intersection of  set/events X and Y, and another event (X or Y) which is the union (i.e.,  either/or) of sets X and Y. Although this is very simple, it says relatively little about how event X influences event Y and vice versa. If P (X and Y) is 0, indicating that events X and Y do not intersect (i.e., they are mutually exclusive), then we have P (X or Y) = P (X) + P (Y). On the other hand if P (X and Y) is not 0, then there are interactions between the two events X and Y. Usually it could be a physical interaction between them. This makes the  relationship P (X or Y) = P (X) + P (Y) - P (X and Y) nonlinear because the  P(X and Y) term is subtracted from which influences the result. <P></P> 
<p>
The above law is known also as the <FONT color=#dc143c>Inclusion-Exclusion Formula</FONT>.  It can be extended to more than two events.   For example, for three events A, B, and C, it becomes:
<P>  <CENTER>
P(A or B or C) =<br> P(A) + P(B) + P(C) - P(A and B) - P(A and C) - P(B and C) + P(A and B and C)
<P>  </CENTER>
<LI><FONT color=#dc143c>Special Law of Addition:</FONT> When two or more  events will happen at the same time, and the events <B>are</B> mutually  exclusive, then: <P>  <CENTER>P(X or Y) = P(X) + P(Y)</CENTER>  <P></P><LI><FONT color=#dc143c>General Law of Multiplication:</FONT> When two or more events will happen at the same time, and the events <B>are</B> dependent, then the general rule of multiplicative law is used to find the joint probability:   <P><CENTER>P(X and Y) = P(Y) <font face=symbol>&#180;</font> P(X|Y),</CENTER>  <P>where P(X|Y) is a conditional probability.   <P></P><LI><FONT color=#dc143c>Multiplicative Law:</FONT> When two or more  events will happen at the same time, and the events <B>are</B> independent,  then the special rule of multiplication law is used to find the joint  probability:   <P>  <CENTER>P(X and Y) = P(X) <font face=symbol>&#180;</font> P(Y)</CENTER>  <P></P>  <LI><FONT color=#dc143c>Conditional Probability Law:</FONT> A conditional   probability is denoted by P(X|Y). This phrase is read: the probability that X   will occur <B>given that</B> Y is known to have occurred. <P>Conditional probabilities are based on knowledge of one of the variables.   The conditional probability of an event, such as X, occurring given that   another event, such as Y, has occurred is expressed as: <P>  <CENTER>P(X|Y) = P(X and Y) <FONT FACE="Symbol">&#184;</FONT> P(Y),</CENTER>  <P>provided P(Y) is not zero. Note that when using the conditional law of   probability, you always divide the joint probability by the probability of the   event after the word <FONT color=#dc143c>given</FONT>. Thus, to get P(X given  Y), you divide the joint probability of X and Y by the unconditional   probability of Y. In other words, the above equation is used to find the  conditional probability for any two <B>dependent</B> events.   <P>The simplest version of the Bayes' Theorem is: 
  <P>  <CENTER>P(X|Y) = P(Y|X) <font face=symbol>&#180;</font> P(X) <FONT FACE="Symbol">&#184;</FONT> P(Y)</CENTER>  <P>If two events, such as X and Y, are <B>independent</B> then:  <P>  <CENTER>P(X|Y) = P(X),</CENTER><BR>and   <CENTER>P(Y|X) = P(Y)</CENTER>  <P></P><LI><FONT color=#dc143c><B>The Bayes' Law:</B></FONT>   <P>
  <CENTER>P(X|Y) = [ P(X) <font face=symbol>&#180;</font> P(Y|X) ]  <FONT FACE="Symbol">&#184;</FONT>  [P(X) <font face=symbol>&#180;</font>P(Y|X) + P(not X) <font face=symbol>&#180;</font> P(Y| not X)]</CENTER>  <P>Bayes' Law provides posterior probability [i.e, P(X|Y)] sharpening the  prior probability [i.e., P(X)] by the availability of accurate and relevant  information in probabilistic terms.   <P></P></LI></OL><BLOCKQUOTE><FONT size=+0>
  <P><B>An Application:</B> Suppose two machines, A and B, produce identical  parts. Machine A has probability 0.1 of producing a defective each time,  whereas Machine B has probability 0.4 of producing a defective. Each machine  produces one part. One of these parts is selected at random, tested, and found 
  to be defective. What is the probability that it was produced by Machine B?  <P><B>Probability tree diagrams</B> depict events or sequences of events as branches  of a tree. Tree diagrams are useful for visualizing the conditional  probabilities:  <P><CENTER><IMG alt="A tree diagram"  src="Condition_Proba.gif"> </CENTER>
  <P>The probabilities at the end of each branch are the probability that events  leading to that end will happen simultaneously. The above tree diagram  indicates that the probability of a part testing Good is 9/20 + 6/20 = 3/4,   therefore the probability of Bad is 1/4. Thus, P(made by B | it is bad) = (4/20) / (1/4) = 4/5.  <P>Now using the Bayes' Law we are able to obtain useful information such as:   <P>  <CENTER>P(it is bad | made by B) = 1/4(4/5) / [1/4(4/5) + 3/4(2/5)] = 2/5.   </CENTER>  <P>Equivalently, using the above conditional probability, results in:   <P>  <CENTER>P(it is bad | made by B) = P(it is bad &amp; made by B)/P(made by B) = 
  (4/20)/(1/2) = 2/5. </CENTER><p>
<P>You may like using the <A href="matrix/matrix.htm" 
  target=new>Bayes' Revised Probability</A> JavaScript.
<P><A name=rmuexindev></A>  <HR>
  <H4><FONT color=#dc143c>Mutually Exclusive versus Independent  Events</FONT></H4><FONT color=#dc143c><B>Mutually Exclusive (ME):</B></FONT> Event A and B are ME if both cannot occur simultaneously. That is, P[A and B]   = 0.   <P><FONT color=#dc143c><B>Independency (Ind.):</B></FONT> Events A and B are  independent if having the information that B already occurred does not change 
  the probability that A will occur. That is P[A given B occurred] = P[A].  <P>If two events are ME they are also Dependent: P(A given B) = P[A and   B] <FONT FACE="Symbol">&#184;</FONT> P[B], and since P[A and B] = 0 (by ME), then P[A given B] = 0. Similarly, 
  <P>If two events are Independent then they are also not ME.   <P>If two events are Dependent then they may or may not be ME.   <P>If two events are not ME, then they may or may not be Independent.   <P>The following Figure contains all possibilities. The notations used in this   table are as follows: <B>X</B> means does not imply, question mark <B>?</B>   means it may or may not imply, while the <B>check mark</B> means it implies.   <P><CENTER><IMG alt="Mutual exclusive vs. Independent events" src="me.gif"> </CENTER>Notice that the (probabilistic)  pairwise independency and mutual independency for a collection of events   A<FONT size=+0><SUB>1</SUB></FONT>,..., A<FONT size=+0><SUB>n</SUB></FONT> are  two different notions.   <P><A name=rwissoimpabNor></A>
  <HR><H4><FONT color=#dc143c>What Is so Important About the Normal Distributions?</FONT></H4> The term "normal" possibly arose because of the   various attempts made to establish this <A    href="#rdensityDist">distribution</A> as the underlying law governing all continuous variables. These attempts were  based on false premises and consequently failed. Nonetheless, the normal   distribution rightly occupies a preeminent place in the field of probability.   In addition to portraying the distribution of many types of natural and   physical phenomena (such as the heights of men, diameters of machined parts,   etc.), it also serves as a convenient approximation of many other   distributions which are less tractable. Most importantly, it describes the   manner in which certain estimators of population characteristics vary from  sample to sample and, thereby, serves as the foundation upon which much <A  href="#rstatInferentia">statistical  inference</A> from a random sample to population are made.   <P>Normal Distribution (called also Gaussian) curves, which have a bell-shaped  appearance (it is sometimes even referred to as the "bell-shaped curves") are   very important in statistical analysis. In any normal distribution is  observations are distributed symmetrically around the mean, 68% of all values  under the curve lie within one standard deviation of the mean and 95% lie 
  within two standard deviations.   <P>There are many reasons for their popularity. The following are the most 
  important reasons for its applicability:   <P></P></FONT></BLOCKQUOTE><FONT size=+0> <OL>
  <LI>One reason the normal distribution is important is that a wide variety of    <I>naturally occurring</I> <A 
  href="#rrandomva">random  variables</A> such as heights and weights of all creatures are distributed 
    evenly around a central value, average, or norm (hence, the name normal distribution).  Although the distributions are only approximately normal, they are usually  quite close.   <P>Whenever there are too many  factors influencing the outcome of a random outcome, then the underlying <A   href="#rdensityDist">distribution</A>  is approximately normal. For example, the height of a tree is determined  by the "sum" of such factors as rain, soil quality, sunshine, disease, etc.  <P>As <A 
  href="http://www-history.mcs.st-and.ac.uk/history/Mathematicians/Galton.html"   target=new>Francis Galton</A> wrote in 1889, "Whenever a large sample of chaotic elements are taken in hand and arranged in the order of their magnitude,  an unsuspected and most beautiful form of regularity proves to have been 
      latent all along."  <P></P> <LI>Almost all <FONT color=#dc143c><B>statistical tables are limited </B></FONT>by  the size of their parameters. However, when these parameters are large enough 
    one may use normal distribution for calculating the critical values for these  tables. For example, the F-statistic is related to standard normal z-statistic  as follows: F = z<FONT size=+0><SUP>2</SUP></FONT>, where F has (d.f.<FONT size=+0><SUB>1</SUB></FONT>  = 1, and d.f.<FONT   size=+0><SUB>2</SUB></FONT> is the largest available in the F-table).  For more, visit the <a href="http://ubmail.ubalt.edu/~harsham/stat-data/opre330.htm#rradast" target ="new">Relationships among Common Distributions</a>.
<P><B>Approximation of the binomial:</B> For example, the normal distribution  provides a very accurate approximation of the binomial when n is large and  p is close to 1/2. Even if n is small and p is not extremely close to 0 or to 1, the approximation is adequate. In fact, the normal approximation  of the binomial will be satisfactory for most purposes provided that np  <FONT 
  face=symbol>&gt;</FONT> 5 and nq <FONT face=symbol>&gt;</FONT> 5.  <P>Here is how the approximation is made. First, set <FONT  face=symbol>m</FONT> = np and <FONT face=symbol>s</FONT><FONT 
  size=+0><SUP>2</SUP></FONT> = npq. To allow for the fact that the binomial is a discrete distribution, we conventionally use <FONT color=#dc143c>a continuity correction factor</FONT> of 1/2 unit added to or subtracted from X on the  grounds that the discrete value (x = a) should correspond on a continuous  scale to (a - 1/2) <FONT face=symbol>&lt;</FONT> x <FONT   face=symbol>&lt;</FONT> (a + 1/2). Then we compute the value of the standard  normal variable by: <P>  <CENTER> z = [(a - 1/2) - <FONT face=symbol>m</FONT>]/<FONT 
  face=symbol>s</FONT>&nbsp; &nbsp;OR&nbsp; &nbsp; z = [(a + 1/2) - <FONT  face=symbol>m</FONT>]/<FONT face=symbol>s</FONT>  </CENTER> <P>Now one may used the standard normal table for the numerical values.  <P><B>An Application:</B> The probability of a defective item coming off a  certain assembly line is <I>p</I> = 0.25. A sample of 400 items is selected  from a large lot of these items. What is the probability 90 or less items are defective?  <P></P><LI>If the mean and standard deviation of a normal distribution are known, it is easy to convert back and forth from <FONT color=#dc143c><B>raw scores to percentiles</B></FONT>. <P></P><LI>It has been proven that the underlying distribution is normal if and only if the sample mean is independent of the sample variance, <FONT color=#dc143c><B>this characterizes the normal distribution.</B></FONT>  Therefore many effective <B>transformations</B> can be applied to convert almost any shaped distribution into a normal one. <P></P> <LI>The most important reason for popularity of normal distribution is the <FONT color=#dc143c><B>Central Limit Theorem (CLT)</B></FONT>. The distribution of the sample averages of a large number of independent <A  href="#rrandomva">random variables</A> will be approximately normal <B>regardless</B> of the distributions of the individual random variables. 
<p>
<li><FONT color=#dc143c><B>The Sampling distribution</B></FONT> of normal populations provide more information</B></FONT> than any other distributions.  For example, the following standard (i.e., having the same unit as the data have) errors are readily available:
<p>  
<ul>
<LI>Standard Error of the Median =  (<FONT  face=symbol>p</FONT>/2n)<FONT><SUP>½</SUP></FONT>S. 
    <P></P> <LI>Standard Error of the Standard Deviation = S/(2n)<FONT  size=+0><SUP>½</SUP></FONT>.<br>  Therefore, the test statistic for the null hypothesis <FONT face=symbol>s</FONT> = <FONT face=symbol>s</FONT><FONT><SUB>0</SUB></FONT>, is  Z = (2n)<FONT  size=+0><SUP>½</SUP></FONT> (S - <FONT face=symbol>s</FONT><FONT><SUB>0</SUB></FONT>)/<FONT face=symbol>s</FONT><FONT><SUB>0</SUB></FONT>.
    <P></P> 
<LI>Standard Error of the Variance =  S<FONT><SUP>2</SUP></FONT>[(2/(n-1)]<FONT  size=+0><SUP>½</SUP></FONT>. 
    <P></P>

<LI>Standard Error of the Interquartiles Half-Range (Q) = 1.166Q/n<FONT 
  size=+0><SUP>½</SUP></FONT>  <P></P> <LI>Standard Error of the Skewness = (6/n)<FONT size=+0><SUP>½</SUP></FONT>. 
<p>
<li>Standard Error of the Skewness of Sample Mean = Skewness/n<FONT  size=+0><SUP>½</SUP></FONT>  <P>Notice that the skewness in sampling distribution of the mean rapidly disappears as n gets larger.  <P></P> <LI>Standard Error of the Kurtosis = (24/n)<FONT size=+0><SUP>½</SUP></FONT> = 2 times the standard error of skewness. 
 <P></P>  <LI>Standard Error of the Correlation (r) =  [(1 - r<FONT  size=+0><SUP>2</SUP></FONT>)/(n-1)]<FONT size=+0><SUP>½</SUP></FONT>.
</ul>
<p>Moreover,<p><center>Quartile deviation <FONT FACE="Symbol">&#187;</FONT> 2S/3, &nbsp; &nbsp; and,
&nbsp; &nbsp;Mean absolute deviation <FONT FACE="Symbol">&#187;</FONT> 4S/5.<p></center>
<P></P> <LI>The other reason the normal distributions are so important is that the <A  href="#rnormalcond">normality condition</A> is required by almost all kinds of parametric <B>statistical tests</B>. The Central Limit Theorem is a useful tool when you are dealing with a population  with an unknown distribution. Often, you may analyze the mean (or the sum) of a sample of size n. For example instead of analyzing the weights of individual items you may analyze the batch of size n, that is, the packages each containing  n items. </LI></OL><BLOCKQUOTE><A name=rwsd></A>  <HR><H4><FONT color=#dc143c>What Is A Sampling Distribution?</FONT></H4>  A sampling distribution describes probabilities associated with a statistic when a random sample is drawn from the entire population. <P>The sampling distribution is the density (for a continuous statistic, such as an estimated mean), or probability function (for discrete statistic, such as an estimated proportion). <P>Derivation of the sampling distribution is the first step in calculating  a confidence interval or carrying out a <A   href="#restimateHypoth">hypothesis testing for a parameter</A>. <P>Example: Suppose that x1,.......,xn are a simple random sample from a normally distributed population with expected value <FONT face=symbol>m </FONT>and known variance <FONT face=symbol>s</FONT><FONT  size=+0><SUP>2</SUP></FONT>. Then, the sample mean is normally distributed with expected value <FONT face=symbol>m </FONT>and variance <FONT  face=symbol>s</FONT><FONT size=+0><SUP>2</SUP></FONT>/n. <P>  <P>The main idea of <A   href="#rstatInferentia">statistical inference</A> is to take a random sample from the entire particular <A  href="#rPopulation">population</A> and then to use the information from the sample to make inferences about the  particular population characteristics such as the mean <FONT face=symbol>m</FONT>(measure of central tendency), the standard deviation (measure of dispersion, spread)  <FONT face=symbol>s</FONT> or the proportion of units in the population that have a certain characteristic. Sampling saves money, time, and effort. Additionally,  a sample can provide, in some cases, as much or more accuracy than a corresponding study that would attempt to investigate an entire population. Careful collection  of data from a sample will often provide better information than a less careful study that tries to look at everything.  <P>Often, one must also study the behavior of the mean of sample values taken from different specified populations; e.g., for comparison purposes.  
<P>Because a sample examines only part of a population, the sample mean will not exactly equal the corresponding mean of the population <FONT face=symbol>m </FONT>. Thus, an important consideration for those planning and interpreting sampling results is the degree to which sample estimates, such as the sample mean, will agree with the corresponding population characteristic. <P>In practice, only one sample is usually taken. In some cases a small "pilot sample" is used to test the data-gathering mechanisms and to get preliminary information for planning the main sampling scheme. However, for purposes of understanding the degree to which sample means will agree with the corresponding  population mean <FONT face=symbol>m </FONT>, it is useful to consider what 
    would happen if 10, or 50, or 100 separate sampling studies, of the same type,  were conducted. How consistent would the results be across these different studies? If we could see that the results from each of the samples would be nearly the same (and nearly correct!), then we would have confidence in the single sample that will actually be used. On the other hand, seeing that answers from the repeated samples were too variable for the needed accuracy would suggest that a different sampling plan (perhaps with a larger sample size) 
    should be used. <P>A sampling distribution is used to describe the distribution of outcomes  that one would observe from replication of a particular sampling plan. <P>Know that estimates computed from one sample will be different from estimates  that would be computed from another sample.  <P>Understand that estimates are expected to differ from the population characteristics  (parameters) that we are trying to estimate, but that the properties of sampling distributions allow us to quantify, based on probability, how they will differ.   <P>Understand that different statistics have different sampling distributions  with distribution shape depending on (a) the specific statistic, (b) the sample size, and (c) the parent <A  href="#rdensityDist">distribution</A>. 
 <P>Understand the relationship between sample size and the distribution of sample estimates. <P>Understand that increasing the sample size can reduce the variability in a sampling distribution. 
 <P>See that in large samples, many sampling distributions can be approximated with a normal distribution. 
<p>
<FONT color=#dc143c><B>Sampling Distribution of the Mean and the Variance for Normal Populations: </B></FONT> Given the random variable X is distributed normally with mean <FONT face=symbol>m</FONT> and standard deviation <FONT face=symbol>s</FONT>, then for a random sample of size n:
<p>
<ul>
<li>The sampling distribution of [<IMG src="xbaru.gif"> - <FONT face=symbol>m</FONT>] <font face=symbol>&#180; </font>n<FONT size=+0><SUP>½</SUP></FONT> <FONT FACE="Symbol">&#184;</FONT> <FONT face=symbol>s</FONT>, is the standard normal distribution.
<p>
<li>The sampling distribution of [<IMG src="xbaru.gif"> - <FONT face=symbol>m </FONT>] <font face=symbol>&#180;</font> n<FONT size=+0><SUP>½</SUP></FONT>  <FONT FACE="Symbol">&#184;</FONT> S, is  a t-distribution with parameter d.f. = n-1.
<p>
<li>The sampling distribution of [S<FONT size=+0><SUP>2</SUP></FONT>(n-1) <FONT FACE="Symbol">&#184;</FONT> <FONT face=symbol>s</FONT><FONT size=+0><SUP>2</SUP></FONT>], is  a <FONT face=symbol>c</FONT><FONT size=+0><SUP>2</SUP></FONT> distribution with parameter d.f. = n-1.
<p>
<li>For two independent samples, the sampling distribution of [S <FONT size=+0><SUB>1</SUB></FONT><FONT size=+0><SUP>2</SUP></FONT> / S<FONT size=+0><SUB>2</SUB></FONT><FONT size=+0><SUP>2</SUP></FONT>],  is  an F distribution with parameters d.f.<FONT size=+0><SUB>1</SUB></FONT> = n <FONT size=+0><SUB>1</SUB></FONT>-1, and d.f.<FONT size=+0><SUB>2</SUB></FONT>= n <FONT size=+0><SUB>2</SUB></FONT>-1.
</ul>
<p>
 <P>   <P><A name=rwclt></A> <HR><H4><FONT color=#dc143c>What Is The Central Limit Theorem?</FONT></H4>  The central limit theorem (CLT) is a "limit" that is "central" to statistical  practice. For practical purposes, the main idea of the CLT is that the average (center of data) of a sample of observations drawn from some population is approximately distributed as a normal distribution if certain conditions are met. In theoretical statistics there are several versions of the central limit theorem depending on how these conditions are specified. These are concerned with the types of conditions made about the <A   href="#rdensityDist">distribution</A>  of the parent population (population from which the sample is drawn) and the actual sampling procedure. <P>One of the simplest versions of the central limit theorem stated by many textbooks is: if we take a random sample of size (n) from the entire population, then, the sample mean which  is a <A   href="#rrandomva">random variable</A> defined by:<p><center> <FONT face=symbol>S</FONT> x<FONT 
  size=+0><SUB>i</SUB></FONT> / n,<p></center> has a histogram which converges to a normal distribution shape if n is large enough . Equivalently, the sample mean distribution approaches to normal distribution as the sample size increases.
<p>
Some students having difficulty reconciling their own understanding of the central limit theorem with some of the textbooks statements.  Some textbooks do not emphasize the on the <B>independent, random samples of fixed-size</B> n (say more than 30).
<p>
The shape of the sampling distributions for means - becomes increasingly normal as the sample size n becomes larger.  The increasing sample size is what causes the distribution to become increasingly normal and the independence condition provides the <FONT FACE="Symbol">&#214;</FONT>n  contraction of the standard deviation.
<p>
<B>The CLT for proportion data</B>, such as binary 0, 1, again the sampling distribution-- while becoming increasingly "bell-shaped"-- remains confined to the domain [0,1]. This domain represents a dramatic difference from a normal distribution, with has an unbounded domain. However, as n increases without bound, the "width" of the bell becomes very small so that the CLT "still works".
<P>In applications of the central limit theorem to practical problems in <A   href="#rInferentiaStatist">statistical inference</A>, however, we are more interested in how closely the approximate distribution of the sample mean follows a normal distribution  for finite sample size, than in the limiting distribution itself. Sufficiently close agreement with a normal distribution allows us to use normal theory for making inferences about population parameters (such as the mean ) using the sample mean, irrespective of the actual form of the parent population.  <P>It can be shown that, if the parent population has mean <FONT face=symbol>m</FONT> and a finite standard deviation <FONT face=symbol>s</FONT>, then the   sample mean distribution has the same mean <FONT face=symbol>m </FONT>but  with smaller standard deviation which is <FONT face=symbol>s</FONT> divided  by n<FONT><SUP>½</SUP></FONT>. <P>You know by now that, whatever the parent population is, the standardized  variable Z = (X - <FONT face=symbol>m </FONT> )/<FONT face=symbol>s</FONT> will have a distribution with a mean <FONT face=symbol>m </FONT> = 0 and standard deviation <FONT face=symbol>s</FONT> =1 under random sampling.  Moreover, <B>if</B> the parent population is normal, then Z is distributed exactly as the standard normal. The central limit theorem states the remarkable result that, even when the parent population is non-normal, the standardized 
    variable is approximately normal if the sample size is large enough. It is generally not possible to state conditions under which the approximation given by the central limit theorem works and what sample sizes are needed before  the approximation becomes good enough. As a general guideline, statisticians have used the prescription that, if the parent <A  href="#rdensityDist">distribution</A>  is symmetric and relatively short-tailed, then the sample mean <B>more</B> closely approximates normality for smaller samples than if the parent population is <A   href="#rskewKur">skewed</A>  or long-tailed. <P>Under certain conditions, in large samples, the sampling distribution of  the sample mean can be approximated by a normal distribution. The sample size  needed for the approximation to be adequate depends strongly on the shape of the parent distribution. Symmetry (or lack thereof) is particularly important. <P>For a symmetric parent distribution, even if very different from the shape  of a normal distribution, an adequate approximation can be obtained with small samples (e.g., 15 or more for the uniform distribution). For symmetric, short-tailed parent distributions, the sample mean more closely approximates normality for smaller  sample sizes <B>than</B> if the parent population is skewed and long-tailed. In some extreme cases (e.g. binomial) sample sizes far exceeding the typical guidelines  (e.g., over 30) <B>are needed</B> for an adequate approximation. For some distributions  without first and second moments (e.g., one is known as the <A  href="http://ubmail.ubalt.edu/~harsham/stat-data/opre330.htm#rCauchy" target="new">Cauchy</a> distribution), the central limit theorem does not hold.  <P>For some distributions, extremely large (impractical) samples would be required  to approach a normal distribution. In manufacturing, for example, when defects  occur at a rate of less than 100 parts per million, using, a <a href="#rBeta">Beta</a> distribution yields an honest <A   href="#rwci">Confidence Interval (CI)</a> of total defects in the population. 
<P><P><A name=rWhatIsDofF></A>   <HR><H4><FONT color=#dc143c>What Is "Degrees of Freedom"?</FONT></H4>  Recall that in estimating the population's variance, we used (n-1) rather than  n, in the denominator. The factor (n-1) is called "degrees of freedom."  <P><FONT color=#dc143c>Estimation of the Population Variance:</FONT> Variance in a population is defined as the average of squared deviations from the population mean. If we draw a random sample of n cases from a population where the mean  is known, we can estimate the population variance in an intuitive way. We  sum the deviations of scores from the population mean and divide this sum by n. This estimate is based on n independent pieces of information, and we have n degrees of freedom. Each of the n observations, including the last one, is unconstrained ('free' to vary).  <P>When we do not know the population's mean, we can still estimate the population  variance; but, now we compute deviations around the sample mean. This introduces  an important constraint because the sum of the deviations around the sample  mean is known to be zero. If we know the value for the first (n-1) deviations, the last one is known. There are only n-1 independent pieces of information  in this estimate of variance.  <P>If you study a system with n parameters x<FONT size=+0><SUB>i</SUB></FONT>,  i =1..., n, you can represent it in an  n-dimension space. Any point of this space shall represent a potential state of your system. If your n parameters  could vary independently, then your system would be fully described in a n-dimension hyper-volume (for n over 3). Now, imagine you have one constraint between the parameters  (an equation with your n parameters), then your system would be described  by a (n-1)-dimension hyper-surface (for n over 3). For example, in three dimensional space,  a linear relationship means a plane which is 2-dimensional.  <P>In statistics, your n parameters are your n data. To evaluate variance, you  first need to infer the mean <FONT face=symbol>m </FONT>. So when you evaluate the variance, you have one constraint on your system (which is the expression of the mean), and  it remains only  (n-1) degrees of freedom to your system. <P>Therefore, we divide the sum of squared deviations by n-1, rather than by n, when we have sample data. On average, deviations around the sample mean  are smaller than deviations around the population mean. This is because our  sample mean is always in the middle of our sample scores; in fact, the minimum  possible sum of squared deviations for any sample of numbers is around the mean for that sample of numbers. Thus, if we sum the squared deviations from  the sample mean and divide by n, we have an underestimate of the variance  in the population (which is based on deviations around the population mean). <P>If we divide the sum of squared deviations by n-1 instead of n, our estimate is a bit larger, and it can be shown that this adjustment gives us <FONT color=#dc143c>an unbiased estimate</FONT> of the population variance. However, for large n,  say, over 30, it does not make too much difference if we divide by n, or   n-1.  <P><FONT color=#dc143c>Degrees of Freedom in ANOVA:</FONT> You will see the  key parse "degrees of freedom" also appearing in the Analysis of Variance (ANOVA) tables. If I tell you about 4 numbers, but don't say what they are,  the average could be anything. I have 4 degrees of freedom in the data set.   If I tell you 3 of those numbers, and the average, you can guess the fourth  number. The data set, given the average, has 3 degrees of freedom. If I tell  you the average and the standard deviation of the numbers, I have given you  2 pieces of information, and reduced the degrees of freedom from 4 to 2.  You only need to know 2 of the numbers' values to guess the other 2.   <P>In an ANOVA table, degree of freedom (df) is the divisor in (Sum of Squared deviations)/df which will  result in an unbiased estimate of the variance of a population.  <P>In general, a degree of freedom d.f. = N - k, where N is the sample size, and k is a small number, equal to  the number of "constraints", the number of "bits of information" already "used  up". As we will see in the ANOVA section, degree of freedom is an additive quantity; total amounts of it can be  "partitioned" into various components.  For example, suppose we have a sample of size 13 and calculate its mean,  and then the deviations from the mean; only 12 of the deviations are free  to vary. Once one has found 12 of the deviations, the thirteenth one is determined.    <P>In bivariate correlation or regression situations, k = 2. The calculation  of the sample means of each variable "uses up" two bits of information, leaving   N - 2 independent bits of information. <P>In a one-way analysis of variance (ANOVA) with g groups, there are three  ways of using the data to estimate the population variance. If all the data  are pooled, the conventional SST/(n-1) would provide an estimate of the population   variance.  <P>If the treatment groups are considered separately, the sample means can also  be considered as estimates of the population mean, and thus SSb/(g - 1) can   be used as an estimate. The remaining ("within-group", "error") variance can  be estimated from SSw/(n - g). This example demonstrates the partitioning  of d.f.:<br> d.f. total = n - 1 = d.f.(between) + d.f.(within) = (g - 1) + (n - g).   <P>Therefore, the simple 'working definition' of d.f. is sample size minus the  number of estimated parameters'. A more complete answer would have to explain why  there are situations in which the degrees of freedom is not an integer. After  we said all this, the best explanation, is mathematical in that we <B>use d.f. to obtain an unbiased estimate</B>.  <P>In summary, the concept of degrees of freedom is used for the following two  different purposes: </P></BLOCKQUOTE><UL>  <LI>Parameter(s) of certain distributions, such as F and t-distribution, are called degrees of freedom.   <P></P>  <LI>Most importantly, the degrees of freedom are used to obtain unbiased estimates for the population parameters. </LI></UL><BLOCKQUOTE><P><P><A name=rappTable></A> <HR>  <H4><FONT color=#dc143c>Applications of and Conditions for Using Statistical Tables</FONT></H4> Some widely used applications of the popular statistical tables can be categorized   as follows:   <P><FONT color=#dc143c><B>T - Table:</B></FONT>   <OL> <LI><A  href="#rsinglepoputest"   target=new>Single Population µ Test</A>.     <LI><A href="#rTwoIndTest"     target=new>Two Independent Populations µ's Test</A>. <LI><A href="#rTwoDepdTest"    target=new>The Before-and-After µ's Test</A>.  <LI><A  href="#rregconditions"    target=new>Tests Concerning Regression Coefficients </A>.     <LI><A  href="#rcals"   target=new>Test Concerning Correlation</A>. </LI>  </OL> <P><B>Conditions for using this table:</B> Test for <A  href="otherapplets/Randomness.htm"   target=new>randomness</A> of the data is needed before using this table. Test  for <A  href="#rnormalcond">normality condition</A> of the population distribution is also needed if the sample  size is small, or it may not be possible to invoke the central limit theorem.  <P> <P><FONT color=#dc143c><B>Z - Table:</B></FONT>  <OL>    <LI><A  href="#rrunstest"   target=new>Test for Randomness</A>.  <LI>Tests concerning µ for <A  href="otherapplets/MeanTest.htm"    target=new>one population</A> or <A    href="otherapplets/TwoPopTest.htm"  target=new>two populations</A> based on their large-size, random sample(s),   (say over 30) to invoke the central limit theorem. This includes test  concerning <B>proportions</B>, with large-size, random sample size n (say over  30) to invoke distribution convergence results. <LI>To Compare <A  href="#rhccc"  target=new>Two Correlation Coefficients</A>. </LI>  </OL>  <P><FONT color=#dc143c>Notes:</FONT> As you know by now, in test of hypotheses  concerning <FONT face=symbol>m</FONT>, and construction of confidence interval  for it, we start with <FONT face=symbol>s</FONT> known, since the critical  value (and the p-value) of the Z-Table distribution can be used. Considering  the more realistic situations, when we don't know <FONT face=symbol>s</FONT>,  the T-Table is used. In both cases, we need to verify the <A  href="#rnormalcond">normality condition</A> of the population's distribution; however, if the sample size n is very large, we can in fact switch back to Z-Table by  virtue of the central limit theorem. For perfectly normal populations, the t-distribution  corrects for any errors introduced by estimating <FONT face=symbol>s</FONT> with s when doing inference. <P>Note also that, in hypothesis testing concerning the parameter of binomial and Poisson distributions for large sample sizes, the standard deviation is  known under the null hypotheses. That's why you may use the normal approximations  for both of these distributions. <P><B>Conditions for using this table:</B> Test for <A  href="otherapplets/Randomness.htm"   target=new>randomness</A> of the data is needed before using this table. Test for <A   href="#rnormalcond">normality condition</A> of the population distribution is also needed if the sample size is small, or it may not be possible to invoke the Central Limit Theorem. <P><P><FONT color=#dc143c><B>Chi-square - Table:</B></FONT>  <OL><LI><A   href="#rCrossTab"    target=new>Test for Cross-table Relationship</A>.     <LI><A href="#rHomCrossTab"   target=new>Identical-Populations Test for Crosstable Data</A>.  <LI><A   href="#rHomPropor">Test for Equality of Several Population Proportions</A>. <LI><A  href="#rEualMedian">Test  for Equality of Several Population Medians</A>. <LI><A  href="#rgoodnessofforDrv"  target=new>Goodness-of-Fit Test for Probability Mass Functions</A>. <LI><A  href="#rCompMultiCo"   target=new>Compatibility of Multi-Counts</A>.<LI><A  href="#rmulticorr" target=new>Correlation-Coefficient  Testing</A>. 
<LI><A  href="#rchiconditions">Necessary  Conditions in Applying the Above Tests</A>. <LI><A   href="#rchivariance"  target=new>Testing the Variance: Is the Quality that Good?</A>. <LI><A   href="#rMultivariances"   target=new>Testing the Equality of Multi-Variances</A>. </LI></OL>  <P><B>Conditions for using this table:</B> The necessary conditions for using this table for all the above tests, except for the last one, can be found  at <A href="#rchiconditions"  target=new>Conditions for the Chi-square  Based Tests</A>. The last application requires <A  href="#rnormalcond">normality</a> (condition) of the population distribution.  <P><P><FONT color=#dc143c><B>F - Table:</B></FONT> <OL>    <LI><A 
    href="#rANOVA"   target=new>Multi-Means Comparisons: Analysis of Variance (ANOVA)</A>. 
    <LI><A  href="otherapplets/twopoptest.htm" 
    target=new>Tests Concerning Two Variances</A>.  <LI><A href="#rregconditions"   target=new>Overall Assessment of Regression Models </A>. </LI></OL>  <P><B>Conditions for using this table:</B> Tests for <A  href="otherapplets/Randomness.htm" 
  target=new>randomness</A> of the data and <A  href="#rnormalcond">normality </A> (condition) of the populations are needed before using this table for ANOVA.  Same conditions must be satisfied for the residuals in regression analysis.  <P>The following chart summarizes application of statistical tables with respect 
    to test of hypotheses and construction of confidence intervals for mean <FONT   face=symbol>m</FONT>and variance <FONT face=symbol>s </FONT><FONT   size=+0><SUP>2</SUP></FONT> in one population or the comparison of two or more populations.   <P>   <CENTER><font size="+0"><a href="selection.gif"><IMG alt="Selection of an appropriate statistical table"  src="selection.gif" width="268" height="184" border="0"></a></font><p>Selection of of an Appropriate Statistical Table<br><b><font color="#dc143c">Click on the image to enlarge it and THEN print it</font></b> <P></CENTER><P>You may like using <A href="http://www.physics.csbsju.edu/stats/Index.html" 
  target=new>Online Statistical Computation</A> in performing most of these tests. The <A 
  href="otherapplets/pvalues.htm"  target=new>P-values for the Popular Distributions</A> Web site provides P-values  useful in major statistical testing. The results are more accurate than those  that can be obtained (by interpolation) from statistical tables of your textbook  are. <P>   <P><B>Further Reading:</B><BR><FONT face="Bookman Old Style" size=-2>Evans M., N. Hastings, and B. Peacock, <I>Statistical Distributions</I>, Wiley, 2000.<BR>    Kanji G., <I>100 Statistical Tests</I>, Sage Publisher, 1995. </FONT>  <p> <P><A name=rBinomial></A> <HR>
  <H4><FONT color=#dc143c>Binomial Probability Function</FONT></H4>  An important class of decision problems under uncertainty involves situations  for which there are only two possible random outcomes. 
  <P>The binomial probability function gives probability of exact number of "successes"  in n independent trials, when probability of success p on single trial is  a constant. Each single trial is called a <FONT 
  color=#dc143c>Bernoulli Trial</FONT> satisfying the following conditions: <P> <OL>    <LI>Each trial results in one of two possible, mutually exclusive, outcomes.  One of the possible outcomes is denoted (arbitrarily) as a success, and   the other is denoted a failure.  <LI>The probability of a success, denoted by p, remains constant from trial  to trial. The probability of a failure, 1-p, is denoted by q. <LI>The trials are independent; that is, the outcome of any particular trial  is not affected by the outcome of any other trial. </LI> </OL>  <P>The number of ways of getting r successes in n trials is: <P>     <CENTER>P (r successes in n trials) = <SUP>n</SUP>C<SUB>r</SUB> . p<SUP>r</SUP> . (1- p)<SUP>(n-r)</SUP><BR> = n! / [r!(n-r)!] . [p<SUP>r</SUP> . (1- p)<SUP>(n-r)</SUP>]. </CENTER>  <P>The mean and variance of <A   href="#rrandomva">random variable</A> r, are np and np(1-p), respectively, where q = 1 - p. The skewness and kurtosis are (2q -1)/ (npq)<FONT size=+0><SUP>½</SUP></FONT>, and (1- 6pq)/(npq), respectively.  
From its skewness, we notice that the distribution is symmetric for p =1/2 and most skewed when p is 0 or 1.
<p>Its mode is within interval [(n+1)p -1, (n+1)p], therefore if (n+1) p is not an integer, then the mode is an integer within the interval. However if (n+1)p is an integer, then its probability function has two but adjacent modes:  (n+1)p -1, and (n+1)p. <P><B>Determination of probabilities for p over 0.5:</B> The binomial tables in some textbooks are limited to deterring the probabilities for values of p up to 0.5. However, these tables can be used for values of p over 0.5. By  recasting a problem in terms of p to 1 -p, and setting r to n-r, then the probability of obtaining r successes in n trials for a given value of p is equal to the probability of obtaining n-r failures in n trials with 1-p. <P><B>An Application:</B> A large shipment of purchased parts is received at a warehouse, and a sample of 10 parts is checked for quality. The manufacturer's  claim is that at most 5% might be defective. What is the chance that the sample 
 includes one defective? <P>   <CENTER>P (one defective out of ten) = {10! /[(1!)(9!)]}(0.05)<FONT 
  size=+0><SUP>1</SUP></FONT>(0.95)<FONT size=+0><SUP>9</SUP></FONT> = 32%. 
</CENTER>  <P>Know that the binomial distribution is to satisfy the five following requirements: 
 (1) each trial can have only two outcomes or its outcomes can be reduced to two  categories which are called pass and fail, (2) there must be a fixed number of  trials, (3) the outcome of each trail must be independent, (4) the probabilities must  remain constant, (5) and the outcome of interest is the number of successes.  <P><B>Normal approximation for binomial:</B> All binomial tables are limited  in their scope; therefore it is necessary to use standard normal distribution  in computing the binomial probabilities. The following numerical example illustrates  how good the approximation could be. This provides an indication for real applications 
    when n is beyond the given values in the available binomial tables. <P>Numerical Example: A sample of 20 items are taken randomly from a manufacturing  process with defective probability p = 0.40. What is the probability of obtaining exactly 5 defective?<P><CENTER> P (5 out of 20) = {20!/[(5!)(15!)]} <font face=symbol>&#180;</font> (0.40)<FONT    size=+0><SUP>5</SUP></FONT>(0.6)<FONT size=+0><SUP>15</SUP></FONT>= 7.5% </CENTER>  <P>Since the mean and standard deviation of distribution are:  <P><CENTER>      <FONT face=Symbol>m</FONT> = np = 8, and <FONT face=Symbol>s</FONT> = (npq)<FONT size=+0><SUP>1/2</SUP></FONT>  = 2.19, </CENTER>  <P>respectively; therefore, the standardized observation for r = 5, by using  the continuity factor (which always enlarges) are:  <P>   <CENTER>z<FONT size=+0><SUB>1</SUB></FONT> = [(r-1/2) - <FONT   face=Symbol>m</FONT>] / <FONT face=Symbol>s</FONT> = (4.5 -8)/2.19 = -1.60,     and     <P>z<FONT size=+0><SUB>2</SUB></FONT> = [(r+1/2) - <FONT face=Symbol>m</FONT>]  / <FONT face=Symbol>s</FONT> = (5.5 -8)/2.19 = -1.14.   </CENTER><P>Therefore, the approximated P (5 out of 20) is P (z being within interval   -1.60, -1.14). Now, by using the standard normal table, we obtain:  <P>   <CENTER>P (5 out of 20) = 0.44520 - 0.37286 = 7.2% </CENTER>  <P>   <P><B>Comments:</B> The approximation for binomial distribution is used frequently in quality control, reliability, survey  sampling, and other industrial problems.    <p>
You might like to use the <a href="otherapplets/ConfIntPro.htm" target ="new"> Exact Confidence Interval Construction and Test of Hypothesis for Binomial Population </a>, and <A  href="otherapplets/pvalues.htm"   target=new>Binomial Probability Function Applet</A> JavaScript in performing some numerical experimentation for validating the above assertions for a deeper understanding. 
  <P> <P><A name=rExponential></A> <HR><H4><FONT color=#dc143c>Exponential Density Function</FONT></H4>An important class of decision problems under uncertainty concerns the random durations between events. For example, the the length of time between breakdowns  of a machine not exceeding a certain time interval,  such as the copying machine in your office not breaking down during this week.  <P>Exponential distribution gives distribution of time between independent events  occurring at a constant rate. Its density function is: <P>  <CENTER> f(t) = <FONT face=Symbol>l</FONT> exp(-<FONT face=Symbol>l</FONT>t),     </CENTER><P>where <FONT face=Symbol>l</FONT> is the average number of events per unit of time, which is a positive number. <P>The mean and the variance of the <A  href="#rrandomva">random variable</A> t (time between events) are 1/ <FONT face=Symbol>l</FONT>, and  1/<FONT face=Symbol>l</FONT><FONT size=+0><SUP>2</SUP></FONT>, respectively. 

 <P><B>Applications</B> include probabilistic assessment of the time between  arrivals of patients to the emergency room of a hospital, and time between arrivals of ships  at a particular port.   <P><B>Comments:</B> Itis a special case of  <a href="#rGamma">Gamma</a> distribution.   <P>You might like to use <A  href="otherapplets/pvalues.htm" target=new>Exponential Density Applet</A> to perform your computations, and  <A   href="otherapplets/LilliExpon.htm"   target=new>Lilliefors Test for Exponentiality</A> to perform the goodness-of-fit  test.   <P>
<P><A name=rFisherF></A> <HR><H4><FONT color=#dc143c>F-Density Function</FONT></H4> 
The F distribution is the distribution of the ratio of two independent sampling (of size of n<FONT><SUB>1</SUB></FONT>, and n<FONT><SUB>2</SUB></FONT>, respectively) estimates of variance from standard normal distributions.  It is also formed by the ratio of two independent chi-square variables divided by their respective independent degrees of freedom. 
<p>
 Its main applications are in <A   href="#rTwoIndTest">testing equality of two</a> independent population variances based on two independent random samples,  <A   href="#rANOVA">ANOVA</A>, and <A   href="#rregplandevmain">regression analysis</a>.<P>You might like to use <A  href="otherapplets/pvalues.htm"  target=new>F-Density Function</A> to obtain its P-values.
<P><A name=rtChiSquare></A> 
  <HR><H4><FONT color=#dc143c>Chi-square Density Function</FONT></H4>  The probability density curve of a Chi-square distribution is an asymmetric curve  stretching over the positive side of the line and having a long right tail.   The form of the curve depends on the value of a parameter known as the degree of freedom (d.f.).  <P>The expected value of Chi-square statistic is its d.f., its variance is twice  of its d.f., and its mode is equal to (d.f.- 2).  <P><B>Chi square Distribution relation to Normal Distribution:</B> The Chi-square distribution is  related to the sampling distribution of the variance when the sample is from a  normal distribution. The sample variance is a sum of squares of standard normal variables N (0, 1). Hence, the of square of  N (0,1) <A   href="#rrandomva">random variable</A> is a Chi-square with 1 d.f.. 

 <P>Notice that the Chi-square is related to F-statistics as follows: F = Chi-square/d.f.<FONT size=+0><SUB>1</SUB></FONT>,  where F has (d.f.<FONT   size=+0><SUB>1</SUB></FONT> = d.f. of the Chi-square-table, and d.f.<FONT  size=+0><SUB>2</SUB></FONT> is the largest available in the F-table) 
  <P>Similar to Normal <A   href="#rrandomva">random variable</A>s, the Chi-square has the additive property. For example, for  two independent Chi-square variables, their sum is also Chi-square with degrees of freedom equal to the sum of the d.f. of the individual d.f.s. Thus the <B>unbiased</B> sample variance for a sample of size n from  N (0,1) is a sum of n-1 Chi-squares,  each with d.f. = 1, hence Chi-square with d.f. = n-1.  <P>The most widely used applications of Chi-square distribution are:  <P><B>The Chi-square Test for Association</B> which is a non-parametric test; therefore, it can be used  for nominal data too. It is a  test of statistical significance widely used bivariate tabular  association analysis. Typically, the hypothesis is whether or not two populations are different  in some characteristic or aspect of their  behavior based on two random samples. This test procedure is also known as     the Pearson Chi-square test. <P><B>The Chi-square Goodness-of-Fit Test</B> is used to test if an observed distribution conforms to any particular distribution. Calculation of this goodness-of-fit  test is by comparison of observed data with data expected based on a particular distribution.   <P>You might like to use  <A href="otherapplets/pvalues.htm" 
  target=new>Chi-square Density</A> to find its P-values. <P><P><A name=rmultinomial></A> 
  <HR><H4><FONT color=#dc143c>Multinomial Probability Function</FONT></H4>   <DD>A multinomial <A   href="#rrandomva">random  variable</A> is an extended binomial. However, the difference is that in a 
    multinomial case, there are more than two possible outcomes. There are a fixed  number of independent outcomes, with a given probability for each outcome.  <P>The Expected Value (i.e., averages): <P>       <CENTER>Expected Value = <FONT face=symbol>m</FONT> = <FONT  face=symbol>S</FONT>X<FONT size=+0><SUB>i</SUB></FONT> <font face=symbol>&#180;</font> P<FONT size=+0><SUB>i</SUB></FONT>, &nbsp; &nbsp; the sum is over all i's.     </CENTER><P>Expected value is known also as <FONT color=#dc143c>the First Moment</FONT>,  borrowed from Physics, because it is the point of balance where the data  and the probabilities are the distances and the weights, respectively.  <P>The Variance is:  <P>    <CENTER>  Variance = <FONT face=symbol>s</FONT><FONT size=+0><SUP>2</SUP></FONT>   = <FONT face=symbol>S</FONT> [X<FONT size=+0><SUB>i</SUB></FONT><FONT  size=+0><SUP>2</SUP></FONT>  <font face=symbol>&#180;</font> P<FONT size=+0><SUB>i</SUB></FONT>] - <FONT   face=symbol>m</FONT><FONT size=+0><SUP>2</SUP></FONT>, &nbsp; &nbsp; the sum is over all i's. </CENTER>  <P>The variance is not expressed in the same units as the expected value.  So, the variance is hard to understand and to explain as a result of the squared  term in its computation. This can be alleviated by working with the square  root of the variance, which is called the <B>Standard (i.e., having the same unit as the data have) Deviation</B>:  <P><CENTER> Standard Deviation = <FONT face=symbol>s</FONT> = (Variance) <FONT size=+0><SUP>½</SUP></FONT>   </CENTER>  <P>Both variance and standard deviation provide the same information and,  therefore, one can always be obtained from the other. In other words, the  process of computing standard deviation always involves computing the variance.  Since standard deviation is the square root of the variance, it is always  expressed in the same units as the expected value.  <P>For the dynamic process, the Volatility as a measure for risk includes  the time period over which the standard deviation is computed. The <B>Volatility measure</B> is defined as standard deviation divided by the square root 
 of the time duration.  <P><B>Coefficient of Variation</B>: Coefficient of Variation (CV) is the <I>absolute 
  relative deviation </I>with respect to size <IMG   src="xbaru.gif"> provided <IMG   src="xbaru.gif"> is not zero, expressed in percentage:  <P> <CENTER> CV =100 |<FONT face=symbol>s</FONT>/<IMG src="xbaru.gif">| %      </CENTER>
    <P>Notice that the CV is independent from the expected value measurement.  The coefficient of variation demonstrates the relationship between standard   deviation and expected value, by expressing the risk as a percentage of    the expected value. The inverse of CV (namely 1/CV) is called the <B>Signal-to-Noise 
      Ratio</B>.  <P>You might like to use <A   href="otherapplets/multinomial.htm"   target=new>Multinomial Applet</A> for checking your computation and performing computer-assisted  experimentation. <P> <P><B>An Application:</B> Consider two investment alternatives, Investment  I and Investment II with the characteristics outlined in the following table:  <P> 
    <table align=center border=0 height=185 width=342>
    <tbody> 
    <tr bgcolor=#FFE8E8 valign=bottom align="center"> 
      <td  bordercolor=#333333 colspan=5 height=13><b><font face="Arial, Helvetica, sans-serif" size="2" color="#996666">- 
        </font><font face="Arial, Helvetica, sans-serif" size="2"><font color="#CC9999"><font color="#994848">Two 
        Investments</font></font><font color="#996666"> - </font></font></b></td>
    </tr>
    <tr bgcolor=#cccccc valign=bottom> 
      <td align=middle bordercolor=#333333 colspan=2 height=6 bgcolor="#f1f1f1"> 
        <div align=center><font face="Arial, Helvetica, sans-serif" size="2"><b><font color="#666666">Investment 
          I</font></b></font></div>
      </td>
      <td align=middle bgcolor=#ffffff bordercolor=#333333 height=6  width=4%><font face="Arial, Helvetica, sans-serif" size="2"></font></td>
      <td align=middle bordercolor=#333333 colspan=2 height=6 bgcolor="#f1f1f1"> 
        <div align=center><font face="Arial, Helvetica, sans-serif" size="2"><b><font color="#666666">Investment 
          II</font></b></font></div>
      </td>
    </tr>
    <tr bgcolor=#c6d5c6 valign=bottom> 
      <td bordercolor=#333333 height=2 width=24% bgcolor="#DBE3DB"> 
        <div align=center><font face="Arial, Helvetica, sans-serif" size="2">Payoff 
          %</font></div>
      </td>
      <td bordercolor=#333333 height=2 width=24% bgcolor="#DBE3DB"> 
        <div align=center><font face="Arial, Helvetica, sans-serif" size="2">Prob.</font></div>
      </td>
      <td bgcolor=#ffffff bordercolor=#333333 height=2 width=4%><font face="Arial, Helvetica, sans-serif" size="2"></font></td>
      <td bordercolor=#333333 height=2 width=24% bgcolor="#DBE3DB"> 
        <div align=center><font face="Arial, Helvetica, sans-serif" size="2">Payoff 
          %</font></div>
      </td>
      <td bordercolor=#333333 height=2 width=24% bgcolor="#DBE3DB"> 
        <div align=center><font face="Arial, Helvetica, sans-serif" size="2">Prob.</font></div>
      </td>
    </tr>
    <tr bordercolor=#000000> 
      <td bgcolor=#ffffff height=14 width=24%> 
        <div align=center><font face="Arial, Helvetica, sans-serif" size="2">1</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=24%> 
        <div align=center><font face="Arial, Helvetica, sans-serif" size="2">0.25</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=4%> 
        <div align=center><font face="Arial, Helvetica, sans-serif"><font face="Arial, Helvetica, sans-serif"><font size="3"><font size="2"></font></font></font></font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=24%> 
        <div align=center><font face="Arial, Helvetica, sans-serif" size="2">3</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=24%> 
        <div align=center><font face="Arial, Helvetica, sans-serif" size="2">0.33</font></div>
      </td>
    </tr>
    <tr bordercolor=#000000> 
      <td bgcolor=#ffffff height=14 width=24%> 
        <div align=center><font face="Arial, Helvetica, sans-serif" size="2">7</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=24%> 
        <div align=center><font face="Arial, Helvetica, sans-serif" size="2">0.50</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=4%> 
        <div align=center><font face="Arial, Helvetica, sans-serif"><font face="Arial, Helvetica, sans-serif"><font size="3"><font size="2"></font></font></font></font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=24%> 
        <div align=center><font face="Arial, Helvetica, sans-serif" size="2">5</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=24%> 
        <div align=center><font face="Arial, Helvetica, sans-serif" size="2">0.33</font></div>
      </td>
    </tr>
    <tr bordercolor=#000000> 
      <td bgcolor=#ffffff height=14 width=24%> 
        <div align=center><font face="Arial, Helvetica, sans-serif" size="2">12</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=24%> 
        <div align=center><font face="Arial, Helvetica, sans-serif" size="2">0.25</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=4%> 
        <div align=center><font face="Arial, Helvetica, sans-serif"><font face="Arial, Helvetica, sans-serif"><font size="3"><font size="2"></font></font></font></font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=24%> 
        <div align=center><font face="Arial, Helvetica, sans-serif" size="2">8</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=24%> 
        <div align=center><font face="Arial, Helvetica, sans-serif" size="2">0.34</font></div>
      </td>
    </tr>
    </tbody> 
  </table><DIV align=center><P><FONT face="Courier New, Courier, mono" size=3><B>Performance of Two Investments</B></FONT></P></DIV><P>To rank these two investments under the <I>Standard Dominance Approach  in Finance</I>, first we must compute the mean and standard deviation and  then analyze the results. Using the <A   href="otherapplets/multinomial.htm"   target=new>Multinomial Applet</A> for calculation, we notice  that the Investment I has mean = 6.75% and standard deviation = 3.9%, while the second investment has mean = 5.36% and standard deviation = 2.06%. First  observe that under the usual mean-variance analysis, these two investments  cannot be ranked. This is because the first investment has the greater mean; it also has the greater standard deviation; therefore, the <B>Standard Dominance  Approach</B> is not a useful tool here. We have to resort to the coefficient of variation (C.V.) as a systematic basis of comparison. The C.V. for Investment  I is 57.74% and for Investment II is 38.43%. Therefore, Investment II has  preference over the Investment  I. Clearly, this approach can be used to rank any number of alternative  investments. Notice that less variation in return on investment implies less risk. <P><P>You might like to use <A 
  href="otherapplets/MultiVariate.htm" 
  target=new>this Applet</A> in performing some numerical experimentation to: <P> <OL>    
<LI>Show that E[aX + b] = aE(X) + b.
<LI>Show that V[aX + b] = a<FONT size=+0><SUP>2</SUP></FONT>V(X).
<LI>Show that: E(X<FONT size=+0><SUP>2</SUP></FONT>)= V(X) + (E(X))<FONT size=+0><SUP>2</SUP></FONT>.
</ol><p>
  <P> <P><A name=rnd></A> <HR><H4><FONT color=#dc143c>Normal Density Function</FONT></H4>    In the Descriptive Statistic Section of this Web site, we have been concerned with how empirical  scores are distributed and how best to describe their distribution. We have discussed several different measures, but the mean <FONT face=symbol>m </FONT>will  be the measure that we use to describe the center of the distribution, and  the standard deviation <FONT face=symbol>s</FONT> will be the measure we use  to describe the spread of the distribution. Knowing these two facts gives us ample information to make statements about the probability of observing  a certain value within that distribution. If I know, for example, that the average Intelligence Quotient (I.Q.) score is 100 with a standard deviation of <FONT face=symbol>s</FONT> = 20, then I know that someone with an I.Q. of 140 is very  smart. I know this because 140 deviates from the mean <FONT face=symbol>m</FONT>by twice the average amount as the rest of the scores in the distribution.  Thus, it is unlikely to see a score as extreme as 140 because most of the I.Q. scores are clustered around 100 and  only deviate 20 points from  the mean <FONT face=symbol>m </FONT>. <P>Many applications arise from the central limit theorem (CLT). The CLT states that, average of values of n observations approaches normal distribution, irrespective of the form of original  distribution under quite general conditions. Consequently, normal distribution is an appropriate model for many, but not all, physical phenomena, such as distribution of physical measurements on living organisms, intelligence  test scores, product dimensions, average temperatures, and so on. <P>Know that the Normal distribution is to satisfy seven requirements: (1) the graph should be bell shaped curve; (2) mean, median and mode are all equal; (3) mean, median and mode are located at the center of the distribution; (4) it has only one mode, (5) it is symmetric about mean, (6) it is a continuous function; (6) it never touches x-axis; and (7) the area under curve equals one. <P>Many methods of statistical analysis presume normal distribution. 
<p>
When we know the mean and variance of a Normal then it allows us to find probabilities. So,  if, for example, you knew some things about the average height of women in the nation, including the fact that heights are distributed normally, you could measure all the women in your extended family and find the average height. This enables you to determine a probability associated with your result, if the probability of getting your result, given your knowledge of women nationwide, is high.  Then your family's female height cannot be said to be different from average. If that probability is low, then your result is rare (given the knowledge about women nationwide), and you can say your family is different. You have just completed a test of the  hypothesis that the average height of women in your family is different from   the overall average.  

<p>

The ratio of two independent observations from the standard normal is distributed as the Cauchy Distribution which  has thicker tails than a normal distribution. It density function is f(x) = 1/[<FONT FACE="Symbol">p</FONT>(1+x<FONT><SUP>2</SUP></FONT>)], for all real value x.

    <P>You might like to use <A href="otherapplets/pvalues.htm"   target=new>Standard Normal</A> Applet instead of using tabular values from your textbook, and the 
well-known <A href="otherapplets/Normality.htm" target=new>Lilliefors' Test for Normality</A> to assess the goodness-of-fit. 
    <P><P><A name=rPoisson></A> <HR><H4><FONT color=#dc143c>Poisson Probability Function</FONT></H4>
   
<p>
<BLOCKQUOTE>

<font  color="#00700f">Life is good for only two things, discovering mathematics and teaching mathematics.<br>  <div align="right"><cite>-- Simeon Poisson</a>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </cite></font>
</BLOCKQUOTE>
<p>
<div align="left">
An important class of decision problems under uncertainty is characterized by the small chance of the occurrence of a particular event, such as an accident. Poisson probability function computes the probability of exactly x independent occurrences during a given period of time, if events take place independently and at a constant rate. Poisson probability function also represent number of occurrences over constant areas or volumes: <P>Poisson probabilities are often used; for example in quality control, software and hardware reliability, insurance claim, number of incoming telephone calls, and queuing theory. 
    <P><B>An Application:</B> One of the most useful applications of the Poisson distribution is in the field of queuing theory. In many situations where queues occur it has been shown that the number of people joining the queue in a given time period follows the Poisson model. For example, if the rate of arrivals to an emergency room is <FONT face=Symbol>l</FONT> per unit of time period (say 1 hr), then: <P> 
<CENTER>P ( n arrivals) = <FONT face=Symbol>l</FONT><SUP>n</SUP>&nbsp; e<SUP>-<FONT face=Symbol>l 
</FONT></SUP>/ n!</CENTER><P>The mean and variance of <A href="#rrandomva">random 
      variable</A> n are both <FONT face=Symbol>l </FONT>. However if the mean and variance of a <A 
  href="#rrandomva">random variable</A> have equal numerical values, then it is not necessary that 
      its distribution is a Poisson. Its mode is within interval [<FONT  face=Symbol>l</FONT> -1, <FONT face=Symbol>l</FONT>]. <P><B>Applications:</B> <P> <CENTER>P ( 0 arrival) = e<SUP>-<FONT face=Symbol>l</FONT></SUP></CENTER><CENTER>P ( 1 arrival) = <FONT face=Symbol>l</FONT>&nbsp; e<SUP>-<FONT  face=Symbol>l </FONT></SUP>/ 1!</CENTER><CENTER>P ( 2 arrival) = <FONT face=Symbol>l</FONT><SUP>2</SUP>&nbsp; e<SUP>-<FONT face=Symbol>l</FONT></SUP>/ 2!
</CENTER><P>and so on. In general: <P> <CENTER>P ( n+1 arrivals ) = <FONT face=Symbol>l </FONT>P ( n arrivals ) / n. </CENTER><P><P><B>Normal approximation for Poisson:</B> All Poisson tables are limited  in their scope; therefore, it is necessary to use standard normal distribution in computing the Poisson probabilities. The following numerical example illustrates how good the approximation could be.  <P><B>Numerical Example:</B> Emergency patients arrive at a large hospital at the rate of 0.033 per minute. What is the probability of exactly two arrivals during the next 30 minutes? <P>The arrival rate during 30 minutes is <FONT face=Symbol>l</FONT> = (30)(0.033) = 1. Therefore, <P><CENTER>P (2 arrivals) = [1<FONT size=+0><SUP>2</SUP></FONT> /(2!)] e<FONT size=+0><SUP>-1</SUP></FONT> = 18% </CENTER>
    <P>The mean and standard deviation of distribution are: <P><CENTER><FONT face=Symbol>m</FONT> = <FONT face=Symbol>l</FONT> = 1, and <FONT face=Symbol>s</FONT> = <FONT face=Symbol>l</FONT> <FONT  size=+0><SUP>1/2</SUP></FONT> = 1, </CENTER>
    <P>respectively; therefore, the standardized observation for n = 2, by using the continuity factor (which always enlarges) are: <P><CENTER>z<FONT size=+0><SUB>1</SUB></FONT> = [(r-1/2) - <FONT 
  face=Symbol>m</FONT>] / <FONT face=Symbol>s</FONT> = (1.5 -1)/1 = 0.5, and <P>z<FONT size=+0><SUB>2</SUB></FONT> = [(r+1/2) - <FONT face=Symbol>m</FONT>] / <FONT face=Symbol>s</FONT> = (2.5 -1)/1 = 1.5.  </CENTER><P>Therefore, the approximated P (2 arrivals) is P (z being within the interval 0.5, 1.5). Now, by using the standard normal table, we obtain: <P> <CENTER>P (2 arrivals) = 0.43319 - 0.19146 = 24% </CENTER><P>As you see the approximation is slightly overestimated, therefore the error is on the safe side. For large values of <FONT face=Symbol>l</FONT>, say over 20, one may use the Normal approximation to calculate Poisson probabilities. <P>Notice that by taking the square root of a Poisson <A   href="#rrandomva">random variable</A>, the transformed variable is more symmetric. This is a useful 
 transformation in regression analysis of Poisson observations. <P>You might like to use <A 
  href="otherapplets/pvalues.htm" target=new>Poisson Probability Function Applet</A> to perform your computation, and <A 
  href="otherapplets/PoissonTest.htm" 
  target=new>Testing Poisson</A> to perform the goodness-of-fit test. 

<P><B>Further Reading:</B><BR>
<FONT face="Bookman Old Style" size=-2>Barbour <I>et al.</I>, <I>Poisson Approximation</I>, Oxford University Press, 1992. </FONT>
<p>
<P><A name=rtdistributions></A> 
    <HR><H4><FONT color=#dc143c>Student T-Density Function</FONT></H4>The t distributions were discovered in 1908 by <A href="http://www-history.mcs.st-and.ac.uk/~history/Mathematicians/Gosset.html" 
  target=new>William Gosset</A>, who was a chemist and a statistician employed by the Guinness brewing company. He considered himself a student still learning statistics, so that is how he signed his papers as pseudonym "Student". Or, perhaps he used a pseudonym due to "trade secret" restrictions by Guinness. 
<P>Note that there are different t-distributions; it is a class of distributions. When we speak of a specific t distribution, we have to specify the degrees of freedom. The t density curves are symmetric and bell-shaped like the normal distribution and have their peak at 0. However, the spread is more than that of the standard normal distribution. The larger the degrees of  freedom, the closer the t-density is to the normal density. 
<p>   
The shape of a t-distribution depends on a parameter called "degree-of-freedom". As the degree-of-freedom gets larger, the t-distribution gets closer and closer to the standard normal distribution. For practical purposes, the t-distribution is treated as the standard normal distribution when degree-of-freedom is greater than 30. <P>Suppose we have two independent <A href="#rrandomva">random variable</A>s, one is Z, distributed as the standard normal distribution, while the other  has a Chi-square distribution with (n-1) d.f.; then the <A 
  href="#rrandomva">random variable</A>: <P> <CENTER>(n-1)Z / <FONT face=symbol>c</FONT><FONT size=+0><SUP>2</SUP> </FONT></CENTER><FONT size=+0><P>has a t-distribution with (n-1) d.f. For large sample size (say, n over 30), the new <A href="#rrandomva">random variable</A> has an expected value equal to zero, and its variance is (n-1)/(n-3) which is close to one. <P>Notice that the t- statistic is related to F-statistic as follow: F = t<FONT size=+0><SUP>2</SUP></FONT>, where F has (d.f.<FONT  size=+0><SUB>1</SUB></FONT> = 1, and d.f.<FONT size=+0><SUB>2</SUB></FONT> = d.f. of the t-table) <P>You might like to use <A 
  href="otherapplets/pvalues.htm" 
  target=new>Student t-Density</A> to obtain its P-values. 


<LI><A href="#rTriangular"  target=new>Triangular Density Function</A> 

<P><A name=Triangular></A> <HR>   <H4><FONT color=#dc143c>Triangular Density Function</FONT></H4>
The triangular distribution shows the number of successes when you know the minimum, maximum, and most likely values. For example, you could describe the number of intakes seen per week when past intake data show the minimum, maximum, and most likely number of cases seen. It has a continuous probability distribution.
<p>
The parameters for the triangular distribution are Minimum, Maximum, and Likeliest. There are three conditions underlying triangular distribution: 
<p>
<ul>
<Li>The minimum number of items is fixed. 
<Li>The maximum number of items is fixed. 
<Li>The most likely number of items falls between the minimum and maximum values.
</ul>
<p>
These three parameters forming a triangular shaped distribution, which shows that values near the minimum and maximum are less apt to occur than those near the most likely value.
<P><B>Further Reading:</B><BR>
<FONT face="Bookman Old Style" size=-2>
 Evans M., Hastings N., and B., Peacock, <I>Triangular Distribution</I>,  Ch. 40 in Statistical Distributions, Wiley, pp. 187-188, 2000.</FONT>
<p>
<P><A name=rUniform></A> <HR>   <H4><FONT color=#dc143c>Uniform Density Function</FONT></H4> The uniform density function gives the probability that observation will occur within a particular interval [a, b] when probability of occurrence within that interval is directly proportional to interval length. Its mean and variance are:<p>
<center><FONT face=symbol>m</FONT> = (a+b)/2, &nbsp;&nbsp;&nbsp;<FONT face=symbol>s</FONT><FONT><SUP>2</SUP></FONT> = (b-a)<FONT><SUP>2</SUP></FONT>/12.
</center>

<p> <B>Applications:</B> Used to generate random numbers in sampling and Monte Carlo simulation.  <P>Comments: Special case of beta distribution.   <P>You might like to use <A href="otherapplets/Uniform.htm"  target=new>Goodness-of-Fit Test for Uniform</A> and performing some numerical experimentation for a deeper understanding of the concepts. 
<p>
Notice that any Uniform distribution has uncountable number of modes having equal density value; therefore it is considered as a homogeneous population.
<P><B>Further Reading:</B><BR>
<FONT face="Bookman Old Style" size=-2>
Balakrishnan N., and V. Nevzorov, <I>A Primer on Statistical Distributions</I>, Wiley, 2003.</FONT>
<p>
<P><A name=rConditionTest></A> 
  <HR>
  <H4><FONT color=#dc143c>Necessary Conditions for Statistical Decision Making</FONT></H4>
<B>Introduction to Inferential Data Analysis Necessary Conditions:</B> Do not just learn formulas and number-crunching. Learn about the <I>conditions</I>  under which statistical testing procedures apply. The following conditions are   common to almost all statistical tests: 
  <P></P></BLOCKQUOTE><OL>  <P>   <LI> Any undetected <A  href="#routlier">outliers</a> may have major impact and may influence the results of almost all statistical estimation and testing procedures.    <P></P> <LI>Homogeneous population. That is, there is not more than one mode. Perform <A 
  href="histograming/topframe.html"  target="new">Test for Homogeneity of a Population</A>  <P></P> <LI>The sample must be random. Perform <A 
  href="otherapplets/Randomness.htm"   target=new>Test for Randomness</A>.  <P></P> <LI>In addition to the Homogeneity requirement, each population has a normal distribution. 
    Perform the <A  href="otherapplets/Normality.htm" 
  target=new>Lilliefors' Test for Normality</A>. <P></P><LI>Homogeneity of variances. Variation in each population is almost the same  as in the other(s). Perform <A  href="otherapplets/BartletTest.htm"  target=new>The Bartlett's Test</A>.  <P>For two populations use the F-test. For 3 or more populations, there is a  practical rule known as the "Rule of 2". In this rule, one divides the highest variance of a sample by the lowest variance of the other sample. Given that the sample sizes are almost the same, and the value of this division is less than 2, then the variations of the populations are almost the same. 
    <P><B>Notice:</B> This important condition in analysis of variance (ANOVA and the t-test for mean differences) is commonly tested by the Levene test or its modified test known as the Brown-Forsythe test. Interestingly, both  tests rely on the homogeneity of variances condition! </P></LI></OL>
<BLOCKQUOTE>These conditions are crucial, not for the method of computation,  but for the testing using the resultant statistic. Otherwise, we can do ANOVA and regression without any assumptions, and the numbers come   out the same. Simple computations give us least-square fits, partitions of  variance, regression coefficients, and so on.  We do need the above conditions when  test of hypotheses are our main concern. <P><B>Further Readings:</B><BR><FONT face="Bookman Old Style"   size=-2>
Good Ph., and  J.  Hardin, <I>Common Errors in Statistics</I>, Wiley, 2003.<br>
Wang  H., Improved confidence estimators for  the usual one-sided confidence intervals for the ratio of two normal variances,  <I>Statistics &amp; Probability Letters</I>, Vol. 59, No.3, 307-315, 2002. </font> 
  <P> 

<P><A name=routlier></A> <HR> <H4><font color="#dc143c" >Measure of Surprise for Outlier Detection</font></H4>
Robust statistical techniques are needed to cope with any undetected outliers; otherwise they are more likely to invalidate <a href="#rConditionTest">the conditions underlying statistical techniques</a>, and they may seriously distort estimates and produce misleading conclusions in test of hypotheses. A common approach consists of  assuming that contaminating models, different from the one generating the rest of the data, generate the (possible) outliers. 
<p><p>
Because of a potentially large variance, outliers could be the outcome of sampling errors or clerical errors such as recording data. Therefore, you must be very careful and cautious. Before declaring an observation "an outlier," find out why and how such observation occurred. It could even be an error at the data entering stage while using any computer package.
<p>

In practice, any observation with a standardized value greater than 2.5 in absolute value is a candidate for being an outlier. In such a case, one must first investigate the source of the datum. If there is no doubt about the accuracy or veracity of the observation, then it should be removed, and the model should be refitted. 
<p>
<ol><li>Compute the mean (<IMG   src="xbaru.gif">) and standard deviation (S) of the whole sample.
<p><li>Set limits for the mean <IMG   src="xbaru.gif">:  
<center><IMG   src="xbaru.gif"> -  k <font face=symbol>&#180;</font> S,&nbsp; &nbsp;&nbsp; &nbsp; <IMG   src="xbaru.gif"> + k <font face=symbol>&#180;</font> S.</center> A typical value for k is 2.5
<p><li>Remove all sample values outside the limits.      
<p><li>Now, iterate through the algorithm, the sample set may reduce after removing the outliers by applying step 3.
<p><li>In most cases, we need to iterate through this algorithm several times until all outliers are removed.
<p></ol>
<font color="#dc143c" ><b>An Application:</b></font> Suppose you ask ten of your classmates to measure a given length X.  The results (in mm) are:
<p>
<center>
46, 48, 38, 45, 47, 58, 44, 45, 43, 44
<p>
</center>
Is 58 an outlier?  Computing the mean and the variance of the ten measurement using the <a href="otherapplets/Descriptive.htm" target ="new">Descriptive Sampling Statistics</a> JavaScript, are 45.8, and 5.1(after the needed adjustment), respectively.  The Z-value for 58 is Z (58) = 2.4.  Since the measurements, in general, follow a normal distribution, therefore,
<p>
<center>
Probability [X as large as 2.4 times standard deviation] = 0.008,
<p>
</center>
obtained by using the <a href="otherapplets/pvalues.htm#rnorm" target ="new">Standard Normal  P-value</a> JavaScript, or from the normal table in your textbook.
<p>
According this probability, one expects only .09 of the ten measurements as bad as this one.  This is a very rare event, however, in spite of such small probability, it has occurred, therefore, it might be an outlier.
<p>
The next most suspected measurement is 38, is it an outlier? It is a question for you.
<p>
<B>A Notice:</B>  Outlier detection in the single population setting is not too difficult. Quite often, however, one can argue that the detected outliers are not really outliers, but <font color="#dc143c" ><b>form a second population</b></font> . If this is the case, a data separation approach needs to be taken. 
<p>
You might like to use the <a href="otherapplets/Outlier.htm" target ="new">Identification of Outliers</a> JavaScript in performing some numerical experimentation for validating and for a deeper understanding of the concepts<p><B> Further Reading:</B><br>
<FONT SIZE="-2" FACE="Bookman Old Style">
Rothamsted V., V. Barnett, and T. Lewis, <I>Outliers in Statistical Data</I>, Wiley, 1994.</font>
<p>
  <P><A name=rhomoPop></A> <HR>  <H4><font color="#dc143c" >Homogeneous Population</font></H4>  A homogeneous population is a statistical  population which has <FONT color=#dc143c><B>a unique  mode</B></FONT>.
<p>
Notice that, e.g.,  a <A  href="#rUniform">Uniform</A> distribution has uncountable number of modes having equal density value; therefore it is considered as a homogeneous population.
<p>
 To determine   if a given population is homogeneous or not, construct the histogram of a random  sample from the entire population. If there is more than one mode, then you  have a mixture of two or more different populations. Know that to perform any statistical testing, 
  you need to make sure you are dealing with a homogeneous population. </font><P><font size="+0">One of the main applications of histogramming is to 
<A href="histograming/topframe.html"  target="new">Test for Homogeneity of a Population</A>. The unimodality of the histogram  is a necessary condition for the homogeneity of a population in order to conduct any meaningful statistical analysis. However, notice that, e.g.,  a <A  href="#rUniform">Uniform</A> distribution has uncountable number of modes having equal density value; therefore it is considered as a homogeneous population. </font> <P><font size="+0"><A name=rrunstest></A> </font> <HR><H4><font color="#dc143c" size="+0">Test for Randomness: The Runs' Test</font></H4> <font size="+0">  A basic condition in almost all inferential statistics is that a set of data constitutes a random sample from a given  homogeneous  population.  The condition of  randomness is essential to make sure <FONT color=#dc143c>the sample is truly representitive of the population</FONT>.  The widely used test for randomness is the Runs test. </font><P><font size="+0">A "run" is a maximal subsequence of like elements. </font><P><font size="+0">Consider the following sequence (D for Defective items, N for Non-defective 
 items) from a production line: DDDNNDNDNDDD. Number of runs is R = 7, with  n<SUB>1</SUB> = 8, and n<SUB>2</SUB> = 4 which are number of D's and N's.  </font><P><font size="+0">A sequence is a random sequence if it is neither "over-mixed"  nor "under-mixed". An example of over-mixed sequence is DDDNDNDNDNDD, with  R = 9 while under-mixed looks like DDDDDDDDNNNN with R = 2. There the above 
    sequence seems to be a random sequence. </font><P><font size="+0">The Runs Tests, which is also known as Wald-Wolfowitz Test,  is designed to test the randomness of a given sample at 100(1- <FONT face=symbol>a</FONT>)%  confidence level. To conduct a runs test on a sample, perform the following 
    steps: </font><P><font size="+0"><B>Step 1:</B> compute the mean of the sample. </font>
  <P><font size="+0"><B>Step 2: </B>going through the sample sequence, replace  any observation with +, or - depending on whether it is above or below the mean. Discard any ties. </font><P><font size="+0"><B>Step 3:</B> compute R, n<SUB>1</SUB>, and n<SUB>2</SUB>.  </font><P><font size="+0"><B>Step 4:</B> compute the expected mean and variance of  R, as follows: </font>  <P><font face="symbol" size="+0">a</font><font size="+0"> =1 + 2n<SUB>1</SUB>n<SUB>2</SUB>/(n<SUB> 1</SUB> + n<SUB>2</SUB>). </font>
  <P><font face="symbol" size="+0">s</font><font size="+0"><SUP>2</SUP> = 2n<SUB>1</SUB>n<SUB>2</SUB>(2n<SUB> 1</SUB>n<SUB>2</SUB>-n<SUB>1</SUB>- n<SUB>2</SUB>)/[[n<SUB>1</SUB> + n<SUB>2</SUB>)<SUP>2</SUP> (n<SUB>1</SUB> + n<SUB>2</SUB> -1)]. </font> <P><font size="+0"><B>Step 5: </B>Compute z = (R-<FONT face=symbol>m</FONT>)/ <FONT 
  face=symbol>s</FONT>. </font> <P><font size="+0"><B>Step 6:</B> Conclusion: </font>
  <P><font size="+0">If z <FONT face=symbol>&gt;</FONT> Z<FONT face=symbol><SUB>a</SUB></FONT>, 
    then there might be cyclic, seasonality behavior (under-mixing). </font> <P><font size="+0">If z <FONT face=symbol>&lt;</FONT> - Z<FONT face=symbol><SUB>a</SUB></FONT>, then there might be a trend. </font>
  <P><font size="+0">If z <FONT face=symbol>&lt;</FONT> - Z<FONT   face=symbol><SUB>a/2</SUB></FONT>, or z <FONT face=symbol>&gt;</FONT> Z<FONT  face=symbol><SUB>a/2</SUB></FONT>, reject the randomness. </font> <P><font size="+0"><B>Note:</B> This test is valid for cases for which both n<SUB>1</SUB> and n<SUB>2</SUB> are large, say greater than 10. For small sample sizes, special tables must be used. </font>
  <P><font size="+0">For example, suppose for a given sample of size 50, we have R = 24, n<SUB>1</SUB> = 14 and n<SUB>2</SUB> = 36. Test for randomness at <FONT face=symbol>a</FONT> = 0.05. <BR> The Plugging these into the above formulas we have <FONT face=symbol>a</FONT>  = 16.95, <FONT face=symbol>s</FONT> = 2.473, and z = -2.0 From Z-table, we  have Z = 1.645. Therefore, there might be a trend, which means that the sample is not random. </font><P><font size="+0">You may use the following JavaScript to <A 
  href="otherapplets/Randomness.htm"  target=new>Test for Randomness</A>. </font><P><font size="+0"><A name=rTestNormal></A> </font>
  <HR><H4><font color="#dc143c" size="+0">Test for Normality</font></H4><font size="+0">The standard test for normality is the Lilliefors' statistic. A histogram and normal probability plot will also help you distinguish between a systematic departure from normality when it shows up as a curve. </font>
  <P><font color="#dc143c">Lilliefors' Test for Normality:</font>    This test is a special case of the <A   href="otherapplets/Uniform.htm"   target=new>Kolmogorov-Smirnov goodness-of-fit test</A>, developed for testing the normality of population's distribution. When  applying the Lilliefors test,  a comparison is made between the standard normal <A   href="#rcdffunc">cumulative distribution function</A>, and a sample cumulative distribution function with standardized <A   href="#rrandomva">random variable</A>. If there is a close agreement between the two cumulative distributions,  the hypothesis that the sample was drawn from population with a normal distribution  function is supported. If, however, there is a discrepancy between the two cumulative distribution functions too great to be attributed to chance alone,  then the hypothesis is rejected. <P>The difference between the two cumulative distribution functions is measured by the statistic D, which is the greatest vertical distance between the two functions.  <P>You might like to use the well-known <A  href="otherapplets/Normality.htm"   target=new>Lilliefors' Test for Normality</A> to assess the goodness-of-fit. 
 <P><B>Further Readings</B><BR><FONT face="Bookman Old Style"  size=-2>Thode T., <I>Testing for Normality</I>, Marcel Dekker, Inc., 2001. Contains the major tests for normality. </font> 
  <P><A name=rqualestiunbsuff></A> <HR><H4><font color="#dc143c">Introduction to Estimation</font></H4>To estimate means to esteem (to give value to). An estimator is any quantity calculated from the sample data which is used to give information about an unknown quantity in the population. For example, the sample mean is an estimator of the population mean <FONT face=symbol>m</FONT>. </font> <P><font size="+0">Results of estimation can be expressed as a single value; known as a point estimate, or a range of values, referred to as a confidence     interval. Whenever we use point estimation, we calculate the margin of error associated with that point estimation. </font><P><font size="+0">Estimators of population parameters are sometimes distinguished     from the true value by using the symbol 'hat'. For example, true population standard deviation <FONT face=symbol>s</FONT> is estimated from a sample population standard deviation. </font>
  <P><font size="+0">Again, the usual estimator of the population mean is <IMG  src="xbaru.gif"> = <FONT face=symbol>S</FONT>x<FONT><SUB>i</SUB></FONT> / n, where n is the size of  the sample and x<FONT><SUB>1</SUB></FONT>, x<FONT><SUB>2</SUB></FONT>, x<FONT><SUB>3</SUB></FONT>,.......,x<FONT><SUB>n</SUB></FONT> are the values of the sample. If the value of the estimator in a particular sample is found to be 5, then 5 is 
    the estimate of the population mean µ. 

<P><A name=rgoodquali></A> <HR><H4><font color="#dc143c">Qualities of a Good Estimator</font></H4>
A "Good" estimator is the one which provides an estimate with the following qualities: </font>
  <P><font color="#dc143c" size="+0"><B>Unbiasedness:</B></font><font size="+0">  An estimate is said to be an unbiased estimate of a given parameter when the expected value  of that estimator can be shown to be equal to the parameter being estimated. For example, the mean of a sample is an unbiased estimate 
    of the mean of the population from which the sample was drawn. Unbiasedness is a good quality for an estimate, since, in such a case, using weighted average of several estimates provides a better estimate than each one of those estimates. Therefore, unbiasedness allows us to upgrade our estimates. For example, if 
    your estimates of the population mean µ are say, 10, and 11.2 from two independent samples of sizes 20, and 30 respectively, then a better estimate of the population mean µ based on both samples is [20 (10) + 30 (11.2)] (20 + 30) = 10.75. </font><P><font color="#dc143c" size="+0"><B>Consistency:</B></font><font size="+0"> 
    The standard deviation of an estimate is called the standard error of that  estimate. The larger the standard error the more error in your estimate. The standard deviation of an estimate is a commonly used index of the error entailed in estimating a population parameter based on the information in a random sample of size n from the entire population. 
  <P>An estimator is said to be "consistent" if increasing the sample size produces an estimate with smaller standard error. Therefore, your estimate is "consistent" with the sample size. That is, spending more money to obtain a larger sample produces a better estimate. </font><P><font color="#dc143c"><B>Efficiency:</B></font> An efficient estimate is one which has the smallest standard error among  all <U>unbiased</U> estimators. <P>The "best" estimator is the one which is the closest to  the population parameter being estimated.  <P>     <CENTER>
     <a href="BiasVaraince.gif"><IMG alt="The Concept of distance for an estimator"   src="BiasVaraince.gif" width="268" height="184" border="0"></a>
<p>The Concept of Distance for an Estimator<br>      <b> <font color="#dc143c">Click on the image to enlarge it  and THEN print it</font></b> <P>
    </CENTER>  <P>The above figure illustrates the concept of closeness by  means of aiming at the center for <FONT color=#dc143c>unbiased with minimum  variance</FONT>. Each dart board has several samples: </font>  <P><font size="+0">The first one has all its shots clustered tightly together, but none of them hit the center. The second one has a large spread, but around  the center. The third one is worse than the first two. Only the last one has a tight cluster around the center, therefore has good efficiency.  </font>
  <P><font size="+0">If an estimator is unbiased, then its variability will determine its reliability. If an estimator is extremely variable, then the estimates it produces may not on average be as close to the population parameter as a biased estimator with small variance. </font><P><font size="+0">The following chart depicts the quality of a few popular estimators for the population mean µ: </font><P> <CENTER> <font size="+0"><IMG alt="Quality of a few estimators"   src="GoodEstimator.gif"> </font>    </CENTER>  <P><font size="+0">The widely used estimator of the population mean µ is <IMG   src="xbaru.gif"> = <FONT face=symbol>S</FONT>x<FONT><SUB>i</SUB></FONT>/n, where n is the size of the sample and x<FONT><SUB>1</SUB></FONT>, x<FONT><SUB>2</SUB></FONT>, x<FONT><SUB>3</SUB></FONT>,......., x<FONT><SUB>n</SUB></FONT> are the values of the sample that have all of the above good properties. Therefore, it is a "good" estimator. <P>If you want an estimate of central tendency as a parameter for a test or for comparison, then small sample sizes are unlikely to yield any stable estimate. The mean is sensible in a symmetrical distribution as a measure of central tendency; but, e.g., with ten cases, you will not be able to judge whether you have a symmetrical distribution. However, the mean estimate is useful if you are trying to estimate the population sum, or some other function of the expected value of the distribution. Would the median be a better measure? In some distributions (e.g., shirt size) the mode may be better. BoxPlot will indicate <A  href="#routlier">outliers</a> in the data set. If there are outliers, the median is better than the mean as a measure of central tendency.  <P>You might like to use <A   href="otherapplets/Descriptive.htm"   target=new>Descriptive Statistics</A> Applet for obtaining "good" estimates.   <P><B>Further Readings</B><BR> <FONT face="Bookman Old Style" 
  size=-2>Casella G., and R. Berger, <I>Statistical Inference</I>, 
    Wadsworth Pub. Co., 2001.<BR>Lehmann E., and G. Casella, <I>Theory of Point Estimation</I>, Springer Verlag, New York, 1998.<BR></font> <P> <P><A name=rwci></A>
  <HR>  <H4><font color="#dc143c">Statistics with Confidence</font></H4>  In practice, a confidence interval is used to express the uncertainty   in a quantity being estimated. There is uncertainty because inferences are based   on a random sample of finite size from the entire population or process of interest.   To judge the statistical procedure we can ask what would happen if we were to   repeat the same study, over and over, getting different data (and thus different   confidence intervals) each time.  <P>In most studies, investigators are usually interested in determining     the size of difference of a measured outcome between groups, rather than a simple indication of whether or not it is statistically significant. Confidence  intervals present a range of values, on the basis of the sample data, in which the  value of such a difference may lie. </font><P><font size="+0">Know that a confidence interval computed from one sample  will be different from a confidence interval computed from another sample.     </font>  <P><font size="+0">Understand the relationship between sample size and width  of confidence interval, moreover, know that sometimes the computed confidence interval does not contain the true value. <P><font size="+0"> Let's say you compute a 95% confidence interval for a mean <FONT face=symbol>m </FONT>. The way  to interpret this is to imagine an infinite number of samples from the same population, 95% of the computed intervals will contain the population mean <FONT face=symbol>m </FONT>, and at most 5% will not. However, it is wrong to state, "I am 95% confident 
    that the population mean <FONT face=symbol>m </FONT>falls within the interval."  </font><P><font size="+0">Again, the usual definition of a 95% confidence interval is an interval constructed by a process such that the interval will contain  the true value 95% of the time. This means that "95%" is a property of the 
    process, not the interval. </font><P><font size="+0">Is the probability of occurrence of the population mean greater in the confidence interval (CI) center and lowest at the boundaries? Does the probability of occurrence of the population mean in a confidence interval vary in a measurable way from the center to the boundaries? In a general sense,  <A   href="#rnormalcond">normality condition</A> is assumed, and then the interval between CI limits is represented by a bell shaped t distribution. The expectation (E) of another value is highest at the calculated mean value, and decreases as the values approach the CI limits. 

<P><B>Tolerance Interval and CI:</B> A good approximation for the single measurement tolerance interval is n<SUP>½</SUP> times confidence interval of the mean. </font>  <P><center><font size="+0"><a href="math.gif"><img src="math.gif" width="161" height="165" border="0"></a></font><br>Statistics with Confidence<br>  <FONT size=+0><FONT size=+0><b><font color="#dc143c">Click on the image to enlarge it and THEN print it</font></b> </FONT></FONT>
</center>  <P>   
<p>
<p>
You need to use <a href="otherapplets/SampleSize.htm" target= "new">Sample Size Determination</a> JavaScript at the design stage of your statistical investigation in decision making with specific subjective requirements.
<p>
<P><font color="#dc143c"><B>A Note on Multiple Comparison via Individual Intervals:</B></font> Notice that, if the confidence intervals from two samples do not overlap, there is a statistically significant difference, say at 5%. However, the other way is not true; two confidence intervals can overlap even when there is a significant difference between them. </font><P><font size="+0">As a numerical example, consider the means of two independent samples. Suppose their values are 10 and 22 with equal standard error of 4.  The 95% confidence interval for the two statistics (using the critical value of 1.96) are:  [2.2, 17.8] and [14.2, 29.8], respectively. As you see they display considerable overlap. However, the z-statistic for the two-population mean is: |22 -10|/(16 + 16)<SUP>½</SUP> = 2.12 which is clearly significant under the same conditions as applied for constructing the confidence intervals. </font><P><font size="+0">One should examine the confidence interval for the difference explicitly. Even if the confidence intervals are overlapping, it is hard to find the exact  overall confidence level. However, the sum of individual confidence levels can serve as an upper limit. This is evident from the fact that: P(A and B) <FONT face=symbol>£</FONT> P(A) + P(B). 
  <P>The <A href="http://www.stat.sc.edu/~west/javahtml/ConfidenceInterval.html"   target=new>Confidence Interval</A> JavaScript demonstrates the precision vs confidence. <P><B>Further Reading:</B><BR>
    <FONT face="Bookman Old Style" size=-2>Cohen J., <I>Statistical Power Analysis for 
    the Behavioral Sciences</I>, L. Erlbaum Associates, 1988.<BR>Kraemer H., and S. Thiemann, <I>How Many Subjects?</I> Provides basic sample size tables , explanations, and power analysis.<BR>Murphy K., and B. Myors, <I>Statistical Power Analysis</I>, L. Erlbaum Associates, 1998. Provides a simple and general sample size determination for hypothesis  tests.<BR>Newcombe R., Interval estimation for the difference between independent proportions: Comparison of eleven methods, <I>Statistics in Medicine</I>, 17, 873-890, 1998.<BR>
 Hahn G. and W. Meeker, <I>Statistical Intervals: A Guide for Practitioners</I>, 
  Wiley, 1991.<br>Schenker N., and J. Gentleman, On judging the significance of differences by examining the overlap between confidence intervals, <I>The American Statistician</I>, 55(2), 135-139, 2001.
</FONT> <P>  <P><font size="+0"><A name=rmarginerror></A> </font> <HR> <H4><font color="#dc143c" size="+0">What Is the Margin of Error?</font></H4>
  <font size="+0">Estimation is the process by which sample data are used to indicate 
  the value of an unknown quantity in a population. </font>  <P><font size="+0">Results of estimation can be expressed as a single value, known as a point estimate; or a range of values, referred to as a confidence 
    interval. </font>  <P><font size="+0">Whenever we use point estimation, we calculate the margin  of error associated with that point estimate. For example, for the estimation of the population proportion, by the means of sample proportion (p), the margin of error is calculated <I><B>often</B></I> as follows: </font>
  <P>     <CENTER> <font size="+0">±1.96 [p(1-p)/n]<SUP>1/2</SUP></font>  </CENTER><P><font size="+0">In newspapers and television reports on public opinion polls, the margin of error often appears in a <I><B>small font</B></I> at the bottom  of a table or screen. However, reporting the amount of error only, is not informative enough by itself, what is missing is the <B>degree of the confidence</B> in the findings. The more important missing piece of  information is the sample size n; that is, <FONT color=#dc143c>how many people 
    participated in the survey, 100 or 100000?</FONT> By now, you know well that the larger the sample size the more accurate is the finding, right? </font> <P><font size="+0">The reported margin of error is the margin of "sampling error".  There are many non-sampling errors that can and do affect the accuracy of polls.  Here we talk about sampling error. The fact that sub-groups might have sampling error larger than the group, one must include the following statement in the report:<p>
<BLOCKQUOTE>
 "Other sources of error include, but are not limited to, individuals refusing to participate in the  interview and inability to connect with the selected number. Every feasible  effort was made to obtain a response and reduce the error, but the reader (or the viewer) should be aware that some error is inherent in all research." 
</BLOCKQUOTE> <P><font size="+0">If you have a yes/no question in a survey, you probably want to calculate a proportion P of Yes's (or No's). In a simple random sample survey, the variance of p is p(1-p)/n, ignoring the finite population correction,  for large n, say over 30. Now a 95% confidence interval is </font>
  <P>     <CENTER>  <font size="+0">p - 1.96 [p(1-p)/n]<SUP>1/2</SUP>,&nbsp;&nbsp; p + 1.96 
      [p(1-p)/n]<SUP>1/2</SUP>. </font>   </CENTER>  <P>     <CENTER>    </CENTER>    <font size="+0">A conservative interval can be calculated, since p(1-p) takes  its maximum value when p = 1/2. Replace 1.96 by 2, put p = 1/2 and you have  a <FONT color=#dc143c>95% consevative confidence interval</FONT> of 1/n<SUP>1/2</SUP>.  This approximation works well as long as p is not too close to 0 or 1. This useful approximation allows you to calculate approximate 95% confidence intervals.     </font>  <P><font size="+0">For continuous random variables, such as the estimation of the   population mean <FONT face=symbol>m</FONT>, the margin of error is  calculated  <I><B>often</B></I> as follows: </font>
  <P>     <CENTER>      <font size="+0">±1.96 S/n<SUP>1/2</SUP>. </font>    </CENTER>  <P><font size="+0">The margin of error can be reduced by one or a combination of  the following strategies: </font>
  <P></P></BLOCKQUOTE><FONT size=+0><FONT size=+0><FONT size=+0><FONT size=+0>
<OL>  <LI>Decreasing the confidence in the estimate -- an undesirable strategy since confidence relates to the chance of drawing the wrong conclusion (i.e., increases  the Type II error). <LI>Reducing the standard deviation -- something we cannot do since it is usually a static property of the population.  <LI>Increasing the sample size -- this provides more information for a better decision. </LI></OL><BLOCKQUOTE> 
  <P>You might like to use <A   href="otherapplets/Descriptive.htm"   target=new>Descriptive Statistics</A> Applet to check your computations, and <a href="otherapplets/SampleSize.htm" target= "new">Sample Size Determination</a> JavaScript at the design stage of your statistical investigation in decision making with specific subjective requirements.
<p>
  <P><B>Further Reading</B><BR>    <FONT face="Bookman Old Style" size=-2>Levy P., and S. Lemeshow, <I>Sampling  of Populations: Methods and Applications</I>, Wiley, 1999.<BR>    </FONT>   <P><A name=rrst></A>   <HR>  <H4><FONT color=#dc143c>Bias Reduction Techniques: Bootstrapping and Jackknifing </FONT></H4>  Some  inferencial  statistical techniques do not require distributional assumptions about the statistics involved. These modern non-parametric methods use large amounts of computation to explore the empirical variability of a statistic, rather than making a priori assumptions about this variability, as is done in the traditional parametric t- and z- tests. 
<p>
<FONT color=#dc143c>Bootstrapping:</font> Bootstrapping method is to obtain an estimate by combining estimators to each of many sub-samples of a data set. Often M randomly drawn samples of T observations are drawn from the original data set of size n with replacement, where T is less n. 
<p>
<FONT color=#dc143c>Jackknife Estimator:</font> A jackknife estimator creates a series of estimate, from a single data set by generating that statistic repeatedly on the data set leaving one data value out each time. This produces a mean estimate of the parameter and a standard deviation of the estimates of the parameter. 
<P>Monte Carlo simulation allows for the evaluation of the behavior of a statistic when its mathematical analysis is intractable. Bootstrapping and jackknifing allow inferences to be made from a sample when traditional parametric inference  fails. These techniques are especially useful to deal with statistical problems, such as small sample size, statistics with no well-developed distributional  theory, and parametric inference condition violations. Both are computer intensive. Bootstrapping means you take repeated samples from a sample and then make statements about a population. Bootstrapping entails sampling-with-replacement from a sample. Jackknifing involves systematically doing n steps, of omitting 1 case from a sample at a time, or, more generally, n/k steps of omitting k cases; computations that compare "included" vs. "omitted" can be used (especially) to reduce the bias of estimation.  Both have applications in reducing bias in estimations. <P>Resampling -- including the bootstrap, permutation, and other non-parametric tests -- is a method for hypothesis testing, confidence limits, and other applied problems in statistics and probability. It involves no formulas or tables.  <P>Following the first publication of the general technique (and the bootstrap) in 1969 by Julian Simon and subsequent independent development by Bradley Efron, resampling has become an alternative approach for testing hypotheses. <P>There are other findings:  "The bootstrap started out as a good notion in that it presented, in theory, an elegant statistical procedure that was free of distributional conditions. In practice the bootstrap technique doesn't work very well, and the attempts to modify it make it more complicated and more confusing than the parametric procedures that it was meant to replace." <P>While resampling techniques may reduce the bias, they achieve this at the expense of increase in variance. The two major concerns are: </P></BLOCKQUOTE><OL>  <LI>The loss in accuracy of the estimate as measured by variance can be very large.   <LI>The dimension of the data affects drastically the quality of the samples and therefore the estimates. </LI></OL><BLOCKQUOTE>   <P><B>Further Readings:</B><BR>    <FONT face="Bookman Old Style" size=-2>Young G., Bootstrap: More than a Stab in the Dark?, <I>Statistical Science</I>, l9, 382-395, 1994. Provides the  pros and cons on the bootstrap methods.<BR>Yatracos Y., Assessing the quality of bootstrap samples and of the bootstrap estimates obtained with finite resampling, <I>Statistics and Probability Letters</I>,     59, 281-292, 2002.<BR>    </FONT>   <P><A name=rpredictsamplemean></A> 
  <HR>  <H4><FONT color=#dc143c>Prediction Intervals</FONT></H4>
  In many application of business statistics, such as forecasting, we are interested in construction of a statistical interval for <A href="#rrandomva">random variable</A>, rather than a parameter of a population distribution. </BLOCKQUOTE><UL>  <LI>The Tchebysheff's inequality is often used to put bounds on the probability  that  a proportion of <A   href="#rrandomva">random variable</A> X will be within k <FONT face=symbol>&gt;</FONT> 1 standard deviation of the mean <FONT face=symbol>m</FONT> for any probability distribution. In other words:  <P> <CENTER>P [|X - <FONT face=symbol>m</FONT>| <FONT face=symbol>³</FONT> k <FONT 
  face=symbol>s</FONT>] <FONT face=symbol>£</FONT> 1/k<FONT  size=+0><SUP>2</SUP></FONT>,&nbsp; &nbsp;for any k greater than 1 </CENTER> <P>The symmetric property of Tchebysheff's inequality is useful;  e.g., in constructing control limits in the quality control process. However, the limits are very conservative due to lack of knowledge about the underlying  distribution. <P></P> <LI>The above bounds can be improved (i.e., becomes tighter) if we have some  knowledge about the population distribution. For example, if the population  is homogeneous; that is, its distribution is unimodal; then, <P> <CENTER>   P [|X - <FONT face=symbol>m</FONT>| <FONT face=symbol>³</FONT> k <FONT   face=symbol>s</FONT>] <FONT face=symbol>£</FONT> 1/(2.25k<FONT   size=+0><SUP>2</SUP></FONT>),&nbsp; &nbsp;for any k greater than 1. </CENTER>    <P>The above inequality is known as the Camp-Meidell inequality. <P></P>
  <LI>Now, let X be a random variable distributed normally with estimated mean <IMG src="xbaru.gif"> and standard deviation S, then a prediction interval  for the sample mean <IMG   src="xbaru.gif"> with 100(1- <FONT 
  face=symbol>a</FONT>)% confidence level is:  <P>  <CENTER>  <P><IMG src="xbaru.gif"> ± t<SUB><FONT 
  face=symbol>a</FONT><FONT size=+0></FONT>/2</SUB> <font face=symbol>&#180;</font> S <font face=symbol>&#180;</font> (1+1/n)<FONT   size=+0><SUP>1/2</SUP></FONT>.       <P>  </CENTER>  <P>This is the range of a random variable <IMG  src="xbaru.gif"> with 100(1- <FONT 
  face=symbol>a</FONT>)% confidence, using t-table. Relaxing the <A   href="#rnormalcond">normality 
  condition</A> for sample-mean prediction interval, requires a large sample size, say n over 30. </UL>
 <BLOCKQUOTE><P><B>Further Readings:</B><BR>  <FONT face="Bookman Old Style" size=-2>Grant E., and R. Leavenworth, <I>Statistical Quality Control</I>, McGraw-Hill, 1996.<BR>Ryan T., <I>Statistical Methods for Quality Improvement</I>, John Wiley &amp; Sons, 2000. A very good book for a starter. </FONT> <P><A name=rwstanderrors></A> <HR><H4><FONT color=#dc143c>What Is a Standard Error?</FONT></H4>For statistical inference, namely statistical testing and estimation, one needs to estimate the population's parameter(s). Estimation involves the determination, with a possible error due to sampling, of the unknown value of a population parameter, such as the proportion having a specific attribute or the average value <FONT face=symbol>m</FONT> of some numerical measurement. To express the accuracy of the estimates of population characteristics, one must also compute the <FONT color=#dc143c>standard errors</FONT> of the estimates. These are measures of accuracy that determine the possible errors arising from the fact that the estimates are based on random samples from the entire population, and not on a complete population census. <P>Standard error is a statistic indicating the accuracy of an estimate. That  is, it tells us to assess how different the estimate (such as <IMG src="xbaru.gif">) is from the population parameter (such as <FONT face=symbol>m</FONT>).   It is therefore, the standard deviation of a sampling distribution of the estimator such as <IMG src="xbaru.gif">.  The following is a collection of standard errors for the widely used statistics: 
<UL>  <LI>Standard Error for the  Mean  <IMG 
src="xbaru.gif"> is:  S/n<FONT size=+0><SUP>½</SUP></FONT>.  <P></P>
As one expects, the standard error decreases as the sample size increases.  However the standard deviation of the estimate decreases by a factor of n<FONT><SUP>½</SUP></FONT> not n.  For example, if you wish to reduce the error by 50%, the sample size must be 4 times n, which is expensive.  Therefore, as an alternative to increasing sample size, one may reduce the error by obtaining "quality" data that provide a more accurate estimate. <P></P><LI>For a finite population of size N, the standard error of the sample mean of size n, is:  <P>S <font face=symbol>&#180;</font> [(N -n)/(nN)]<FONT size=+0><SUP>½</SUP></FONT>. <P></P>


<p><li>Standard Error for the Multiplication of Two Independent Means  <IMG 
src="xbaru.gif"><FONT><SUB>1</SUB></FONT> <font face=symbol>&#180;</font> <IMG 
src="xbaru.gif"><FONT><SUB>2</SUB></FONT> is:<p> {<IMG 
src="xbaru.gif"><FONT><SUB>1</SUB></FONT> S<FONT><SUB>2</SUB></FONT><FONT><SUP>2</SUP></FONT>/n<FONT><SUB>2</SUB></FONT> + <IMG src="xbaru.gif"><FONT><SUB>2</SUB></FONT> S<FONT><SUB>1</SUB></FONT><FONT><SUP>2</SUP></FONT>/n<FONT><SUB>1</SUB></FONT>}<FONT><SUP>½</SUP></FONT>. 
<p><li>Standard Error for Two Dependent Means  <IMG 
src="xbaru.gif"><FONT><SUB>1</SUB></FONT> <font face=symbol>&#177;</font> <IMG 
src="xbaru.gif"><FONT><SUB>2</SUB></FONT> is:<p> {S<FONT><SUB>1</SUB></FONT><FONT><SUP>2</SUP></FONT>/n<FONT><SUB>1</SUB></FONT> +  S<FONT><SUB>2</SUB></FONT><FONT><SUP>2</SUP></FONT>/n<FONT><SUB>2</SUB></FONT> + 2 r <font face=symbol>&#180;</font> [(S<FONT><SUB>1</SUB></FONT><FONT><SUP>2</SUP></FONT>/n<FONT><SUB>1</SUB></FONT>)(S<FONT><SUB>2</SUB></FONT><FONT><SUP>2</SUP></FONT>/n<FONT><SUB>2</SUB></FONT>)]<FONT><SUP>½</SUP></FONT>}<FONT><SUP>½</SUP></FONT>.
<p><li>Standard Error for the Proportion P is:<p>  [P(1-P)/n]<FONT><SUP>½</SUP></FONT><p>
<p><li>Standard Error for P<FONT><SUB>1</SUB></FONT> <font face=symbol>&#177;</font> P<FONT><SUB>2</SUB></FONT>, Two Dependent Proportions is:<p> {[P<FONT><SUB>1</SUB></FONT> + P<FONT><SUB>2</SUB></FONT> - (P<FONT><SUB>1</SUB></FONT>-P<FONT><SUB>2</SUB></FONT>)<FONT><SUP>2</SUP></FONT>] / n}<FONT><SUP>½</SUP></FONT>.<p>

 <LI>Standard Error of the Proportion (P) from a finite population is:<p>  [P(1-P)(N -n)/(nN)]<FONT size=+0><SUP>½</SUP></FONT>. <P>The last two formulas for finite population are frequently used when we wish to compare a sub-sample of size n with a larger sample of size N, which contains the sub-sample. In such a comparison, it would be wrong to treat the two samples "as if" there were two independent samples. For example, in comparing the two means one may use the t-statistic but with the standard error: <P>S<FONT size=+0><SUB>N</SUB></FONT> [(N -n)/(nN)]<FONT  size=+0><SUP>½</SUP></FONT> 
    <P>as its denominator. Similar treatment is needed for proportions. 

   <P></P><li>Standard Error of the Slope (m) in  Linear Regression is<p>  S<FONT><SUB>res</SUB></FONT> / S<FONT size=+0><SUB>xx</SUB></FONT><FONT><SUP>½</SUP></FONT>,   where  S<FONT><SUB>res</SUB></FONT> is the residual' standard deviation.<P></P><li>Standard Error of the Intercept (b) in  Linear Regression is:<p>   S<FONT><SUB>res</SUB></FONT>[(S<FONT size=+0><SUB>xx</SUB></FONT> + n <font face=symbol>&#180;</font> <IMG src="xbaru.gif"><FONT><SUP>2</SUP></FONT>) /(n <font face=symbol>&#180;</font> S<FONT size=+0><SUB>xx</SUB></FONT>] <FONT><SUP>½</SUP></FONT>.  <P></P> <LI>Standard Error of the Predicted Value using a Linear Regression is:<p>  S<FONT><SUB>y</SUB></FONT>(1 - r<FONT><SUP>2</SUP></FONT>)<FONT><SUP>½</SUP></FONT>.
<p>The term (1 - r<FONT><SUP>2</SUP></FONT>)<FONT><SUP>½</SUP></FONT> is called the coefficient of alienation. Therefore if r = 0, the error of prediction is S<FONT><SUB>y</SUB></FONT> as expected.
<p><li>Standard Error of  the  Linear Regression is:<p>  S<FONT><SUB>y</SUB></FONT> (1 -  r<FONT><SUP>2</SUP></FONT>)<FONT><SUP>½</SUP></FONT>. <p>Note that if r = 0, then the standard error reaches its maximum possible value, which is standard deviation in Y.
</UL>
<p>
<br>
<FONT color=#dc143c><b>Stability of an estimator:</b></font> An estimator is stable if, by taking two different samples of the same size, they produce two estimates having "small" absolute difference.   The stability of an estimator is measured by its reliability:
<p>
<center>
Reliability of an estimator = 1 / (its standard error)<FONT><SUP>2</SUP></FONT>
</center>
<p>
The larger the standard error, the less reliable is the estimate.   Reliability of estimators is often used to select the "best" estimator among all unbiased estimators.
<P><A name=rssss></A> <HR><H4><FONT color=#dc143c>Sample Size Determination</FONT></H4>
At the planning stage of a statistical investigation, the question of sample size (n) is critical. This is an important matter NOT to be taken lightly. To take a larger sample than is needed to achieve the desired results is wasteful of resources, whereas very small samples often lead to what are no practical use of making good decisions. The main objective is to obtain both a desirable accuracy and a desirable confidence level with minimum cost. <P>Students sometimes ask me, what fraction of the population do you need for 
  good estimation? I answer, "It's irrelevant; accuracy is determined by sample size." This answer has to be modified if the sample is a sizable fraction  of the population. <P>The confidence level of conclusions drawn from a set of data depends on the size of the data set. The larger the sample, the higher is the associated confidence. However, larger samples also require more effort and resources. Thus, your goal must be to find the smallest sample size that will provide the desirable confidence. <P>For an item scored 0 or 1, for no or yes, the standard error (SE) of the estimated proportion p, based on your random sample observations, is given by: 
<P>  <CENTER> SE = [p(1-p)/n]<FONT size=+0><SUP>1/2</SUP></FONT> </CENTER><P> <CENTER>
  </CENTER> where p is the proportion obtaining a score of 1, and n is the sample size. This SE is the standard deviation of the range of possible estimate values. <P>The SE is at its maximum when p = 0.5, therefore the worst case scenario occurs when 50% are yes, and 50% are no. <P>Under this extreme condition, the sample size, n, can then be expressed as the largest integer less than or equal to: <P> <CENTER>n = 0.25/SE<FONT size=+0><SUP>2</SUP></FONT> </CENTER><P>To have some notion of the sample size, for example for SE to be 0.01 (i.e. 1%), a sample size of 2500 will be needed; 2%, 625; 3%, 278; 4%, 156, 5%, 100. 
<P>Note, incidentally, that as long as the sample is a small fraction of the total population, the actual size of the population is entirely irrelevant for the purposes of this calculation. <P><FONT color=#dc143c>Pilot Studies:</FONT> When the needed estimates for sample size calculation is not available from an existing database, a pilot study is needed for adequate estimation with a given precision. A pilot, or preliminary, sample   must be drawn from the population, and the statistics computed from this sample are used in determination of the sample size. Observations used in the pilot sample may be counted as part of the final sample, so that the computed sample size minus the pilot sample size is the number of observations needed to satisfy 
  the total sample size requirement. <P> <P><FONT color=#dc143c>Sample Size with Acceptable Absolute Precision:</FONT> The following present the widely used method for determining the sample size 
  required for estimating a population mean and proportion. <P>Let us suppose we want an interval that extends <FONT face=symbol>d</FONT> unit on either side of the estimator. We can write <CENTER>
  <P><FONT face=symbol>d</FONT> = Absolute Precision = (reliability coefficient) <font face=symbol>&#180;</font>  (standard error) = Z<FONT size=+0><SUB><FONT face=symbol>a</FONT>/2</SUB></FONT> <font face=symbol>&#180;</font> (S/n<FONT size=+0><SUP>1/2</SUP></FONT>)  <P></CENTER><P>Suppose, based on a pilot sample of size n, the estimated proportion is p, then  the required sample size with the absolute error size not exceeding <FONT 
face=symbol>d</FONT>, with 1- <FONT face=symbol>a</FONT> confidence is: <P> <CENTER>
    [t<FONT size=+0><SUP>2</SUP></FONT> n p(1-p)] / [t<FONT size=+0><SUP>2</SUP></FONT> p(1-p) - <FONT face=symbol>d</FONT><FONT size=+0><SUP>2</SUP></FONT> (n-1)],</CENTER><P>where t = t <FONT face=symbol><SUB>a/2</SUB></FONT> being the value taken from the t-table with parameter d.f. = <FONT face=symbol>n</FONT> = n-1, corresponding to the desired 1- <FONT face=symbol>a</FONT> confidence interval. <P>For large pilot sample sizes (n), say over 30, the simplest sample size determinate is: <P> 
<CENTER>[(Z<FONT face=symbol><SUB>a/2</SUB></FONT>)<FONT 
size=+0><SUP>2</SUP></FONT> S<FONT size=+0><SUP>2</SUP></FONT>] / <FONT 
face=symbol>d</FONT><FONT size=+0><SUP>2</SUP></FONT>&nbsp; &nbsp;&nbsp; for the 
  Mean <FONT face=symbol>m</FONT>  <P></CENTER><P> <CENTER>[(Z<FONT face=symbol><SUB>a/2</SUB></FONT>)<FONT size=+0><SUP>2</SUP></FONT> p(1-p)] / <FONT face=symbol>d</FONT><FONT size=+0><SUP>2</SUP></FONT>&nbsp; &nbsp;&nbsp; for the proportion, 
  <P></CENTER><P>where <FONT face=symbol>d</FONT><FONT size=+0> is the desirable margin of error (i.e., the absolute error), which is the half-length of the confidence interval with 100(1- <FONT face=symbol>a</FONT>)% confidence interval. <P><FONT color=#dc143c>Sample Size with Acceptable Type I and Type II Errors:</FONT>   One may use the following sample size determinate, which is based on the size 
  of type I and Type II errors: <P>   <CENTER> 2(Z<FONT face=symbol><SUB>a/2</SUB></FONT> + Z<FONT face=symbol><FONT size=+0><SUB>b/2</SUB></FONT></FONT>)<FONT size=+0><SUP>2</SUP></FONT>S<FONT size=+0><SUP>2</SUP></FONT>/<FONT face=symbol>d</FONT><FONT size=+0><SUP>2</SUP></FONT>,   </CENTER><P>where <FONT face=symbol>a</FONT> and <FONT face=symbol>b</FONT> are the desirable type I, and type II errors, respectively. S<FONT size=+0><SUP>2</SUP></FONT> is the variance obtained from the pilot run, and <FONT face=symbol>d</FONT> is the difference between the null and alternative (<FONT  face=symbol>m</FONT><FONT size=+0><SUB>0</SUB></FONT> -<FONT face=symbol>m</FONT><FONT size=+0><SUB>a</SUB></FONT>). <P><FONT color=#dc143c>Sample Size with Acceptable Relative Precision:</FONT> You may use the following sample size determinate for a <B>desirable relative error</B> 
  <FONT face=symbol>D</FONT> in %, which requires an estimate of the coefficient of variation (CV in %) from a pilot sample with size over 30: <P> <CENTER> [(Z<FONT face=symbol><SUB>a/2</SUB></FONT>)<FONT 
size=+0><SUP>2</SUP></FONT> (C.V.)<FONT size=+0><SUP>2</SUP></FONT>] / <FONT 
face=symbol>D</FONT><FONT size=+0><SUP>2</SUP></FONT>&nbsp; <P></CENTER><P><FONT color=#dc143c>Sample Size Based on the Null and an Alternative:</FONT> One may use <B>power of the test</B> to determine the sample size. The functional  relation of the power and the sample size is known as the <B>operating characteristic curve</B>. On this curve, as sample size increases, the power function increases rapidly. Let <FONT face=symbol>d</FONT> be such that: <P> <CENTER> <FONT face=symbol>m</FONT><FONT size=+0><SUB>a</SUB></FONT> = <FONT 
face=symbol>m</FONT><FONT size=+0><SUB>0</SUB></FONT> + <FONT face=symbol>d</FONT> 
  </CENTER><P>  <CENTER> </CENTER> is an alternative to represent departure from the null hypothesis. We wish to be reasonably confident to find evidence against the null, if in fact the particular  alternative holds. That is, the type error <FONT face=symbol>b</FONT>, is the probability of failing to find evidence at least at level of <FONT face=symbol>a</FONT>, when the alternative holds. This implies <P>  <CENTER> Required sample size = (z<FONT size=+0><SUB>1</SUB></FONT> + z<FONT size=+0><SUB>2</SUB></FONT>) S<FONT size=+0><SUP>2</SUP></FONT>/ <FONT face=symbol>d</FONT><FONT size=+0><SUP>2</SUP></FONT> 
  </CENTER><P>   <CENTER>  </CENTER>  Where: z<FONT size=+0><SUB>1</SUB></FONT> = |mean - <FONT 
face=symbol>m</FONT><FONT size=+0><SUB>0</SUB></FONT>|/ SE, z<FONT 
size=+0><SUB>2</SUB></FONT> = |mean - <FONT face=symbol>m</FONT><FONT 
size=+0><SUB>a</SUB></FONT>|/ SE, the mean is the current estimate for <FONT 
face=symbol>m</FONT>, and S is the current estimate for <FONT face=symbol>s</FONT>. 
<P> <P>All of the above sample size determinates could also be used for estimating the mean of any unimodal population, with discrete or continuous <A href="#rrandomva">random variables</A>, provided the pilot run size (n) is larger than (say) 30. <P>In estimating the sample size, when the standard deviation is not known, instead of S<FONT size=+0><SUP>2</SUP></FONT> one may use 1/4 of the range for sample 
   size over 30 as a "good" estimate for the standard deviation. It is a good practice to compare the result with IQR/1.349. <P>One may extend the sample size determination to other useful statistics, such as <FONT color=#dc143c>correlation coefficient (r)</FONT> based on acceptable Type I and Type II errors: </FONT>
<P>  <CENTER><FONT size=+0>2 + [(Z<FONT face=symbol><SUB>a/2</SUB></FONT> + Z<FONT 
face=symbol><FONT size=+0><SUB>b/2</SUB></FONT></FONT>( 1- r<FONT 
size=+0><SUP>2</SUP></FONT>) <SUP>½</SUP></font>)/r] <FONT size=+0><SUP>2</SUP></FONT> 
  </CENTER><P>provided r is not equal to  -1, 0, or 1.
<p> The aim of applying any one of the above sample size determinates is at improving 
  your pilot estimates at feasible costs. <P>You might like to use <A  href="otherapplets/SampleSize.htm"  target=new>Sample Size Determination</A> JavaScript to check your computations. <P><B>Further Reading:</B><BR>
  <FONT face="Bookman Old Style" size=-2>Kish L., <I>Survey Sampling</I>, Wiley, 1995.<BR>
  Murphy K., and B. Myors, <I>Statistical Power Analysis</I>, L. Erlbaum Associates, 1998. Provides a simple and general sample size determination for hypothesis  tests.<BR> </FONT> 
<P><A name=rrevisemv></A> <HR><H4><FONT color=#dc143c>Revising the Expected Value and the  Variance</FONT></H4>
<P><FONT color=#dc143c><B>Averaging Variances:</B></FONT> What is the mean variance  
 of k variances without regard to differences in their sample sizes? The answer  is simply: <P><CENTER>
      Average of Variances = [<FONT face=symbol>S</FONT>S<FONT  size=+0><SUB>i</SUB></FONT><FONT size=+0><SUP>2</SUP></FONT>] / k </CENTER><P>However, what is the variance of all k groups combined?  The answer must consider the sample size n<FONT><SUB>i</SUB></FONT> of the ith group: <P><CENTER>Combined Group Variance = <FONT face=symbol>S</FONT> n<FONT size=+0><SUB>i</SUB></FONT>[S<FONT size=+0><SUB>i</SUB></FONT><FONT  size=+0><SUP>2</SUP></FONT> + d<FONT size=+0><SUB>i</SUB></FONT><FONT  size=+0><SUP>2</SUP></FONT>]/N,  </CENTER>
  <P> where d<FONT size=+0><SUB>i</SUB></FONT> = mean<FONT  size=+0><SUB>i</SUB></FONT> - grand mean, and N = <FONT face=symbol>S</FONT> n<FONT size=+0><SUB>i</SUB></FONT>, for all i = 1, 2, .., k. 
  <P>Notice that the above formula allows us to split up the total variance into its two component parts. This splitting process permits us to determine the extent to which the overall variation is inflated by the difference between group means. What the variation would be if all groups had the same mean?  ANOVA is a well-known application of this concept where the equality of several means is tested.

<p>
<FONT color=#dc143c><b>Subjective Mean and Variance:</b></font> In many applications, we saw how to make decisions based on objective data; however, an informative decision-maker might be able to combine  his/her subjective input and the two sources of information.
<p>
<B>Application:</B> Suppose the following information is available from two independent sources:
<p>
<P> <CENTER> <TABLE border=1 borderColor=#ffffff cellSpacing=0> <TBODY><TR><TD align=middle colSpan=5><FONT color=#dc143c><FONT size=3><B>Revising the Expected Value and the  Variance</B></FONT></FONT></TD>
</TR><TR> <TD align=middle><B>Estimate Source</B></TD><TD align=middle>Expected value</TD> <TD align=middle>Variance</TD>
<TR> <TD align=middle>Sales manager
</TD><TD align=middle><FONT face=symbol>m</FONT><FONT   size=+0><SUB>1</SUB></FONT> = 110</TD>
<TD align=middle><FONT face=symbol>s</FONT><FONT   size=+0><SUB>1</SUB></FONT><FONT size=+0><SUP>2</SUP></FONT> = 100</TD></TR>
<TR> <TD align=middle>Market survey
</TD><TD align=middle><FONT face=symbol>m</FONT><FONT   size=+0><SUB>2</SUB></FONT> = 70</TD>
<TD align=middle><FONT face=symbol>s</FONT><FONT   size=+0><SUB>2</SUB></FONT><FONT size=+0><SUP>2</SUP></FONT> = 49</TD></TR>
</TBODY></TABLE></CENTER>
  <P>
<p>
The combined expected value is:
<p>
<center>
[<FONT face=symbol>m</FONT><FONT   size=+0><SUB>1</SUB></FONT>/<FONT face=symbol>s</FONT><FONT   size=+0><SUB>1</SUB></FONT><FONT size=+0><SUP>2</SUP></FONT> + <FONT face=symbol>m</FONT><FONT   size=+0><SUB>2</SUB></FONT>/<FONT face=symbol>s</FONT><FONT   size=+0><SUB>2</SUB></FONT><FONT size=+0><SUP>2</SUP></FONT> ] / [1/<FONT face=symbol>s</FONT><FONT   size=+0><SUB>1</SUB></FONT><FONT size=+0><SUP>2</SUP></FONT> + 1/<FONT face=symbol>s</FONT><FONT   size=+0><SUB>2</SUB></FONT><FONT size=+0><SUP>2</SUP></FONT>]
<p></center>
The combined variance is:<p><center>
2 / [1/<FONT face=symbol>s</FONT><FONT   size=+0><SUB>1</SUB></FONT><FONT size=+0><SUP>2</SUP></FONT> + 1/<FONT face=symbol>s</FONT><FONT   size=+0><SUB>2</SUB></FONT><FONT size=+0><SUP>2</SUP></FONT>]<p></center>
For our application, using the above tabular information, the combined estimate of expected sales is 83.15 units with combined variance of 65.77.
<p>
<p>
You might like to use <a href="otherapplets/InaccracyAssessmet.htm" target= "new">Revising the Mean and Variance</a> JavaScript in performing some numerical experimentation.  You may apply it  for validating the above example and for a deeper understanding of the concept where more than two sources of information are to be combined.
<P><A name= rsubsevest></A> <HR><H4><FONT color=#dc143c>Subjective Assessment of Several Estimates Based on Relative Precision</FONT></H4>

<dd>In many cases, we may wish to compare several estimates of the same parameter. The simplest approach is to measure the closeness among the estimates in an attempt to determine that at least one of the estimates is more than r times the parameter away from the parameter, where r is a subjective, non-negative number  less than one. 
<p>
You might like to use <a href="otherapplets/InaccracyAssessmet.htm" target= "new">Subjective Assessment of Estimates</a> JavaScript to isolate any inaccurate estimate.  By repeating the same process you might be able to remove all inaccurate estimates. 
<P><B>Further Reading:</B><BR>
  <FONT face="Bookman Old Style" size=-2>Tsao H. and T. Wright, On the maximum ratio: A tool for assisting inaccuracy assessment,  <I>The American Statistician</I>, 37(4), 1983.
 <BR> </FONT> 
<P><A name=rrstatdecision></A> 
<HR><H4><FONT color=#dc143c>Managing the Producer's or the Consumer's Risk</FONT></H4>
The logic behind a statistical test of hypothesis is similar to the following logic. Draw two lines on a paper and determine whether they are of different lengths. You compare them and say, "Well, certainly they are not equal. Therefore they must be of different lengths.  By rejecting equality, that is, the null hypothesis, you assert that there is a difference. 
<p>
The power of a statistical test is best explained by the overview of the Type I and Type II errors. The following matrix shows the basic representation of these errors.

<P>   <CENTER>   <IMG src="error.gif">   </CENTER>  As indicated in the above matrix a <FONT color=#dc143c>Type-I error</FONT> occurs   when, based on your data, you reject the null hypothesis when in fact it is true.   The probability of a type-I error is the level of significance of the test of 
  hypothesis and is denoted by <FONT face=symbol>a </FONT>. 
<p>
Type-I error is often called <FONT color=#dc143c><b>the producer's risk</b></font> that consumers reject a good product or service indicated by the null hypothesis. That is, a producer introduces a good product, in doing so, he or she take a risk that consumer will reject it.
<P>A <FONT color=#dc143c>type II error</FONT> occurs when you do not reject the  null hypothesis when it is in fact false. The probability of a type-II  error is denoted by <FONT face=symbol>b </FONT>. The quantity 1 - <FONT face=symbol>b </FONT>is known as the <FONT color=#dc143c>Power of a Test</FONT>.  A Type-II error can be evaluated for any specific alternative hypotheses stated  in the form "Not Equal to" as a competing hypothesis.
<p>
Type-II error is often called <FONT color=#dc143c><b>the consumer's risk</b></font> for not rejecting possibly a worthless product or service indicated by the null hypothesis.
 <P>Students often raise questions, such as what are the 'right' confidence intervals,  and why do most people use the 95% level? The answer is that the decision-maker must consider both the Type I and II errors and work out the best tradeoff. Ideally one wishes to reduce the probability of making these types of error; however, for a fixed sample size, we cannot reduce one type of error without at the same time increasing the probability of another type of error. Nevertheless,  to reduce the probabilities of both types of error simultaneously is to increase  the sample size. That is, <FONT color=#dc143c>by having more information one makes a better decision</FONT>. 
<p>
The following example highlights this concept. A electronics firm, Big Z, manufactures and sells a component part to a radio manufacturer, Big Y. Big Z consistently maintain a component part failure rate of 10% per 1000 parts produced. Here Big Z is the producer and Big Y is the consumer. Big Y, for reasons of practicality, will test sample of 10 parts out of lots of 1000. Big Y will adopt one of two rules regarding lot acceptance:
<p>
<ul>
<li>Rule 1: Accept lots with one or fewer defectives; therefore, a lot has either 0 defective or 1 defective.
<li>Rule 2: Accept lots with two or fewer defectives; therefore, a lot has either 0,1, or 2 defective(s).
</ul>
<p>
On the basis of the binomial distribution, the P(0 or 1) is 0.7367. This means that, with a defective rate of .10, the Big Y will accept 74% of tested lots and will reject  26% of the lots even though they are good lots. The 26% is the producer's risk or the <FONT face=symbol>a </FONT> level. This <FONT face=symbol>a </FONT> level is analogous to a Type I error -- rejecting a true null. Or, in other words, rejecting a good lot. In this example, for illustration purposes, the lot represents a null hypothesis. The rejected lot goes back to the producer; hence, producer's risk. If Big Y is to take rule 2, then the producer's risk decreases. The P(0 or, or 1, or 2) is 0.9298 therefore, Big Y will accept 93% of all tested lots, and  7% will be rejected, even though the lot is acceptable. The primary reason for this is that, although the probability of defective is .10, the Big Y through rule 2 allows for a higher defective acceptance rate. Big Y increases its own risk (consumer's risk), as stated previously.
<P><FONT color=#dc143c><B>Making Good Decision:</B></FONT> Given that there is   a relevant profit (which could be negative) for the outcome of your decision, and a prior probability (before testing) for the null hypothesis to be true,  the objective is to make a good decision. Let us denote the profits for each  cell in the decision table as $a, $b, $c and $d (column-wise), respectively.  The expectation of profit is [<FONT face=symbol>a</FONT>a + (1-<FONT face=symbol>a</FONT>)b], and + [(1-<FONT face=symbol>b</FONT>)c + <FONT face=symbol>b</FONT>d], depending whether the null is true. 
<P>Now having a prior (i.e., before testing) subjective probability of p that  the null is true, then the expected profit of your decision is: <P> <CENTER> Net Profit = [<FONT face=symbol>a</FONT>a + (1-<FONT 
face=symbol>a</FONT>)b]p + [(1-<FONT face=symbol>b</FONT>)c + <FONT 
face=symbol>b</FONT>d](1-p) - Sampling cost </CENTER><P>A good decision makes this profit as large as possible. To this end, we must suitably choose the sample size and all other factors in the above profit 
  function. <P>Note that, since we are using a subjective probability expressing the strength   of belief assessment of the truthfulness of the null hypothesis, it is called a <FONT color=#dc143c><B>Bayesian Approach</B></FONT> to statistical decision making, which is a standard approach in <A 
href="http://ubmail.ubalt.edu/~harsham/opre640a/partIX.htm" target=new>decision theory</A>.
<p>You might like to use the <a href="otherapplets/SubjTest.htm" target ="new">Subjectivity in Hypothesis Testing</a> JavaScript applet in performing some numerical experimentation for validating the above assertions for a deeper understanding. 
<p><P><B>Further Reading:</B><BR>
  <FONT face="Bookman Old Style" size=-2>Cochran W., <I>Planning and Analysis of Observational Studies</I>, Wiley, 1983. </FONT> 
<P><A name=rht></A> <HR><H4><FONT color=#dc143c>Hypothesis Testing: Rejecting a Claim</FONT></H4>
To perform a hypothesis test, one must be very specific about the test one wishes to perform. The null hypothesis must be clearly stated, and the data must be collected in a repeatable manner. If there is 
any subjectivity, the results are technically not valid. All of the analyses, including the sample size, significance level, the time, and the budget, must be planned in advance, or else the user runs the risk of "data diving". 
<P><FONT color=#dc143c>Hypothesis testing is mathematical proof by contradiction</FONT>. 
  For example, for a Student's t test comparing two groups, we assume that the two  groups come from the same population (same means, standard deviations, and in   general same distributions). Then we do our best  to prove that this   assumption is false. Rejecting H<FONT size=+0><SUB>0</SUB></FONT> means either H<FONT size=+0><SUB>0</SUB></FONT> is false, or a rare event as has occurred. <P>The real question is in statistics not whether a null hypothesis is correct, but  whether it is close enough to be used as an approximation. 
<p>
<center>
<a href="TEST1.jpe"><IMG alt="Test of Hypotheses" src="TEST1.jpe" width="203" height="250" border="0"></a>
  <p>Test of Hypotheses<br><FONT size=+0><FONT size=+0><b><font color="#dc143c">Click on the image to enlarge it and THEN print it</font></b></center>
<P> 
In most statistical tests concerning <FONT face=symbol>m</FONT>, we start by assuming the <FONT face=symbol>s</FONT><FONT  size=+0><SUP>2</SUP></FONT>, and the higher moments, such as <A 
  href="#rskewKur">skewness and kurtosis</A>, are equal. Then, we hypothesize that the <FONT 
  face=symbol>a</FONT>'s are equal wich is null hypothesis.  <P>The "null" often suggests no difference between group means, or no relationship between quantitative variables, and so on.  <P>Then we test with a calculated t-value. For simplicity, suppose we have a  two-sided test. If the calculated t is close to 0, we say "it is good", as we expected.  If the calculated t is far from 0, we say, "the chance of getting this value  of t, given my assumption that the populations are statistically the same,  is so small that I will not believe the assumption. We will say that the populations  are not equal; specifically the means are not equal."   <P>As an example, sketch a normal distribution with mean <IMG   src="xbaru.gif">1 - <IMG  src="xbaru.gif">2 and standard deviation s. If the null hypothesis is true,   then the mean is 0. We calculate the 't' value, as per the equation. We look  up a "critical" value of t. The probability of calculating a t value more extreme ( + or - ) than this, given that the null hypothesis is true, is equal or less than the <FONT face=symbol>a</FONT> risk we used in pulling the critical value from the table. Mark the calculated t, and critical t (both sides) on the sketch of the distribution. Now, if the calculated t is more extreme than  the critical value, we say, "the chance of getting this t, by shear chance,  when the null hypothesis is true, is so small that I would rather say the  null hypothesis is false, and accept the alternative, that the means are not equal." When the calculated value is less extreme than the calculated value,  we say, "I could get this value of t by shear chance. I cannot detect a difference in the means of  the two groups at the <FONT  face=symbol>a</FONT> significance level." 
  <P>In this test,  we need (among others) the condition that the population variances  (i.e., treatment impacts on central tendency but not variability) are equal. However, this test is robust to violations of that condition if n's are large and almost the same size. A counter example would be to try a t-test between (11, 12, 13) and (20, 30, 40). The pooled and unpooled tests both give t statistics of 3.10, but the degrees of freedom are different: d.f. = 4  (for pooled) or d.f. about 2 (for unpooled). Consequently the pooled test gives p = .036 and the unpooled p = .088. We  could go down to n = 2 and get something still more extreme. <P>You might like to use <A href="http://www.physics.csbsju.edu/stats/Index.html"  target=new>Online Statistical Computation</A>, <A 
  href="otherapplets/MeanTest.htm" 
  target=new>Testing the Mean</A>, and <A   href="otherapplets/variationtest.htm"   target=new>Testing the Variance</A> in performing more of these tests. 
  <p>
You might need to use <a href="otherapplets/SampleSize.htm" target= "new">Sample Size Determination</a> JavaScript at the design stage of your statistical investigation in decision making with specific subjective requirements.
<p>
<P><A name=rclassicalTest></A>  <HR> <H4><FONT color=#dc143c>Classical Approach to Testing  Hypotheses</FONT></H4> In this treatment there are two parties: One party (or a person) proposes the 
  null hypothesis (the claim). Another party proposes an alternative hypothesis. A significance level <FONT face=symbol>a</FONT> and a sample size n are agreed upon by both parties. The next step is to compute the relevant statistic based on the null hypothesis and the random sample of size n. Finally, 
  one determines the <B>rejection region</B>. The conclusion based on this approach is as follows: 
  <P>If the computed statistic falls within the rejection region, then <FONT   color=#dc143c>Reject</FONT> the null hypothesis; otherwise <FONT  color=#dc143c>Do Not Reject</FONT> the null hypothesis (the claim). 
  <P>You may ask: How do you determine the critical value (such as z-value) for the rejection interval for one and two-tailed hypotheses?. What is the rule? <P>First, you have to choose a significance level <FONT face=symbol>a</FONT>.  Knowing that the null hypothesis is always in "equality" form then, the alternative 
    hypothesis has one of the three possible forms: "greater-than", "less-than", or "not equal to". The first two forms correspond to a one-tail hypothesis while the last one corresponds to a two-tail hypothesis.  <P></P>
</BLOCKQUOTE><UL> <LI>If your alternative is in the form of <FONT   color=#dc143c>"greater-than"</FONT>, then <FONT color=#dc143c>z is the value</FONT>  that gives you an area to the <FONT color=#dc143c>right tail</FONT> of the distribution  that is equal to <FONT face=symbol>a</FONT>.  <P></P> <LI>If your alternative is in the form of <FONT   color=#dc143c>"less-than"</FONT>, then <FONT color=#dc143c>z </FONT>is the value 
    that gives you an area to the <FONT color=#dc143c>left tail</FONT> of the distribution that is equal to <FONT face=symbol>a</FONT>.  <P></P><LI>If your alternative is in the form of <FONT color=#dc143c>"not equal to"</FONT>,  then there are two z values, one positive and the other negative. The <FONT color=#dc143c>positive z</FONT> is the value that gives you an <FONT   face=symbol>a</FONT>/2 area to the <FONT color=#dc143c>right tail</FONT> of the distribution. While, the <FONT color=#dc143c>negative z</FONT> is the value that gives you an <FONT face=symbol>a</FONT>/2 area to the <FONT  color=#dc143c>left tail</FONT> of the distribution. </LI></UL><BLOCKQUOTE> <P>The above rule can be generalized and implemented for determining the critical value for any test of hypothesis, you must first master reading the statistical tables, because, as you see, not all tables in your textbook are presented in the same format. <A name=rmip></A> 
  <HR> <H4><FONT color=#dc143c>The Meaning and Interpretation of P-values (what the data  say?)</FONT></H4> The p-value, which directly depends on a given sample attempts to provide a 
  measure of the strength of the results of a test for the null hypothesis, in contrast to a simple reject or do not reject in the classical approach to the test of hypotheses. If the null hypothesis is true, and if the chance of random variation is the only reason for sample differences, then the p-value is a quantitative measure to feed into the decision-making process as evidence. The following table provides a reasonable interpretation of p-values:  <P> <CENTER> <TABLE border=1 borderColor=#dddddd cellSpacing=0> <TBODY> 
      <TR>  <TD>  <CENTER> <STRONG>P-value</STRONG> </CENTER></TD><TD> <CENTER>
            <STRONG>Interpretation</STRONG></CENTER></TD></TR><TR> <TD>P <FONT face=symbol>&lt;</FONT> 0.01 </TD> <TD>very strong evidence against H<FONT size=+0><SUB>0</SUB></FONT> </TD> </TR> <TR> <TD>0.01<FONT face=symbol>£</FONT> P <FONT face=symbol>&lt;</FONT> 0.05 </TD> <TD>moderate evidence against H<FONT size=+0><SUB>0</SUB></FONT> </TD> </TR> <TR>  <TD>0.05 <FONT face=symbol>£</FONT> P <FONT face=symbol>&lt;</FONT> 0.10 
        </TD><TD>suggestive evidence against H<FONT size=+0><SUB>0</SUB></FONT> </TD> </TR>
      <TR>  <TD>0.10 <FONT face=symbol>£</FONT> P </TD> <TD>little or no real evidences against H<FONT 
        size=+0><SUB>0</SUB></FONT> </TD> </TR> </TBODY> </TABLE></CENTER><P>This interpretation is widely accepted, and many scientific journals routinely publish papers using this interpretation for the result of a  test of hypothesis. <P>For the fixed-sample size, when the number of realizations is decided in 
    advance, the distribution of p is uniform, assuming the null hypothesis is true. We  would express this as P(p <FONT face=symbol>£</FONT> x) = x. That means the criterion of p <FONT face=symbol>£</FONT> 0.05 achieves <FONT   face=symbol>a</FONT> of 0.05.  <P>Understand that the distribution of p-values under null hypothesis H<FONT  size=+0><SUB>0</SUB></FONT> is uniform, and thus does not depend on a particular 
    form of the statistical test. In a statistical hypothesis test, the P value  is the probability of observing a test statistic at least as extreme as the  value actually observed, assuming that the null hypothesis is true. The value  of p is defined with respect to a distribution. Therefore, we could call it "model-distribution hypothesis" rather than "the null hypothesis".  <P>In short, it simply means that, if the null had been true, the p-value is 
    the probability against the null in that case. The p-value is determined by the observed value; however, this makes it difficult to even state the inverse of p.  <P>Finally, since the p-values are <A    href="#rrandomva">random variables</A>, one cannot compare several p-values for any statistical conclusions 
    (nor order them).  This is a common mistake many people do, therefore, the above table is not intended for such a comparison. <P>You might like to use <A   href="otherapplets/pvalues.htm"   target=new>The P-values for the Popular Distributions</A> JavaScript. 
  <P><B>Further Readings:</B><BR> <FONT face="Bookman Old Style" size=-2>Arsham H., Kuiper's P-value as a Measuring Tool and Decision Procedure for the Goodness-of-fit Test, <I>Journal of Applied Statistics</I>, Vol. 15, No.3, 131-135, 1988.<BR> Good Ph.., <I>Resembling Methods: A Practical Guide to Data Analysis</I>, 
    Springer Verlag, 1999.<BR> </FONT>  <P> <HR>  <P><A name=rcombinpandalpfa></A>   <H4><FONT color=#dc143c>Blending the Classical and the P-value Based Approaches in Test of Hypotheses</FONT></H4>
  A p-value is a measure of how much evidence you have against the null hypothesis. <FONT color=#dc143c>Notice that the null hypothesis is always in = form, and does not contain any forms of inequalities.</FONT> The smaller the p-value, the more evidence you have. In this setting, the p-value is based on the hull hypothesis and has nothing to do with an alternative hypothesis and therefore with the rejection region. In recent years, some authors try to use the mixture of the classical and the p-value approaches. It is  based on the critical value obtained from given <FONT face=symbol>a</FONT>,  the computed statistics and the p-value. This is a blend of two different schools of thought. In this setting, some textbooks compare 
  the p-value with the significance level to make decisions on a given test of hypothesis. The larger the p-value is when compared with <FONT   face=symbol>a</FONT> (in one-sided alternative hypothesis, and <FONT 
  face=symbol>a</FONT>/2 for the two sided alternative hypotheses), the  less evidence we have for rejecting the null hypothesis. In such a comparison, if the p-value is less than some threshold (usually 0.05, sometimes a bit larger like 0.1 or a bit smaller like 0.01) then you reject the null hypothesis. The following deal with such a combined approach. <P><FONT color=#dc143c><B>Use of P-value and <FONT  
  face=symbol>a</FONT>:</B></FONT> In this setting, we must also consider the alternative hypothesis in drawing the rejection region. There is only one p-value to compare with <FONT face=symbol>a</FONT> (or <FONT   face=symbol>a</FONT>/2). Know that, for any test of hypothesis, there is only  one p-value. The following outlines the computation of the p-value and the decision process involved in a given test of hypothesis:  <P></P></BLOCKQUOTE><OL><LI><FONT color=#dc143c>P-value for One-sided  Alternative Hypotheses:</FONT> The p-value is defined as the area under the right tail of distribution, if the rejection region in on the right tail; if the rejection region is on the left  tail, then the p-value is the area under  the left tail (in one-sided alternative  hypotheses).  <P></P><LI><FONT color=#dc143c>P-value for Two-sided  Alternative Hypotheses:</FONT> If the alternative hypothesis is  two-sided (that is, rejection regions are both on the left and on the right tails), then the p-value is the area under the right tail or to the left tail of  the distribution, depending on whether the computed statistic is closer to the right rejection region or left rejection region.     
<p>
For symmetric densities (such as t-density), the left and right tails p-values are the same. However, for non-symmetric densities (such as Chi-square) use the smaller of the two. This makes the test more conservative. Notice that,  for a two sided-test alternative hypotheses, the p-value is never greater than 0.5.  <P></P>
  <LI>After finding the p-value as defined here, you compare it with a pre-set <FONT face=symbol>a</FONT> value for one-sided tests, and with <FONT  face=symbol>a</FONT>/2 for two sided-test. The larger the p-value is when compared with <FONT face=symbol>a</FONT> (in one-sided alternative hypothesis, and  <FONT face=symbol>a</FONT>/2 for the two sided alternative hypotheses), the less evidence we have for rejecting the null hypothesis. </LI></OL><P> <BLOCKQUOTE>To avoid looking-up the p-values from the limited statistical tables  given in your textbook, most professional statistical packages such as <a href="http://ubmail.ubalt.edu/~harsham/stat-data/opre330SPSSSAS.htm" target ="new"> SAS and  SPSS</a> provide the two-tailed p-value. Based on where the rejection region is, you must find out what p-value to use.  <P>Some textbooks have many misleading statements about p-value and its applications. For example, in many textbooks you find the authors double the p-value to compare it with <FONT face=symbol>a</FONT> when dealing with the two-sided test of hypotheses. One wonders how they do it in the case when "their" p-value exceeds 0.5? Notice that, while it is correct to compare the p-value with <FONT face=symbol>a</FONT> for a one sided tests of hypotheses <FONT face=symbol>a</FONT>,  for two-sided hypotheses, one must compare the p-value with <FONT face=symbol>a</FONT>/2, NOT <FONT   face=symbol>a</FONT> with 2 times p-value, as some textbooks advise.  While the decision is the same, there is a clear distinction here and 
    an important difference, which the careful reader will note. <P><FONT color=#dc143c><b>How to set the appropriate <FONT face=symbol>a</FONT> value?</b></FONT> You may have wondered why <FONT face=symbol>a</FONT> = 0.05 is so popular in a test of hypothesis. <FONT face=symbol>a</FONT> = 0.05 is 
    traditional for tests, but is arbitrary in its origins suggested by R.A. Fisher, who suggested it in the spirit of 0.05 being the biggest p-value at which one would think maybe the null hypothesis in a statistical experiment was to be considered false. This was also a tradeoff between "type I error" and  "type II error"; that we do not want to accept the wrong null hypothesis, but  we do not want to fail to reject the false null hypothesis, either. 
As a final note, the average of these two p-values is often called the mid-p value. 
<P><FONT color=#dc143c><b>Conversions from two-sided to one-sided probabilities:</b></FONT> Let C be the probability for a two-sided confidence interval (CI) constructed for an estimate. The probability (C<FONT><SUB>1</SUB></FONT>) that either the estimate is greater than the lower limit or that it is less than the upper limit can be computed by using:
<p>
<center>
C<FONT><SUB>1</SUB></FONT> = C/2 + 1/2, &nbsp; &nbsp;&nbsp; &nbsp;for conversion to one-sided
<p>
</center>
<B>Numerical Example:</B> Suppose you wish to convert a C = 90% two-sided CI into a one-sided, then   C<FONT><SUB>1</SUB></FONT> = 0.90/2  + 1/2 = 95%.
<p>
You might need to use <a href="otherapplets/SampleSize.htm" target= "new">Sample Size Determination</a> JavaScript at the design stage of your statistical investigation in decision making with specific, subjective requirements.
<P><A name=rbm></A> 
  <HR>  <H4><FONT color=#dc143c>Bonferroni Method for Multiple P-Values Procedure</FONT></H4>
  One may combine several t-tests by using the Bonferroni method. It works reasonably well when there are only a few tests, but as the number of comparisons increases above 8, the value of 't' required to conclude that a difference exists becomes much larger than it really needs to be, and the method becomes over conservative. 
  <P>One way to make the Bonferroni t-test less conservative is to use the estimate of the population variance computed from within the groups in the analysis  of variance.  <P>  <CENTER>  t = ( <IMG src="xbaru.gif">1 -<IMG   src="xbaru.gif">2 )/ ( <FONT   face=symbol>s</FONT><FONT size=+0><SUP>2</SUP></FONT> / n1 + <FONT   face=symbol>s</FONT><FONT size=+0><SUP>2</SUP></FONT> / n2 )<FONT 
  size=+0><SUP>1/2</SUP></FONT>,  </CENTER>  <P>where <FONT face=symbol>s</FONT><FONT size=+0><SUP>2</SUP></FONT> is the     population variance computed within the groups.   <P>   <P><FONT color=#dc143c>Hommel's Multiple P-Values Procedure:</FONT> This test can be summarized as follows:   <P>Suppose we have n number of P-values: p(i), i =1, .., n, in ascending order corresponding  to independent tests. Let j be the largest integer, such as:  <CENTER><P>p(n-j+k) <FONT face=symbol>&gt;</FONT> k<FONT face=symbol>a</FONT>/j, &nbsp; &nbsp;for all k=1,.., ,j. </CENTER> <P>  <CENTER> </CENTER> If no such j exists, reject all hypotheses; otherwise, reject all hypotheses  with p(i) <FONT face=symbol>£</FONT> <FONT face=symbol>a</FONT> / j. This provides a strong control of the family-wise error rate at <FONT 
  face=symbol>a</FONT> level. <P>There are other improvements on the Bonferroni adjustment when multiple tests  are independent or positively dependent. However, the Hommel's method is the  most powerful compared with other methods. <P><B>Further Readings:</B><BR> <FONT face="Bookman Old Style" size=-2>Hommel G., Bonferroni procedures for logically related hypotheses, <I>Journal of Statistical Planning and Inference</I>, 
    82, 119-128, 1999.<BR>Kost  J., and  M. McDermott, Combining dependent P-values, <I>Statistics and Probability Letters</I>, 60,  183-190, 2002.<br>Wasteful P., and S. Young, <I>Resembling-Based Multiple Testing: Examples and Methods for P-Value Adjustment</I>, Wiley, 1992.<BR> Wright S., Adjusted P-values for simultaneous inference, <I>Biometrics</I>,  48, 1005-1013, 1992. </FONT>  <P> <P><A name=rpowt></A>  <HR>
  <H4><FONT color=#dc143c>Power of a Test and the Size Effect</FONT></H4> The power of a test plays the same role in hypothesis testing that Standard Error played in estimation. It is <FONT color=#dc143c>a measuring tool for assessing the accuracy of a test</FONT> or in comparing two competing test procedures. <P>The power of a test is the probability of  rejecting a false null hypothesis when the null hypothesis is false.   This probability is inversely related to the probability of making a Type  II error, not rejecting the null hypothesis when it is false . Recall that we choose the probability of making a Type I error when  we set <FONT face=symbol>a</FONT>. If we decrease the probability of making a Type I error, then  we increase the probability of making a Type II error. Therefore,  there are basically two errors possible when conducting a statistical analysis; type I error and and type II error: <UL><LI>Type I error - (producer's) risk of rejecting the null hypothesis when it is in fact true. <LI>Type II error - (consumer's) risk of not rejecting the null hypothesis  when it is in fact false. </LI></UL><P><B>Power and Alpha (<FONT face=symbol>a</FONT>):</B> Thus, the probability of not rejecting  a true null has the same relationship to Type I errors as the probability of correctly rejecting an untrue null     does to Type II error. Yet, as I mentioned if we decrease the odds of making one type of error we increase the odds of making the other type of error. What is the relationship between Type I and Type II errors? For a fixed sample size, decreasing one type of error increases the size of the other one.  <p><B>Power and the Size Effect:</B>   Anytime we test whether a sample differs from a population, or whether two samples come from 2 separate populations, there is the condition that each of the populations we are comparing has its own mean and standard deviation (even if we do not know it). The distance between the two population means will affect the power of our test. This is known as <FONT color=#dc143c>the size of treatment</FONT>, also known as  <font  color="#DC143C">the effect size</font>, as shown in the following table with the  three popular values for <font face="symbol">a</font>:  <p> <table width="400" border="0" align="center" height="112">  <tr>  <td height="14" bordercolor="#333333" bgcolor="#FFCCCC" align="center" valign="bottom" colspan="2"><b><font size="3" face="Arial, Helvetica, sans-serif">Power as a Function of <font face="symbol">a</font> and the Size Effect</font></b></td> </tr><tr> <td align=center width=10 height="14" bordercolor="#333333" bgcolor="#FFFFFF"> </td><td align=center height="14" bordercolor="#333333" bgcolor="#C6D5C6"><b><font face="symbol">a</font></b></td></tr> <tr> <td height="79" bordercolor="#000000" colspan="2"> <table border="0" align="center" width="455"><tr align="center" valign="bottom"><td align="left" width="120" colspan="1" height="22"><b>Size Effect</b></td><td width="87" height="22"><u>0.10</u></td><td width="111" height="22"><u>0.05</u></td><td width="116" height="22"><u>0.01</u>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td> </tr><tr align="center" valign="middle"><td width="120" height="13" colspan="1" align="left" nowrap><font face="Times New Roman, Times, serif">1.0</font></td><td height="13">.22&nbsp;</td><td height="13">.13&nbsp;</td><td height="13">.03&nbsp;&nbsp;&nbsp;</td></tr><tr align="center" valign="middle"><td width="123" height="2" colspan="1" align="left" nowrap>2.0</td><td height="2">.39&nbsp;</td><td height="2">.26&nbsp;</td>            <td height="2">.09&nbsp;&nbsp;&nbsp;</td></tr><tr align="center" valign="middle"><td width="123" height="2" colspan="1" align="left" nowrap>3.0</td><td height="2">.59&nbsp;</td><td height="2">.44&nbsp;</td>            <td height="2">.20&nbsp;&nbsp;&nbsp;</td></tr><tr align="center" valign="middle"> <td width="123" height="2" colspan="1" align="left" nowrap>4.0</td><td height="2">.76&nbsp;</td><td height="2">.64&nbsp;</td>            <td height="2">.37&nbsp;&nbsp;&nbsp;</td></tr><tr align="center" valign="middle"> <td width="123" height="2" colspan="1" align="left" nowrap>5.0</td><td height="2">.89&nbsp;</td><td height="2">.79&nbsp;</td>            <td height="2">.57&nbsp;&nbsp;&nbsp;</td></tr><tr align="center" valign="middle"><td width="123" height="2" colspan="1" align="left" nowrap>6.0</td><td height="2">.96&nbsp;</td><td height="2">.91&nbsp;</td>            <td height="2">.75&nbsp;&nbsp;&nbsp;</td></tr><tr align="center" valign="middle"><td width="123" height="2" colspan="1" align="left" nowrap>7.0</td><td height="2">.99&nbsp;</td><td height="2">.97&nbsp;</td>            <td height="2">.88&nbsp;&nbsp;&nbsp;</td></tr></table></td></tr></table></div></center>


<p><P><B>Power and the Size of Variance</B> <FONT face=symbol>s</FONT><FONT   size=+0><SUP>2</SUP></FONT>: The greater the variance S<FONT size=+0><SUP>2</SUP></FONT>, the lower the     power 1-<FONT face=symbol>b</FONT>. Anything that effects the extent to which the two distributions share common values will increase <FONT  face=symbol>b</FONT> (the likelihood of making a Type II error)   <P><B>Power and the Sample Size:</B> The smaller the sample sizes n, the lower the power. Very small n produces  power so low that false hypotheses are accepted.   <P>The following is a list of four factors influencing the power:  <UL>
    <LI>effect size (for example, the difference between the means) <LI>variance S<FONT size=+0><SUP>2</SUP></FONT> <LI>significance level <FONT face=symbol>a</FONT>  <LI>number of observations, or the sample size n </LI> </UL><P>In practice, the first three factors are often fixed. Only the <FONT  color=#dc143c>sample size</FONT> can be controlled by the statistician and that only within budget constraint. There exists a tradeoff between budget and achievement of desirable accuracy in any analysis.
  <P><B>A Numerical Example:</B> The power of a test is most easily understood by viewing it in the context of a composite test. A composite test requires the specification of a population mean as the alternative hypothesis. For example, using Z-test of hypothesis in the following Figure. The power is developed from specification of an alternative hypothesis such as <FONT face=symbol>m</FONT> = 2.5, and <FONT face=symbol>m</FONT> = 3. The resultant distribution under this alternative shifts to the right 2.5 units with the shaded area representing the power of the test, correctly rejecting a false null.
 <p><br><p><CENTER><font size=+0><font size=+0><font size=+0><font size=+0><a href="Power.gif"><img alt="Power of a test" src="Power.gif" width="180" height="228" border="0"></a></font></font></font></font> <p>Power of a Test<br><FONT size=+0><FONT size=+0><FONT size=+0><FONT size=+0><FONT size=+0><FONT size=+0><b><font color="#dc143c">Click on the image to enlarge  it</font></b></FONT></FONT></FONT></FONT></FONT></FONT></CENTER>
  <P>Not rejecting the null hypothesis when it is false is defined as a Type II     error, and is denoted by the <FONT face=symbol>b</FONT> region. In the above     Figure this region lies to the left of the critical value. In the configuration     shown in this Figure, <FONT face=symbol>b</FONT> falls to the left of the     critical value (and below the statistic's density (or probability) function     under the alternative hypothesis H<FONT size=+0><SUB>a</SUB></FONT>). The     <FONT face=symbol>b</FONT> is also defined as the probability of      not-rejecting a false null hypothesis when it is false, also called a miss. Related to the     value of <FONT face=symbol>b</FONT> is the power of a test. The power is defined     as the probability of rejecting the null hypothesis given that a specific     alternative is true, and is computed as (1- <FONT   face=symbol>a</FONT>). 
  <P><B>A Short Discussion:</B> Consider testing a simple null versus simple alternative.     In the Neyman-Pearson setup, an upper bound is set for the probability of  a given     Type I error (<FONT face=symbol>a</FONT>), and then it is desirable to find     tests with low probability of type II error (<FONT   face=symbol>b</FONT>) given this. The usual justification for this is that "we  are more concerned about a Type I error, so we set an upper limit on the <FONT   face=symbol>a</FONT> that we can tolerate." I have seen this sort of reasoning in     elementary texts and also in some advanced ones. It doesn't seem to make any   sense. When the sample size is large, for most standard tests, the ratio <FONT   face=symbol>b</FONT>/<FONT face=symbol>a</FONT> tends to 0. If we care more  about Type I error than Type II error, why should this concern dissipate with     increasing sample size?   <P>This is indeed a drawback of the classical theory of testing statistical  hypotheses. A second drawback is that the choice lies between only two test  decisions: reject the null or accept the null. It is worth considering approaches   that overcome these deficiencies. This can be done, for example, by the concept 
    of profile-tests at a 'level' <FONT face=symbol>a</FONT>. Neither the Type  I nor Type II error rates are considered separately, but they are the ratio  of a correct decision. For example, we accept the alternative hypothesis H<FONT size=+0><SUB>a</SUB></FONT>  and reject the null H<FONT   size=+0><SUB>0</SUB></FONT>, if an event is observed which is at least a-times greater under H<FONT size=+0><SUB>a</SUB></FONT> than under H<FONT   size=+0><SUB>0</SUB></FONT>. Conversely, we accept H<FONT   size=+0><SUB>0</SUB></FONT> and reject H<FONT size=+0><SUB>a</SUB></FONT>, if 
    an event is observed which is at least a-times greater under H<FONT 
  size=+0><SUB>0</SUB></FONT> than under H<FONT size=+0><SUB>a</SUB></FONT>. This 
    is a symmetric concept which is formulated within the classical approach.     <p><FONT color=#dc143c><b>Power of Parametric versus Non-parametric Tests:</font></b> As a general rule, for a given  sample size n, the parametric tests are more powerful than their non-parametric counterparts.  
The primarily  reason for this is that we have emphasized parametric tests.  Moreover, among the parametric tests,  those which use correlation are more powerful, such as 
<a href="otherapplets/Paired.htm" target ="new"> the before-and-after test</a>.  This is known as a Variance Reduction Technique used in <a href="http://ubmail.ubalt.edu/~harsham/simulation/sim.htm" target ="new">system simulation</a> to increase the accuracy (i.e., reduce variation) without increasing the sample size. <p>

<p><FONT color=#dc143c><b>Correlation Coefficient as a Measuring Tool and Decision Criterion for the Effect Size:</font></b>  The correlation coefficient could be obtained and used as a measuring tool and decision criteron for the strength of the effect size based on the computed test-statistic for major hypothesis testing.
<p>
 The correlation coefficient r stands as a very useful and accessible index of the magnitude of effect. It is commonly accepted that the small, medium, and large effect sizes correspond to r-values over 0.1, 0.3, and 0.5, respectively.  The following are needed transformation of some major inferential statistics to the r-value:
<p>
<ul>
<li>For the t(df)-statistic: &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;  r = [t<FONT size=+0><SUP>2</SUP></FONT>/(t<FONT size=+0><SUP>2</SUP></FONT> + df)]<FONT size=+0><SUP>½</SUP></FONT>
<p><li>For the F(1,df<FONT><SUB>2</SUB></FONT>)-statistic: &nbsp; &nbsp; &nbsp; &nbsp;  r = [F/(F + df)]<FONT size=+0><SUP>½</SUP></FONT>
<p><li>For the <FONT face=symbol>c</FONT><FONT size=+0><SUP>2</SUP></FONT>(1)-statistic: &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; r = [<FONT face=symbol>c</FONT><FONT size=+0><SUP>2</SUP></FONT>/n] <FONT size=+0><SUP>½</SUP></FONT>
<p><li>For the Standard Normal Z: &nbsp; &nbsp;  r = (Z<FONT size=+0><SUP>2</SUP></FONT>/n)<FONT size=+0><SUP>½</SUP></FONT>
<p>
</ul>
You might like to use <a href="otherapplets/SampleSize.htm" target= "new">Sample Size Determination</a> JavaScript at the design stage of your statistical investigation in decision making with specific subjective requirements.
<p>
<B>Further Reading:</B><BR> <FONT face="Bookman Old Style" size=-2>Murphy K., and B. Myors, <I>Statistical Power Analysis</I>, L. Erlbaum Associates, 1998.<BR>    </FONT>  <P>  <P>  <A name=rwunp>   <HR>  </A> 
  <H4><FONT color=#dc143c>Parametric vs. Non-Parametric vs. Distribution-free Tests</FONT></H4>
  One must use a statistical technique called non-parametric if it satisfies at least one of the following five types of criteria:   <P></P></BLOCKQUOTE><OL>  <LI>The data entering the analysis are enumerative; that is, counted data represent  the number of observations in each category or cross-category.  <P></P> <LI>The data are measured and/or analyzed using a nominal scale of measurement.  <P></P><LI>The data are measured and/or analyzed using an ordinal scale of measurement.  <P></P> <LI>The inference does not concern a parameter in the population distribution; for example, the hypothesis that a time-ordered set of observations 
    exhibits a random pattern.  <P></P><LI>The probability distribution of the statistic upon which the analysis is  based is not dependent upon specific information or conditions (i.e., assumptions) about the population(s) from which the sample(s) are drawn, but only upon general assumptions, such as a continuous and/or symmetric population distribution. <P></P></LI></OL><BLOCKQUOTE>According to these creteria, the distinction of non-parametric is accorded either because of the level of measurement used or required for the analysis, as in types 1 through 3; the type of inference, as in type 4, or the generality of the assumptions made about the population distribution, as in type 5. <P>For example, one may use the Mann-Whitney Rank Test as a non-parametric alternative  to Students T-test when one does not have normally distributed data. <P><B>Mann-Whitney:</B> To be used with two independent groups (analogous to the independent groups t-test) <BR><B>Wilcoxon:</B> To be used with two related (i.e., matched or repeated) groups (analogous to the related samples t-test) <BR> <B>Kruskall-Wallis:</B> To be used with two or more independent groups (analogous to the single-factor between-subjects ANOVA) <BR> <B>Friedman:</B> To be used with two or more related groups (analogous to  the single-factor within-subjects ANOVA) <P><FONT color=#dc143c>Non-parametric vs. Distribution-free Tests:</FONT> 
  <P>Non-parametric tests are those used when some specific conditions for the ordinary tests are violated. 
  <P>Distribution-free tests are those for which the procedure is valid for all different shape of the population distribution. <P>For example, the Chi-square test concerning the variance of a given population is parametric since this test requires that the population distribution be  normal. The Chi-square test of independence does not assume <A   href="#rnormalcond">normality condition</A>, or even that the data are numerical. The <a  href="#rkstest">Kolmogorov-Smirnov test</a> is a distribution-free test, which is applicable to comparing two populations with any distribution of continuous random variable.<P>

<p>
<A name=rustatistic></A>
The following section is an interesting non-parametric procedure with various and useful applications.
<p>
<FONT color=#dc143c><b>Comparison of Two Random Variables:</b></font> Consider two independent observations X = (x<FONT><SUB>1</SUB></FONT>, x<FONT><SUB>2</SUB></FONT>,, x<FONT><SUB>r</SUB></FONT>) and Y = (y<FONT><SUB>1</SUB></FONT>, y<FONT><SUB>2</SUB></FONT>,, y<FONT><SUB>s</SUB></FONT>) for two random variables X and Y respectively. To estimate the reliability function:
<p>
<center>
R = Pr (X <font face=symbol>></font> Y)
<p>
</center>
One may use:
<p>
<center>
The estimator RS = U/(r <font face=symbol>&#180;</font> s), 
<p>
</center>

where U is the number of pairs (x<FONT><SUB>i</SUB></FONT>, y<FONT><SUB>j</SUB></FONT>) such that x<FONT><SUB>i</SUB></FONT> <font face=symbol>></font> y<FONT><SUB>j</SUB></FONT>, for all i = 1, 2,  ,r, &nbsp;and  j = 1, 2,..,s.  
<p>
This estimator is an unbiased one with the minimum variance for R. It is important to know that the estimate has an upper limit,  non-negative delta value for its accuracy:
<p>
<center>
Pr{R <font face=symbol>&#179;</font> RS - <FONT face=symbol>d</FONT>} <font face=symbol>&#179;</font> max {1- exp(-2n<FONT face=symbol>d</FONT><FONT><SUP>2</SUP></FONT>), 4n<FONT face=symbol>d</FONT><FONT><SUP>2</SUP></FONT>/(1-4n<FONT face=symbol>d</FONT><FONT><SUP>2</SUP></FONT>)}.
<p>
</center>

Application areas include the insurance ruin problem. Let  random variable Y denote the claims per unit of time and let random variable X denote the return on investment (ROI) for the Insurance Company. Finally, let z denote the constant premium amount collected; then the probability that the insurance company will survive is:
<p>
<center>
 R = Pr [X + z <font face=symbol>></font> Y}.
<p>
</center>
  
<p>You might like to use the  <A   href="otherapplets/KS.htm"  
    target=new> Kolmogorov-Smirnov Test for Two Populations</A> and  <A href="otherapplets/Ustat.htm"  target=new>Comparing Two Random Variables</A> in checking your computations and performing some numerical experiment for a deeper understanding of these concepts.<p>
<p>
<B>Further Readings:</B><BR> <FONT face="Bookman Old Style" size=-2>

Arsham H., A generalized confidence region for stress-strength reliability, <I>IEEE Transactions on Reliability</I>, 35(4), 586-589, 1986.<br> 
Conover W., <I>Practical Nonparametric  Statistics</I>, Wiley, 1998.<BR>
Hollander M., and D. Wolfe, <I>Nonparametric Statistical Methods</I>, Wiley,  1999.<BR>
Kotz S., Y.  Lumelskii, and M. Pensky, <em>The Stress-Strength Model and Its Generalizations: Theory and Applications</em>, <br> Imperial College Press, London, UK, 2003, distributed by World Scientific
Publishing.
<p>
</FONT> 
   <P><A name=rTwoIndIntroTest></A>   <HR>  <H4><FONT color=#dc143c>Hypotheses Testing</FONT></H4>  Remember that, in the t-tests for differences in means, there is a condition of   equal population variances that must be examined. One way to test for possible   differences in variances is to do an F test. However, the F test is very sensitive   to violations of the <A   href="#rnormalcond">normality condition</A>; i.e., if populations appear not to be normal, then the F test  will tend to reject too often the null of no differences in population variances.  <P>You might like to use the following JavaScript to check your computations and to perform some statistical experiments for deeper understanding of these concepts:<UL><LI><A  href="otherapplets/MeanTest.htm"   target=new>Testing the Mean</A>.<LI><A  href="otherapplets/variationtest.htm"  target=new>Testing the Variance</A>.  <LI><A href="otherapplets/twopoptest.htm"  target=new>Testing Two Populations</A>. <LI><A  href="otherapplets/Paired.htm"  target=new>Testing the Difference: The Before-and-After Test</A>. <LI><A  href="otherapplets/ANOVA.htm"   target=new>ANOVA</A>. <LI>For <I>statistical</I> equality of two populations, you might like to use the <A  href="otherapplets/KS.htm"    target=new>Kolmogorov-Smirnov Test</A>. </LI> </UL> <P>  <P><A name=rsinglepoputest></A> <HR> <H4><FONT color=#dc143c>Single Population t-Test</FONT></H4> The purpose is to compare the sample mean with the given population mean. The aim is to judge the claimed mean value, based on a set of random observations   of size n. A necessary condition for validity of the result is that the population distribution is normal, if the sample size n is small (say less than 30). <P>The task is to decide whether to accept a null hypothesis:   <P>H<FONT size=+0><SUB>0</SUB></FONT> = <FONT face=symbol>m</FONT> = <FONT 
  face=symbol>m</FONT><FONT size=+0><SUB>0</SUB></FONT> <P>or to reject the null hypothesis in favor of the alternative hypothesis: <P>H<FONT size=+0><SUB>a</SUB></FONT>: <FONT face=symbol>m</FONT> is significantly  different from <FONT face=symbol>m</FONT><FONT  size=+0><SUB>0</SUB></FONT> 
  <P>The testing framework consists of computing a the t-statistics: <P>  <CENTER>  T = [(<IMG src="xbaru.gif"> - <FONT  face=symbol>m</FONT><FONT size=+0><SUB>0</SUB></FONT>) n<FONT 
  size=+0><SUP>1/2</SUP></FONT>] / S </CENTER><P> <CENTER> </CENTER>Where <IMG src="xbaru.gif">      is the estimated mean and S<FONT><SUP>2</SUP></FONT> is  the estimated variance based on n random observations. 
  <P>The above statistic is distributed as a  t-distribution with parameter d.f. = <FONT  face=symbol>n</FONT> = (n-1).  If the absolute value of the computed T-statistic is "too large" compared with the critical value of the t-table, then one rejects  the claimed value for the population's mean.  <P>This test could also be used for testing similar claims for other unimodal populations including those with discrete <A href="#rrandomva">random variables</A>, such as proportion, provided there are sufficient observations (say, over 30). 
<p>
You might like to use <A 
  href="otherapplets/MeanTest.htm" target=new>Testing the Mean</A> JavaScript in checking your computations. and <a href="otherapplets/SampleSize.htm" target= "new">Sample Size Determination</a> JavaScript at the design stage of your statistical investigation in decision making with specific subjective requirements
 <P><HR><P><A name=rTwoIndTest></A> 
  <P> <CENTER><a href="P2.gif"><IMG alt="Two-Populations Independent Means"  src="P2.gif" width="175" height="201" border="0"></a> <br><p>The Procedure for Two Populations Independent Means Test 
<br><b><font color="#dc143c">Click on the image to enlarge it and THEN print it</font></b></CENTER>
<p>
<p>
<p>  You might like to use  JavaScript <A  href="otherapplets/TwoPopTest.htm" target=new>Testing Two Populations</A>.
<P> 
 <P> <P><A name=rwwspve></A> 
  <HR><H4><FONT color=#dc143c>When  Should We Pool Variance Estimates?</FONT></H4>
  We should pool variance estimates only if there is a good reason for doing so, and then (depending on that reason) the conclusions might have to be made explicitly conditional on the validity of the equal-variance model. There are several different good reasons for pooling: <P>(a) to get a single stable estimate from several relatively small samples,     where variance fluctuations seem not to be systematic; or <P>(b) for convenience, when all the variance estimates are near enough to equality; or <P>(c) when there is no choice but to model variance (as in simple linear regression  with no replicated X values), and deviations from the constant-variance model do not seem systematic; or <P>(d) when group sizes are large and nearly equal, so that there is essentially  no difference between the pooled and unpooled estimates of standard errors of pairwise contrasts, and degrees of freedom are nearly asymptotic. <P>Note that this last rationale can fall apart for contrasts other than pairwise ones. One is not really pooling variance in case (d), rather one is merely taking a shortcut in the computation of standard errors of pairwise contrasts. <P>If you calculate the test without the assumption, you have to determine the degrees of freedom (d.f.). The formula works in such a way that d.f. will be less if the larger sample variance is in the group with the smaller number of observations. This is the case in which the two tests will differ considerably. A study of the formula for the d.f. is most enlightening, and one must understand the correspondence between the unfortunate design (having the most observations in the group with little variance) and  the low d.f. and accompanying large t-value. <P><B>Example:</B> When doing t tests for differences in means of populations  (a classic independent samples case):  <P></P>
</BLOCKQUOTE><OL><LI>For differences in means that do not make any assumption about equality 
    of population variances, use the standard error formula: <P>  <CENTER>  <B>[S<FONT size=+0><SUP>2</SUP></FONT><FONT  size=+0><SUB>1</SUB></FONT>/n<FONT size=+0><SUB>1</SUB></FONT> + S<FONT  size=+0><SUP>2</SUP></FONT><FONT size=+0><SUB>2</SUB></FONT>/n<FONT  size=+0><SUB>2</SUB></FONT>]<FONT size=+0><SUP>½</SUP></FONT></B>,   </CENTER>   <P>with d.f. = <FONT face=symbol>n</FONT> = n<FONT size=+0><SUB>1</SUB></FONT>  or n<FONT size=+0><SUB>2</SUB></FONT> whichever is smaller. </P>
  <LI>With equal variances, use the statistics:  <P>  <CENTER>  <IMG src="Tpoooled.gif"> </CENTER> <P> with parameter d.f. = <FONT face=symbol>n</FONT> = (n<FONT  size=+0><SUB>1</SUB></FONT> + n<FONT size=+0><SUB>2</SUB></FONT>- 2), n<FONT  size=+0><SUB>1</SUB></FONT>, &nbsp; &nbsp; for n<FONT 
  size=+0><SUB>2</SUB></FONT> greater than or equal to 1, where the pooled variance  is:  <P> 
      <CENTER> <IMG src="Spooled.gif"> </CENTER> <P></P> <LI>If total N is less than 50 and one sample is 1/2 the size of the other (or less), and if the smaller sample has a standard deviation at least twice as large 
    as the other sample, then apply the procedure given in item no. 1, but adjust d.f. parameter of the t-test to the largest integer less than or equal to:  <P>  <CENTER>  d.f. = <FONT face=symbol>n</FONT> = A/(B +C),  </CENTER> <P>where: <P>A = [S<FONT size=+0><SUP>2</SUP></FONT><FONT 
  size=+0><SUB>1</SUB></FONT>/n<FONT size=+0><SUB>1</SUB></FONT> + S<FONT 
  size=+0><SUP>2</SUP></FONT><FONT size=+0><SUB>2</SUB></FONT>/n<FONT 
  size=+0><SUB>2</SUB></FONT>]<FONT size=+0><SUP>2</SUP></FONT>,  <P>B = [S<FONT size=+0><SUP>2</SUP></FONT><FONT  size=+0><SUB>1</SUB></FONT>/n<FONT size=+0><SUB>1</SUB></FONT>]<FONT size=+0><SUP>2</SUP></FONT> / (n<FONT size=+0><SUB>1</SUB></FONT> -1),  <P>C = [S<FONT size=+0><SUP>2</SUP></FONT><FONT 
  size=+0><SUB>2</SUB></FONT>/n<FONT size=+0><SUB>2</SUB></FONT>]<FONT 
  size=+0><SUP>2</SUP></FONT>/ (n<FONT size=+0><SUB>2</SUB></FONT> -1) 
    <P>Otherwise, do not worry about the problem of having an actual <FONT  face=symbol>a </FONT>level that is much different than what you have set it  to be. <P></P>  </LI></OL><BLOCKQUOTE>Statistics with Confidence Section is concerned with the construction of a confidence interval where the equality of variances condition is an important  issue. <P>The last approach, which is very general with conservative results, can be  implemented using <A  href="otherapplets/twopoptest.htm" 
  target=new>Testing Two Populations</A> Applet. <P> 

  <P> <CENTER> <A name=rTwoDepdTest></A> <HR> <a href="P3.gif"><IMG alt="Two Dependent Means" src="P3.gif" width="175" height="188" border="0"></a> <p>The Procedure for Two Dependent Means Test <br>
  <b><font color="#dc143c">Click on the image to enlarge it and THEN print it</font></b><p> </CENTER><p>  You might like to use  JavaScript <A  href="otherapplets/Paired.htm" target=new>Testing the Difference in Means: The Before-and-After Test</A> and <A href="otherapplets/PairedProp.htm"  target=new>Paired Proportion Test</A> for dependent proportions.<p>
<p>
<p>
<P><A name=rMorethanwononpar></A> <HR><H4><FONT color=#dc143c>Non-parametric Multiple Comparison Procedures</FONT></H4>Duncan's multiple-range test: This is one of the many multiple comparison procedures.  It is based on the standardized range statistic by comparing all pairs of means  while controlling the overall Type I error at a desirable level. While it does  not provide interval estimates of the difference between each pair of means,   it does indicate which means are significantly different from the others.  For determining the significant differences between a <i>single control group</I> mean and the other means, one may use the Dunnett's multiple-comparison test. 
<P><A name=requalPopulasTest></A> 
  <HR><H4><FONT color=#dc143c>Introduction to Tests for Statistical Equality of Two or More Populations:</FONT></H4>  Two random variables X and Y having distribution F<FONT><SUB>X</SUB></FONT>(x) and  F<FONT><SUB>Y</SUB></FONT>(y) respectively, are said to be equivalent, or equal in law, or equal in distribution, if and only if they have the same distribution function. That is,  
<p>
<center>
F<FONT><SUB>X</SUB></FONT>(z) = F<FONT><SUB>Y</SUB></FONT>(z),  &nbsp;&nbsp;for all z, 
<p>
</center>
There are different tests depending on the intended applications.  The widely used tests for statistical equality of populations are as follow:
<p>
<ol>
 <li><FONT color=#dc143c><A  href="#requalPopus">Equality of Two Normal Populations:</A></FONT>
 One may use the Z-test and F-test to check the equality of the means, and the equality of variances, respectively. 
<A name=rshiftPopulasTest></A> 
<p>
<li><FONT color=#dc143c><A href="otherapplets/TwoPopTest.htm" target="new">Testing a Shift in Normal Populations:</a></FONT> Often we are interested in testing for a given shift in a given population distribution, that is testing if a random variable Y is equal in distribution to another X + c for some constant c.   In other words, the distribution of Y is the distribution of X shifted.  In testing any shift in distribution one needs to test for normality first, and then testing the difference in expected values by applying the two-sided Z-test with the null hypothesis of: 
<p>
<center>
H<FONT><SUB>0</SUB></FONT>:&nbsp;&nbsp; <FONT  face=symbol>m</FONT><FONT><SUB>Y</SUB></FONT> - <FONT  face=symbol>m</FONT><FONT><SUB>X</SUB></FONT> = c.
</center> 
<p>
<li><FONT color=#dc143c><A  href="#rANOVA">Analysis of Variance:</A></FONT> Analysis of Variance (ANOVA) tests are designed for simultaneous testing of equality of three or more populations.  The preconditions in applying ANOVA are normality of each population's distribution, and the equality of all variances simultaneously (not the pair-wise tests).
 <p>
Notice that ANOVA is an extension of item no. 1 in testing equality of more than two populations.  It can be shown that if one applies ANOVA for testing the equality of two populations based on two independent samples with sizes of n1 and n2 form each population, respectively, then the results of both tests will be identical.   Moreover, the test-statistic obtained by each test are directly related, i.e.,  
<p>
<center>
 F <FONT FACE="symbol"><SUB>a</SUB></FONT> <FONT><SUB>, (1, n1+ n2 - 2)</SUB></FONT> &nbsp;= &nbsp; t <FONT><SUP>2</SUP></FONT> <FONT FACE="symbol"><SUB>a/2</SUB></FONT> <FONT><SUB>, (n1+ n2 - 2)</SUB></FONT>
</center>
<p>
<p>
<li><FONT color=#dc143c><A href="#rHomPropor">Equality of Proportions in Several Populations:</A></FONT>  This test is for discrete random variables. It is one of the many interesting <A href="#rtestothercl">chi-square applications</a>.
<p>
<li><FONT color=#dc143c><A  href="#rkstest" target=new>Distribution-free Equality of Two Populations:</A></FONT> Whenever one is interested in testing the equality of two populations with a common continuous random variable, without any reference to the underlying distribution such as normality condition, one may use the distribution-free known as the K-S test.
<p>
<li><FONT color=#dc143c><A  href="#rustatistic" target=new>Non-parametric Comparison of Two Random Variables:</A></FONT>  Consider two independent observations X = (x<FONT><SUB>1</SUB></FONT>, x<FONT><SUB>2</SUB></FONT>,, x<FONT><SUB>r</SUB></FONT>) and Y = (y<FONT><SUB>1</SUB></FONT>, y<FONT><SUB>2</SUB></FONT>,, y<FONT><SUB>s</SUB></FONT>) for two independent populations with random variables X and Y, respectively. Often we are interested in estimating the Pr (X <font face=symbol>></font> Y).
<p>
</ol>
<p>
 <P><A name=requalPopus></A> 
  <HR><H4><FONT color=#dc143c>Equality of Two Normal Populations:</FONT></H4> The normal or Gaussian distribution is a continuous symmetric distribution that follows the familiar bell-shaped curve. One of its nice features is that, the mean and variance uniquely and independently determines the distribution. 
<p>
Therefore, for testing the statistical equality of two independent normal populations, one must first perform the <A href="otherapplets/Normality.htm" target=new>Lilliefors' Test for Normality</A> to assess this condition. Given that both populations are normally distributed, then one must performing two more tests, namely the test for equality of the two means and the test for equality of the two variances.  Both of these tests can be carried out by using the
<A href="otherapplets/TwoPopTest.htm" target=new>Test of  Hypotheses for Two Populations</A> JavaScript Applets.
<p>
 <P><A name=rANOVA></A> 
  <HR><H4><FONT color=#dc143c>Multi-Means Comparisons: Analysis of Variance (ANOVA)</FONT></H4>
  The tests we have learned up to this point allow us to test hypotheses that examine the difference between only two means. Analysis of Variance or ANOVA will allow us to test the difference between two or more means. ANOVA does this by examining the ratio of variability between two conditions and variability within each condition. For example, say we give a drug that we believe will improve memory to a group of people and give a placebo to another group of people. We might measure memory performance by the number of words recalled from a list we ask everyone to memorize. A t-test would compare the likelihood of observing 
  the difference in the mean number of words recalled for each group. An ANOVA test, on the other hand, would compare the variability that we observe between the two conditions to the variability observed within each condition. Recall that we measure variability as the sum of the difference of each score from the mean. When we actually calculate an ANOVA we will use a short-cut formula  <P>Thus, when the variability that we predict  between the two groups is much greater than the variability we don't predict within each group, then we 
    will conclude that our treatments produce different results. <H4><FONT color=#dc143c>An Illustrative Numerical Example for ANOVA</FONT></H4>Consider the following (small integers, indeed for illustration  while saving space) random samples from three different populations. <P>With the null hypothesis:<br>H<FONT size=+0><SUB>0</SUB></FONT>: µ1 = µ2 = µ3,  <br>and the alternative:<br>H<FONT size=+0><SUB>a</SUB></FONT>: at least two of the means are not equal. <p>At the significance level <FONT face=symbol>a </FONT>= 0.05, the critical value from F-table is <BR> F <FONT size=+0><SUB>0.05, 2, 12</SUB></FONT> = 3.89.  <P>  <CENTER><table width="411" border="0">
    <tr>
      <td bgcolor="#999999" > 
        <table width="450" border="0" cellspacing="1">
          <tr bgcolor="#FFFFFF"> 
            <td colspan="6">&nbsp;</td>
            <td width="12%"> 
              <div align="center"><font color="#003366"><b><font size="2" face="Arial, Helvetica, sans-serif">Sum</font></b></font></div>
            </td>
            <td width="12%"> 
              <div align="center"><font color="#003366"><b><font size="2" face="Arial, Helvetica, sans-serif">Mean</font></b></font></div>
            </td>
          </tr>
          <tr bgcolor="#FFFFFF"> 
            <td width="78"><font size="2" face="Arial, Helvetica, sans-serif"><b><font color="#996699">Sample 
              P1</font></b></font></td>
            <td width="11%"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">2</font></div>
            </td>
            <td width="11%"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">3</font></div>
            </td>
            <td width="11%"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">1</font></div>
            </td>
            <td width="11%"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">3</font></div>
            </td>
            <td width="11%"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">1</font></div>
            </td>
            <td width="11%"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif" color="#996699">10</font></div>
            </td>
            <td width="11%"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif" color="#996699">2</font></div>
            </td>
          </tr>
          <tr bgcolor="#FFFFFF"> 
            <td width="78" height="19"><b><font size="2" face="Arial, Helvetica, sans-serif" color="#CC6633">Sample 
              P2</font></b></td>
            <td width="11%" height="19"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">3</font></div>
            </td>
            <td width="11%" height="19"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">4</font></div>
            </td>
            <td width="11%" height="19"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">3</font></div>
            </td>
            <td width="11%" height="19"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">5</font></div>
            </td>
            <td width="11%" height="19"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">0</font></div>
            </td>
            <td width="11%" height="19"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif" color="#CC6633">15</font></div>
            </td>
            <td width="11%" height="19"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif" color="#CC6633">3</font></div>
            </td>
          </tr>
          <tr bgcolor="#FFFFFF"> 
            <td width="78"><b><font size="2" face="Arial, Helvetica, sans-serif" color="#218363">Sample 
              P3</font></b></td>
            <td width="11%"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">5</font></div>
            </td>
            <td width="11%"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">5</font></div>
            </td>
            <td width="11%"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">5</font></div>
            </td>
            <td width="11%"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">3</font></div>
            </td>
            <td width="11%"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">2</font></div>
            </td>
            <td width="11%"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif" color="#339966">20</font></div>
            </td>
            <td width="11%"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif" color="#339966">4</font></div>
            </td>
          </tr>
        </table>
      </td>
    </tr>
  </table>
  </CENTER>
  <P>
<br>Demonstrate that, SST=SSB+SSW.<br>That is, the sum of squares total (SST) equals sum of squares between (SSB) the groups plus sum of squares within (SSW) the groups.   <P><B>Computation of sample SST:</B> With the grand mean = 3, first, start with  taking the difference between each observation and the grand mean, and then     square it for each data point. <P> <CENTER> <table width="349" border="0">
    <tr>
      <td bgcolor="#999999" >
        <table width="342" border="0" cellspacing="1">
          <tr bgcolor="#FFFFFF"> 
            <td colspan="6">&nbsp;</td>
            <td width="45"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif"><b><font color="#003366">Sum</font></b></font></div>
            </td>
          </tr>
          <tr bgcolor="#FFFFFF"> 
            <td width="30%"><font size="2" face="Arial, Helvetica, sans-serif"><b><font color="#996699">Sample 
              P1</font></b></font></td>
            <td width="11%"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">1</font></div>
            </td>
            <td width="11%"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">0</font></div>
            </td>
            <td width="11%"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">4</font></div>
            </td>
            <td width="11%"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">0</font></div>
            </td>
            <td width="11%"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">4</font></div>
            </td>
            <td width="11%"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif" color="#996699">9</font></div>
            </td>
          </tr>
          <tr bgcolor="#FFFFFF"> 
            <td width="30%" height="19"><b><font size="2" face="Arial, Helvetica, sans-serif" color="#CC6633">Sample 
              P2</font></b></td>
            <td width="11%" height="19"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">0</font></div>
            </td>
            <td width="11%" height="19"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">1</font></div>
            </td>
            <td width="11%" height="19"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">0</font></div>
            </td>
            <td width="11%" height="19"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">4</font></div>
            </td>
            <td width="11%" height="19"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">9</font></div>
            </td>
            <td width="11%" height="19"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif" color="#CC6633">14</font></div>
            </td>
          </tr>
          <tr bgcolor="#FFFFFF"> 
            <td width="30%" height="19"><b><font size="2" face="Arial, Helvetica, sans-serif" color="#339966">Sample 
              P3</font></b></td>
            <td width="11%" height="19"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">4</font></div>
            </td>
            <td width="11%" height="19"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">4</font></div>
            </td>
            <td width="11%" height="19"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">4</font></div>
            </td>
            <td width="11%" height="19"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">0</font></div>
            </td>
            <td width="11%" height="19"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">1</font></div>
            </td>
            <td width="11%" height="19"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif" color="#339966">13</font></div>
            </td>
          </tr>
        </table>
      </td>
    </tr>
  </table></CENTER><P>Therefore SST = 36 with d.f = (n-1) = 15-1 = 14 
  <P><B>Computation of sample SSB:</B> <P>Second, let all the data in each sample have the same value as the mean in  that sample. This removes any variation WITHIN. Compute SS differences from the grand mean. 
  <P> <CENTER><table width="357" border="0">
    <tr>
      <td bgcolor="#999999">
        <table border="0" cellspacing="1">
          <tr bgcolor="#FFFFFF"> 
            <td colspan="6">&nbsp;</td>
            <td width="46"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif"><b><font color="#003366">Sum</font></b></font></div>
            </td>
          </tr>
          <tr bgcolor="#FFFFFF"> 
            <td width="101"><font size="2" face="Arial, Helvetica, sans-serif"><b><font color="#996699">Sample 
              P1</font></b></font></td>
            <td width="36"> 
              <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">1</font></div>
            </td>
            <td width="36"> 
              <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">1</font></div>
            </td>
            <td width="36"> 
              <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">1</font></div>
            </td>
            <td width="37"> 
              <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">1</font></div>
            </td>
            <td width="37"> 
              <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">1</font></div>
            </td>
            <td width="46"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif" color="#996699">5</font></div>
            </td>
          </tr>
          <tr bgcolor="#FFFFFF"> 
            <td width="101" height="19"><b><font size="2" face="Arial, Helvetica, sans-serif" color="#CC6633">Sample 
              P2</font></b></td>
            <td width="36" height="19"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">0</font></div>
            </td>
            <td width="36" height="19"> 
              <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">0</font></div>
            </td>
            <td width="36" height="19"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">0</font></div>
            </td>
            <td width="37" height="19"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">0</font></div>
            </td>
            <td width="37" height="19"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">0</font></div>
            </td>
            <td width="46" height="19"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif" color="#CC6633">0</font></div>
            </td>
          </tr>
          <tr bgcolor="#FFFFFF"> 
            <td width="101"><b><font size="2" face="Arial, Helvetica, sans-serif" color="#339966">Sample 
              P3</font></b></td>
            <td width="36"> 
              <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">1</font></div>
            </td>
            <td width="36"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">1</font></div>
            </td>
            <td width="36"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">1</font></div>
            </td>
            <td width="37"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">1</font></div>
            </td>
            <td width="37"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">1</font></div>
            </td>
            <td width="46"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif" color="#339966">5</font></div>
            </td>
          </tr>
        </table>
      </td>
    </tr>
  </table></CENTER><P>Therefore SSB = 10, with d.f = (m-1)= 3-1 = 2 for m=3 groups.  <P><B>Computation of sample SSW:</B> 
  <P>Third, compute the SS difference within each sample using their own sample means. This provides SS deviation WITHIN all samples.  <P> <CENTER> <table width="345" border="0">
    <tr>
      <td bgcolor="#999999" height="57"> 
        <table width="341" border="0" cellspacing="1">
          <tr bgcolor="#FFFFFF"> 
            <td colspan="6">&nbsp;</td>
            <td width="45"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif"><b><font color="#003366">Sum</font></b></font></div>
            </td>
          </tr>
          <tr bgcolor="#FFFFFF"> 
            <td width="30%"><font size="2" face="Arial, Helvetica, sans-serif"><b><font color="#996699">Sample 
              P1</font></b></font></td>
            <td width="11%"> 
              <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">0</font></div>
            </td>
            <td width="11%"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">1</font></div>
            </td>
            <td width="11%"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">1</font></div>
            </td>
            <td width="11%"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">1</font></div>
            </td>
            <td width="11%"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">1</font></div>
            </td>
            <td width="11%"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif" color="#996699">4</font></div>
            </td>
          </tr>
          <tr bgcolor="#FFFFFF"> 
            <td width="30%" height="19"><b><font size="2" face="Arial, Helvetica, sans-serif" color="#CC6633">Sample 
              P2</font></b></td>
            <td width="11%" height="19"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">0</font></div>
            </td>
            <td width="11%" height="19"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">1</font></div>
            </td>
            <td width="11%" height="19"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">0</font></div>
            </td>
            <td width="11%" height="19"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">4</font></div>
            </td>
            <td width="11%" height="19"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">9</font></div>
            </td>
            <td width="11%" height="19"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif" color="#CC6633">14</font></div>
            </td>
          </tr>
          <tr bgcolor="#FFFFFF"> 
            <td width="30%"><b><font size="2" face="Arial, Helvetica, sans-serif" color="#339966">Sample 
              P3</font></b></td>
            <td width="11%"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">1</font></div>
            </td>
            <td width="11%"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">1</font></div>
            </td>
            <td width="11%"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">1</font></div>
            </td>
            <td width="11%"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">1</font></div>
            </td>
            <td width="11%"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">4</font></div>
            </td>
            <td width="11%"> 
              <div align="center"><font size="2" face="Arial, Helvetica, sans-serif" color="#339966">8</font></div>
            </td>
          </tr>
        </table>
      </td>
    </tr>
  </table></CENTER><P>SSW = 26 with d.f = 3(5-1) = 12. That is, 3 groups times (5 observations in each -1) 
  <P>Results are: SST = SSB + SSW, and d.f<FONT size=+0><SUB>SST</SUB></FONT>  = d.f<FONT size=+0><SUB>SSB</SUB></FONT> + d.f<FONT size=+0><SUB>SSW</SUB></FONT>, as expected. <P>Now, construct the ANOVA table for this numerical example by plugging the results of your computation in the ANOVA Table.  Note that, the Mean Squares are the Sum of squares divided by their Degrees of Freedom. F-statistics is the ratio of the two Mean Squares. <P> <CENTER> <TABLE border=1 borderColor=#ffffff cellSpacing=0> <TBODY><TR><TD align=middle colSpan=5><FONT color=#dc143c><FONT size=3><B>The ANOVA Table</B></FONT></FONT></TD>
</TR><TR> <TD align=middle>Sources of Variation</TD><TD align=middle>Sum of Squares</TD> <TD align=middle>Degrees of Freedom</TD><TD align=middle>Mean Squares</TD> <TD align=middle>F-Statistic</TD></TR><TR> <TD align=middle>Between Samples</TD><TD align=middle>10</TD>
<TD align=middle>2</TD><TD align=middle>5</TD><TD align=middle>2.30</TD></TR><TR> <TD align=middle>Within Samples</TD><TD align=middle>26</TD><TD align=middle>12</TD><TD align=middle>2.17</TD> <TD>&nbsp;</TD> </TR><TR> <TD align=middle>Total</TD><TD align=middle>36</TD>
        <TD align=middle>14</TD><TD>&nbsp;</TD><TD>&nbsp;</TD></TR></TBODY></TABLE></CENTER>
  <P>Conclusion: There is not enough evidence to reject the null hypothesis H<FONT size=+0><SUB>0</SUB></FONT>.  <P> <P><FONT color=#dc143c>The Logic behind ANOVA:</FONT> First, let us try to explain the logic and then illustrate it with a simple example. In performing the ANOVA  test, we are trying to determine if a certain number of population means are equal. To do that, we measure the difference of the sample means and compare that to the variability within the sample observations. That is why the test statistic is the ratio of the between-sample variation (MSB) and the within-sample variation (MSW). If this ratio is close to 1, there is evidence that the population means are equal. <P>Here is a good application for you: Many people believe that men get paid more in the business world than women, simply because they are male. To justify or reject such a claim, you could look at the variation within each group  (one group being women's salaries and the other group being men's salaries) and compare that to the variation between the means of randomly selected samples of each population. If the variation in the women's salaries is much larger than the variation between the men's and women's mean salaries, one could say that because the variation is so large within the women's group that this may not be a gender-related problem. <P> <P>Now, getting back to our numerical example of the drug treatment to improve memory vs the placebo. We notice that: given the test conclusion and the ANOVA test's conditions, we may conclude that these three     populations are in fact the same population. Therefore, the ANOVA technique could be used as a measuring tool and statistical routine for quality control as described below using our numerical example. 
  <P><FONT color=#dc143c><B>Construction of the Control Chart for the Sample Means:</B></FONT> 
    Under the null hypothesis, the ANOVA concludes that µ1 = µ2 = µ3; that is, we have a "hypothetical parent population." The question is, what is its variance? The estimated variance (i.e., the total mean squares) is 36 / 14 = 2.75. Thus, estimated standard deviation is = 1.60 and estimated standard deviation for the means is 1.6 / 5<FONT size=+0><SUP>½</SUP></FONT>  = 0.71. Under the conditions of ANOVA, we can construct a control chart with   the warning limits = 3 ± 2(0.71); the action limits = 3 ± 3(0.71). The following figure depicts the control chart.   <P>  <CENTER><IMG alt="Control Chart" src="ANOVA.gif"> </CENTER><P><A name="rMore thanTwoIndTest"></A> <P>You might like to use <A href="otherapplets/anova.htm"   target=new>ANOVA: Testing Equality of Means</A>, or <A   href="http://www.physics.csbsju.edu/stats/anova_pnp_NGROUP_form.html"  target=new>ANOVA</A> for your computations, and then to interpret the results in managerial (not technical) terms.<p>
You might need to use <a href="otherapplets/SampleSize.htm" target= "new">Sample Size Determination</a> JavaScript at the design stage of your statistical investigation in decision making with specific subjective requirements.<p><P> <P>  <CENTER>
      <font size=+0><font size=+0><font size=+0><font size=+0><a href="P4.gif"><img alt="More Than Two Independent Means"  src="P4.gif" width="162" height="190" border="0"></a></font></font></font></font> 
      <p>The Procedure for More Than Two Independent Means Test <br><b><font color="#dc143c">Click on the image to enlarge it and THEN print it</font></b></CENTER> <P> 
<p>

<P><A name=rANOVACond></A> <HR><H4><FONT color=#dc143c>ANOVA for Normal but Condensed Data Sets</FONT></H4>
 In testing the equality of several means, often the raw data are not available.  In such a case, one must perform the needed analysis based on secondary data using the data summaries; namely, the triple-set: The sample sizes, the sample means, and the sample variances. 
<p>
Suppose one of the samples is of size n having the sample mean <IMG src="xbaru.gif">,  and the sample variance S<FONT><SUP>2</SUP></FONT>. Let:
<p>
<center>
y<FONT><SUB>i</SUB></FONT>  = <IMG src="xbaru.gif"> + (S<FONT><SUP>2</SUP></FONT>/n)<FONT><SUP>½</SUP></FONT> &nbsp; &nbsp; for all i = 1, 2, , n-1,
<p>
</center>
and
<p>
<center>
y<FONT><SUB>n</SUB></FONT> = n <IMG src="xbaru.gif">  -  (n - 1)y<FONT><SUB>1</SUB></FONT>
<p>
</center>
Then, the new random data y<FONT><SUB>i</SUB></FONT>'s are surrogate data having the same mean and variance as the original data set.  Therefore, by generating the surrogate data for each sample, one can perform the standard ANOVA test. The results are identical.
<P>You might like to use <A href="otherapplets/SeveralMeans.htm"   target=new>ANOVA for Condensed Data</A> for your computation and experimentation. 
<p>
The JavaScript <A href="otherapplets/InaccracyAssessmet.htm"  target=new>Subjective Assessment of Estimates</A> tests the claim that at least the ratio of one estimate to   the largest estimate is as large as a given claimed value. 
  <P><B>Further Reading:</B><BR> <FONT face="Bookman Old Style" size=-2>Larson D., Analysis of variance with just summary statistics as input, <I>The American Statistician</I>, 46(2), 151-152, 1992. </FONT> <p>
<P><A name=rMorethantwodep></A> <HR><H4><FONT color=#dc143c>ANOVA for Dependent Populations</FONT></H4>
Populations can be dependent in either of the following ways:
<p>
<ol>
<li>Every subject is tested in every experimental condition.  This kind of dependency is called the repeated-measurement design.
<li>Subjects under different experimental conditions are related in some manner.  This kind of dependency is called matched-subject designed.
</ol>
<p>
<B>An Application:</B>  Suppose we are interested in studying the effect of alcohol on driving ability.  Ten  subjects are given three different alcohol levels and the number of driving errors are tabulated below:
<P>  <table width="400" border="1" align="center" cellpadding="3" cellspacing="0">
        <tr> 
          <td colspan="11">&nbsp;</td>
          <td width="49" height="22"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2"><b><font color="#666666">Mean</font></b></font></div>
          </td>
        </tr>
        <tr> 
          <td width="37" height="15" valign="top"><b><font face="Arial, Helvetica, sans-serif" size="2" color="#006666">0 
            oz</font></b></td>
          <td width="30" height="15" valign="top"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#006666">2</font></div>
          </td>
          <td width="30" height="15" valign="top"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#006666">3</font></div>
          </td>
          <td width="30" height="15" valign="top"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#006666">1</font></div>
          </td>
          <td width="30" height="15" valign="top"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#006666">3</font></div>
          </td>
          <td width="30" height="15" valign="top"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#006666">1</font></div>
          </td>
          <td width="30" height="15" valign="top"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#006666">4</font></div>
          </td>
          <td width="30" height="15" valign="top"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#006666">1</font></div>
          </td>
          <td width="30" height="15" valign="top"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#006666">3</font></div>
          </td>
          <td width="30" height="15" valign="top"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#006666">2</font></div>
          </td>
          <td width="30" height="15" valign="top"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#006666">1</font></div>
          </td>
          <td width="49" height="15" valign="top"> 
            <div align="center"><b><font face="Arial, Helvetica, sans-serif" size="2" color="#006666">2.1</font></b></div>
          </td>
        </tr>
        <tr> 
          <td width="37" valign="top" height="17"><b><font face="Arial, Helvetica, sans-serif" size="2" color="#B0002D">2 
            oz</font></b></td>
          <td width="30" valign="top" height="17"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#B0002D">3</font></div>
          </td>
          <td width="30" valign="top" height="17"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#B0002D">2</font></div>
          </td>
          <td width="30" valign="top" height="17"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#B0002D">1</font></div>
          </td>
          <td width="30" valign="top" height="17"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#B0002D">4</font></div>
          </td>
          <td width="30" valign="top" height="17"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#B0002D">2</font></div>
          </td>
          <td width="30" valign="top" height="17"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#B0002D">3</font></div>
          </td>
          <td width="30" valign="top" height="17"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#B0002D">1</font></div>
          </td>
          <td width="30" valign="top" height="17"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#B0002D">5</font></div>
          </td>
          <td width="30" valign="top" height="17"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#B0002D">1</font></div>
          </td>
          <td width="30" valign="top" height="17"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#B0002D">2</font></div>
          </td>
          <td width="49" valign="top" height="17"> 
            <div align="center"><b><font face="Arial, Helvetica, sans-serif" size="2" color="#B0002D">2.4</font></b></div>
          </td>
        </tr>
        <tr> 
          <td width="37" valign="top" height="2"><b><font face="Arial, Helvetica, sans-serif" size="2" color="#B79300">4 
            oz</font></b></td>
          <td width="30" valign="top" height="2"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#B79300">3</font></div>
          </td>
          <td width="30" valign="top" height="2"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#B79300">1</font></div>
          </td>
          <td width="30" valign="top" height="2"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#B79300">2</font></div>
          </td>
          <td width="30" valign="top" height="2"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#B79300">4</font></div>
          </td>
          <td width="30" valign="top" height="2"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#B79300">2</font></div>
          </td>
          <td width="30" valign="top" height="2"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#B79300">5</font></div>
          </td>
          <td width="30" valign="top" height="2"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#B79300">2</font></div>
          </td>
          <td width="30" valign="top" height="2"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#B79300">4</font></div>
          </td>
          <td width="30" valign="top" height="2"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#B79300">3</font></div>
          </td>
          <td width="30" valign="top" height="2"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#B79300">2</font></div>
          </td>
          <td width="49" valign="top" height="2"> 
            <div align="center"><b><font face="Arial, Helvetica, sans-serif" size="2" color="#B79300">3.1</font></b></div>
          </td>
        </tr>
      </table>
     
<p>

The test null hypothesis is:
<p>
<B>H<FONT size=+0><SUB>0</SUB></FONT>:</B> µ1 = µ2 = µ3,  <p>and the alternative:<p><B>H<FONT size=+0><SUB>a</SUB></FONT>:</B> at least two of the means are not equal.
<p>

<p>
Using the <a href="otherapplets/ANOVADep.htm" target= "new"> ANOVA for Dependent Populations</a> JavaScripts, we obtain the needed information in constructing the following ANOVA table:
<p
<P> <CENTER> <TABLE border=1 borderColor=#ffffff cellSpacing=0> <TBODY><TR><TD align=middle colSpan=5><FONT color=#dc143c><FONT size=3><B>The ANOVA Table</B></FONT></FONT></TD>
</TR><TR> <TD align=middle>Sources of Variation</TD><TD align=middle>Sum of Squares</TD> <TD align=middle>Degrees of Freedom</TD><TD align=middle>Mean Squares</TD> <TD align=middle>F-Statistic</TD></TR>
<TR> <TD align=middle>Subjects
</TD><TD align=middle>31.50</TD>
<TD align=middle>9</TD><TD align=middle>3.50</TD><TD align=middle>-</TD></TR>
<TR> <TD align=middle>Between
</TD><TD align=middle>5.26</TD>
<TD align=middle>2</TD><TD align=middle>2.63</TD><TD align=middle>7.03</TD></TR>
<TR> <TD align=middle>Within
</TD><TD align=middle>6.70</TD><TD align=middle>18</TD><TD align=middle>0.37</TD> <TD>&nbsp;</TD> </TR><TR> <TD align=middle>Total
</TD><TD align=middle>43.46</TD>
        <TD align=middle>29</TD><TD>&nbsp;</TD><TD>&nbsp;</TD></TR></TBODY></TABLE></CENTER>
  <P><B>Conclusion:</B> The p-value is P= 0.006, indicating a strong evidence against the null hypothesis. The means of the populations are not equal.  Here, one may conclude that person who has consumed more than certain level of alcohol commits more driving errors.
<P>
 <CENTER> <font size=+0><font size=+0><font size=+0><font size=+0><a href="P5.gif"><img alt="More Than Two Dependent Populations"   src="P5.gif" width="172" height="194" border="0"></a></font></font></font></font> <p>The Procedure for More Than Two Dependent Populations Test
  <br><b><font color="#dc143c">Click on the image to enlarge it and THEN print it</font></b> </CENTER><P> 
<p>
<p>
<br>
<p>
A "block design sampling" implies studying more than two dependent populations.  For testing the equality of means of more than two populations based on block design sampling, you may use <A href="otherapplets/ANOVATwo.htm"  target=new>Two-Way ANOVA Test</A> JavaScript.  In the case of having block design data with replications, use <A href="otherapplets/ANOVA2Rep.htm"  target=new>Two-Way ANOVA with Replications</A> JavaScript to obtain the needed information for constructing the ANOVA tables.
<p>

 <P><A name=rHomPropor></A> 
    <HR>    <H4><FONT color=#dc143c>Test for Equality of Several Population Proportions</FONT></H4>
  <DD>The Chi-square test of homogeneity provides an alternative method for testing the null hypothesis that two population proportions are equal. Moreover, it extend, to several populations similar to the ANOVA test that compares several means.  <P><B>An Application:</B> Suppose we wish to test the null hypothesis 
    <P>       <CENTER>        <B>H<FONT size=+0><SUB>0</SUB></FONT></B>: P<FONT 
  size=+0><SUB>1</SUB></FONT> = P<FONT size=+0><SUB>2</SUB></FONT> = ..... = P<FONT size=+0><SUB>k</SUB></FONT>  </CENTER> <P>That is, all three population proportions are almost identical. The sample data from each of the three populations are given in the following table: <P> <CENTER> <TABLE borderColor=#000000 cellSpacing=0> <TBODY>  <TR>  <TD align=middle colSpan=11><FONT color=#dc143c>Test for homogeneity of Several Population Proportions</FONT>  <TR><TD><U>Populations</U></TD> <TD><U>Yes </U></TD> <TD><U>No </U></TD> <TD>Total</TD>
</TR>  <TR>  <TD>Sample I</TD> <TD>60</TD> <TD>40</TD> <TD>100</TD>  </TR> <TR> <TD>Sample II</TD><TD>57</TD>  <TD>53</TD><TD>110</TD></TR><TR> <TD>Sample III</TD>
  <TD>48</TD> <TD>72</TD><TD>120</TD></TR> <TR> <TD><U>Total</U></TD> <TD>165</TD> <TD>165</TD>  <TD>330</TD> </TR> </TBODY> </TABLE></CENTER><P>The Chi-square statistic is 8.95 with d.f. = (3-1)(3-1) = 4. The p-value is equal to 0.062, indicating that there is moderate evidence against the null hypothesis that the three populations are statistically identical.  <P> <P>You might like to use <A   href="otherapplets/ProporTest.htm"  target=new>Testing Proportions</A> to perform this test.   
<p>
<P><A name=rkstest></A> <HR> <H4><FONT color=#dc143c>Distribution-free Equality of Two Populations </FONT></H4>
For <I>statistical</I> equality of two populations, one may use the Kolmogorov-Smirnov Test (K-S Test) for two populations. The K-S test seeks differences between the two population's distribution function based on their two independent random samples. The test rejects the null hypothesis of no difference between the two populations if the difference between the two empirical distribution functions is "large".<p>Prior to applying the K-S test it is necessary to arrange each of the two sample observations in a frequency table. The frequency table must have a common classification. Therefore the test is based on the frequency table, which belongs to the family of <A  href="#rwunp">distribution-free</A> tests.<p>The K-STest process is as follows:<ol><li>Some k number of "classes" is selected, each typically covering a different but similar range of values. <p><li>Some much larger number of independent observations (n<FONT><SUB>1</SUB></FONT>, and n<FONT><SUB>2</SUB></FONT>, both larger than 40) are taken. Each is measured and its frequency is recorded in a class.<p><li>Based on the frequency table, the empirical cumulative distribution functions  F1<FONT><SUB>i</SUB></FONT> and F2<FONT><SUB>i</SUB></FONT> for two sample populations are constructed,  for i = 1, 2,..., k.<p><li> The K-S statistic is the largest absolute difference between F1<FONT><SUB>i</SUB></FONT> and F2<FONT><SUB>i</SUB></FONT>; i.e.,
<center><p>K-S statistic = D = Maximum | F1<FONT><SUB>i</SUB></FONT> - F2<FONT><SUB>i</SUB></FONT> |, &nbsp; &nbsp;&nbsp; &nbsp;   for all i = 1, 2, .., k.</center></ol><p>The critical values of K-S statistic can be found at <A  href="http://ubmail.ubalt.edu/~harsham/stat-data/opre330SPSSSAS.htm#rkstesttwo"  target=new>Computers and Computational Statistics with Applications</A><p><B>An Application:</B> The daily sales of the two subsidiaries of The PC & Accessories Company are shown in the following table, with n1 = 44, and n2 = 54: <p><table width="451" border="0" align="center" height="185"><tr bgcolor="#FFCCCC" valign="bottom"><td height="13" bordercolor="#333333" align="center" colspan="5"><b><font size="4" face="Arial, Helvetica, sans-serif">Daily Sales at Two Branches Over 6 Months</font></b></td></tr><tr bgcolor="#C6D5C6" valign="bottom"><td height="2" bordercolor="#333333" width="93"><div align="center"><i>Sales ($1000)</i></div></td><td height="2" bordercolor="#333333" width="15" bgcolor="#FFFFFF">&nbsp;</td><td height="2" bordercolor="#333333" width="108"><div align="center"><i>Frequency I</i></div></td><td height="2" bordercolor="#333333" width="103"><div align="center"><i>Frequency II</i></div></td></tr><tr bordercolor="#000000"><td height="14" bgcolor="#FFFFFF" width="110"><div align="center">0 - 2</div></td><td height="14" bgcolor="#FFFFFF" width="15"><div align="center"></div></td><td height="14" bgcolor="#FFFFFF" width="108"> 
<div align="center">11</div></td><td height="14" bgcolor="#FFFFFF" width="103"><div align="center">1</div>
</td></tr><tr bordercolor="#000000"><td height="14" bgcolor="#FFFFFF" width="110"> 
<div align="center">3 - 5</div><td height="14" bgcolor="#FFFFFF" width="15"><div align="center"></div>
</td><td height="14" bgcolor="#FFFFFF" width="108"><div align="center">7</div></td><td height="14" bgcolor="#FFFFFF" width="103"><div align="center">3</div></td></tr><tr bordercolor="#000000"> 
<td height="14" bgcolor="#FFFFFF" width="110"><div align="center">6 - 8</div><td height="14" bgcolor="#FFFFFF" width="15"><div align="center"></div></td><td height="14" bgcolor="#FFFFFF" width="108"><div align="center">8</div></td><td height="14" bgcolor="#FFFFFF" width="103"><div align="center">6</div></td></tr><tr bordercolor="#000000"><td height="14" bgcolor="#FFFFFF" width="110"> 
<div align="center">9 - 11</div></td><td height="14" bgcolor="#FFFFFF" width="15"><div align="center"></div>
</td><td height="14" bgcolor="#FFFFFF" width="108"><div align="center">3</div></td><td height="14" bgcolor="#FFFFFF" width="103"><div align="center">12</div></td></tr><tr bordercolor="#000000"> 
<td height="14" bgcolor="#FFFFFF" width="110"><div align="center">12 - 14</div><td height="14" bgcolor="#FFFFFF" width="15"><div align="center"></div></td><td height="14" bgcolor="#FFFFFF" width="108"><div align="center">5</div></td><td height="14" bgcolor="#FFFFFF" width="103"> 
<div align="center">12</div></td></tr><tr bordercolor="#000000"><td height="14" bgcolor="#FFFFFF" width="110"><div align="center">15 - 17</div><td height="14" bgcolor="#FFFFFF" width="15"> 
<div align="center"></div></td><td height="14" bgcolor="#FFFFFF" width="108"><div align="center">5</div>
</td><td height="14" bgcolor="#FFFFFF" width="103"><div align="center">14</div></td><tr  bordercolor="#000000"><td height="14" bgcolor="#FFFFFF" width="110"><div align="center">18 - 20</div>
<td height="14" bgcolor="#FFFFFF" width="15"><div align="center"></div></td><td height="14" bgcolor="#FFFFFF" width="108"><div align="center">5</div></td><td height="14" bgcolor="#FFFFFF" width="103"><div align="center">6</div></td></tr>
<tr  bordercolor="#000000"><td height="14" bgcolor="#FFFFFF" width="110"><div align="center">Sums</div>
<td height="14" bgcolor="#FFFFFF" width="15"><div align="center"></div></td><td height="14" bgcolor="#FFFFFF" width="108"><div align="center">44</div></td><td height="14" bgcolor="#FFFFFF" width="103"><div align="center">54</div></td></tr>
</table><div align="/center"><p>The manager of the first  branch is claiming that "since the daily sales are random phenomena, my overall performance is as good as the other manager's performance."   In other words:
<p>
H<FONT><SUB>0</SUB></FONT>: The daily sales at the two stores are almost the same.<br>
H<FONT><SUB>a</SUB></FONT>: The performance of the managers is significantly different.
<p>Following the above process for this test, the K-S statistic is 0.421 with the p-value of 0.0009, indicating a strong evidence against the null hypothesis.  There is enough evidence that the performance of the manager of the second branch is better.
<p>
<P><A name=rcst></A> <HR> <H4><FONT color=#dc143c>Introduction to Applications of the Chi-square Statistic </FONT></H4>The variance is not the only thing for which you use a Chi-square test for. 
<p>
The most widely used applications of Chi-square distribution are: 
<p>The Chi-square Test for Association which is a non-parametric test; therefore, it can be used for nominal data too. It is a test of statistical significance widely used bivariate tabular association analysis. Typically, the hypothesis is whether or not two populations are different in some characteristic or aspect of their behavior based on two random samples. This test procedure is also known as the Pearson Chi-square test. 
<p>The Chi-square Goodness-of-Fit Test is used to test if an observed distribution conforms to any particular
 distribution. Calculation of this goodness-of-fit test is by comparison of observed data with data expected based on a particular distribution. 
<P>One of the disadvantages of some of the Chi-square tests is that they do  not permit the calculation of confidence intervals; therefore, determination of the sample size is not readily available. 
<p><FONT color=#dc143c><b>Treatment of Cases with Many Categories:</b></font> Notice that, although in the following section most of the crosstables have only two categories, it is always possible to convert cases with many categories into similar crosstables. To do so, one must consider all possible pairs of categories and their numerical values while constructing the equivalent "two-categories" crosstable.  <P><A name=rCrossTab></A>  <HR>  <H4><FONT color=#dc143c>Test for Crosstable Relationship</FONT></H4>  <B>Crosstables:</B> Often crosstables are used to test relationships among two categorical  types of  data, or independence of two variables, such as cigarette smoking and drug use. If you were to survey 1000 people on whether or not they smoke and whether   or not they use drugs, you would  get one of four answers: (no, no) (no, yes) (yes, no)   (yes, yes)  <P>By compiling the number of people in each category, you can ultimately test  whether drug usage is independent of cigarette smoking by using the Chi-square  distribution (this is approximate, but works well). Again, the methodology  for this is in your textbook. The degrees of freedom is equal to (number of  rows-1)(number of columns -1). That is, these many numbers needed to fill  in the entire body of the crosstable, the rest will be determined by using the given row sums and the column sums values. <P>Do not forget the conditions for the validity of Chi-square test and related expected values greater than 5 in 80% or more of the cells. Otherwise, one could use an "exact" test, using either a permutation or resampling approach.  <P>Using Chi-square in a 2x2 table requires the Yates's correction. One first  subtracts 0.5 from the absolute differences between observed and expected  frequencies for each of the three genotypes before squaring, dividing by the expected frequency, and summing. The formula for the Chi-square value in a 2x2 table can be derived from the Normal Theory comparison of the two proportions in the table using the total incidence to produce the standard errors. The rationale of the correction is a better equivalence of the area under the normal curve and the probabilities obtained from the discrete frequencies. In other words,  the simplest correction is to move the cut-off point for the continuous distribution from the observed value of the discrete distribution to midway between that and the next value in the direction of the null hypothesis expectation. Therefore, the correction essentially only applied to one d.f. tests where the "square root" of the Chi-square looks like a "normal/t-test" and where a direction can be attached to the 0.5 addition. 
  <P>Chi-square distribution is used as an approximation of the binomial distribution. By applying a continuity correction, we get a better approximation of the binomial distribution for the purposes of calculating tail probabilities.  <P>Given the following 2x2 table, one may compute some relative risk measures:  <P> 
  <table border=1 bordercolor=#dddddd cellspacing=0 align="center" width="60">
  <tbody> 
  <tr> 
    <td width="50%"> 
      <div align="center"><b><font size="2" face="Arial, Helvetica, sans-serif">a 
        </font></b></div>
    </td>
    <td width="50%"> 
      <div align="center"><b><font size="2" face="Arial, Helvetica, sans-serif">b 
        </font></b></div>
    </td>
  </tr>
  <tr> 
    <td width="50%"> 
      <div align="center"><b><font size="2" face="Arial, Helvetica, sans-serif">c 
        </font></b></div>
    </td>
    <td width="50%"> 
      <div align="center"><b><font size="2" face="Arial, Helvetica, sans-serif">d 
        </font></b></div>
    </td>
  </tr>
  </tbody> 
</table>
  <P>The most usual measures are:  <P>Rate-difference: a/(a+c) - b/(b+d)<BR> Rate-ratio: (a/(a+c))/(b/(b+d))<BR>
    Odds-ratio: ad/bc <P>The rate difference and rate ratio are appropriate when you are contrasting  two groups whose sizes (a+c and b+d) are given. The odds ratio is for when the issue is association rather than difference. <P>The risk-ratio (RR) is the ratio of the proportion (a/(a+b)) to the proportion  (c/(c+d)):  <P>   <CENTER>   RR = (a / (a + b)) / (c / (c + d))</CENTER>  <P>RR is thus a measure of how much larger the proportion in the first row is compared to the second.  RR value of <FONT   face=symbol>&lt;</FONT> 1.00 indicating a 'negative' association [a/(a+b) <FONT face=symbol>&lt;</FONT>  c/(c+d)], 1.00 indicating no association [a/(a+b) = c/(c+d)], and <FONT face=symbol>&gt;</FONT>1.00  indicating a 'positive' association [a/(a+b) <FONT face=symbol>&gt;</FONT> c/(c+d)]. The further from 1.00 the RR is, the stronger the association.    <P><B>An Application:</B> Suppose a counselor of a school in a small town is interested whether the curriculum chosen by students is related to the occupation  of their parents. It is necessary to record the data as shown in the following  contingency table with two rows (r1, r2) and three columns (c1, c2, c3):  <P>  <TABLE align=center border=0 height=185 width=451> <TBODY> <TR> 
      <TD align=middle bgColor=#ffffcc borderColor=#333333 height=14   vAlign=bottom><B><I>Relationship between occupation of parents and<BR> curriculum chosen by high school students</I></B></TD>
    </TR>  <TR>  <TD bgColor=#ffffff borderColor=#333333 height=14>  <DIV align=center><I>Curriculum Chosen by Students</I></DIV> </TD> </TR> <TR> <TD borderColor=#000000 height=126> <TABLE align=center border=0 width=455>  <TBODY>  <TR align=middle vAlign=bottom>  <TD height=22 width=80><I>Parental<BR>  Occupation</I></TD> <TD height=22 width=91><I>College prep</I></TD><TD height=22 width=90><I>Vocational</I></TD> <TD height=22 width=93><I>General</I></TD> <TD height=22 width=67><I>Totals</I></TD></TR><TR align=middle vAlign=center> <TD align=left height=33 width=80><FONT 
  face="Times New Roman, Times, serif">Professional</FONT></TD>  <TD colSpan=3 height=45 rowSpan=2>        <TABLE border=1 borderColor=#333333 width=287>  <TBODY> <TR align=middle vAlign=center> <TD width=85> <P>12</P>  </TD><TD width=92>2</TD> <TD width=87>6</TD></TR> <TR align=middle vAlign=center> <TD width=85>6</TD><TD width=92>6</TD> <TD width=87>8</TD></TR></TBODY></TABLE></TD> <TD height=33 width=67> <P><FONT face=Symbol size=2><FONT size=3>20</FONT></FONT></P> </TD></TR><TR align=middle vAlign=center> <TD align=left height=2 width=80>Blue collar</TD> <TD height=2 width=67><FONT face=Symbol size=2><FONT 
  size=3>20</FONT></FONT></TD>  </TR> <TR align=middle borderColor=#0000ff vAlign=center> <TD align=left vAlign=bottom width=80><I>Totals</I></TD> <TD width=91><FONT face=Symbol size=3>18</FONT></TD>            <TD width=90><FONT face=Symbol size=3>8</FONT></TD> <TD width=93><FONT face=Symbol size=3>14</FONT></TD> <TD width=67>&nbsp;</TD> </TR> </TBODY> </TABLE> </TD></TR>
    </TBODY> </TABLE>  <DIV align=center></DIV>  <P>Under the hypothesis that there is no relation, the expected (E) frequency   would be:   <P>     <CENTER>      E<FONT size=+0><SUB>i, j</SUB></FONT> = (<FONT 
  face=symbol>S</FONT>r<FONT size=+0><SUB>i</SUB></FONT>)(<FONT   face=symbol>S</FONT>c<FONT size=+0><SUB>j</SUB></FONT>)/N     </CENTER>  <P>The Observed (O) and Expected (E) frequencies are recorded in the following   table:   <P>   <TABLE align=center border=0 height=211 width=458> <TBODY>  <TR>  <TD align=middle bgColor=#ffffcc borderColor=#333333 height=31  vAlign=center><B><I>Expected frequencies for the data.</I></B></TD>  </TR> <TR> <TD borderColor=#000000 height=168> <TABLE align=center border=0 width=455> <TBODY>  <TR align=middle vAlign=center> <TD height=22 width=80>&nbsp;</TD> <TD height=22 width=91><I>College prep</I></TD><TD height=22 width=90><I>Vocational</I></TD> <TD height=22 width=93><I>General</I></TD> <TD height=22 width=67><I>Totals</I></TD> </TR> <TR align=middle vAlign=center>  <TD align=left height=33 width=80><I><FONT  face="Times New Roman, Times, serif">Professional</FONT></I></TD> <TD colSpan=3 height=45 rowSpan=2> <TABLE border=1 borderColor=#333333 width=287> <TBODY> <TR align=middle vAlign=center> <TD width=85> <P><I>O</I> = 12<BR> <I>E</I> = 9</P> </TD><TD width=92><I>O</I> = 2<BR> <I>E</I> = 4</TD><TD width=87><I>O</I> = 6<BR>    <I>E</I> = 7</TD>  </TR>    <TR align=middle vAlign=center>   <TD width=85><I>O</I> = 6<BR>
 <I>E</I> = 9</TD> <TD width=92><I>O </I>= 6<BR><I>E</I> = 4</TD><TD width=87><I>O</I> = 8<BR>
 <I>E</I> = 7</TD> </TR></TBODY> </TABLE></TD> <TD height=33 width=67> <P><FONT face=Symbol size=2>å<I><FONT size=3>O </FONT></I><FONT size=3>= 20</FONT></FONT><BR><FONT face=Symbol size=2>å<I><FONT  face="Times New Roman, Times, serif" size=3>E</FONT><FONT size=3>  </FONT></I><FONT size=3>= 20</FONT></FONT></P> </TD></TR><TR align=middle vAlign=center> 
 <TD align=left height=2 width=80><I>Blue collar</I></TD><TD height=2 width=67><FONT face=Symbol size=2>å<I><FONT size=3>O</FONT></I><FONT size=3>= 20</FONT></FONT><BR><FONT face=Symbol 
 size=2>å<I><FONT face="Times New Roman, Times, serif" size=3>E </FONT></I><FONT size=3>=  20</FONT></FONT></TD></TR><TR align=middle borderColor=#0000ff vAlign=center> <TD align=left vAlign=bottom width=80><I>Totals</I></TD><TD width=91><FONT face=Symbol size=2>å<I><FONT size=3>O </FONT></I><FONT size=3>=</FONT><I><FONT size=3> </FONT></I><FONT 
  size=3>18</FONT></FONT><BR><FONT face=Symbol size=2>å<I><FONT   face="Times New Roman, Times, serif" size=3>E </FONT></I><FONT  size=3>= 18 </FONT></FONT></TD><TD width=90><FONT face=Symbol size=2>å<I><FONT size=3>O </FONT></I><FONT size=3>=  8</FONT></FONT><BR> <FONT face=Symbol 
              size=2>å<I><FONT face="Times New Roman, Times, serif"   size=3>E</FONT></I><FONT size=3> = 8 </FONT></FONT></TD> <TD width=93><FONT face=Symbol size=2>å<I><FONT  size=3>O</FONT></I><FONT size=3> = 14</FONT></FONT><BR><FONT  face=Symbol size=2>å<I><FONT face="Times New Roman, Times, serif"   size=3>E</FONT></I><FONT size=3> = 14 </FONT></FONT></TD><TD width=67>&nbsp;</TD> </TR>
</TBODY> </TABLE></TD></TR></TBODY>  </TABLE>  <DIV align=center></DIV>  <P>The quantity   <P>     <CENTER> <FONT face=symbol>c </FONT><FONT size=+0><SUP>2</SUP></FONT> = <FONT 
  face=symbol>S</FONT> [(O - E )<FONT size=+0><SUP>2</SUP></FONT> / E] </CENTER> <P>is a measure of the degree of deviation between the Observed  and Expected frequencies. If there is no relationship between the row  variable and the column variable  this measure will be very close to zero. Under the <I>hypothesis that there is a relationship</I> between the rows and the columns, this quantity has a Chi-square distribution with parameter equal to number of rows minus 1, multiplied by number of columns minus 1. <P>For this numerical example we have:  <P> <CENTER> <FONT face=symbol>c </FONT><FONT size=+0><SUP>2</SUP></FONT> = <FONT 
  face=symbol>S</FONT> [(O - E )<FONT size=+0><SUP>2</SUP></FONT> / E] = 30/7 = 4.3 </CENTER>
  <P>with d.f. = (2-1)(3-1) = 2, that has the p-value of 0.14, suggesting little or no real evidences against the null hypothesis. <P>The main question is how large is this measure. The maximum value of this measure is: 
  <P>  <CENTER> <FONT face=symbol>c </FONT><FONT size=+0><SUP>2</SUP></FONT><FONT 
  size=+0><SUB>max</SUB></FONT> = N(A-1),  </CENTER><P>where A is the number of rows or columns, whichever is smaller. For our numerical example it is, 40(2-1) = 40. <P>The coefficient of determination which has a range of [0, 1], provides relative strength of relationship, computed as <P>  <CENTER> <FONT face=symbol>c </FONT><FONT size=+0><SUP>2</SUP></FONT>/<FONT  face=symbol>c </FONT><FONT size=+0><SUP>2</SUP></FONT><FONT  size=+0><SUB>max</SUB></FONT> = 4.3/40 = 0.11  </CENTER>
  <P>Therefore we conclude that the degree of association is only 11% which is fairly weak.  <P> <P>Alternatively, you could also look at the contingency coefficient <FONT  face=symbol>f</FONT> statistic, which is:   <P>    <CENTER> <FONT face=symbol>f</FONT> = [<FONT face=symbol> c</FONT><FONT 
  size=+0><SUP>2</SUP></FONT>/(N + <FONT face=symbol>c</FONT><FONT  size=+0><SUP>2</SUP></FONT>)]<FONT size=+0><SUP>½</SUP></FONT> = 0.31 </CENTER>
  <P>This statistic ranges between 0 and 1 and can be interpreted like the correlation  coefficient. This measure also indicates that the curriculum chosen by students is related to the occupation of their parents. 
  <P>You might like to use <A  href="otherapplets/Catego.htm" 
  target=new>Chi-square Test for Crosstable Relationship</A> in performing this  test, and he <A 
  href="otherapplets/pvalues.htm"  target=new>P-values for the Popular Distributions</A> Applet to findout the p-values of Chi-square statistic. <P><B>Further Readings:</B><BR> <FONT face="Bookman Old Style" size=-2>Agresti A., <I>Categorical Data Analysis</I>, 
    Wiley, 2002.<BR> Fleiss J., <I>Statistical Methods for Rates and Proportions</I>, Wiley, 1981.  </FONT> 
  <P><A name=rHomCrossTab></A>  <HR>  <H4><FONT color=#dc143c>Identical Populations Test for Crosstable Data</FONT></H4>Test of homogeneity is much like the Test for Crosstable Relationship in 
    that both deal with the cross-classification of nominal data; that is, r <font face=symbol>&#180;</font> c  tables. The method of computing Chi-square statistic is the same for both  tests, with the same d.f.  <P>The two tests differ, however, in the following respect. The Test for Crosstable Relationship is made on data drawn from a single population (with <B>fixed</B>  total) where one is concerned with whether one set of attributes is independent of another set. The test for homogeneity, on the other hand, is designed  to test the null hypothesis that  two or more <FONT   color=#dc143c>random samples are drawn from the same population or from different       populations</FONT>, according to some criterion of classification applied  to the samples.  <P>The homogeneity test is concerned with the question: Are the samples drawn  form populations that are homogeneous (i.e., the same) with respect to some   criterion of classification? <P>In the crosstable for this test, either the row or the column categories   may represent the populations from which the samples are drawn.  <P><B>An Application:</B> Suppose a board of directors of a labor union wishes  to survey the opinion of its members regarding a change in its constitution. The following table shows the result of the survey sent to three union locals: <P>     <TABLE align=center border=0 height=185 width=251> <TBODY>  <TR>  <TD align=middle bgColor=#ffffcc borderColor=#333333 height=14   vAlign=bottom><B><I>Reactions of A Sample of Three Locals Group Members</I></B></TD>    </TR>   <TR>     <TD bgColor=#ffffff borderColor=#333333 height=14> <DIV align=center><I>Union Local</I></DIV> </TD>  </TR> <TR> <TD borderColor=#000000 height=126> <TABLE align=center border=0 width=455> <TBODY> <TR align=middle vAlign=bottom> <TD align=left height=22 width=80><I>Reaction</I></TD><TD height=22 width=91><I>A</I></TD><TD height=22 width=90><I>B</I></TD><TD height=22 width=93><I>C</I></TD> </TR><TR align=middle vAlign=center> 
<TD align=left height=44 noWrap vAlign=bottom width=80><FONT  face="Arial, Helvetica, sans-serif" size=2>In Favor</FONT></TD> <TD colSpan=3 rowSpan=3><TABLE border=1 borderColor=#333333 height=83 width=345>
 <TBODY>  <TR align=middle vAlign=center>  <TD width=86>18</TD><TD width=91>22</TD><TD width=88>10</TD> </TR><TR align=middle vAlign=center><P><BR><P></P><TD width=86>7</TD> <TD width=91>14</TD><TD width=88>9</TD> <p></P> </TR><TR align=middle vAlign=center>  <TD width=86>5</TD>
 <TD width=91>4</TD><TD width=88>11</TD></TR></TBODY></TABLE><TR align=middle vAlign=center> 
 <TD align=left height=24 noWrap vAlign=bottom width=80><FONT   face="Arial, Helvetica, sans-serif" size=2>Against</FONT></TD><TR align=middle vAlign=center>  <TD align=left noWrap vAlign=bottom width=80><FONT face="Arial, Helvetica, sans-serif" size=2>No Response</FONT></TD></TR> </TBODY>          </TABLE></TD><TD width=67></TD> <TR align=middle borderColor=#0000ff vAlign=center> <TD width=67>&nbsp;</TD></TR></TBODY></TABLE><DIV align=center></DIV><P> <P>The problem is not to determine whether or not the union members are in favor of the change. The question is to test if there is a significant difference in the proportions of opinion of the three populations' members concerning  the proposed change. <P>The Chi-square statistic is 9.58 with d.f. = (3-1)(3-1) = 4. The p-value  is equal to 0.048, indicating that there is moderate evidence against the null hypothesis that the three union locals are the same. 
    <P>You might like to use <A  href="otherapplets/HomoCrosstable.htm"   target=new>Populations Homogeneity Test</A> to perfor this test.  <P>   <P><B>Further Readings:</B><BR> <FONT face="Bookman Old Style" size=-2>Agresti A., <I>Categorical Data Analysis</I>,  Wiley, 2002.<BR>Clark Ch., and L. Schkade, <I>Statistical Analysis for Administrative Decisions</I>,  South-Western Pub., 1979. </FONT> <p>     <P><A name=rEualMedian></A>     <HR>    <H4><FONT color=#dc143c>Test for Equality of Several Population Medians</FONT></H4> Generally, the median provides a better measure of location than the mean when there are some extremely large or small observations; i.e., when the data are <A   href="#rskewKur">skewed</A>  to the right or to the left. For this reason, median income is used as the measure of location for the U.S. household income. <P>Suppose we are interested in testing the equality of the medians of k number  of populations with respect to the same continuous <A   href="#rrandomva">random variable</A>. <P>The first step in calculating the test statistic is to compute the common  median of the k samples combined. Then, determine for each group the number  of observations  falling above and below the common median. The resulting frequencies are arranged in a 2 by k crosstable. If the k samples are, in  fact, from populations with the same median, one expects about one half the score in each sample to be above the combined median and about one half  to be below. In the case that some observations are equal to the combined  median, one may drop those few observations, in constructing a 2 by k crosstable.  Under this condition, now the Chi-square statistic may be computed and compared  with the p-value of Chi-square distribution with d.f. = k-1. <P><B>An illustrative application:</B> Do public and private primary school  teachers differ with respect to their salary?  The data from a random sample are given in the following table (in thousands of dollars per year). <p><br>
   <CENTER> <TABLE border=3><TBODY> <TR>  <TH align=middle>Public <TH align=middle>Private<TH align=middle>Public <TH align=middle>Private <TR> <TD align=middle>35 <TD align=middle>29   <TD align=middle>25  <TD align=middle>50 <TR> <TD align=middle>26 <TD align=middle>50 
<TD align=middle>27  <TD align=middle>37  <TR> <TD align=middle>27 <TD align=middle>43  <TD align=middle>45 <TD align=middle>34  <TR>  <TD align=middle>21   <TD align=middle>22  <TD align=middle>46  <TD align=middle>31   <TR>   <TD align=middle>27<TD align=middle>42 <TD align=middle>33<TD align=middle>  <TR><TD align=middle>38  <TD align=middle>47 <TD align=middle>26  <TD align=middle>  <TR>  <TD align=middle>23 <TD align=middle>42 <TD align=middle>46 <TD align=middle> 
 <TR>  <TD align=middle>25  <TD align=middle>32 <TD align=middle>41   <TD align=middle> <TR></TR></TBODY> </TABLE>    </CENTER>  <P>The test of hypothesis is: <P><B>H<FONT size=+0><SUB>0</SUB></FONT>:</B> The public and private school  teachers' salaries are almost the same.  <P>The median of all data (i.e., combined) is 33.5. Now determine in each group the number of observations  falling above and below the common median  of 33.5. The resulting frequencies are shown in the following table: <P>  <CENTER>     <TABLE border=1 borderColor=#000000 cellSpacing=0>   <TBODY> <TR> <TD align=middle colSpan=11><B><FONT color=#dc143c>Crosstable for the  public and private school teachers'</FONT></B> </TD> <TR>    <TD></TD>  <TD>Public </TD>  <TD>Private </TD> <TD>Total</TD>        </TR>   <TR>    <TD>Above median</TD>   <TD>6</TD> <TD>8</TD>    <TD>14</TD>  </TR> <TR>     <TD>Below median</TD>  <TD>10</TD>  <TD>4</TD>   <TD>14</TD>  </TR>   <TR>   <TD>Total</TD>  <TD>16</TD>          <TD>12</TD>    <TD>28</TD></TR>   </TBODY> </TABLE>    </CENTER>    <P>The Chi-square statistic based on this table is 2.33. The p-value for   the computed test statistic with d.f. = (2-1)(2-1) = 1 is 0.127, therefore,   we are unable to reject the null hypothesis.     <P>You might like to use <A   href="otherapplets/MediansTest.htm"   target=new>Testing Medians</A> to perform this test.     <P><A name=rgoodnessofforDrv></A>     <HR>    <H4><FONT color=#dc143c>Goodness-of-Fit Test for Probability Mass Functions</FONT></H4>    There are other tests that might use the Chi-square, such as goodness-of-fit     test for discrete <A   href="#rrandomva">random variables</A>. Therefore, Chi-square is a statistical test that measures "goodness-of-fit".     In other words, it measures how much the observed or actual frequencies differ  from the expected or predicted frequencies. Using a Chi-square table will enable you to discover how significant the difference is. A null hypothesis in the context of the Chi-square test is the model that you use to calculate  your expected or predicted values. If the value you get from calculating the  Chi-square statistic is sufficiently high (as compared to the values in the 
    Chi-square table),  it tells you that your null hypothesis is probably wrong.  <P>Let Y<SUB>1</SUB>, Y<SUB> 2</SUB>, . . ., Y<SUB> n </SUB>be a set of independent  and identically distributed <B>discrete</B> random variables. Assume that the probability  distribution of the Y<SUB> i</SUB>'s has the probability mass function f<SUB> o </SUB>(y). We can divide the set of all possible values of Y<SUB>i</SUB>,  i = {1, 2, ..., n}, into m non-overlapping intervals D<SUB>1</SUB>, D<SUB>2</SUB>,  ...., D<SUB>m</SUB>. Define the probability values <EM>p</EM><SUB>1</SUB>,    <EM>p</EM><SUB>2</SUB> , ..., <EM>p</EM><SUB>m</SUB> as; 
    <P><EM>p</EM><SUB>1</SUB> = P(Y<SUB>i</SUB> <FONT face=symbol>Î</FONT> D<SUB>1</SUB>)<BR>
      <EM>p</EM><SUB>2</SUB> = P(Y<SUB>i</SUB> <FONT face=symbol>Î </FONT>D<SUB>2</SUB>) 
    <P>:<BR> <P><EM>p</EM><SUB>m </SUB>= P(Y<SUB>i </SUB><FONT face=symbol>Î</FONT> D<SUB>m</SUB>)     <P>Where the symbol <FONT face=symbol>Î</FONT> means, "an element of". 
    <P>Since the union of the mutually exclusive intervals D<SUB>1</SUB>, D<SUB>2</SUB>,       ...., D<SUB>m</SUB> is the set of all possible values for the Y<SUB>i</SUB>'s,   (<EM>p</EM><SUB>1</SUB> +<EM> p</EM><SUB>2</SUB> + .... +<EM> p</EM><SUB>m</SUB>)   = 1. Define the set of discrete <A 
  href="#rrandomva">random  variables</A> X<SUB>1</SUB>, X<SUB>2</SUB>, ...., X<SUB>m</SUB>, where 
    <P>X<SUB>1</SUB>= number of Y<SUB>i</SUB>'s whose value<FONT face=symbol>Î</FONT>D<SUB>1</SUB><BR>     X<SUB>2</SUB>= number of Y<SUB>i</SUB>'s whose value <FONT face=symbol>Î</FONT> D<SUB>2</SUB> <P>:<BR>: <P>X<SUB>m</SUB>= number of Y<SUB>i</SUB>'s whose value <FONT   face=symbol>Î</FONT> D<SUB>m</SUB>     <P>and (X<SUB>1</SUB>+ X<SUB>2</SUB>+ .... + X<SUB>m</SUB>) = n. Then the  set of discrete random variables X<SUB>1</SUB>, X<SUB>2</SUB>, ...., X<SUB>m</SUB>will  have a multinomial probability distribution with parameters <EM>n </EM>and  the set of probabilities {<EM>p</EM><SUB>1</SUB>, <EM>p</EM><SUB>2</SUB>,  ..., <EM>p</EM><SUB>m</SUB>}. If the intervals D<SUB>1</SUB>, D<SUB>2</SUB>,   ...., D<SUB>m</SUB> are chosen such that <EM>np</EM><SUB>i</SUB> <FONT face=symbol>³</FONT>  5 for i = 1, 2, ..., m, then; 
    <P>       <CENTER>        C = <FONT face=symbol>S</FONT> (X<FONT size=+0><SUB>i</SUB></FONT> - np<FONT size=+0><SUB>i</SUB></FONT>)         <FONT size=+0><SUP>2</SUP></FONT>/ np<FONT size=+0><SUB>i</SUB></FONT>.       </CENTER>    <P>       <CENTER>      </CENTER>      The sum is over i = 1, 2,..., m. The results is distributed as <FONT face=symbol>c</FONT><FONT size=+0><SUP>2</SUP></FONT> 
      <FONT   size=+0><SUB>m-1</SUB></FONT>.     <P>     <P>For the goodness-of-fit sample test, we formulate the null and alternative       hypothesis as     <P>H<FONT><SUB>0</SUB></FONT> : f<SUB>Y</SUB>(y) = f<SUB>o</SUB>(y) <BR>
      H<FONT><SUB>a</SUB></FONT> : f<SUB>Y</SUB>(y) <FONT   face=symbol>¹</FONT> f<SUB>o</SUB>(y)     <P>At the <FONT face=symbol>a</FONT> level of significance, H<FONT><SUB>0</SUB></FONT> will be rejected       in favor of H<FONT><SUB>a</SUB></FONT> if 
    <P>       <CENTER>        C = <FONT face=symbol>S</FONT> (X<FONT size=+0><SUB>i</SUB></FONT> - np<FONT size=+0><SUB>i</SUB></FONT>)         <FONT size=+0><SUP>2</SUP></FONT>/ np<FONT size=+0><SUB>i</SUB></FONT>     </CENTER>   <P>  <CENTER> </CENTER> is greater than <FONT face=symbol>c</FONT><FONT   size=+0><SUP>2</SUP></FONT> <FONT size=+0><SUB>m</SUB></FONT> 
    <P>However, it is possible that in a goodness-of-fit test, one or more of  the parameters of f<SUB>o</SUB>(y) are unknown. Then the probability values  <EM>p</EM><SUB>1</SUB>, <EM>p</EM><SUB>2</SUB>, ..., <EM>p</EM><SUB>m</SUB>  will have to be estimated by assuming that H<SUB>0</SUB> is true and calculating  their estimated values from the sample data. That is, another set of probability  values <EM>p</EM>'<SUB>1</SUB>, <EM>p</EM>'<SUB>2</SUB>, ..., <EM>p</EM>'<SUB>m</SUB>will need to be computed so that the values (<EM>np</EM>'<SUB>1</SUB>,  <EM>np</EM>'<SUB>2</SUB>, ..., <EM>np</EM>'<SUB>m</SUB>) are the estimated  expected values of the multinomial random variable (X<SUB>1</SUB>, X<SUB>2</SUB>,  ...., X<SUB>m</SUB>). In this case, the random variable C will still have       a Chi-square distribution, but its degrees of freedom will be reduced. In   particular, if the probability function f<SUB>o</SUB>(y) has <EM>r</EM>  unknown parameters,  <P>   <CENTER>     C = <FONT face=symbol>S</FONT> (X<FONT size=+0><SUB>i</SUB></FONT> - np<FONT size=+0><SUB>i</SUB></FONT>) 
        <FONT size=+0><SUP>2</SUP></FONT>/ np<FONT size=+0><SUB>i</SUB></FONT> 
      </CENTER> <P>is distributed as <FONT face=symbol>c</FONT><FONT   size=+0><SUP>2</SUP></FONT> <FONT size=+0><SUB>m-1-r</SUB></FONT>. <P>For this goodness-of-fit test, we formulate the null and alternative hypothesis   as  <P>H<SUB>0</SUB>: f<SUB>Y</SUB>(y) = f<SUB>o</SUB>(y) <BR>     H<SUB>a</SUB>: f<SUB>Y</SUB>(y) <FONT face=symbol>¹ </FONT>f<SUB>o</SUB>(y)  <P>At the <FONT face=symbol>a</FONT> level of significance, H<SUB>0</SUB>  will be rejected in favor of H<SUB>a</SUB> if C is greater than <FONT   face=symbol>c</FONT><FONT size=+0><SUP>2</SUP></FONT> <FONT 
  size=+0><SUB>m-1-r</SUB></FONT>.   <P><B>An Application:</B> A die is thrown 300 times and the following frequencies  are observed. Test the hypothesis that the die is fair at level 0.05.   Under  the null hypothesis that the die is fair, the expected frequencies are all  equal to 300/6 = 50. Both the Observed (O) and Expected (E) frequencies  are recorded in the following table together with the random variable Y that represents the number on each sides of the die:  <P> <CENTER> <TABLE border=1 borderColor=#000000 cellSpacing=0>   <TBODY> <TR> <TD align=middle colSpan=11><B><FONT color=#dc143c>Goodness-of-fit Test 
  For Discrete Variables</FONT></B> </TD><TR> <TD>Y</TD><TD>1</TD><TD>2</TD><TD>3</TD><TD>4</TD>
  <TD>5</TD>  <TD>6</TD>  </TR><TR><TD>O</TD><TD>57</TD> <TD>43</TD> <TD>59</TD><TD>55</TD>
   <TD>63</TD>  <TD>23</TD>  </TR><TR> <TD>E</TD>  <TD>50</TD> <TD>50</TD> <TD>50</TD>  <TD>50</TD>
  <TD>50</TD> <TD>50</TD></TR></TBODY></TABLE></CENTER><P> <P>The quantity <P> <CENTER>
        <FONT face=symbol>c </FONT><FONT size=+0><SUP>2</SUP></FONT> = <FONT   face=symbol>S</FONT> [(O - E )<FONT size=+0><SUP>2</SUP></FONT> / E] = 22.04  </CENTER>  <P>is a measure of the goodness-of-fit. If there is a reasonably good fit  to the hypothetical distribution, this measure will be very close to zero. Since <FONT face=symbol>c </FONT><FONT size=+0><SUP>2</SUP></FONT> <FONT   size=+0><SUB>n-1, 0.95</SUB></FONT> = 11.07, we reject the null hypothesis that the die is a fair one.  <P> <P>You might like to  use this <A   href="otherapplets/goodness.htm" 
  target=new>JavaScript</A> to perform this test.     <P>For <I>statistical</I> equality of two <A 
  href="#rrandomva">random  variables</A> characterizing two populations, you might like to use the <A 
  href="otherapplets/KS.htm"   target=new>Kolmogorov-Smirnov Test</A> if you have two independent sets of random  observations, one from each population.  <P><A name=rCompMultiCo></A>    <HR>   <H4><FONT color=#dc143c>Compatibility of Multi-Counts Test</FONT></H4>
    <P>In some applications, such as quality control, it is necessary to check if the process is under control. This can be done by testing if there are significant differences between number of "counts", taken over k equal-periods of times. The counts are supposed to have been obtained under comparable conditions. <p>The null hypothesis is:  <P><B>H<FONT size=+0><SUB>0</SUB></FONT></B>: There is no significant difference   between number of "counts" taken over k equal-periods of times. <P>Under the null hypothesis, the statistic:  <P> <CENTER> <FONTface=symbol>S</FONT> (N<FONT size=+0><SUB>i</SUB></FONT> - N)<FONTsize=+0><SUP>2</SUP></FONT>/N  </CENTER> <P>has a Chi-square distribution with d.f. = k-1. Where i is the count's number,  N<FONT size=+0><SUB>i</SUB></FONT> is its counts, and N = <FONT 
  face=symbol>S</FONT>N<FONT size=+0><SUB>i</SUB></FONT>/k. <P>One may extend this useful test to where the duration of obtaining the  i<FONT size=+0><SUP>th</SUP></FONT> count is t<FONT 
  size=+0><SUB>i</SUB></FONT>. Then the above test statistic becomes:  <P>  <CENTER> <FONT face=symbol>S</FONT> [(N<FONT size=+0><SUB>i</SUB></FONT> - t<FONT size=+0><SUB>i</SUB></FONT>N)<FONT size=+0><SUP>2</SUP></FONT>/  t<FONT 
  size=+0><SUB>i</SUB></FONT>N]    </CENTER> <P>and has a Chi-square distribution with d.f. = k-1, where i is the count's number,  N<FONT size=+0><SUB>i</SUB></FONT> is its counts, and N = <FONT 
  face=symbol>S</FONT>N<FONT size=+0><SUB>i</SUB></FONT>/<FONT  face=symbol>S</FONT>t<FONT size=+0><SUB>i</SUB></FONT>.  <P>You might like to use the <A  href="otherapplets/CountTest.htm"   target=new>Compatibility of Multi-Counts </A> JavaScript to check your computations,  and to perform some numerical experimentation for a deeper understanding   of the concepts.  <P><A name=rchiconditions></A> 
    <HR>   <H4><FONT color=#dc143c>Necessary Conditions for the Above Chi-square Based Testing</FONT></H4>
    Like any statistical test procedures, the Chi-square based testing must meet certain necessary conditions to apply; otherwise, any obtained conclusion might be wrong or misleading. This is  true in particular for using the Chi-square-based  test for  cross-tabulated data. <P>Necessary conditions for the Chi-square based tests for crosstable data are:   <OL> <LI>Expected values greater than 5 in 80% or more of the cells. <LI>Moreover, if number of cells is fewer than 5, then all expected values must be greater than 5.  </OL> <P><B>An  Example:</B> Suppose the monthly number of accidents reported in a factory in three eight-hour shifts is 1, 7, and 7, respectively.  Are the working conditions and the exposure to risk similar for all shifts? Clearly, the answer must be, No they are not. However, applying the goodness-of-fit,  at 0.05, under the null hypothesis that there are no differences in the number  of accidents in three shifts, one expects 5, 5, and 5 accidents in each  shift. The Chi-square test statistic is:  <P>  <CENTER> <FONT face=symbol>c </FONT><FONT size=+0><SUP>2</SUP></FONT> = <FONT   face=symbol>S</FONT> [(O - E )<FONT size=+0><SUP>2</SUP></FONT> / E] = 4.8  </CENTER>  <P>However, since <FONT face=symbol>c </FONT><FONT size=+0><SUP>2</SUP></FONT>  <FONT size=+0><SUB>n-1, 0.95</SUB></FONT> = 5.99, there is no reason to  reject that there is no difference, which is a very strange conclusion.  What is wrong with this application?   <P>You might like to use this <A   href="otherapplets/goodness.htm"   target=new>JavaScript</A> to verify your computation.     <P><A name=rchivariance></A>     <HR>    <H4><FONT color=#dc143c>Testing the Variance: Is the Quality that Good?</FONT></H4>    Suppose a population has a normal distribution. The manager is to test a specific 
    claim made about the quality of the population by way  of testing its variance     <FONT face=symbol>s</FONT><FONT   size=+0><SUP>2</SUP></FONT>. Among three possible scenarios, the interesting   case is in testing the following null hypothesis based on a set of n random sample observations: 
    <P><B>H<FONT size=+0><SUB>0</SUB></FONT></B>: Variation is about the claimed  value.<BR>
      <B>H<FONT size=+0><SUB>a</SUB></FONT></B>: The variation is more than what 
      is claimed, indicating the quality is much lower than expected.  <P>Upon computing the estimated variance S<FONT size=+0><SUP>2</SUP></FONT>  based on n observations, then the statistic: 
    <P>  <CENTER> <FONT face=symbol>c</FONT><FONT size=+0><SUP>½</SUP></FONT> = [(n-1) . 
        <FONT face=symbol>s</FONT><FONT size=+0><SUP>2</SUP></FONT>] / <FONT 
  face=symbol>s</FONT><FONT size=+0><SUP>2</SUP></FONT>  </CENTER> <P>has a Chi-square distribution with degree of freedom <FONT face=symbol>n</FONT> = n - 1. This statistic is then used for testing the above  null hypothesis.  <P>You might like to use <A   href="otherapplets/variationtest.htm"   target=new>Testing the Variance</A> JavaScript to check your computations.     <P> 
    <P><A name=rMultivariances></A>     <HR>    <H4><FONT color=#dc143c>Testing the Equality of Multi-Variances</FONT></H4>    The equality of variances across populations is called homogeneity of variances  or 
    homoscedasticity.  Some statistical tests, such as testing equality of the means by the t-test and ANOVA, assume that the data come from populations that have the same variance, even if the test rejects the null hypothesis of equality of population means.  If this condition of homogeneity of variance is not met, the statistical test results may not     be valid. Heteroscedasticity refers to lack of homogeneity of variances.     <P><FONT color=#dc143c><B>Bartlett's Test</B></FONT> is used to test if k  samples have equal variances. It compares the <A href="#rspecialmean">Geometric Mean</A> of the group  variances to the arithmetic mean; therefore, it is a Chi-square statistic with (k-1) degrees of freedom, where k is the number of categories in the      independent variable. The test is sensitive to departures from normality.  The sample sizes do not have to be equal but each must be at least 6. Just  like the two population t-test, ANOVA can go wrong when the equality of variances  condition is not met.  <P>The Bartlett test statistic is designed to test for equality of variances  across groups against the alternative that variances are unequal for at  least two groups. Formally,    <P><B>H<FONT size=+0><SUB>0</SUB></FONT></B>: All variances are almost equal.   <P>The test statistic: 
    <P>     <CENTER>  B = {<FONT face=symbol>S</FONT> [(n<FONT size=+0><SUB>i</SUB></FONT> -1)LnS<FONT size=+0><SUP>2</SUP></FONT>]  <FONT face=symbol>S</FONT> [(n<FONT 
  size=+0><SUB>i</SUB></FONT> -1)LnS<FONT size=+0><SUB>i</SUB></FONT><FONT 
  size=+0><SUP>2</SUP></FONT>]}/ C  </CENTER>  <P>In the above, S<FONT size=+0><SUB>i</SUB></FONT><FONT   size=+0><SUP>2</SUP></FONT> is the variance of the ith group, n<FONT   size=+0><SUB>i</SUB></FONT> is the sample size of the i<FONT   size=+0><SUP>th</SUP></FONT> group, k is the number of groups, and S<FONT   size=+0><SUP>2</SUP></FONT> is the pooled variance. The pooled variance is a  weighted average of the group variances and is defined as: <P>  <CENTER> S<FONT size=+0><SUP>2</SUP></FONT> = {<FONT face=symbol>S</FONT> [(n<FONT size=+0><SUB>i</SUB></FONT> 
        -1)S<FONT  size=+0><SUB>i</SUB></FONT><FONT size=+0><SUP>2</SUP></FONT>]} / <FONT 
  face=symbol>S</FONT> [(n<FONT size=+0><SUB>i</SUB></FONT> -1)], over all i = 1,  2,..,k      </CENTER>
    <P>and  <P>  <CENTER> C = 1 + {<FONT face=symbol>S</FONT> [1/(n<FONT 
  size=+0><SUB>i</SUB></FONT> -1)] - 1/ <FONT face=symbol>S</FONT> [1/(n<FONT 
  size=+0><SUB>i</SUB></FONT> -1)] }/[3(k+1)].  </CENTER>  <P> <P>You might like to use the <A   href="otherapplets/BartletTest.htm"   target=new>Equality of Multi-Variances </A>JavaScript tor check your computations,  and to perform some numerical experimentation for a deeper understanding of the concepts. <p>  <P><FONT color=#dc143c><B>Rule of 2:</B></FONT> For 3 or more populations,   there is a practical rule known as the "Rule of 2". According to this rule,  one divides the highest variance of a sample by the lowest variance of the  other sample. Given that the sample sizes are almost the same, and the value  of this division is less than 2, then, the variations of the populations  are almost the same.  <P><B>Example:</B> Consider the following three random samples from three  populations, P1, P2, P3: 
<p>
<CENTER> <TABLE border=1 borderColor=#dddddd cellSpacing=0>
      <TBODY> <TR> <TD></TD> <TH>Sample P1</TH><TH>Sample P2</TH><TH>Sample P3</TH></TR><TR> <TD></TD><TD align=middle>25</TD><TD align=middle>17</TD><TD align=middle> 8</TD></TR><TR>  <TD></TD> <TD align=middle>25</TD> <TD align=middle>21</TD><TD align=middle>10</TD></TR><TR> <TD></TD><TD align=middle>20</TD><TD align=middle>17</TD><TD align=middle>14</TD></TR><TR> <TD></TD><TD align=middle>18</TD><TD align=middle>25</TD><TD align=middle>16</TD></TR><TR> <TD></TD><TD align=middle>13</TD><TD align=middle>19</TD><TD align=middle>12</TD><TR><TD></TD><TD align=middle>6</TD><TD align=middle>21</TD><TD align=middle>14</TD></TR><TR>  <TD></TD> <TD align=middle>5</TD> <TD align=middle>15</TD><TD align=middle>6</TD></TR><TR> <TD></TD><TD align=middle>22</TD><TD align=middle>16</TD><TD align=middle>16</TD></TR><TR> <TD></TD><TD align=middle>25</TD><TD align=middle>24</TD><TD align=middle>13</TD></TR><TR> <TD></TD><TD align=middle>10</TD><TD align=middle>23</TD><TD align=middle>6</TD></TR><TR> <TD><B>N</B></TD><TD align=middle>10</TD><TD align=middle>10</TD><TD align=middle>10</TD></TR><TR><TD><B>Mean</B></TD><TD align=middle>16.90</TD><TD align=middle>19.80</TD><TD align=middle>11.50</TD> </TR><TR> <TD><B>Std.Dev.</B></TD><TD align=middle>7.87</TD><TD align=middle>3.52</TD><TD align=middle>3.81</TD></TR><TR> <TD><B>SE Mean</B></TD><TD align=middle>2.49</TD><TD align=middle>1.11</TD><TD align=middle>1.20</TD></TR></TBODY></TABLE>  </CENTER> <P> <P><p>
  <CENTER> <TABLE border=1 borderColor=#ffffff cellSpacing=0> <TBODY><TR><TD align=middle colSpan=5><FONT color=#dc143c><FONT size=3><B>The ANOVA Table</B></FONT></FONT></TD>
</TR><TR> <TD align=middle>Sources of Variation</TD><TD align=middle>Sum of Squares</TD> <TD align=middle>Degrees of Freedom</TD><TD align=middle>Mean Squares</TD> <TD align=middle>F-Statistic</TD></TR><TR> <TD align=middle>Between Samples</TD><TD align=middle>79.40</TD>
<TD align=middle>2</TD><TD align=middle>39.70</TD><TD align=middle>4.38</TD></TR><TR> <TD align=middle>Within Samples</TD><TD align=middle>244.90</TD><TD align=middle>27</TD><TD align=middle>9.07</TD> <TD>&nbsp;</TD> </TR><TR> <TD align=middle>Total</TD><TD align=middle>324.30</TD><TD align=middle>29</TD><TD>&nbsp;</TD><TD>&nbsp;</TD></TR></TBODY></TABLE></CENTER>  
<p>
  <P>With an F = 4.38 and a p-value of .023, we reject the null at <FONT   face=symbol>a</FONT> = 0.05. This is not good news, since ANOVA, like the two-sample  t-test, can go wrong when the equality of variances condition is not met.    <p>
    <P><B>Further Readings:</B><BR> <FONT face="Bookman Old Style" size=-2>Hand D., and C. Taylor, <I>Multivariate  Analysis of Variance and Repeated Measures</I>, Chapman and Hall, 1987.<BR>Miller R. Jr, <I>Beyond ANOVA: Basics of Applied Statistics</I>, Wiley,  1986. </FONT>   
 <P><A name=rmulticorr></A> 
    <HR>    <H4><FONT color=#dc143c>Correlation Coefficients Testing</FONT></H4>
The <A  href="#rhccc">Fisher's Z-transformation</a> is a useful tool in the circumstances in which two or more independent correlation coefficients are to be compared simultaneously.   To perform such a test one may evaluate the Chi-square statistic:
<center>
<p><FONT face=symbol>c</FONT><FONT size=+0><SUP>2</SUP></FONT> = <FONT face=symbol>S</FONT>[(n<FONT  size=+0><SUB>i</SUB></FONT> - 3).Z<FONT 
  size=+0><SUB>i</SUB></FONT><FONT size=+0><SUP>2</SUP></FONT>] - [<FONT face=symbol>S</FONT>(n<FONT  size=+0><SUB>i</SUB></FONT> - 3).Z<FONT 
  size=+0><SUB>i</SUB></FONT>]<FONT size=+0><SUP>2</SUP></FONT> / [<FONT face=symbol>S</FONT>(n<FONT   size=+0><SUB>i</SUB></FONT> - 3)],&nbsp; &nbsp; the sums are over all i = 1, 2, .., k.
</center><p>Where the Fisher Z-transformation is <p><CENTER>Z<FONT 
  size=+0><SUB>i</SUB></FONT> = 0.5[Ln(1+r<FONT  size=+0><SUB>i</SUB></FONT>) - Ln(1-r<FONT 
  size=+0><SUB>i</SUB></FONT>)],&nbsp; &nbsp;provided | r<FONT  size=+0><SUB>i</SUB></FONT> | <font face=symbol>&#185;</font> 1.</center><p>Under the null hypothesis:<p><B>H<FONT size=+0><SUB>0</SUB></FONT></B>: All correlation coefficients are almost equal.   <p>
The test statistic <FONT face=symbol>c</FONT><FONT size=+0><SUP>2</SUP></FONT> has  (k-1) degrees of freedom, where k is the number of  populations.
<p>  <P><B>An Application:</B> Consider the following correlation coefficients obtained by random sampling form ten independent populations. <p><CENTER><table width="330" border="0"  align="center">
  <tr>
    <td bgcolor="#999999" height="202"> 
      <table width="330" border="0" cellspacing="1" cellpadding="1">
        <tr bordercolor="" bgcolor="#F1f1f1"> 
          <th> 
            <div align="center"><font size="2" face="Arial, Helvetica, sans-serif" color="#003399">Population 
              P<sub>i</sub></font></div>
          </th>
          <th> 
            <div align="center"><font size="2" face="Arial, Helvetica, sans-serif" color="#003399">Correlation 
              r<sub>i</sub></font></div>
          </th>
          <th> 
            <div align="center"><font size="2" face="Arial, Helvetica, sans-serif" color="#003399">Sample 
              Size n<sub>i</sub></font></div>
          </th>
        </tr>
        <tr bordercolor="#dddddd" bgcolor="#FFFFFF"> 
          <td align=middle> 
            <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">1</font></div>
          </td>
          <td align=middle> 
            <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">0.72</font></div>
          </td>
          <td align=middle> 
            <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">67</font></div>
          </td>
        </tr>
        <tr bordercolor="#dddddd" bgcolor="#FFFFFF"> 
          <td align=middle> 
            <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">2</font></div>
          </td>
          <td align=middle> 
            <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">0.41</font></div>
          </td>
          <td align=middle> 
            <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">93</font></div>
          </td>
        </tr>
        <tr bordercolor="#dddddd" bgcolor="#FFFFFF"> 
          <td align=middle> 
            <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">3</font></div>
          </td>
          <td align=middle> 
            <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">0.57</font></div>
          </td>
          <td align=middle> 
            <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">73</font></div>
          </td>
        </tr>
        <tr bordercolor="#dddddd" bgcolor="#FFFFFF"> 
          <td align=middle> 
            <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">4</font></div>
          </td>
          <td align=middle> 
            <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">0.53</font></div>
          </td>
          <td align=middle> 
            <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">98</font></div>
          </td>
        </tr>
        <tr bordercolor="#dddddd" bgcolor="#FFFFFF"> 
          <td align=middle height="21"> 
            <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">5</font></div>
          </td>
          <td align=middle height="21"> 
            <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">0.62</font></div>
          </td>
          <td align=middle height="21"> 
            <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">82</font></div>
          </td>
        <tr bordercolor="#dddddd" bgcolor="#FFFFFF"> 
          <td align=middle> 
            <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">6</font></div>
          </td>
          <td align=middle> 
            <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">0.21</font></div>
          </td>
          <td align=middle> 
            <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">39</font></div>
          </td>
        </tr>
        <tr bordercolor="#dddddd" bgcolor="#FFFFFF"> 
          <td align=middle> 
            <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">7</font></div>
          </td>
          <td align=middle> 
            <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">0.68</font></div>
          </td>
          <td align=middle> 
            <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">91</font></div>
          </td>
        </tr>
        <tr bordercolor="#dddddd" bgcolor="#FFFFFF"> 
          <td align=middle> 
            <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">8</font></div>
          </td>
          <td align=middle> 
            <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">0.53</font></div>
          </td>
          <td align=middle> 
            <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">27</font></div>
          </td>
        </tr>
        <tr bordercolor="#dddddd" bgcolor="#FFFFFF"> 
          <td align=middle> 
            <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">9</font></div>
          </td>
          <td align=middle> 
            <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">0.49</font></div>
          </td>
          <td align=middle> 
            <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">75</font></div>
          </td>
        </tr>
        <tr bordercolor="#dddddd" bgcolor="#FFFFFF"> 
          <td align=middle height="15"> 
            <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">10</font></div>
          </td>
          <td align=middle height="15"> 
            <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">0.50</font></div>
          </td>
          <td align=middle height="15"> 
            <div align="center"><font size="2" face="Arial, Helvetica, sans-serif">49</font></div>
          </td>
        </tr>
      </table>
    </td>
  </tr>
</table>  </CENTER> <P> <P>  <P> Using the above formula  <FONT face=symbol>c</FONT><FONT size=+0><SUP>2</SUP></FONT>-statistic = 19.916, that has a p-value of 0.02.  Therefore, there is  moderate evidence against the null hypothesis.<p>In such a case, one may omit a few <A  href="#routlier">outliers</a> from the group, then use the <a href="otherapplets/MultiCorr.htm" target= "new">Test for Equality of Several Correlation Coefficients</a> applet. Repeat this process until a possible homogeneous sub-group may emerge.<p>
You might need to use <a href="otherapplets/SampleSize.htm" target= "new">Sample Size Determination</a> JavaScript at the design stage of your statistical investigation in decision making with specific subjective requirements.
<p>

<P><A name=rwlls></A> 
    <HR>    <H4><FONT color=#dc143c>Regression Modeling and Analysis</FONT></H4>
    Many problems in analyzing data involve describing how variables are related. The simplest of all models describing the relationship between two variables is a linear, or straight-line, model. Linear regression is always linear in  the coefficients being estimated, not necessarily linear in the variables.  <P>The simplest method of drawing a linear model is to "eye-ball" a line through the data on a plot, but a more elegant, and conventional method is that  of least squares, which finds the line minimizing the sum of the vertical distances between observed points and the fitted line. Realize that fitting the "best" line by eye is difficult, especially when there is much residual variability   in the data. <P>Know that there is a simple connection between the numerical coefficients  in the regression equation and the slope and intercept of the regression line.  <P>Know that a single summary statistic, like a correlation coefficient, does not tell the whole story. A <a href="otherapplets/scatter.htm" target= "new">scatterplot</a> is an essential complement to examining  the relationship between the two variables. <P>Again, the regression line is a group of estimates for the variable plotted  on the Y-axis. It has a form of y = b + mx, where m is the slope of the line. 
      The slope is the rise over run. If a line goes up 2 for each 1 it goes over,  then its slope is 2. <P><FONT color=#dc143c><STRONG>Formulas and Notations:</STRONG></FONT>   <UL>      <LI><IMG src="xbaru.gif"> = <FONT 
    face=symbol>S</FONT> x(i)/n<BR>        This is just the mean of the x values. 
      <LI><IMG src="ybar.gif"> = <FONT     face=symbol>S</FONT> y(i)/n <BR>        This is just the mean of the y values.       <LI>S<FONT size=+0><SUB>xx</SUB></FONT> = <FONT face=symbol>S</FONT>(x(i) 
        - <IMG src="xbaru.gif">)<FONT     size=+0><SUP>2</SUP></FONT> = <FONT face=symbol>S</FONT>x(i)<FONT     size=+0><SUP>2</SUP></FONT> - [<FONT face=symbol> S</FONT>x(i) ] <FONT     size=+0><SUP>2</SUP></FONT> / n       <LI>S<FONT size=+0><SUB>yy</SUB></FONT> = <FONT face=symbol>S</FONT>(y(i)         - <IMG src="ybar.gif">)<FONT     size=+0><SUP>2</SUP></FONT> = <FONT face=symbol>S</FONT>y(i)<FONT     size=+0><SUP>2</SUP></FONT> - [<FONT face=symbol> S</FONT>y(i) ] <FONT     size=+0><SUP>2</SUP></FONT> / n       <LI>S<FONT size=+0><SUB>xy</SUB></FONT> = <FONT face=symbol>S</FONT>(x(i)         - <IMG src="xbaru.gif">)(y(i) - <IMG     src="ybar.gif">) = <FONT 
    face=symbol>S</FONT>x(i).y(i) - [<FONT face=symbol> S</FONT>x(i) . <FONT 
    face=symbol>S</FONT>y(i)] / n       <LI>Slope m = S<FONT size=+0><SUB>xy</SUB></FONT> / S<FONT 
    size=+0><SUB>xx</SUB></FONT>       <LI>Intercept, b = <IMG src="ybar.gif"> - m . <IMG 
    src="xbaru.gif">       <LI>y-predicted = yhat = mx + b.<li>Residual = Error = y - yhat.<li>S<FONT><SUB>errors</SUB></FONT> = <FONT face=symbol> S</FONT>(y - yhat)<FONT><SUP>2</SUP></FONT>.<li>Standard deviation of residuals = S<FONT><SUB>res</SUB></FONT> = [S<FONT><SUB>errors</SUB></FONT> / (n-2)]<FONT><SUP>½</SUP></FONT>.<li>Standard error of the slope (m) = S<FONT><SUB>res</SUB></FONT> / S<FONT size=+0><SUB>xx</SUB></FONT><FONT><SUP>½</SUP></FONT>.<li>Standard error of the intercept (b) =  S<FONT><SUB>res</SUB></FONT>[(S<FONT size=+0><SUB>xx</SUB></FONT> + n. <IMG src="xbaru.gif"><FONT><SUP>2</SUP></FONT>) /(n.S<FONT size=+0><SUB>xx</SUB></FONT>] <FONT><SUP>½</SUP></FONT>.  <P></P>   </LI>    </UL> The regression line goes through a point with coordinates of (mean of x values, mean of y values), known as the mean-mean point.  <P>If you plug each x in the regression equation, then you obtain a predicted value for y. The difference between the predicted y and the observed y is called a residual, or an error term.  Some errors are positive and some are negative. The sum of squares of the errors plus the sum of squares of the estimates add up to the sum of squares of Y. The regression line is the line that minimizes the variance of the errors. The mean error is zero; so, this means that it minimizes the sum of the squares errors.<P>The reason for finding the best fitting line is so that you can make a reasonable prediction of what y will be if x is known (not vise-versa). <P>r<FONT size=+0><SUP>2</SUP></FONT> is the variance of the estimates divided by the variance of Y.   r is the size of the slope of the regression line, in terms of standard deviations. In other words, it is the slope of the regression line if we use the standardized X and Y. It is how many standard deviations of Y you would go up, when you go one standard deviation of X to the right. <P><FONT color=#dc143c><B>Coefficient of Determination:</B></FONT> Another measure of the closeness of the points to the regression line is the Coefficient  of Determination:     <P>      <CENTER>r<FONT size=+0><SUP>2</SUP></FONT> = S<FONT size=+0><SUP><SUB>yhat yhat</SUB></SUP></FONT> / S<FONT size=+0><SUB>yy</SUB></FONT> </CENTER>
    <P>which is the amount of the squared deviation in Y, that is explained by the points on the least squares regression line. <P><FONT color=#dc143c><B>Homoscedasticity and Heteroscedasticity:</B></FONT> 
      Homoscedasticity (homo = same, skedasis = scattering) is a word used to describe the distribution of data points around the line of best fit. The opposite  term is heteroscedasticity. Briefly, homoscedasticity means that data points are distributed equally about the line of best fit. Therefore, homoscedasticity means constancy of variance over all the levels of factors. Heteroscedasticity means that the data points cluster or clump above and below the line in a non-equal pattern. <P><FONT color=#dc143c><B>Standardized Regression Analysis:</b></FONT> The scale of measurements used to measure  X and Y has major impact on 
   the regression equation and correlation coefficient. This impact is more drastic comparing two regression equations having different scales of measurement. To overcome these drawbacks, one must standardize both X and Y prior to constructing the regression and interpreting the results. In such a model, the slope is equal to the correlation coefficient r. Notice that the derivative of function Y with respect to dependent variable X is the correlation coefficient.  Therefore, there is a nice similarity in the meaning of r in statistics and the derivative from calculus, in that its sign and its magnitude reveal the increasing/decreasing and the rate of change, as the derivative of a function do.
<p>In the usual regression modeling the <FONT color=#dc143c><B>estimated slope and intercept are correlated</b></FONT>; therefore, any error in estimating the slope influences the estimate of the intercept. One of the main advantages of using the standardized data is that the intercept is always equal to zero.

 <P> <FONT color=#dc143c><B>Regression when both X and Y are random:</b></FONT> Simple linear least-squares regression has among its conditions that the  data for the independent (X) variables are known without error. In fact,  the estimated results are conditioned on whatever errors happened to be present in the independent data sets. When the X-data have an error associated  with them the result is to bias the slope downwards. A procedure known as Deming regression can handle this problem quite well. Biased slope estimates  (due to error in X) can be avoided using Deming regression. 
<p>If X and Y are random variables, then the correlation coefficient R is often referred to as the <FONT color=#dc143c><B>Coefficient of Reliability</b></FONT>.


 <P><FONT color=#dc143c><B>The Relationship Between Slope and Correlation Coefficient:</b></FONT>  By a little bit of algebraic manipulation, one can show that the coefficient of correlation is related to the slope of the two regression lines:  Y on X, and X on Y, denoted by m <FONT  size=+0><SUB>yx</SUB></FONT> and m<FONT size=+0><SUB>xy</SUB></FONT>, respectively:  <P>   <CENTER>R<FONT size=+0><SUP>2</SUP></FONT> = m <FONT   size=+0><SUB>yx</SUB></FONT> . m<FONT size=+0><SUB>xy</SUB></FONT>  </CENTER>
<p> <P><FONT color=#dc143c><B>Lines of regression through the origin:</b></FONT>  Often the conditions of a practical problem require that the regression line go through the origin (x = 0, y = 0).  In such a case, the regression line has one parameter only, which is its slope:<p><center> m = <FONT face=symbol> S</FONT> (x<FONT><SUB>i</SUB></FONT> <font face=symbol>&#180;</font> y<FONT><SUB>i</SUB></FONT>)/ <FONT face=symbol> S</FONT>x<FONT><SUB>i</SUB></FONT><FONT><SUP>2</SUP></FONT>
</center><p> 
Notice that, for the models with the omission of the intercept, it is generally agreed that R<FONT><SUP>2</SUP></FONT> should not be defined or even considered.
<p>
<P><FONT color=#dc143c><B>Parabola models:</b></FONT>  Parabola regressions have three coefficients with a general form: <p><center>Y = a + bX + cX<FONT><SUP>2</SUP></FONT>,</center>
<p>where
<p>
<center>c = {<FONT face=symbol> S</FONT> (x<FONT><SUB>i</SUB></FONT><FONT>  - xbar)<SUP>2</SUP></FONT><FONT FACE="Symbol">&#215;</FONT>y<FONT><SUB>i</SUB></FONT> - n[<FONT face=symbol>S</FONT>(x<FONT><SUB>i</SUB></FONT> - xbar)
<FONT><SUP>2</SUP></FONT><FONT FACE="Symbol">&#215;</FONT><FONT face=symbol> S</FONT>y<FONT><SUB>i</SUB></FONT>]} / {n<FONT face=symbol> S</FONT>(x<FONT><SUB>i</SUB></FONT> - xbar)
<FONT><SUP>4</SUP></FONT>  - [<FONT face=symbol>S</FONT>(x<FONT><SUB>i</SUB></FONT> - xbar)<FONT><SUP>2</SUP></FONT>]<FONT><SUP> 2</SUP></FONT>}
</center>
<p><center>b = [<FONT face=symbol>S</FONT>(x<FONT><SUB>i</SUB></FONT>- xbar) y<FONT><SUB>i</SUB></FONT>]/[<FONT face=symbol> S</FONT>(x<FONT><SUB>i</SUB></FONT> - xbar)<FONT><SUP>2</SUP></FONT>] - 2<FONT FACE="Symbol">&#215;</FONT>c<FONT FACE="Symbol">&#215;</FONT>xbar </center>
<p><center>a = {<FONT face=symbol>S</FONT>y<FONT><SUB>i</SUB></FONT> - [c<FONT FACE="Symbol">&#215;</FONT><FONT face=symbol> S</FONT>(x <FONT><SUB>i</SUB></FONT> - xbar) <FONT><SUP>2</SUP></FONT>)}/n  -  (c<FONT FACE="Symbol">&#215;</FONT>xbar<FONT FACE="Symbol">&#215;</FONT>xbar + b<FONT FACE="Symbol">&#215;</FONT>xbar),<p></center>
<p>
where xbar is the mean of x<FONT><SUB>i</SUB></FONT>'s.
<p>Applications of quadratic regression include fitting the supply and demand curves in econometrics and fitting the ordering cost and holding cost functions in inventory control for finding the optimal ordering quantity.
<p>
You might like to use <a href="otherapplets/QuadReg.htm" target="new">Quadratic Regression</a> JavaScript to check your hand computation.  For higher degrees than quadratic, you may like to use the 
<a href="otherapplets/PolynoReg.htm" target="new">Polynomial Regressions</a> JavaScript.
<p>
<br>
<p>
<font   color="#DC143C"><b>Multiple Linear Regression:</b></font> The objectives in a multiple regression problem are essentially the same as for a simple regression.  While the objectives remain the same, the more predictors we have the calculations and interpretations become more complicated.  With multiple regression, we can use more than one predictor. It is always best, however, to be parsimonious, that is to use as few variables as predictors as necessary to get a reasonably accurate forecast. Multiple regression is best modeled with commercial package such as <a href="http://ubmail.ubalt.edu/~harsham/stat-data/opre330SPSSSAS.htm" target ="new"> SAS and  SPSS</a>. The forecast takes the form: 
<p>
<center>
Y = <FONT FACE="Symbol">b</FONT><FONT><SUB>0</SUB></FONT> + <FONT FACE="Symbol">b</FONT><FONT><SUB>1</SUB></FONT>X<FONT><SUB>1</SUB></FONT> + <FONT FACE="Symbol">b</FONT><FONT><SUB>2</SUB></FONT>X<FONT><SUB>2</SUB></FONT> + . . .+ <FONT FACE="Symbol">b</FONT><FONT><SUB>n</SUB></FONT>X<FONT><SUB>n</SUB></FONT>,
</center>
<p>
where <FONT FACE="Symbol">b</FONT><FONT><SUB>0</SUB></FONT> is the intercept, <FONT FACE="Symbol">b</FONT><FONT><SUB>1</SUB></FONT>,  <FONT FACE="Symbol">b</FONT><FONT><SUB>2</SUB></FONT>,  . . . <FONT FACE="Symbol">b</FONT><FONT><SUB>n</SUB></FONT> are coefficients representing the contribution of the independent variables X<FONT><SUB>1</SUB></FONT>, X<FONT><SUB>2</SUB></FONT>,..., X<FONT><SUB>n</SUB></FONT>.
<p>
For small sample size, you may like to use the <a href="otherapplets/MultRgression.htm" target="new">Multiple Linear Regression</a> JavaScript.
<br>
<p> 

<P><FONT color=#dc143c><B>What Is Auto-Regression:</b></FONT> In time series analysis and forecasting techniques, often linear regression is use to combine present and past values of an observation in order to forecast its future value.  The model is called an autoregressive model.  For details and implementation process visit <a href="http://www.mirror.ac.uk/sites/ubmail.ubalt.edu/~harsham/Business-stat/otherapplets/Autoreg.htm"  target=new>Autoregressive Modeling</A> JavaScript.
<br>
  <P><FONT color=#dc143c><B>What Is Logistic Regression:</b></FONT> Standard logistic regression is a method for modeling binary data (e.g., does a person smoke or not?, does a person survive a disease, or not?). <I>Polygamous</I> logistic regression is a method for modeling more than two options (e.g., does a person take the bus, drive a car or take the subway? does an office use WordPerfect, Word, or other office-ware?).
<p>   <P><FONT color=#dc143c><B>Why Linear Regression?</b></font> The study of corn shell (i.e., ear of corn) height versus rainfall has shown to have the following regression curve:
<P> <CENTER><IMG src="whylinear.gif"></center> <p>
Clearly, the relationship is highly nonlinear; however, if we are interested in a "small" range (say, for a specific geographical area, like southern region of the state of Maryland) then the condition of linearity might be satisfactory.  A typical application is depicted in the above figure where we are interested in predicting the height of corn in an area with rainfall in the range of [a, b].  Magnifying process of scale for this range allows us to fit a useful linear regression.   If the range is not short enough, then one may sub-divide the range accordingly by applying the same process of fitting a few lines, one for each sub-interval. 
<p>   <P><FONT color=#dc143c><B>Structural Changes:</b></font>  When a regression model has been estimated using the available data set, an additional data set may sometimes become available.  To test if previous model is still valid or the two separate models are equivalent or not, one may use the <A href="#rconariance">analysis of covariance</A> testing described on this site.
<P>You might like to use the <A   href="otherapplets/Regression.htm"   target=new>Regression Analysis</A> JavaScript to check your computations and to perform some numerical experimentation for a deeper understanding of the concepts.   
<P><B>Further Reading:</B><BR><FONT face="Bookman Old Style"   size=-2>Chatterjee S., B. Price, and A. Hadi, <I>Regression Analysis by Example</I>,  Wiley, 1999.<BR></FONT> 
<P><A name=rranaproces></A> 
    <HR>
    <H4><FONT color=#dc143c>Regression Modeling Selection Process</FONT></H4>
 When you  have more than one regression equation based on data, to select the "best model", you should compare: <P></P> </BLOCKQUOTE><OL> <LI>R-squares: That is, the percentage of variance [in fact, the sum of  squares] in Y accounted for by variance in X captured by the model. <LI>When you want to compare models of different sizes (different numbers of  independent variables (p) and/or different sample sizes n), you must use  the Adjusted R-Square, because the usual r-square tends to grow with the  number of independent variables.  <P>  <CENTER>r<FONT size=+0><SUP>2</SUP></FONT> <FONT size=+0><SUB>a</SUB></FONT> 
          = 1 - (n - 1)(1 - r<FONT size=+0><SUP>2</SUP></FONT>)/(n - p - 1)
        </CENTER> <P></P> <LI>Standard deviation of error terms, i.e., observed y-value - predicted y-value for each x.<LI>Trends in errors as a function of control variable x. Systematic trends are not uncommon. <LI>The T-statistic of individual parameters. <LI>The values of the parameters and its content to content underpinnings.  <LI>F<FONT size=+0><SUB>df1 df2</SUB></FONT> value for overall assessment.  Where df1 (numerator degrees of freedom) is the number of linearly independent  predictors in the assumed model minus the number of linearly independent predictors in the restricted model; i.e., the number of linearly independent restrictions imposed on the assumed model, and df2 (denominator degrees of freedom) is the number of observations minus the number of linearly independent 
      predictors in the assumed model. <P>The observed F-statistic should exceed not merely the selected critical  value of F-table, but at least four times the critical value. </P></LI></OL><P>  <CENTER>
      <a href="regression.gif"><IMG alt="Regression Analysis Process" src="regression.gif" width="174" height="189" border="0"></a> <br><p> Regression Analysis Process <br>  <b><font color="#dc143c">Click on the image to enlarge it and THEN print it</font></b>    </CENTER>  <P>     <P>  <BLOCKQUOTE>Finally in statistics for business, there exists an opinion that with more  than 4 parameters, one can fit an elephant so that if one attempts to fit  a regression funtion  that depends on many parameters, the result should not be regarded  as very reliable. <P><B>Further Reading:</B><BR><FONT face="Bookman Old Style"   size=-2>Draper N., and H. Smith, <I>Applied Regression Analysis</I>, Wiley, 1998.<BR></FONT> 
<p>
<P><A name=rcorrIationCovar></A> 
    <HR>    <H4><FONT color=#dc143c>Covariance and Correlation</FONT></H4>
    Suppose that X and Y are two <A   href="#rrandomva">random  variables</A> for the outcome of a random experiment. The covariance of X and Y is defined by <P>  <CENTER> Cov (X, Y) = E{[X - E(X)][Y - E(Y)]} 
      </CENTER> <P>  <CENTER> </CENTER>  and, given that the variances are strictly positive, the correlation of X and   Y is defined by  <P>  <CENTER>    <FONT face=symbol>r</FONT> (X, Y) = Cov(X, Y) / [sd(X) . sd(Y)]  </CENTER> <P>       <CENTER> </CENTER> Correlation is a scaled version of covariance; note that the two parameters  always have the same sign (positive, negative, or 0). When the sign is positive,  the variables are said to be positively correlated; when the sign is negative, the variables are said to be negatively correlated; and when it is 0, the variables are said to be uncorrelated.<p>Notice that the correlation between two random variables is often due only to the fact that both variables are correlated with the same third variable. 
 <P>As these terms suggest, covariance and correlation measure a certain kind  behavior in both variables. Correlation is very similar to the derivative of a function that you may have studies in high school.<p><FONT color=#dc143c><b>Coefficient of Determination:</b></font> The square of correlation coefficient <FONT face=symbol>r</FONT> <FONT size=+0><SUP>2</SUP></FONT> indicates the proportion of the variation in one variable that can be associated with the variance in the other variable. The three typical possibilities are depicted in the following figure:
<P> <CENTER><IMG src="corrsqr.gif"> 
<p>The proportion of shared variance by two variables for the different values of the coefficient of determination:<br><FONT face=symbol>r</FONT><FONT><SUP>2</SUP></FONT> = 0, <FONT face=symbol>r</FONT><FONT><SUP>2</SUP></FONT> = 1, and <FONT face=symbol>r</FONT><FONT><SUP>2</SUP></FONT> = 0.25,<br></font> as shown by the shaded areas in this figure. 
</CENTER><P>
 <P><FONT color=#dc143c><B>Properties:</B></font> The following exercises give some basic properties of  expected values. The main tool that you will need is the fact that expected  value is a linear operation.  <P>You might like to use <A  href="otherapplets/MultiVariate.htm"  target=new>this Applet</A> in performing some numerical experimentation to:  <P> 
    <OL>
<LI>Show that E[X/Y] <FONT face=symbol>¹</FONT> E(X)/E(Y). <LI>Show that E[X <font face=symbol>&#180;</font> Y] <FONT face=symbol>¹</FONT> E(X) <font face=symbol>&#180;</font> E(Y). <LI>Show that [E(X <font face=symbol>&#180;</font>	Y)<FONT size=+0><SUP>2</SUP></FONT>] <FONT 
face=symbol>£</FONT> E(X<FONT size=+0><SUP>2</SUP></FONT>)  <font face=symbol>&#180;</font> E(Y<FONT 
size=+0><SUP>2</SUP></FONT>). <LI>Show that [E(X/Y)<FONT size=+0><SUP>n</SUP></FONT>] <FONT 
face=symbol>³</FONT> E(X<FONT size=+0><SUP>n</SUP></FONT>)/E(Y<FONT 
size=+0><SUP>n</SUP></FONT>), for any n. <LI>Show that Cov(X, Y) = E(XY) - E(X)E(Y). 
<LI>Show that Cov(X, Y) = Cov(Y, X). <LI>Show that Cov(X, X) = V(X). <LI>Show that: If X and Y are independent random variables, then<BR>  Var(XY) = 2 V(X) <font face=symbol>&#180;</font>	V(Y) + V(X)(E(Y))<FONT size=+0><SUP>2</SUP></FONT> + V(Y)(E(X))<FONT size=+0><SUP>2</SUP></FONT>.   </OL>    <P><A name=rppc></A> 
    <HR>      

 <H4><FONT color=#dc143c>Pearson, Spearman, and Point-Biserial Correlations</FONT></H4>
    
There are measures that describe the degree to which two variables are linearly related. For the majority of these measures, the correlation is expressed as a coefficient that ranges from 1.00 to -1.00.  A value of 1 is indicating a perfect linear relationship, such that knowing the value of one variable will allow perfect prediction of the value of the related value.  A value of 0 is indicating no predictability by a linear model. With negative values indicating that, when the value of one variable is higher than average, the other is lower than average (and vice versa); and positive values indicating that, when the value of one variable is high, so is the other (and vice versa). 
<p>
 Correlation is similar to the derivative you have learned in calculus (a deterministic course). <P>The Pearson's product correlation is an index of the <FONT color=#dc143c>linear relationship</FONT> between two variables. <P><FONT color=#dc143c><B>Formulas and Notations:</B></FONT> <UL><LI><IMG src="xbaru.gif"> = <FONT 
    face=symbol>S</FONT> x<FONT size=+0><SUB>i</SUB></FONT> / n.<p> This is just the mean of the x values.  <LI><IMG src="ybar.gif"> = <FONT 
    face=symbol>S</FONT> y<FONT size=+0><SUB>i</SUB></FONT> / n.<p> This is just the mean of the y values.  <LI>S<FONT size=+0><SUB>xx</SUB></FONT> = <FONT face=symbol>S</FONT>(x<FONT 
    size=+0><SUB>i</SUB></FONT> - <IMG  src="xbaru.gif">)<FONT size=+0><SUP>2</SUP></FONT> = <FONT  face=symbol>S</FONT>x<FONT size=+0><SUB>i</SUB></FONT><FONT
 size=+0><SUP>2</SUP></FONT> - [<FONT face=symbol> S</FONT>x<FONT  size=+0><SUB>i</SUB></FONT> ] <FONT size=+0><SUP>2</SUP></FONT> / n.<p><LI>S<FONT size=+0><SUB>yy</SUB></FONT> = <FONT face=symbol>S</FONT>(y<FONT  size=+0><SUB>i</SUB></FONT> - <IMG  src="ybar.gif">)<FONT size=+0><SUP>2</SUP></FONT> = <FONT face=symbol>S</FONT>y<FONT size=+0><SUB>i</SUB></FONT><FONT 
    size=+0><SUP>2</SUP></FONT> - [<FONT face=symbol> S</FONT>y<FONT  size=+0><SUB>i</SUB></FONT> ] <FONT size=+0><SUP>2</SUP></FONT> / n.<p> <LI>S<FONT size=+0><SUB>xx</SUB></FONT> = <FONT face=symbol>S</FONT>(x<FONT  size=+0><SUB>i</SUB></FONT> - <IMG  src="xbaru.gif">)(y<FONT 
    size=+0><SUB>i</SUB></FONT> - <IMG  src="ybar.gif">) = <FONT  face=symbol>S</FONT>(x<FONT size=+0><SUB>i</SUB></FONT> y<FONT  size=+0><SUB>i</SUB></FONT>) - [<FONT face=symbol> S</FONT>x<FONT  size=+0><SUB>i</SUB></FONT> . <FONT face=symbol>S</FONT>y<FONT 
    size=+0><SUB>i</SUB></FONT> ] / n. </LI></UL>
    <P><FONT color=#dc143c><B>The Pearson's correlation</B></FONT> is 
    <P>  <CENTER>r = S<FONT size=+0><SUB>xy</SUB></FONT> / (S<FONT size=+0><SUB>xx</SUB></FONT> <font face=symbol>&#180;</font> S<FONT size=+0><SUB>yy</SUB></FONT>)<FONT size=+0><SUP>0.5 </SUP></FONT> </CENTER> <P>A positive relationship indicates that if an individual value of x is above the  mean of x's, then this individual x is likely to have a y value that is above the mean of y's, and vice versa.  A negative relationship would be an x score above the mean of x and a y score below the mean of y. It is a measure of the relationship between variables and an index of the proportion of individual differences in one variable that can be associated with the individual differences in another variable.  

<p>Notice that, the correlation coefficient is the mean of the cross-products of scores. Therefore, if you have three values for  r of .40, .60, and .80, you cannot say that the difference between   r = .40 and  r = .60 is the same as the difference between  r =.60 and  r = .80, or that  r = .80 is twice as large as  r = .40 because the scale of values for the correlation coefficient is not interval or ratio, but ordinal. Therefore, all you can say is that, for example, a correlation coefficient of +.80 indicates a high positive linear relationship and a correlation  coefficient of +.40 indicates a some what lower positive linear relationship. 
<p> The square of the correlation coefficient equals the proportion of the total variance in Y that can be associated  with the variance in x. It can tell us how much of the total variance of one variable can be associated with the variance of another variable.
<P> Note that a correlation coefficient is done on linear correlation. If the data forms a parabola, then a linear correlation of x and y will produce an r-value equal to zero. So one must be careful and look at data.  <P>The standard statistics for hypothesis testing: H<FONT><SUB>0</SUB></FONT>: <FONT face=symbol>r</FONT> = <FONT face=symbol>r</FONT><FONT><SUB>0</SUB></FONT>,  is the Fisher's normal transformation: <P> <CENTER>z = 0.5[Ln(1+r) - Ln(1-r)],&nbsp; &nbsp; with mean <FONT face=symbol>m</FONT> = 0.5[Ln(1+ <FONT face=symbol>r</FONT><FONT><SUB>0</SUB></FONT>) - Ln(1-<FONT face=symbol>r</FONT><FONT><SUB>0</SUB></FONT>)],&nbsp; &nbsp;and standard deviation <FONT face=symbol>s</FONT> =  (n-3)<FONT><SUP>-½</SUP></FONT>.<p></CENTER>  Having constructed a desirable confidence interval, say [a, b], based on statistic Z, it has to be transformed back to the original scale. That is, the confidence interval is:<P> <CENTER>(e<FONT><SUP>2a</SUP></FONT> -1)/ (e<FONT><SUP>2a</SUP></FONT> +1), &nbsp; &nbsp;&nbsp; &nbsp;
(e<FONT><SUP>2b</SUP></FONT> -1)/ (e<FONT><SUP>2b</SUP></FONT> +1).
<p></CENTER>
<p>
Provided | r<FONT><SUB>0</SUB></FONT> | <font face=symbol>&#185;</font> 1,  and  | <FONT face=symbol>r</FONT><FONT><SUB>0</SUB></FONT> | <font face=symbol>&#185;</font> 1, and n is greater than 3.
<p>
Alternatively, 
<P> <CENTER>&nbsp; &nbsp;&nbsp; &nbsp;{1+ r - (1-r) exp[2z<FONT FACE="symbol"><SUB>a</SUB></FONT>/(n-3)<FONT><SUP>½</SUP></FONT>]} / {1+ r + (1-r) exp[2z<FONT FACE="symbol"><SUB>a</SUB></FONT>/(n-3)<FONT><SUP>½</SUP></FONT>]} , &nbsp; &nbsp;and
<p> 
{1+ r - (1-r) exp[-2z<FONT FACE="symbol"><SUB>a</SUB></FONT>/(n-3)<FONT><SUP>½</SUP></FONT>]} / {1+ r + (1-r) exp[-2z<FONT FACE="symbol"><SUB>a</SUB></FONT>/(n-3)<FONT><SUP>½</SUP></FONT>]}
<p></CENTER>
<p>
You might like to use <a href="otherapplets/scientificCal.htm" target="new" >this calculator</a> for your needed computation. You may perform <a href="otherapplets/correlation.htm" target= "new">Testing the Population Correlation Coefficient </a>.
       <P><FONT color=#dc143c><B>Spearman rank-order correlation</B></FONT> is used as a non-parametric version of Pearson's. It is expressed as:  <P> <CENTER><FONT face=symbol>r </FONT> = 1 - (6<FONT face=symbol> S </FONT>d<FONT size=+0><SUP>2</SUP></FONT>) / [n(n<FONT size=+0><SUP>2</SUP></FONT> - 1)],  </CENTER> <P>where d is the difference rank between each X and Y pair. <P>Spearman correlation coefficient can be algebraically derived from the Pearson correlation formula by making use of sums of series. Pearson contains expressions for <FONT face=symbol>S </FONT>X(i), <FONT face=symbol>S </FONT>Y(i),  <FONT face=symbol>S </FONT>X(i)<FONT size=+0><SUP>2</SUP></FONT>, and <FONT face=symbol>S</FONT>Y(i)<FONT size=+0><SUP>2</SUP></FONT>. <P>In the Spearman case, the X(i)'s and Y(i)' are ranks, and so the sums of the ranks, and the sums of the ranks squared, are entirely determined by  the number of cases (without any ties).  <P>  <CENTER><FONT face=symbol>S </FONT>i = (n+1)n/2, <FONT face=symbol>S </FONT>i<FONT size=+0><SUP>2</SUP></FONT> = n(n+1)(2n+1)/6.</CENTER>
    <P>The Spearman formula then is equal to: <P> <CENTER>[12P - 3n(n+1)<FONT size=+0><SUP>2</SUP></FONT>] / [n(n<FONT size=+0><SUP>2</SUP></FONT> - 1)], </CENTER> <P>where P is the sum of the product of each pair of ranks X(i)Y(i). This reduces to: <P> <CENTER><FONT face=symbol>r </FONT>= 1 - (6<FONT face=symbol> S </FONT>d<FONT  size=+0><SUP>2</SUP></FONT>) / [n(n<FONT size=+0><SUP>2</SUP></FONT> - 1)],  </CENTER><P>where d is the difference rank between each x(i) and y(i) pair. <P>An important consequence of this is that if you enter ranks into a Pearson formula, you get precisely the same numerical value as that obtained by entering the ranks into the Spearman formula. This comes as a bit of a shock to those who like to adopt simplistic slogans, such as "Pearson is for interval data, Spearman is for ranked data". Spearman doesn't work too well if there are many tied ranks. That's because the formula for calculating the sums of squared ranks no longer holds true. If one has many tied ranks, use the Pearson formula.  <P> One may use this measure as a decision-making tool: <P> <CENTER> <TABLE border=1 borderColor=#000000 cellSpacing=0><TBODY> <TR> <TD> <CENTER><STRONG>Value of |<FONT  
   face=symbol>r</FONT>|</STRONG> </CENTER></TD><TD> <CENTER> <STRONG>Interpretation</STRONG>
</CENTER> </TD></TR><TR>  <TD>0.00 - 0.40 </TD> <TD>Poor</TD></TR><TR> <TD>0.41 - 0.75 </TD>
<TD>Fair</TD></TR><TR><TD>0.76 - 0.85 </TD><TD>Good</TD></TR><TR> <TD>0.86 - 1.00 </TD>
<TD>Excellent</TD></TR></TBODY></TABLE></CENTER><P>This interpretation is widely accepted, and many scientific journals routinely publish papers using this interpretation for the estimated result, and even 
for the test of hypothesis. <P><FONT color=#dc143c><B>Point-Biserial Correlation</B></FONT> is used when  one <A  href="#rrandomva">random variable</A> is binary (0, 1) and the other is a continuous random variable;  the strength of relationship is measured by the point-biserial correlation: <P> <CENTER>
r = (X<FONT><SUB>1</SUB></FONT> - X<FONT><SUB>0</SUB></FONT>)[pq/S<FONT size=+0><SUP>2</SUP></FONT>] <FONT size=+0><SUP>½</SUP></FONT> 
 </CENTER><P>Where X<FONT><SUB>1</SUB></FONT>and X<FONT><SUB>0</SUB></FONT> are the means of scores having 1, and 0 values, and p and q are their proportions, respectively. S<FONT size=+0><SUP>2</SUP></FONT> is the <I>sample variance</I> of the continuous random variable. This is a simplified version of the Pearson correlation for the case when one of 
      the two random variables is a (0, 1) Nominal random variable.
<P>Note also that r has the shift-invariant property for any positive scale. That is ax + c, and by + d, have same r as x and y, for any positive a and b. 
<p><A name=rcals></A> 
    <HR> <H4><FONT color=#dc143c>Correlation, and Level of Significance</FONT></H4> It is intuitive that with very few data points, a high correlation may not be statistically significant. You may see statements such as, "correlation is significant between x and y at the <FONT face=symbol>a</FONT> = .005 level"  and "correlation is significant at the <FONT face=symbol>a</FONT> = .05 level." The question is:  how do you determine these numbers? <p>Using the simple correlation r,  the formula for F-statistic is:
<P> <CENTER>  F= (n-2) r<FONT size=+0><SUP>2</SUP></FONT> / (1-r<FONT 
  size=+0><SUP>2</SUP></FONT>), &nbsp; &nbsp; where n is at least 2. </CENTER> <P>As you see, F statistic  is monotonic function with respect to both: r<FONT size=+0><SUP>2</SUP></FONT>, and the sample size n.  
<p>
Notice that the test for the statistical significance of a correlation coefficient requires that the two variables be distributed as a bivariate normal.<P>
 <p>
<P><A name=rcorrInd></A>  <HR> <H4><FONT color=#dc143c>Independence vs. Correlated</FONT></H4>
    In the sense that it is used in statistics; i.e., as an assumption in applying a statistical test; a random sample from the entire population provides a set of <A  href="#rrandomva">random variables</A> X1,...., Xn, that are identically distributed and mutually independent. Mutually independent is stronger than pairwise independence. The random variables are mutually independent if their joint distribution is equal to  the product of their marginal distributions.  <P>In the case of joint normality, independence is equivalent to zero correlation,   but not in general. Independence will imply zero correlation but not conversely. Not that not all random variables have a first moment, let alone a second moment, and hence there may not be a correlation coefficient. <P>However; if the correlation coefficient of two random variables is not zero then the random variables are not independent. <P><P><A name=rhccc></A> 
    <HR> <H4><FONT color=#dc143c>How to Compare Two Correlation Coefficients?</FONT></H4>
Given that two populations have normal distributions,  we wish to test for the following null hypothesis regarding the equality of correlation coefficients:
<P>
<B>H<FONT><SUB>o</SUB></FONT>:</B> <FONT   face=symbol>r </FONT><FONT size=+0><SUB>1</SUB></FONT> = <FONT face=symbol>r 
    </FONT><FONT size=+0><SUB>2</SUB></FONT>,
<p>
 based on two observed correlation coefficients r<FONT size=+0><SUB>1</SUB></FONT>, and r<FONT size=+0><SUB>2</SUB></FONT>, obtained from two random sample of size n<FONT size=+0><SUB>1</SUB></FONT> and n<FONT size=+0><SUB>2</SUB></FONT>, respectively, provided | r<FONT   size=+0><SUB>1</SUB></FONT> | <font face=symbol>&#185;</font> 1,  and  | r<FONT   size=+0><SUB>2</SUB></FONT> | <font face=symbol>&#185;</font> 1, and  n<FONT   size=+0><SUB>1</SUB></FONT>,  n<FONT size=+0><SUB>2</SUB></FONT> both are greater than 3.  Under the null hypothesis and normality condition , the test statistic is:<p>  <CENTER>
        Z = (z<FONT size=+0><SUB>1</SUB></FONT> - z<FONT   size=+0><SUB>2</SUB></FONT>) / [ 1/(n<FONT size=+0><SUB>1</SUB></FONT>-3) + 1/(n<FONT size=+0><SUB>2</SUB></FONT>-3) ]<FONT 
  size=+0><SUP>½</SUP></FONT>  </CENTER>  where: <CENTER> z<FONT size=+0><SUB>1</SUB></FONT> = 0.5 Ln [ (1+r<FONT  size=+0><SUB>1</SUB></FONT>)/(1-r<FONT size=+0><SUB>1</SUB></FONT>) ], <BR>   z<FONT size=+0><SUB>2</SUB></FONT> = 0.5 Ln [ (1+r<FONT 
  size=+0><SUB>2</SUB></FONT>)/(1-r<FONT size=+0><SUB>2</SUB></FONT>) ],  </CENTER>
    <P>and n<FONT size=+0><SUB>1</SUB></FONT>= sample size associated with r<FONT 
  size=+0><SUB>1</SUB></FONT>, and n<FONT size=+0><SUB>2</SUB></FONT> =sample size 
      associated with r<FONT size=+0><SUB>2</SUB></FONT>.   <P>The distribution of the Z-statistic is the standard  Normal(0,1); therefore, you may reject H<FONT size=+0><SUB>0</SUB></FONT> if |Z|<FONT face=symbol>&gt;</FONT> 1.96 at the 95% confidence level. 
<p>
<B>An Application:</B> Suppose r<FONT><SUB>1</SUB></FONT> = 0.47, r<FONT><SUB>2</SUB></FONT> = 0.63 are obtained from two independent random samples of size  n<FONT><SUB>1</SUB></FONT>=103, and n<FONT><SUB>2</SUB></FONT> = 103, respectively. Therefore, the z<FONT><SUB>1</SUB></FONT> = 0.510, and z<FONT><SUB>2</SUB></FONT> = 0.741, with Z-statistics:
<CENTER>
<p>
Z = (0.510 - 0.7)/ [1/(103-3) + 1/(103-3)]<FONT   size=+0><SUP>½</SUP></FONT> = -1.63
<p>
</CENTER>
This result is not within the rejection region of the two-tails critical values at <FONT face=symbol>a</FONT> = 0.05, therefore is not significant. Therefore, there is not sufficient evidence to reject the null hypothesis that the two correlation coefficients are equal
<p>
Clearly, this test can be modified and applied for test of hypothesis regarding population correlation <FONT face=symbol>r</FONT> based on observed r obtained from a random sample of size n:
<p>
<CENTER>
        Z = (z<FONT size=+0><SUB>r</SUB></FONT> - z<FONT   size=+0><SUB><FONT face=symbol>r</FONT> </SUB></FONT>) / [1/(n-3) ]<FONT 
  size=+0><SUP>½</SUP></FONT>, </CENTER>
<p>
provided | r | <font face=symbol>&#185;</font> 1,  and  | <FONT face=symbol>r</FONT> | <font face=symbol>&#185;</font> 1, and n is greater than 3.
<p>
<FONT color=#dc143c>Testing the Equality of Two Dependent Correlations:</font> In testing the hypothesis of no difference between two population correlation coefficients:
<p>

<B>H<FONT><SUB>0</SUB></FONT>:</B> <FONT face=symbol>r</FONT> (X, Y) = <FONT face=symbol>r</FONT> (X, Z)
<p>
Against the alternative:
<p>
<B>H<FONT><SUB>a</SUB></FONT>: </B><FONT face=symbol>r</FONT> (X, Y) <font face=symbol>&#185;</font> <FONT face=symbol>r</FONT> (X, Z)
<p>
 with a common covariare X, one may use the following test statistics:
<p>
t = { (r<FONT><SUB>xy</SUB></FONT> - r<FONT><SUB>xz</SUB></FONT> ) [ (n-3)(1 + r<FONT><SUB>yz</SUB></FONT>)]<FONT   size=+0><SUP>½</SUP></FONT> ] } / {2(1-r<FONT><SUB>xy</SUB></FONT><FONT   size=+0><SUP>2</SUP></FONT> - r<FONT><SUB>xz</SUB></FONT><FONT   size=+0><SUP>2</SUP></FONT> - r<FONT><SUB>yz</SUB></FONT><FONT   size=+0><SUP>2</SUP></FONT> + 2r<FONT><SUB>xy</SUB></FONT>r<FONT><SUB>xz</SUB></FONT>r<FONT><SUB>yz</SUB></FONT> )}<FONT   size=+0><SUP>½</SUP></FONT>,    
<p>
with n - 3 degree of freedom, where n is the tripled-ordered sample size, provided all absolute value of r's are not equal to 1. 
<p>
<B>Numerical example:</B> Suppose n = 87, r<FONT><SUB>xy</SUB></FONT> = 0.631, r<FONT><SUB>xz</SUB></FONT> = 0.428, and r<FONT><SUB>yz</SUB></FONT> = 0.683, then t-statistic is equal to 3.014,  with p-value equal to 0.002, indicating a strong evidence against the null hypothesis.
<p>
<br>
<FONT color=#dc143c><B>Adjusted R<FONT><SUP>2</SUP></FONT>:</B></FONT> 

In modeling selection process based of  R<FONT><SUP>2</SUP></FONT> values, it is often necessary and meaningful to adjust the  R<FONT><SUP>2</SUP></FONT>'s for their degrees of freedom. Each <B>Adjusted</B>  R<FONT><SUP>2</SUP></FONT> is calculated by:
<p>
<p>
<center>1 - [(n - i)(1 - R<FONT><SUP>2</SUP></FONT>)] / (n - p),</center>
<p>
where i is equal to 1 if there is an intercept and 0 otherwise; n is the number of observations used to fit the model; and p is the number of parameters in the model. 
<p>
You might like to use  the <a href="otherapplets/correlation.htm" target= "new">Testing the Population Correlation Coefficient</a> JavaScript in performing some numerical experimentation for validating and a deeper understanding of the concepts. 
 <P><A name=rregplandevmain></A> 
<HR><H4><FONT color=#dc143c>Planning, Development, and Maintenance of a Linear Model</FONT></H4>
  </BLOCKQUOTE><FONT color=#dc143c>A. Planning:</FONT> 
  <OL><LI>Define the problem; select response; suggest variables. <P></P><LI>Are the proposed variables fundamental to the problem, and are they variables? Are they measurable/countable? Can one get a complete set of observations at the same time?  Ordinary regression analysis does not assume that the independent variables are measured without error. However, they are conditioned on whatever errors happened to be present in the independent data set. <P></P><LI>Is the problem potentially solvable?  <P></P><LI>Find  correlation matrix and first regression runs (for a subset of data). <BR> Find the basic statistics, correlation matrix. <BR> How difficult is the problem?  Compute the Variance Inflation Factor:
<p>
<center>

 VIF = 1/(1 -r<FONT size=+0><SUB>ij</SUB></FONT>),  for all i, j. 
<p>
</center>
For moderate VIF's, say between  2 and 8, you might be able to come-up with a good' model.     <P>Inspect r<FONT size=+0><SUB>ij</SUB></FONT>'s; one or two must be large. If all are small, perhaps the ranges of the X variables are too small. <P></P><LI>Establish goal; prepare budget and time table.       <P>a.  The final equation should have <B>Adjusted</B> R<FONT size=+0><SUP>2 </SUP></FONT> =  0.8 (say). <BR>
        b. Coefficient of Variation of say; less than 0.10 <BR> c. Number of predictors should not exceed p (say, 3), (for example for  p = 3, we need at least 30 points). Even if all the usual assumptions for a regression model are satisfied, over-fitting can ruin a model's usefulness. The widely used approach is the data reduction method to deal with the cases where the number of potential predictors is large in comparison with the number of observations. <BR> d. All estimated coefficients must be significant at <FONT face=symbol>m </FONT>= 0.05 (say). <BR> e. No pattern in the residuals <P></P><LI>Are goals and budget acceptable? </LI>
  </OL><P><FONT color=#dc143c>B. Development of the Model:</FONT> <P> <OL> <LI>Collect date; check the quality of date; plot; try models;  check the regression conditions. <P></P>
    <LI>Consult experts for criticism. <BR> Plot new variable and examine same fitted model. Also transformed Predictor Variable may be used.  <P></P> <LI>Are goals met? <BR> Have you found "the best" model? </LI></OL> <P><FONT color=#dc143c>C. Validation and Maintenance of the Model:</FONT> 
  <P> <OL> <LI>Are parameters stable over the sample space?  <P></P> <LI>Is there a lack of fit? <BR>
      Are the coefficients reasonable? Are any obvious variables missing? Is the equation usable for control or for prediction?  <P></P> <LI>Maintenance of the Model. <BR> Need to have control chart to check the model periodically by statistical  techniques. </LI></OL><P> <BLOCKQUOTE> <P>You might like to use <A href="otherapplets/Regression.htm"   target=new>Regression Analysis with Diagnostic Tools</A> in performing regression analysis. <P> 

<P> <P><A name=rregconditions></A> <HR> <H4><FONT color=#dc143c>Conditions and the Check-list for Linear Models</FONT></H4> Almost all models of reality, including regression models, have assumptions that must be verified in order that the model has power to test hypotheses and for it to be able to predict accurately. 
    <P>The following is the list of basic assumptions (i.e., conditions) and the tools to check these necessary conditions. <P></P></BLOCKQUOTE><ol> <LI>Any undetected outliers may have major impact on the regression model. Outliers are a few observations that are not well fitted by the "best" available model.  In such case one, must first investigate the source of data, if there is no doubt about the accuracy or veracity of the observation, then it should be removed and the model should be refitted. 
<p>
You might like to use the <a href="otherapplets/scatter.htm" target ="new">Determination of the Outliers</a> JavaScript to perform some numerical experimentation for validating and for a deeper understanding of the concepts<p>
 <P></P><LI>The dependent variable Y is a linear function of the independent variable X.  This can be checked by carefully examining all the points in the <a href="otherapplets/scatter.htm" target= "new">scatter diagram</a>, and see if it is possible to bound them all  within two parallel lines. You may also use the <A   href="otherapplets/Trend.htm"  target=new>Detective Testing for Trend</A> to check this condition.  <P></P><LI>The distribution of the residual must be normal. You may check this condition by using the <A  href="otherapplets/Normality.htm"  target=new>Lilliefors Test for Normality</A>.  <P></P><LI>The residuals should have a mean equal to zero, and a constant standard deviation (i.e., homoskedastic condition). You may check this condition by dividing the residuals data into two or more groups; this approach is known as the Goldfeld-Quandt test. You may use the <A href="otherapplets/Stationary.htm"  target=new>Stationary Testing Process</A> to check this condition.    <P></P><LI>The residuals constitute a set of random variables. You may use the <A 
  href="otherapplets/Randomness.htm"  target=new>Test for Randomness</A> and  <a href="otherapplets/Fluctuation.htm" target= "new">Test for Randomness of Fluctuations</a> to check this condition .  <P></P><LI>Durbin-Watson (D-W) statistic quantifies the serial correlation of least-squares errors in its original form. D-W statistic is defined by:  <P> <CENTER> D-W statistic = <FONT face=symbol>S</FONT><SUB>2</SUB><SUP>n</SUP> (e<SUB>j</SUB> 
          - e<SUB>j-1</SUB>)<SUP>2</SUP> / <FONT   face=symbol>S</FONT><SUB>1</SUB><SUP>n</SUP> e<SUB>j</SUB><SUP>2</SUP>,   </CENTER> <P>   where e<SUB>j</SUB> is the j<FONT size=+0><SUP>th</SUP></FONT> error.  D-W takes values within [0, 4]. For no serial correlation, a value close     to 2 is expected. With positive serial correlation, adjacent deviates  tend to have the same sign, therefore D-W becomes less than 2; whereas, with negative serial correlation, alternating signs of error, D-W takes values larger than 2. For a least-squares fit where the value of D-W is significantly different from 2, the estimates of the variances and covariances of the parameters (i.e., coefficients) can be in error, being either too large or too small. The serial correlation of the deviates arise also time series analysis and forecasting. You may use the <A href="otherapplets/MeasurAccur.htm"  target=new>Measuring for Accuracy</A> JavaScript to check this condition. </P></ol>  <BLOCKQUOTE>The "good" regression equation candidate is further analyzed using a plot of the residuals versus the independent variable(s). If any patterns are seen in the graph; e.g., an indication of non-constant variance; then  there is a need for data transformation. The following are the widely used transformations: </BLOCKQUOTE>
  <UL>    <P>     <LI>X' = 1/X, &nbsp; &nbsp;for non-zero X.     <LI>X' = Ln (X), &nbsp; &nbsp;for positive X.
    <LI>X' = Ln(X), Y' = Ln (Y), &nbsp; &nbsp;for positive X, and Y.     <LI>Y' = Ln (Y), &nbsp; &nbsp;for positive Y.     <LI>Y' = Ln (Y) - Ln (1-Y), &nbsp; &nbsp;for Y positive, less than one.    <LI>Y' = Ln [Y/(100-Y)], &nbsp; &nbsp; known as the <I>logit transformation</I>,   which is useful for the S-shape functions.  <LI>Taking square root of a Poisson <A   href="#rrandomva">random variable</A>, the transformed variable is more symmetric. This is a useful transformation in regression analysis with Poisson observations. It also stabilizes the residual variation.
<p>
<font   color="#DC143C"><b>Box-Cox Transformations:</b></font> The Box-Cox transformation, below, can be applied to a regressor, a combination of regressors, and/or to the dependent variable (y) in a regression. The objective of doing so is usually to make the residuals of the regression more homoskedastic (ie., independently  and identically distributed) and closer to a normal distribution:
<p>
<center>
   (y<FONT FACE="Symbol"><SUP>l</SUP></FONT> - 1) / <FONT FACE="Symbol">l</FONT>
&nbsp; &nbsp;for a constant <FONT FACE="Symbol">l</FONT> not equal to zero, and log(y) &nbsp; &nbsp;for <FONT FACE="Symbol">l</FONT> = 0.
<p>
</center>


 </LI></UL><BLOCKQUOTE>  <P>You might like to use the <A href="otherapplets/Regression.htm"  target=new>Regression Analysis with Diagnostic Tools</A> JavaScript to  check your computations, and to perform some numerical experimentation for a deeper understanding of the concepts. 
<p>
<a name="rconariance" >
</a>
<hr>
<H4><font   color="#DC143C"> Analysis of Covariance: Comparing the Slopes</font></H4>
Consider the following two samples of before-and-after independent treatments.  
<p><table width="451" border="0" align="center" height="185"><tr bgcolor="#FFCCCC" valign="bottom"> 
<td height="13" bordercolor="#333333" align="center" colspan="5"><b><font size="4" face="Arial, Helvetica, sans-serif">Values of Covariate X and a Dependent Variable Y</font></b></td></tr><tr bgcolor="#CCCCCC" valign="bottom"><td height="8" bordercolor="#333333" align="center" colspan="2"><div align="center"><b><font face="Courier New, Courier, mono" color="#000000">Treatment-I</font></b></div>
</td><td height="8" bordercolor="#333333" align="center" colspan="1" width="15" bgcolor="#FFFFFF">&nbsp;</td><td height="8" bordercolor="#333333" align="center" colspan="2"> <div align="center"><b><font face="Courier New, Courier, mono" color="#000000">Treatment-II</font></b></div>    </td></tr><tr bgcolor="#C6D5C6" valign="bottom"><td height="2" bordercolor="#333333" width="110"> 
<div align="center"><i>X</i></div></td><td height="2" bordercolor="#333333" width="93"><div align="center"><i>Y</i></div></td><td height="2" bordercolor="#333333" width="15" bgcolor="#FFFFFF">&nbsp;</td><td height="2" bordercolor="#333333" width="108"><div align="center"><i>X</i></div></td><td height="2" bordercolor="#333333" width="103"><div align="center"><i>Y</i></div></td></tr><tr bordercolor="#000000"><td height="14" bgcolor="#FFFFFF" width="110"><div align="center">5</div></td><td height="14" bgcolor="#FFFFFF" width="93"><div align="center">11</div></td><td height="14" bgcolor="#FFFFFF" width="15"><div align="center"></div>
</td><td height="14" bgcolor="#FFFFFF" width="108"><div align="center">2</div></td><td height="14" bgcolor="#FFFFFF" width="103"><div align="center">1</div></td></tr><tr bordercolor="#000000"> 
<td height="14" bgcolor="#FFFFFF" width="110"><div align="center">3</div></td><td height="14" bgcolor="#FFFFFF" width="93"><div align="center">9</div></td><td height="14" bgcolor="#FFFFFF" width="15"><div align="center"></div></td><td height="14" bgcolor="#FFFFFF" width="108"> 
<div align="center">6</div></td><td height="14" bgcolor="#FFFFFF" width="103"><div align="center">7</div>
</td></tr><tr bordercolor="#000000"><td height="14" bgcolor="#FFFFFF" width="110"><div align="center">1</div></td><td height="14" bgcolor="#FFFFFF" width="93"><div align="center">5</div>
</td><td height="14" bgcolor="#FFFFFF" width="15"><div align="center"></div></td><td height="14" bgcolor="#FFFFFF" width="108"><div align="center">4</div></td><td height="14" bgcolor="#FFFFFF" width="103"><div align="center">3</div></td></tr><tr bordercolor="#000000"><td height="14" bgcolor="#FFFFFF" width="110"><div align="center">4</div></td><td height="14" bgcolor="#FFFFFF" width="93"><div align="center">8</div></td><td height="14" bgcolor="#FFFFFF" width="15"> 
<div align="center"></div></td><td height="14" bgcolor="#FFFFFF" width="108"><div align="center">7</div></td><td height="14" bgcolor="#FFFFFF" width="103"><div align="center">8</div>
</td></tr><tr bordercolor="#000000"><td height="14" bgcolor="#FFFFFF" width="110"><div align="center">6</div></td><td height="14" bgcolor="#FFFFFF" width="93"><div align="center">12</div>
</td><td height="14" bgcolor="#FFFFFF" width="15"><div align="center"></div></td><td height="14" bgcolor="#FFFFFF" width="108"><div align="center">3</div></td><td height="14" bgcolor="#FFFFFF" width="103"><div align="center">2</div></td></tr></table></center><p>
<p>We wish to test the following test of hypothesis on the two means of the dependent variable Y1, and Y2:
<p><B>H<FONT><SUB>0</SUB></FONT>:</B>  The difference between the two means is about a given value M.<br><B>H<FONT><SUB>a</SUB></FONT>:</B>  The difference between the two means is quite different than it is claimed.  <p>Since we are dealing with dependent variables, it's natural to investigate the linear regression coefficients of the two samples; namely, the slopes and the intercepts.<p>Suppose we are interested in testing the equality of two slopes. In other words, we wish to determine if two given lines are statistically parallel.  Let m<FONT><SUB>1</SUB></FONT> represent the regression coefficient for explanatory variable X<FONT><SUB>1</SUB></FONT> in sample 1 with size n<FONT><SUB>1</SUB></FONT>.  Let m<FONT><SUB>2</SUB></FONT> represent the regression coefficient for X<FONT><SUB>2</SUB></FONT> in sample 2 with size n<FONT><SUB>2</SUB></FONT>.    The difference between the two estimated slopes has the following variance:<p><center>V= Var [m<FONT><SUB>1</SUB></FONT> - m<FONT><SUB>2</SUB></FONT>] = {S<FONT size=+0><SUB>xx1</SUB></FONT> <font face=symbol>&#180;</font> S<FONT size=+0><SUB>xx2</SUB></FONT>[(n<FONT><SUB>1</SUB></FONT> -2)S<FONT><SUB>res1</SUB></FONT><FONT><SUP>2</SUP></FONT> + (n<FONT><SUB>2</SUB></FONT> -2)S<FONT><SUB>res2</SUB></FONT><FONT><SUP>2</SUP></FONT>] /[(n<FONT><SUB>1</SUB></FONT> + n<FONT><SUB>2</SUB></FONT> - 4)(S<FONT size=+0><SUB>xx1</SUB></FONT> + S<FONT size=+0><SUB>xx2</SUB></FONT>].  
<p></center>Then, the quantity:<p><CENTER>(m<FONT><SUB>1</SUB></FONT> - m<FONT><SUB>2</SUB></FONT>) / V<FONT><SUP>½</SUP></FONT>
</CENTER><p>has a t-distribution with d.f. = n1 + n2 - 4.
<p><p>This test and its generalization in comparing more than two slopes are called the Analysis of Covariance (ANOCOV).  The ANOCOV test is the same as in the ANOVA test; however there is an additional variable called covariate.   ANOCOV enables us to conduct and to extend the before-and-after test for two different populations. The process is as follows:<p><ol><li>Find a linear model for (X<FONT><SUB>1</SUB></FONT>, Y<FONT><SUB>1</SUB></FONT>) = (before<FONT><SUB>1</SUB></FONT>, after<FONT><SUB>1</SUB></FONT>), and one for (X<FONT><SUB>2</SUB></FONT>, Y<FONT><SUB>2</SUB></FONT>) = (before<FONT><SUB>2</SUB></FONT>, after<FONT><SUB>2</SUB></FONT>) that fit best.<p>
<li>Perform the test of the hypothesis m<FONT><SUB>1</SUB></FONT> = m<FONT><SUB>2</SUB></FONT>. 
<li>If the test result indicates that the slopes are almost equal, then compute the common slope of the two parallel regression lines:<p><center>Slope<FONT><SUB>par</SUB></FONT> = (m<FONT><SUB>1</SUB></FONT>S<FONT size=+0><SUB>xx1</SUB></FONT> + m<FONT><SUB>2</SUB></FONT>S<FONT size=+0><SUB>xx2</SUB></FONT>) / (S<FONT size=+0><SUB>xx1</SUB></FONT> + S<FONT size=+0><SUB>xx2</SUB></FONT>).</center>
<p>The variance of the residuals is: <p><center>S<FONT><SUB>res</SUB></FONT><FONT><SUP>2</SUP></FONT> = [S<FONT size=+0><SUB>yy1</SUB></FONT> + S<FONT size=+0><SUB>yy2</SUB></FONT> - (S<FONT size=+0><SUB>xx1</SUB></FONT> + S<FONT   size=+0><SUB>xx2</SUB></FONT>) Slope<FONT><SUB>par</SUB></FONT>] / ( n<FONT><SUB>1</SUB></FONT> +  n<FONT><SUB>1</SUB></FONT> -3).
</center> <p>
<li>Now, perform the test for the difference between the two the intercepts, which is the vertical difference between the two parallel lines:<p>
<center>Intercepts' difference =<IMG src="ybar.gif"><FONT><SUB>1</SUB></FONT>  -<IMG src="ybar.gif"><FONT><SUB>2</SUB></FONT> - (<IMG src="xbaru.gif"><FONT><SUB>1</SUB></FONT> - <IMG src="xbaru.gif"><FONT><SUB>2</SUB></FONT>) Slope<FONT><SUB>par</SUB></FONT>.
</center><p>The test statistic is:<p><center>(Intercepts' difference) / {S<FONT><SUB>res</SUB></FONT>  [1/n<FONT><SUB>1</SUB></FONT> + 1/n<FONT><SUB>2</SUB></FONT> + (<IMG src="xbaru.gif"><FONT><SUB>1</SUB></FONT> - <IMG src="xbaru.gif"><FONT><SUB>2</SUB></FONT>)<FONT><SUP>2</SUP></FONT>/(S<FONT size=+0><SUB>xx1</SUB></FONT> + S<FONT size=+0><SUB>xx2</SUB></FONT>)]<FONT><SUP>½</SUP></FONT>},<p></center>
which has a t-distribution with parameter d.f. = n<FONT><SUB>1</SUB></FONT> +  n<FONT><SUB>1</SUB></FONT> -3.<p>
Depending on the outcome of the last test, one may reject the null hypothesis.
</ol><P>
For our numerical example, using the <a href="otherapplets/ANOCOV.htm" target= "new">Analysis of Covariance</a> JavaScripts, we obtained the following statistics: <br>
Slope 1 = 1.3513514; its standard error = .2587641<br>
Slope 2 = 1.4883721; its standard error = 1.0793906
<p>
These indicate that there is no evidence against equality of the slopes.  Now, we may test for any differences in the intercepts. Suppose we wish to test the null hypothesis that the vertical distance between the two parallel lines is about 4 units. 
<p>
Using the second function in the <a href="otherapplets/ANOCOV.htm" target= "new">Analysis of Covariance</a> JavaScripts,  we obtained the  statistics: Common Slope = 1.425, Intercept =5.655,  providing a moderate evidence against the null hypothesis.
<P><B>Further Reading:</B><BR> <FONT face="Bookman Old Style" size=-2>Wall F., <I>Statistical Data Analysis Handbook</I>, by  McGraw-Hill, New York, 1986.  </FONT>

<p>
<A name=rApplyRegress></A>     <HR>    <H4><FONT color=#dc143c>
Residential Properties Appraisal Application</FONT></H4>
Estimating the market value of large numbers of residential properties is of interest to a number of socio-economic stakeholders, such as mortgage and insurance companies, banks and real-estate agencies, and investment property companies, etc.  It is both a science and an art. It is a science, because it is based on formal, rigorous and proven methods. It is an art because interaction with socio-economic stakeholders and the methods used give rise to all sorts of tradeoffs and compromises that assessors and their organizations must take into account when making decisions on the basis of their experience and skills.
<p>
The market value assessment of a set of selected houses involves performing an assessment by a few individual appraisers for each property and then computing an average obtained from the few individuals. 
<p>
Individual appraisal refers to the process of estimating the exchange value of a house on the basis of a direct comparison between its profiles and the profiles of a set of other comparable properties sold on acceptable conditions. The profile of a property consists of all the relevant attributes of each house, such as the location, size, gross living space, age, one-story, two-story or more, garage, swimming pool, basement, etc.  Data on prices and characteristics of individual houses are available; e.g., from the U.S Bureau of the Census. 
<p>
Often regression analysis is used to determine what characteristics influence the price of the houses.  Thus it is  important to correct the subjective elements in the appraisal value before carrying out the regression analysis.  Coefficients that are not significantly different from zero as indicated by insignificant t-statistics at a 5% level are dropped from the regression model.
<p>
There are several practical questions to be answered before the actual data collection can take place.
<p>
The first step is to use statistical techniques, such as geographic clustering, to define  <A  href="histograming/topframe.html"   target="new">homogeneous</A> groupings of houses within an urban area.
<p>
How many houses should we look at?   Ideally, one would collect information on as many houses as time and money allow.  It is these practical considerations that make statistics so useful.  Hardly anyone could spend the time, money, and effort needed to look at every house for sale.  It is unrealistic to obtain information on every house of interest, or in statistical terms, on every item in the population.  Thus, we can look only at a sample of houses -- a subset of the population -- and hope that this sample will give us reasonably accurate information about the population.  Let us say we can afford to look at 16 houses.
<p>
We would probably choose to select a simple random sample-that is, a sample in which, roughly speaking, every house in the population has equal chance of being included.  Then we would expect to get a reasonably representative sample of houses throughout this selected size range, reflecting prices for the whole neighborhood.  This sample should give us some information about all houses of all sizes within this range, since a simple random sample tends to select as many larger houses as smaller houses, and as many expensive as less expensive ones. 
<p>
Suppose that the 16 houses in our random sample have the sizes and prices shown in the following Table.   If 160 houses are randomly selected, variables Y, X1, and X2 are random variables.  We have no control over them and cannot know what specific values will be selected.  It is chance only that determines them. 

<p><br> <center><table align=center border=0 width=451>  <tbody>   <tr valign=bottom bgcolor="#FFF2FF"> 
    <td align=middle bordercolor=#333333 colspan=8 height=13><b><font face="Arial, Helvetica, sans-serif" size="2">- <font color="#660066">Sizes, Ages, and Prices of Twenty Houses</font> -</font></b></td>
  </tr><tr valign=bottom> <td bordercolor=#333333 height=37 width=13% bgcolor="#E8E8FF"> 
<div align=center><font face="Arial, Helvetica, sans-serif" size="2" color="#660066">X1 = Size</font></div>
    </td><td bordercolor=#333333 height=37 width=13% bgcolor="#E8E8FF"> <div align=center><font face="Arial, Helvetica, sans-serif" size="2" color="#660066">X2 = Age</font></div> </td><td bordercolor=#333333 height=37 width=13% bgcolor="#E8E8FF"> <div align=center><font face="Arial, Helvetica, sans-serif" size="2" color="#660066">Y = Price</font></div></td><td bordercolor=#333333 height=37 width=6%>&nbsp;</td>
    <td bordercolor=#333333 height=37 width=6%>&nbsp;</td> <td bordercolor=#333333 height=37 width=13% bgcolor="#E8E8FF"> <div align=center><font face="Arial, Helvetica, sans-serif" size="2" color="#660066">X1 
  = Size</font></div> </td><td bordercolor=#333333 height=37 width=13% bgcolor="#E8E8FF"> 
<div align=center><font face="Arial, Helvetica, sans-serif" size="2" color="#660066">X2 = Age</font></div></td>
    <td bordercolor=#333333 height=37 width=13% bgcolor="#E8E8FF"> <div align=center><font face="Arial, Helvetica, sans-serif" size="2" color="#660066">Y = Price</font></div> </td> </tr> <tr bordercolor=#000000> 
    <td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">1.8</font></div> </td> <td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">30</font></div> </td> <td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">32</font></div> </td> <td bgcolor=#ffffff height=14 width=6%>&nbsp;</td> <td bgcolor=#ffffff height=14 width=6%>&nbsp;</td> <td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">2.3</font></div>
    </td><td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">30</font></div> </td><td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">44</font></div></td></tr><tr bordercolor=#000000> <td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">1.0</font></div> </td>
    <td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">33</font></div> </td> <td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">24</font></div> </td><td bgcolor=#ffffff height=14 width=6%>&nbsp;</td>
    <td bgcolor=#ffffff height=14 width=6%>&nbsp;</td><td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">1.4</font></div> </td> <td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">17</font></div>
    </td><td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">27</font></div> </td></tr><tr bordercolor=#000000> <td bgcolor=#ffffff height=14 width=13%> 
      <div align=center><font face="Arial, Helvetica, sans-serif" size="2">1.7</font></div></td> <td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">25</font></div>
    </td> <td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">27</font></div> </td> <td bgcolor=#ffffff height=14 width=6%>&nbsp;</td> <td bgcolor=#ffffff height=14 width=6%>&nbsp;</td><td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">3.3</font></div></td><td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">16</font></div></td><td bgcolor=#ffffff height=14 width=13%> 
      <div align=center><font face="Arial, Helvetica, sans-serif" size="2">50</font></div></td></tr>
  <tr bordercolor=#000000> <td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">1.2</font></div> </td><td bgcolor=#ffffff height=14 width=13%> 
      <div align=center><font face="Arial, Helvetica, sans-serif" size="2">12</font></div> </td>
    <td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">25</font></div> </td> <td bgcolor=#ffffff height=14 width=6%>&nbsp;</td> <td bgcolor=#ffffff height=14 width=6%>&nbsp;</td><td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">2.2</font></div></td> <td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">22</font></div> </td> <td bgcolor=#ffffff height=14 width=13%> 
      <div align=center><font face="Arial, Helvetica, sans-serif" size="2">37</font></div> </td></tr>
  <tr bordercolor=#000000> <td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">2.8</font></div> </td><td bgcolor=#ffffff height=14 width=13%> 
      <div align=center><font face="Arial, Helvetica, sans-serif" size="2">12</font></div> </td><td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">47</font></div>
    </td><td bgcolor=#ffffff height=14 width=6%>&nbsp;</td><td bgcolor=#ffffff height=14 width=6%>&nbsp;</td>
    <td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">1.5</font></div> </td><td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">29</font></div> </td><td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">28</font></div> </td></tr>
  <tr bordercolor=#000000> <td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">1.7</font></div></td> <td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">1</font></div> </td> <td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">30</font></div> </td> <td bgcolor=#ffffff height=14 width=6%>&nbsp;</td> <td bgcolor=#ffffff height=14 width=6%>&nbsp;</td><td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">1.1</font></div> </td> <td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">29</font></div> </td><td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">20</font></div> </td>
  </tr><tr bordercolor=#000000> <td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">2.5</font></div> </td> <td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">12</font></div> </td> <td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">43</font></div>
 </td> <td bgcolor=#ffffff height=14 width=6%>&nbsp;</td><td bgcolor=#ffffff height=14 width=6%>&nbsp;</td>
    <td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">2.0</font></div> </td> <td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">25</font></div> </td> <td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">38</font></div> </td></tr> <tr bordercolor=#000000>  <td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">3.6</font></div> </td> <td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">28</font></div> </td> <td bgcolor=#ffffff height=14 width=13%> 
<div align=center><font face="Arial, Helvetica, sans-serif" size="2">52</font></div> </td> <td bgcolor=#ffffff height=14 width=6%>&nbsp;</td> <td bgcolor=#ffffff height=14 width=6%>&nbsp;</td> <td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">2.6</font></div>
    </td><td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">2</font></div></td> <td bgcolor=#ffffff height=14 width=13%> <div align=center><font face="Arial, Helvetica, sans-serif" size="2">45</font></div></td></tr></tbody> </table></center>    <P>
<br>
<p> 
What can we tell about the relationship between size and price from our sample?   Reading the data from the above table row-wise, and entering them in the <A  href="otherapplets/Regression.htm"  target=new>Regression Analysis with Diagnostic Tools</A> JavaScript, we found the following simple regression model:
<p>
<center>
Price = 9.253 + 12.873(Size)
</center>
<p>
Now consider the problem of estimating the price (Y) of a house from knowing its size (X1) and also its age (X2).  The sizes and prices will be the same as in the simple regression problem.  What we have done is add ages of houses to the existing data.  Note carefully that in real life, one would not first go out and collect data on sizes and prices and then analyze the simple regression problem.  Rather, one would collect all data, which might be pertinent on all twenty houses at the outset.  Then the analysis performed would throw out predictors which turn out not to be needed.
<p>
The objectives in a multiple regression problem are essentially the same as for a simple regression.  While the objectives remain the same, the more predictors we have the calculations and interpretations become more complicated.  For large data set one may use  the multiple regression module of any statistical package such as <a href="http://ubmail.ubalt.edu/~harsham/stat-data/opre330SPSSSAS.htm" target ="new"> SAS and  SPSS</a>. Using the 
<a href="otherapplets/MultRgression.htm" target="new">Multiple Linear Regression</a> JavaScript, for our numerical example with X1 = Size, X2 = Age, and Y = Price, we obtain the following statistical model:

<p>
<center>
Price = 9.959 + 12.800(Size) - 0.027(Age)
<p>
</center>
The regression results suggest that, on average, as the Size of house increases the Price increases. However, the coefficient of the variable Age is significantly small with negative value indicating an inverse relationship. Older houses tend to cost less than newer houses.   Moreover, the correlation between Price and Age is -0.236.  This result indicates that only 6% of variation in price can be accounted by the different in ages of the houses.  This result supports our suspicion that the Age is not a significant predictor of price.  Therefore, the simple regression:
<p>
<center>
Price = 9.253 + 12.873(Size)
<p>
</center>
Now, the question is: Is this model is good enough to satisfy the usual conditions of the regression analysis.

    <P>The following is the list of basic assumptions (i.e., conditions) and the tools to check these necessary conditions. <P></P></BLOCKQUOTE><ol> <LI>Any undetected outliers may have major impact on the regression model. Using the <a href="otherapplets/scatter.htm" target ="new">Determination of the Outliers</a> JavaScript we found that there is no outlier in the above data set.
<P></P><LI>The dependent variable Price is a linear function of the independent variable Size.  By carefully examining the <a href="otherapplets/scatter.htm" target= "new">scatter diagram</a> we found that the linearity condition is satisfied. <P></P><LI>The distribution of the residual must be normal. Reading the data from the above table row-wise, and entering them in the <A  href="otherapplets/Regression.htm"  target=new> Regression Analysis with Diagnostic Tools</A> JavaScript, we found that the normality condition is also satisfied.  <P></P><LI>The residuals should have a mean equal to zero, and a constant standard deviation (i.e., homoskedastic condition). By the <A  href="otherapplets/Regression.htm"  target=new> Regression Analysis with Diagnostic Tools</A> JavaScript, the results are satisfactory. 

<P></P><LI>The residuals constitute a set of random variables. The persistent non-randomness in the residuals violates the best linear unbiased estimator condition. However, since the numerical statistics corresponding to the residuals obtained by using <A  href="otherapplets/Regression.htm"  target=new> Regression Analysis with Diagnostic Tools</A> JavaScript, are not significant, therefore our ordinary least square regression is adequate for our analysis.

<P></P><LI>Durbin-Watson (D-W) statistic quantifies the serial correlation of least-squares errors in its original form. D-W statistic for this model is 1.995, which is good enough in rejecting any serial correlation.
<p>
<li>More Useful Statistics for the Model: The standard errors for the Slope and the Intercept are0.881, and 1.916, respectively, which are small enough. The F-statistic is 213.599, which is large enough indicating that the model is good enough overall for prediction purposes. 
<p>
</ol>
<BLOCKQUOTE>
Notice that since the above analysis is performed on a specific set of data, as always, one must be careful in generalizing its findings.
<P><B>Further Reading:</B><BR>
    <FONT face="Bookman Old Style" size=-2>
Lovell, R., and French, N., Estimated realization price: what do the banks want and what can be realistically provided? <I>Journal of property finance</I>, 6, 7-16, 1995.<br>
Newsome, B.A. and Zeitz, J., 1992. Adjusting comparable sales using multiple regression analysis-the need for segmentation, <I>The Appraisal Journal</I>, 8, 129-135.
</font>
<p>
<A name=runfyviewthests></A>     <HR>    <H4><FONT color=#dc143c>
Introduction to Integrating Statistical Concepts
</FONT></H4>Statistical thinking for decision-making requires a deeper understanding than merely <FONT color=#dc143c>memorizing each isolated technique</font>.  Understanding involves ever expansion of neural network by means of correct connectivity between concepts.  The aim of this chapter is to  look closely at some of the concepts and techniques that we have learned up to now in a unifying theme. The following case studies,  improve your statistical thinking <FONT color=#dc143c>to see the wholeness and manifoldness of statistical tools</font>.
 <P>As you will see, although one would hope that all tests give the same results this is not always the case. It all depends on how  informative the data are and to what extend they have been condensed before presenting them  to you  for analysis (while becoming a good statistician). The following sections are illustrations in examining how much useful information they provide and how they may result in opposite conclusions, if one is not careful enough.
<p>
<A name=rhyptestconf></A>
    <HR>    <H4><FONT color=#dc143c>Hypothesis Testing with Confidence</FONT></H4>  
One of the main advantages of constructing a confidence interval (CI) is to provide a degree of confidence for the point estimate for the population parameter.  Moreover, one may utilize CI for the test of hypothesis purposes. Suppose you wish to test the following general test of hypothesis:
<p>
<B>H<FONT><SUB>0</SUB></FONT>:</B> The population parameter is almost equal to a given claimed value,
<p>
against the alternative:
<p>
<B>H<FONT><SUB>a</SUB></FONT>:</B> The population parameter is not even close to the claimed value.
<p>
The process of executing the above test of hypothesis at <FONT face=symbol>a</FONT> level of significance using CI is as follows:
<p>
<ol>
<li>Ignore the claimed value in the null hypothesis, for the time being.
<li>Construct a 100(1- <FONT face=symbol>a</FONT>)% confidence interval based on the available data.
<li>If the constructed CI does not contain the claimed value, then there is enough evidence to reject the null hypothesis; otherwise, there is no reason to reject the null hypothesis.
</ol>
<p>
You might like to use the <a href="otherapplets/ConfIntPro.htm" target ="new">Hypothesis Testing with Confidence </a> JavaScript applet to  perform some numerical experimentation for validating the above assertions and for a deeper understanding. 
<p>
<p>
<A name=rreganovachi></A>
    <HR>    <H4><FONT color=#dc143c>Regression Analysis, ANOVA, and Chi-square Test</FONT></H4> 
There are close relationships among linear regression, analysis of variance and the Chi-square test.  To illustrate the relationship, consider the following application:
<P> <P><FONT color=#dc143c><B>Relationship between age and income in a given neighborhood:</B></FONT>  A random survey sampling of size 33 individuals in a neighborhood revealed  the following pairs of data. For each pair age is in years and the indicated income is in thousands of dollars:   <P> 
  <table align=center border=0 width=451>
    <tbody> 
    <tr align="center" valign=bottom> 
      <td bordercolor=#333333 colspan=8 height=13 bgcolor="#DDFAFF"><b><font face="Arial, Helvetica, sans-serif" size="2" color="#000066">- 
        Relation between Age and Income($1000) -</font><font face="Arial, Helvetica, sans-serif" size="2"> 
        </font></b></td>
    </tr>
    <tr valign=bottom> 
      <td bordercolor=#333333 height=2 width=15% bgcolor="#EEEEEE"> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif" color="#000066">Age</font></div>
      </td>
      <td bordercolor=#333333 height=2 width=15% bgcolor="#EEEEEE"> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif" color="#000066">Income</font></div>
      </td>
      <td bordercolor=#333333 height=2 width=5%><i></i></td>
      <td bordercolor=#333333 height=2 width=15% bgcolor="#EEEEEE"> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif" color="#000066">Age</font></div>
      </td>
      <td bordercolor=#333333 height=2 width=15% bgcolor="#EEEEEE"> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif" color="#000066">Income</font></div>
      </td>
      <td bordercolor=#333333 height=2 width=5%><i></i></td>
      <td bordercolor=#333333 height=2 width=15% bgcolor="#EEEEEE"> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif" color="#000066">Age</font></div>
      </td>
      <td bordercolor=#333333 height=2 width=15% bgcolor="#EEEEEE"> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif" color="#000066">Income</font></div>
      </td>
    </tr>
    <tr bordercolor=#000000> 
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">20</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">15</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=5%>&nbsp;</td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">42</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">19</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=5%>&nbsp;</td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">61</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">13</font></div>
      </td>
    </tr>
    <tr bordercolor=#000000> 
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">22</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">13</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=5%>&nbsp;</td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">47</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">17</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=5%>&nbsp;</td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">62</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">14</font></div>
      </td>
    </tr>
    <tr bordercolor=#000000> 
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">23</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">17</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=5%>&nbsp;</td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">53</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">13</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=5%>&nbsp;</td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">65</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">9</font></div>
      </td>
    </tr>
    <tr bordercolor=#000000> 
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">28</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">19</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=5%>&nbsp;</td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">55</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">18</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=5%>&nbsp;</td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">67</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">7</font></div>
      </td>
    </tr>
    <tr bordercolor=#000000> 
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">35</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">15</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=5%>&nbsp;</td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">41</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">21</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=5%>&nbsp;</td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">72</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">7</font></div>
      </td>
    </tr>
    <tr bordercolor=#000000> 
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">24</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">21</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=5%>&nbsp;</td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">53</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">39</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=5%>&nbsp;</td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">65</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">22</font></div>
      </td>
    </tr>
    <tr bordercolor=#000000> 
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">26</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">26</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=5%>&nbsp;</td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">57</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">28</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=5%>&nbsp;</td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">65</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">24</font></div>
      </td>
    </tr>
    <tr bordercolor=#000000> 
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">29</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">27</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=5%>&nbsp;</td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">58</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">22</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=5%>&nbsp;</td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">69</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">27</font></div>
      </td>
    </tr>
    <tr bordercolor=#000000> 
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">39</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">31</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=5%>&nbsp;</td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">58</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">29</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=5%>&nbsp;</td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">71</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">22</font></div>
      </td>
    </tr>
    <tr bordercolor=#000000> 
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">31</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">16</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=5%>&nbsp;</td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">46</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">27</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=5%>&nbsp;</td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">69</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">9</font></div>
      </td>
    </tr>
    <tr bordercolor=#000000> 
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">37</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">19</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=5%>&nbsp;</td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">44</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">35</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=5%>&nbsp;</td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">62</font></div>
      </td>
      <td bgcolor=#ffffff height=14 width=15%> 
        <div align=center><font size="2" face="Arial, Helvetica, sans-serif">21</font></div>
      </td>
    </tr>
    </tbody> 
  </table>
    <P>Constructing a <A  href="otherapplets/Regression.htm" 
  target=new>linear regression</A> gives us: <P>  <CENTER> Income = 22.88 - 0.05834 (Age) </CENTER> <P>This suggests a negative relationship; as people get older, they have lower income, on average. Although slope is small, it cannot be considered as zero, since the t-statistic for it is -0.70, which is significant. <P> 
    <P>Now suppose you have only the following <A   href="#rseconprim">secondary data</A>, where the original data have  been condensed:  <P> <DIV align=center> <table align=center border=0 width=339>
      <tbody> 
      <tr valign=bottom> 
        <td align=middle bordercolor=#333333 colspan=5 height=13 bgcolor="#990033"><font face="Arial, Helvetica, sans-serif" size="2"><b><font color="#FFFFFF">- 
          Relation between Age and Income($1000) -</font></b></font></td>
      </tr>
      <tr valign=bottom> 
        <td bordercolor=#333333 height=11 width=33% bgcolor="#FFF0E1"> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2"><font color="#990033">Age 
            ( 29 - 39 ) </font></font></div>
        </td>
        <td bordercolor=#333333 height=11 width=1%>&nbsp;</td>
        <td bordercolor=#333333 height=11 width=33% bgcolor="#FFF0E1"> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2"><font color="#990033">Age 
            ( 40 - 59 )</font></font></div>
        </td>
        <td bordercolor=#333333 height=11 width=1%>&nbsp;</td>
        <td bordercolor=#333333 height=11 width=32% bgcolor="#FFF0E1"> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2"><font color="#990033">Age 
            ( 60 &amp; Over )</font></font></div>
        </td>
      </tr>
      <tr bordercolor=#000000> 
        <td bgcolor=#ffffff height=14 width=33%> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2">15</font></div>
        </td>
        <td height=14 width=1%>&nbsp;</td>
        <td bgcolor=#ffffff height=14 width=33%> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2">19</font></div>
        </td>
        <td height=14 width=1%>&nbsp;</td>
        <td bgcolor=#ffffff height=14 width=32%> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2">13</font></div>
        </td>
      </tr>
      <tr bordercolor=#000000> 
        <td bgcolor=#ffffff height=14 width=33%> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2">13</font></div>
        </td>
        <td height=14 width=1%>&nbsp;</td>
        <td bgcolor=#ffffff height=14 width=33%> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2">17</font></div>
        </td>
        <td height=14 width=1%>&nbsp;</td>
        <td bgcolor=#ffffff height=14 width=32%> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2">14</font></div>
        </td>
      </tr>
      <tr bordercolor=#000000> 
        <td bgcolor=#ffffff height=20 width=33%> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2">17</font></div>
        </td>
        <td height=20 width=1%>&nbsp;</td>
        <td bgcolor=#ffffff height=20 width=33%> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2">13</font></div>
        </td>
        <td height=20 width=1%>&nbsp;</td>
        <td bgcolor=#ffffff height=20 width=32%> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2">9</font></div>
        </td>
      </tr>
      <tr bordercolor=#000000> 
        <td bgcolor=#ffffff height=14 width=33%> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2">21</font></div>
        </td>
        <td height=14 width=1%>&nbsp;</td>
        <td bgcolor=#ffffff height=14 width=33%> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2">21</font></div>
        </td>
        <td height=14 width=1%>&nbsp;</td>
        <td bgcolor=#ffffff height=14 width=32%> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2">7</font></div>
        </td>
      </tr>
      <tr bordercolor=#000000> 
        <td bgcolor=#ffffff height=14 width=33%> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2">15</font></div>
        </td>
        <td height=14 width=1%>&nbsp;</td>
        <td bgcolor=#ffffff height=14 width=33%> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2">39</font></div>
        </td>
        <td height=14 width=1%>&nbsp;</td>
        <td bgcolor=#ffffff height=14 width=32%> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2">21</font></div>
        </td>
      </tr>
      <tr bordercolor=#000000> 
        <td bgcolor=#ffffff height=14 width=33%> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2">26</font></div>
        </td>
        <td height=14 width=1%>&nbsp;</td>
        <td bgcolor=#ffffff height=14 width=33%> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2">28</font></div>
        </td>
        <td height=14 width=1%>&nbsp;</td>
        <td bgcolor=#ffffff height=14 width=32%> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2">24</font></div>
        </td>
      </tr>
      <tr bordercolor=#000000> 
        <td bgcolor=#ffffff height=14 width=33%> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2">27</font></div>
        </td>
        <td height=14 width=1%>&nbsp;</td>
        <td bgcolor=#ffffff height=14 width=33%> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2">22</font></div>
        </td>
        <td height=14 width=1%>&nbsp;</td>
        <td bgcolor=#ffffff height=14 width=32%> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2">27</font></div>
        </td>
      </tr>
      <tr bordercolor=#000000> 
        <td bgcolor=#ffffff height=14 width=33%> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2">31</font></div>
        </td>
        <td height=14 width=1%>&nbsp;</td>
        <td bgcolor=#ffffff height=14 width=33%> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2">26</font></div>
        </td>
        <td height=14 width=1%>&nbsp;</td>
        <td bgcolor=#ffffff height=14 width=32%> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2">22</font></div>
        </td>
      </tr>
      <tr bordercolor=#000000> 
        <td bgcolor=#ffffff height=14 width=33%> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2">16</font></div>
        </td>
        <td height=14 width=1%>&nbsp;</td>
        <td bgcolor=#ffffff height=14 width=33%> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2">27</font></div>
        </td>
        <td height=14 width=1%>&nbsp;</td>
        <td bgcolor=#ffffff height=14 width=32%> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2">9</font></div>
        </td>
      </tr>
      <tr bordercolor=#000000> 
        <td bgcolor=#ffffff height=14 width=33%> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2">19</font></div>
        </td>
        <td height=14 width=1%>&nbsp;</td>
        <td bgcolor=#ffffff height=14 width=33%> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2">35</font></div>
        </td>
        <td height=14 width=1%>&nbsp;</td>
        <td bgcolor=#ffffff height=14 width=32%> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2">22</font></div>
        </td>
      </tr>
      <tr bordercolor=#000000> 
        <td bgcolor=#ffffff height=14 width=33%> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2">19</font></div>
        </td>
        <td height=14 width=1%>&nbsp;</td>
        <td bgcolor=#ffffff height=14 width=33%> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2">18</font></div>
        </td>
        <td height=14 width=1%>&nbsp;</td>
        <td bgcolor=#ffffff height=14 width=32%> 
          <div align=center><font face="Arial, Helvetica, sans-serif" size="2">7</font></div>
        </td>
      </tr>
      </tbody> 
    </table> <P>One may use <A 
  href="otherapplets/ANOVA.htm" 
  target=new>ANOVA</A> in testing that there is no relationship among age and  income. Performing the analysis provides the F-statistic equal to 3.87  which is quite significant; i.e., rejecting the hypothesis of no difference   in population average income for the three age groups.  <P>Now, suppose  more <A 
  href="#rseconprim">condensed  secondary data</A> are provided as in the following table:    <P>&nbsp;</P>
      <TABLE align=center border=0 height=112 width=451>  <TBODY> <TR> 
          <TD align=middle bgColor=#ffcccc borderColor=#333333 colSpan=2 height=14 
      vAlign=bottom><B><FONT face="Arial, Helvetica, sans-serif"  size=3>Relation between Age and Income($1000):</FONT></B></TD></TR>  <TR> <TD align=middle bgColor=#ffffff borderColor=#333333 height=14 
        width=10><I></I></TD> <TD align=middle bgColor=#c6d5c6 borderColor=#333333    height=14><I><B>Age</B></I></TD></TR><TR><TD borderColor=#000000 colSpan=2 height=79> <TABLE align=center border=0 width=455> <TBODY>   <TR align=middle vAlign=bottom> <TD align=left height=22 width=123><I><B>Income</B></I></TD> <TD height=22 width=87><I>20-39</I></TD><TD height=22 width=111><I>40-59</I></TD><TD height=22 width=116><I>60 and over</I></TD>  </TR><TR align=middle vAlign=center>  <TD align=left height=13 noWrap width=123><FONT  face="Times New Roman, Times, serif">Up to $20,000</FONT></TD><TD height=13>7&nbsp; </TD> <TD height=13>4&nbsp;</TD>  <TD height=13>6&nbsp;</TD> </TR> <TR align=middle vAlign=center>   <TD align=left height=2 noWrap width=123>$20,000 and over</TD>   <TD height=2>4&nbsp;</TD> <TD height=2>7&nbsp;</TD>  <TD height=2>5&nbsp;</TD>  </TR>              </TBODY>            </TABLE>          </TD>        </TR>        </TBODY>      </TABLE>    </DIV> <p>One may use the <A   href="otherapplets/Catego.htm"   target=new>Chi-square test</A> for the null hypothesis that age and income are unrelated. The Chi-square statistic is 1.70, which is not significant; therefore there is no reason to believe income and age are related!  But of course, these data are over-condensed, because when all data in the sample were used, there was an observable relationship.
<p><br><A name=rreganovattes></A><HR><H4><FONT color=#dc143c>Regression Analysis, ANOVA, T-test, and Coefficient of Determination</FONT></H4>There are very direct relationships among linear regression, analysis of variance,  t-test and the coefficient of determination.  The following  small data set is for illustrating the connections among the above statistical procedures, and therefore relationships among statistical tables: <p>
<center> <table width="253" border="0">
    <tr>
      <td bgcolor="#00007D"> 
        <table border=0 cellspacing=1 width="253" height="55" cellpadding="1">
          <tr bgcolor="#FFFFFF"> 
            <td width="10%"> 
              <div align="center"><b><font face="Arial, Helvetica, sans-serif" size="2" color="#CC3399">X1</font></b></div>
            </td>
            <td width="9%"> 
              <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">4</font></div>
            </td>
            <td width="9%"> 
              <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">5</font></div>
            </td>
            <td width="9%"> 
              <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">4</font></div>
            </td>
            <td width="9%"> 
              <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">6</font></div>
            </td>
            <td width="9%"> 
              <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">7</font></div>
            </td>
            <td width="9%"> 
              <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">7</font></div>
            </td>
            <td width="9%"> 
              <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">8 
                </font></div>
            </td>
            <td width="9%"> 
              <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">9</font></div>
            </td>
            <td width="9%"> 
              <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">9</font></div>
            </td>
            <td width="9%"> 
              <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">11</font></div>
            </td>
          </tr>
          <tr bgcolor="#FFFFFF"> 
            <td width="10%"> 
              <div align="center"><b><font face="Arial, Helvetica, sans-serif" size="2" color="#336633">X2</font></b></div>
            </td>
            <td width="9%"> 
              <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">8</font></div>
            </td>
            <td width="9%"> 
              <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">6</font></div>
            </td>
            <td width="9%"> 
              <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">8</font></div>
            </td>
            <td width="9%"> 
              <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">10</font></div>
            </td>
            <td width="9%"> 
              <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">10</font></div>
            </td>
            <td width="9%"> 
              <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">11</font></div>
            </td>
            <td width="9%"> 
              <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">13 
                </font></div>
            </td>
            <td width="9%"> 
              <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">14</font></div>
            </td>
            <td width="9%"> 
              <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">14</font></div>
            </td>
            <td width="9%"> 
              <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">16</font></div>
            </td>
          </tr>
        </table>
      </td>
    </tr>
  </table></center><p>Suppose we apply the <a href="otherapplets/TwoPopTest.htm" target ="new">t-test</a>.  The statistic is  t = 3.207, with d.f. = 18.  The p-value is 0.003 indicating a very strong evidence against the null hypothesis.<p>Now, by introducing a dummy variable x with two values, say 0 and 1, representing the two data sets,  respectively, we are able to apply <a href="otherapplets/Regression.htm" target ="new">regression analysis</a>:  <p><center>
<table width="268" border="0" align="center">
  <tr>
    <td bgcolor="#DDBB99"> 
      <table border=0 cellspacing=1 align="center" width="268" height="123" cellpadding="1">
        <tr bgcolor="#FFFFFF"> 
          <td width="9%"> 
            <div align="center"><font color="#8D5289"><b><font face="Arial, Helvetica, sans-serif" size="2">x</font></b></font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">0</font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">0</font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">0</font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">0</font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">0</font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">0</font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">0 
              </font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">0</font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">0</font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">0</font></div>
          </td>
        </tr>
        <tr bgcolor="#FFFFFF"> 
          <td width="9%"> 
            <div align="center"><font color="#8D5289"><b><font face="Arial, Helvetica, sans-serif" size="2">y</font></b></font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">4</font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">5</font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">4</font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">6</font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">7</font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">7</font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">8 
              </font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">9</font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">9</font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">11</font></div>
          </td>
        </tr>
        <tr bgcolor="#ffffff" > 
		
          <td  bgcolor="#ffffff" colspan="11" ></td>
        </tr>
        <tr bgcolor="#FFFFFF"> 
          <td width="9%"> 
            <div align="center"><font color="#351EA2"><b><font face="Arial, Helvetica, sans-serif" size="2">x</font></b></font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">1</font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">1</font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">1</font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">1</font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">1</font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">1</font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">1 
              </font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">1</font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">1</font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">1</font></div>
          </td>
        </tr>
        <tr bgcolor="#FFFFFF"> 
          <td width="9%"> 
            <div align="center"><font color="#351EA2"><b><font face="Arial, Helvetica, sans-serif" size="2">y</font></b></font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">8</font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">6</font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">8</font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">10</font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">10</font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">11</font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">13 
              </font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">14</font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">14</font></div>
          </td>
          <td width="9%"> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">16</font></div>
          </td>
        </tr>
      </table>
    </td>
  </tr>
</table></center>
<p>Among other statistics, we obtain a large slope = m = 4 <font face=symbol>&#185;</font> 0, indicating the rejection of the null hypothesis.  Notice that, the t-statistic for the slope is: t-statistic = slope/(its standard error) = 4/ 1.2472191 =  3.207, which is the t-statistic we obtained from the t-test. In general, the square of t-statistic of the slope is the F-statistic in the ANOVA table;  i.e.,<p><center>t<FONT><SUB>m</SUB></FONT><FONT><SUP>2</SUP></FONT> = F-statistic</center>
<p>Moreover, the <a href="#rcorrIationCovar">coefficient of determination</a> r <FONT><SUP>2</SUP></FONT> = 0.36, which is always obtainable from the t-test, as follows: 
<p><center>r<FONT><SUP>2</SUP></FONT>  = t <FONT><SUP>2</SUP></FONT> / (t <FONT><SUP>2</SUP></FONT> + d.f.).</center><p>
For our numerical example, the r <FONT><SUP>2</SUP></FONT> is (3.207) <FONT><SUP>2</SUP></FONT> / [(3.207) <FONT><SUP>2</SUP></FONT> + 18] = 0.36, as expected.<p>
Now, applying <a href="#rANOVA">ANOVA</a> on the two sets of data, we obtain the F-statistic = 10.285, with d.f.<FONT><SUB>1</SUB></FONT> = 1, and d.f.<FONT><SUB>2</SUB></FONT> = 18. The F-statistic is not large enough; therefore, one must reject the null hypothesis.  Note that, in general, <p>
<center> F <FONT FACE="symbol"><SUB>a</SUB></FONT> <FONT><SUB>, (1, n)</SUB></FONT> &nbsp;= &nbsp; t <FONT><SUP>2</SUP></FONT> <FONT FACE="symbol"><SUB>a/2</SUB></FONT> <FONT><SUB>, n</SUB></FONT>.<p></center>For our numerical example, F = t <FONT><SUP>2</SUP></FONT> = (3.207) <FONT><SUP>2</SUP></FONT> = 10.285, as expected.<p>As expected, by just looking at the data, all three tests indicate strongly that the means of the two sets are quite different. 
<p>   
 <P><A name=rradast></A>   <HR>
<H4><STRONG><FONT color=#dc143c>Relationships among Distributions and Unification of Statistical Tables</FONT></STRONG></H4> Particular attention must be paid to a first course in statistics. When I first  began studying statistics, it bothered me that there were different tables for  different tests. It took me a while to learn that this is not as haphazard as it appeared. Binomial, Normal, Chi-square, t, and F distributions that you will learn are actually closely connected. <P>A problem with elementary statistical textbooks is that they not only don't provide information of this kind, to permit a useful understanding of the  principles involved, but they usually don't provide these conceptual links.  If you want to understand connections between statistical concepts, then you  should practice making these connections. Learning by doing statistics  lends itself to active rather than passive learning. Statistics is a highly interrelated set of concepts, and to be successful at it, you must learn to make these links conscious in your mind. <P>Students often ask: Why T- table values with d.f. = 1 are much larger compared  with other d.f. values? Some tables are limited. What should I do when the sample size is too large?, How can I become familiar with tables and their differences. Is there any type of integration among tables? Is there any connection  between test of hypotheses and confidence interval under different scenarios? For example, testing with respect to one, two, more than two populations, and so on.  <P>The following Figure demonstrates useful relationships among distributions  and a unification of statistical tables: <P><P><CENTER><a href="relationship.jpe"><IMG alt="Unification of common statistical tables" src="relationship.jpe" width="179" height="137" border="0"></a> <P><p>A Unification of Common Statistical Tables<br><FONT size=+0><b><font color="#dc143c">Click on the image to enlarge it and THEN print it</font></b></FONT></CENTER><P>For example, the following are some nice connections between major tables:   <P></P></BLOCKQUOTE>
<UL>  <LI>Standard normal z and F-statistics: F = z<FONT  size=+0><SUP>2</SUP></FONT>, where F has (d.f.<FONT   size=+0><SUB>1</SUB></FONT> = 1, and d.f.<FONT size=+0><SUB>2</SUB></FONT> is 
    the largest available in the F-table) <P></P><LI>T- statistic and F-statistic: F = t<FONT size=+0><SUP>2</SUP></FONT>, where F has (d.f.<FONT size=+0><SUB>1</SUB></FONT> = 1, and d.f.<FONT 
  size=+0><SUB>2</SUB></FONT> = d.f. of the t-table) <P></P><LI>Chi-square and F-statistics: F = Chi-square/d.f.<FONT size=+0><SUB>1</SUB></FONT>, where F has (d.f.<FONT size=+0><SUB>1</SUB></FONT> = d.f. of the Chi-square-table, and d.f.<FONT size=+0><SUB>2</SUB></FONT> is the largest available in the F-table) 
<P></P><LI>T-statistic and Chi-square: (Chi-square)<FONT    size=+0><SUP>½</SUP></FONT> = t,    where Chi-square has d.f.=1, and t has d.f. = <font face=symbol>&#165;</font>.
<P></P><LI>Standard normal z and T-statistic: z = t, where  t  has d.f. = <font face=symbol>&#165;</font>.
<P></P><LI>Standard normal z and Chi-square: (2 Chi-square)<FONT    size=+0><SUP>½</SUP></FONT> - (2d.f.-1)<FONT size=+0><SUP>½</SUP></FONT> = z,    where d.f. is the largest available in the Chi-square table).  <P></P><LI>Standard normal z, Chi-square, and  T- statistic: z/[Chi-aquare/n)<FONT   size=+0><SUP>½</SUP></FONT> = t with d.f. = n. <P></P><LI>F-statistics and its Inverse: F<FONT face=symbol><SUB>a</SUB></FONT>(n1, n2) = 1/F<FONT face=symbol><SUB>1-a</SUB></FONT>(n2, n1), therefore it is only necessary to tabulate, say the upper tail probabilities. </LI>
<p><li>Correlation coefficient r and T-statistic:   t =  [r(n-2)<FONT size=+0><SUP>½</SUP></FONT>]/[1 - r<FONT size=+0><SUP>2</SUP></FONT>]<FONT size=+0><SUP>½</SUP></FONT>.
<p>
<FONT color=#dc143c><b>Transformation of Some Inferential Statistics to the Standard normal Z:</b></FONT>
<p>
<li>For the t(df):  Z = {df <font face=symbol>&#180;</font> Ln[1 + (t<FONT size=+0><SUP>2</SUP></FONT>/df)]}<FONT size=+0><SUP>½</SUP></FONT> <font face=symbol>&#180;</font> {1 - [1/(2df)]}<FONT size=+0><SUP>½</SUP></FONT>.
<p><li>For the F(1,df): Z = {df <font face=symbol>&#180;</font> Ln[1 + (F/df)]}<FONT size=+0><SUP>½</SUP></FONT> <font face=symbol>&#180;</font> {1 - [1/(2df)]}<FONT size=+0><SUP>½</SUP></FONT>,
<p>
 where Ln is the natural logarithm. </UL>
<BLOCKQUOTE>
<p>
Visit also the <a href="http://ubmail.ubalt.edu/~harsham/stat-data/opre330.htm#rradast" target ="new">Relationships among Common Distributions</a>.
<p>
You may like using the statistical tables at the back of your book and/or <a href="otherapplets/pvalues.htm" target ="new">P-values</a> JavaScript in performing some numerical experimentation for validating the above relationships for a deeper understanding of the concepts. You might need to use a <a href="otherapplets/scientificCal.htm" target ="new">scientific calculator</a>, too.
<p>
<B>Further Reading:</B><BR><FONT face="Bookman Old Style" size=-2>Kagan. A., What students can learn  from tables of basic distributions,<I> Int. Journal of Mathematical Education in Science and Technology</I>, 30(6), 1999. </FONT> <P>
 <P>   <A name=rAnalVisualStat>    <HR>  </A> 
      <H4><FONT color=#dc143c>Introduction to Visualization of Statistics</FONT></H4>    Most of statistical data processing involves algebraic operations on the  dataset. However, if the dataset contains more than 3 numbers, it is not possible to visualize it by geometric representation, mainly due to human sensory limitation. Geometry has a much longer history than algebra. Ancient Greeks applied geometry to  <I>measure land</I>, and developed the <I>geo-metric</I> models. The <I>analytic-geometry</I> is to find <I>equivalency between algebra and geometry</I>. The aim is a better understanding by visualization in 2-or-3 dimensional space, and to generalize the ideas for higher dimensions by analytic thinking.  <P>Without the loss of generality, and conserving space, the following presentation is in the context of small sample size, allowing us to see statistics in 1, or 2-dimensional space.  
<p>
<P><A name=rneanmedian></A>     <HR>    <H4><FONT color=#dc143c>The Mean and The Median</FONT></H4> 
Suppose that four people want to get together to play poker. They live on 1<FONT size=+0><SUP>st</SUP></FONT> Street, 3<FONT 
  size=+0><SUP>rd</SUP></FONT> Street, 7<FONT size=+0><SUP>th</SUP></FONT> Street, 
 and 15<FONT size=+0><SUP>th</SUP></FONT> Street. They want to select a house that involves the minimum amount of driving for all parties concerned. <P>Let's suppose that they decide to minimize the absolute amount of driving. If they met at 1<FONT size=+0><SUP>st</SUP></FONT> Street, the amount of driving would be 0 + 2 + 6 + 14  = 22 blocks. If they met at 3<FONT size=+0><SUP>rd</SUP></FONT> Street, the amount of driving would be 2 + 0+ 4 + 12 = 18 blocks. If they met at 7<FONT size=+0><SUP>th</SUP></FONT> Street, 6 + 4 + 0 + 8 = 18 blocks. Finally, at 15<FONT size=+0><SUP>th</SUP></FONT> Street, 14 + 12 + 8 + 0 = 34 blocks. <P>So the two houses that would minimize the amount of driving would be 3<FONT size=+0><SUP>rd</SUP></FONT> or 7<FONT size=+0><SUP>th</SUP></FONT> Street.  Actually, if they wanted a neutral site, any place on 4<FONT 
  size=+0><SUP>th</SUP></FONT>, 5<FONT size=+0><SUP>th</SUP></FONT>, or 6<FONT 
  size=+0><SUP>th</SUP></FONT> Street would also work. <P>Note that any value between 3 and 7 could be defined as the median of 1, 3, 7, and 15. So the median is the value that minimizes the absolute distance  to the data points.  <P>Now, the person at 15<FONT size=+0><SUP>th</SUP></FONT> is upset at always having to do more driving. So the group agrees to consider a different rule. In deciding to minimize the square of the distance driving, we are using the least square principle.  By squaring,  we give more weight to a single very long commute than to a bunch of shorter commutes. With this rule, the 7<FONT size=+0><SUP>th</SUP></FONT> Street house (36 + 16 + 0 + 64 = 116 square blocks) is preferred to the 3<FONT size=+0><SUP>rd</SUP></FONT> Street house (4 + 0 + 16 + 144 = 164 square blocks). If you consider any location, and not just the houses themselves, then 9<FONT size=+0><SUP>th</SUP></FONT> Street is the location that minimizes the square of the distances driven.  <P>Find the value of x that minimizes: 
<p>
<center>
(1 - x)<FONT  size=+0><SUP>2</SUP></FONT> + (3 - x)<FONT size=+0><SUP>2</SUP></FONT> +(7 - 
      x)<FONT size=+0><SUP>2</SUP></FONT> + (15 - x)<FONT   size=+0><SUP>2</SUP></FONT>. 
</center>    
<P>The value that minimizes the sum of squared values is 6.5, which is also equal to the arithmetic mean of 1, 3, 7, and 15. With calculus, it's easy to show that this holds in general.  <P>Consider a small sample of scores with an even number of cases; for example,  1, 2, 4, 7, 10, and 12. The median is 5.5, the midpoint of the interval  between the scores of 4 and 7. <P>As we discussed above, it is true that the median is a point around which  the sum of absolute deviations is minimized. In this example the sum of absolute deviations is 22. However, <FONT color=#dc143c>it is not a unique point</FONT>. Any point in the 4 to 7 region will have the same value of 22 for the sum of the absolute deviations. <P>Indeed, medians are tricky. The 50% above -- 50% below is not quite correct.  For example, 1, 1, 1, 1, 1, 1, 8 has no median. The convention says that, 
 the median is 1; however, about 14% of the data lie strictly above it; 100% of the data are greater than or equal to the median.  <P>We will make use of this idea in regression analysis. In an analogous argument, the regression line is a unique line, which minimizes the sum of the squared deviations from it. There is no unique line that minimizes the sum of the absolute deviations from it.

<P><A name=rartandgeo></A>     <HR>    <H4><FONT color=#dc143c>Arithmetic and  Geometric Means</FONT></H4> 
<p>
<FONT color=#dc143c><b>Arithmetic Mean:</b></font> Suppose you have two data points x and y, on real number- line axis:
<p>
<center>
<img src="linescale.jpg">  
</center>
<p>
The arithmetic mean (a) is a point such that the following <B>vectorial relation</B> holds: ox - oa = oa - oy.
<p>
<FONT color=#dc143c><b>Geometric Mean:</b></font> Suppose you have two positive data points x and y, on the above  real number- line axis, then the <A href="#rspecialmean">Geometric Mean</A> (g) of these numbers is a point g such that |ox| / |og| = |og| / |oy|, where |ox| means the <B>length of line segment</B> ox, for example.
  
<P><A name=rvarcovcoef></A>     <HR>    <H4><FONT color=#dc143c>Variance, Covariance, and Correlation Coefficient </FONT></H4> 
Consider a data set containing n = 2 observations (5, 1). Upon centralizing  the data, one obtains the vector V1 = (5-3 = 2, 1-3 = -2), as shown in  the following n = 2 dimensional coordinate system:       <P> 
      <CENTER>        <IMG alt="Seeing Statistics" 
  src="STAT_GEO.gif"> 
        <P>      </CENTER>      <P>Notice that the vector V1 length is:       <P> 
        <CENTER>          |V1| = [(2)<FONT size=+0><SUP>2</SUP></FONT> + (-2)<FONT 
  size=+0><SUP>2</SUP></FONT>]<FONT size=+0><SUP>½</SUP></FONT> = 8<FONT 
  size=+0><SUP>½</SUP></FONT>         </CENTER>      <P>         <CENTER>        </CENTER>
        The variance of V1 is:       <P>         <CENTER>          Var(V1) = <FONT face=symbol>S</FONT> X<FONT 
  size=+0><SUB>i</SUB></FONT><FONT size=+0><SUP>2</SUP></FONT>/ n = |V1|<FONT 
  size=+0><SUP>2</SUP></FONT>/n = 4         </CENTER>      <P>         <CENTER>        </CENTER>
        The standard deviation is:      <P>         <CENTER>          |OS1| = |V1| / n<FONT size=+0><SUP>½</SUP></FONT> = 8<FONT   size=+0><SUP>½</SUP></FONT> / 2<FONT size=+0><SUP>½</SUP></FONT> = 2.        </CENTER>      <P>         <CENTER>        </CENTER>    Now, consider a second observation (2, 4). Similarly, it can be represented by vector V2 = (-1, 1).       <P>The covariance is, 
      <P>  <CENTER>     Cov (V1, V2) = the dot product / n = [(2)(-1) + (-2)(1)]/2 = -4/2 =  -2  </CENTER>      <P> 
   Therefore:       <P>     <CENTER>n Cov (V1, V2) = the dot product of the two vectors V1, and V2      </CENTER>
      <P>  Notice that the dot-product is multiplication of the two lengths times  the cosine of the angle between the two vectors. Therefore,      <P>      <CENTER>    Cov (V1, V2) = |OS1| <font face=symbol>&#180;</font> |OS2| <font face=symbol>&#180;</font>  Cos (V1, V2) = (2) (1) Cos(180<FONT   size=+0><SUP>°</SUP></FONT>) = -2     </CENTER>      <P>  The correlation coefficient is therefore:      <P>    <CENTER>        <FONT face=symbol>r</FONT> = Cos (V1, V2)      </CENTER>  <P> 
   This is possibly the simplest proof that the correlation coefficient is always bounded by the interval [-1, 1]. The correlation coefficient for our numerical  example is Cos (V1, V2) = Cos(180<FONT   size=+0><SUP>°</SUP></FONT>) = -1, as expected from the above figure.  <P>The distance between the two-point data sets V1, and V2 is also a dot-product:       <P>         <CENTER>   |V1 - V2| = (V1-V2) <B>.</B> (V1-V2) = |V1|<FONT   size=+0><SUP>2</SUP></FONT> + |V2|<FONT size=+0><SUP>2</SUP></FONT> - 2 |V1| <font face=symbol>&#180;</font> |V2 |<BR>
          = n[Var(V1) + VarV2 - 2Cov(V1, V2)]         </CENTER>      <P>     Now, construct a matrix whose columns are the coordinates of the two vectors  V1 and V2, respectively. Multiplying the transpose of this matrix by itself provides a new symmetric matrix containing n times the variance of V1 and  variance of  V2 as its main diagonal elements (i.e., 8, 2), and n times Cov (V1, V2) as its off diagonal element (i.e., -4).    
<P>You might like to use a <A  href="http://search.officeupdate.microsoft.com/TemplateGallery/ct146.asp"   target=new>graph paper</A>, and a <A   href="otherapplets/scientificCal.htm"  target=new>scientific calculator</A> to check the results of these numerical examples and to perform some additional numerical experimentation for a deeper understanding of the concepts. 
<P><B>Further Reading:</B><BR> <FONT face="Bookman Old Style" size=-2>Wickens T., <I>The Geometry of  Multivariate Statistics</I>, Erlbaum Pub., 1995. </FONT>
 <P><A name=rindexnum></A>     <HR>    <H4><FONT color=#dc143c>Index Numbers with Applications</FONT></H4>    When facing a lack of a unit of measure, we often use indicators as surrogates  for direct measurement. For example, the height of a column of mercury is a familiar indicator of temperature. No one presumes that the height of mercury  column constitutes temperature in quite the same sense that length constitutes  the number of centimeters from end to end. However, the height of a column of mercury is a dependable correlate of temperature and thus serves as a useful measure of it. Therefore, and indicator is an accessible and dependable correlate of a dimension of interest; that correlate is used as a measure of that dimension   <FONT color=#dc143c>because direct measurement of the dimension is not possible  or practical</FONT>. In like manner index numbers serve as surrogate for actual data.     <P>The primary purposes of an index number are to provide a value useful for   comparing magnitudes of aggregates of related variables to each other, and  to measure the changes in these magnitudes over time. Consequently, many   different index numbers have been developed for special use. There are a  number of particularly well-known ones, some of which are announced on public   media every day.  Government agencies often report time series  data in the form of index numbers. For example, the consumer price index  is an important economic indicator. Therefore, it is useful to understand  how index numbers are constructed and how to interpret them. These index  numbers are developed usually starting with base 100 that indicates a change   in magnitude relative to its value at a specified point in time. <P>For example, in determining the cost of living, the Bureau of Labor Statistics   (BLS) first identifies a "market basket" of goods and services the typical  consumer buys. Annually, the BLS surveys consumers to determine what they  buy and the overall cost of the goods and services they buy: What, where,  and how much.  The Consumer Price Index (CPI) is used to monitor changes  in the cost of living (i.e. the selected market basket) over time. When  the CPI rises, the typical family has to spend more dollars to maintain  the same standard of living. The goal of the CPI is to measure changes in  the cost of living. It reports the movement of prices, not in dollar amounts,  but with an index number.    <HR>   
<P><A name=rgeomean></A>  <H4><FONT color=#dc143c>The Geometric Mean</FONT></H4>The <A href="#rspecialmean">Geometric Means</A> are used extensively by the U.S. Bureau of Labor Statistics, "Geomeans" as they call them, in the computation of the U.S. Consumer Price Index.  The geomeans are also used in price indexes<p><P><A name=rratioindex> </A>     <HR>    <H4><FONT color=#dc143c>Ratio Index Numbers</FONT></H4> The following provides the computational procedures with applications for some  Index numbers, including  the Ratio Index, and Composite Index numbers.  <p>Suppose we are interested in  the labor utilization of two manufacturing plants A and B with the unit  outputs and man/hours, as shown in the following table, together with the  national standard over the last three months:  <p><br><p><table align=center border=0 width=420 height="144">
        <tbody> 
        <tr align=middle> 
          <td height=22 width=9%><font size="2" face="Arial, Helvetica, sans-serif"></font></td>
          <td width="1%"></td>
          <td align=right bgcolor=#D1D58E colspan=2 height=22> 
            <div align=center><font face="Arial, Helvetica, sans-serif" size="2" color="#660066"><b><font color="#FFFFFF">Plant 
              Type - A</font></b></font></div>
          </td>
          <td width="1%"></td>
          <td align=right bgcolor=#9ECFCF colspan=3 height=22> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#000066"><b><font color="#FFFFFF">Plant 
              Type - B</font></b></font> </div>
          </td>
        </tr>
        <tr align=middle> 
          <td width=9%> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2"><b><font color="#333333">Months</font></b></font></div>
          </td>
          <td align=middle width=1%>&nbsp;</td>
          <td align=middle width=19%> 
            <div align=center><font size="2" face="Arial, Helvetica, sans-serif" color="#A0A050"><b>Unit 
              Output</b></font></div>
          </td>
          <td align=middle width=19%> 
            <div align=center><font size="2" face="Arial, Helvetica, sans-serif" color="#A0A050"><b>Man 
              Hours</b></font></div>
          </td>
          <td align=middle width=1%>&nbsp;</td>
          <td align=middle width=19%> 
            <div align=center><font size="2" face="Arial, Helvetica, sans-serif" color="#4B9696"><b>Unit 
              Output</b></font></div>
          </td>
          <td align=middle width=19%> 
            <div align=center><font size="2" face="Arial, Helvetica, sans-serif" color="#4B9696"><b>Man 
              Hours</b></font></div>
          </td>
        </tr>
        <tr align=middle> 
          <td width=9%> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">1</font></div>
          </td>
          <td width=1%>&nbsp;</td>
          <td width=19%> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">0283</font></div>
          </td>
          <td width=19%> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">200000</font></div>
          </td>
          <td width=1%>&nbsp;</td>
          <td width=19%> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">11315</font></div>
          </td>
          <td width=19%> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">680000</font></div>
          </td>
        </tr>
        <tr align=middle> 
          <td width=9%> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" ize="2" size="2">2</font></div>
          </td>
          <td width=1%>&nbsp;</td>
          <td width=19%> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">0760</font></div>
          </td>
          <td width=19%> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">300000</font></div>
          </td>
          <td width=1%>&nbsp;</td>
          <td width=19%> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">12470</font></div>
          </td>
          <td width=19%> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">720000</font></div>
          </td>
        </tr>
        <tr align=middle> 
          <td width=9%> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">3</font></div>
          </td>
          <td width=1%>&nbsp;</td>
          <td width=19%> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">1195</font></div>
          </td>
          <td width=19%> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">530000</font></div>
          </td>
          <td width=1%>&nbsp;</td>
          <td width=19%> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">13395</font></div>
          </td>
          <td width=19%> 
            <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">750000</font></div>
          </td>
        </tr>
        <tr align=middle> 
          <td width=9%> 
            <div align=center><font face="Arial, Helvetica, sans-serif" size="2"><b><font color="#333333">Standard</font></b></font></div>
          </td>
          <td width=1%>&nbsp;</td>
          <td width=19%> 
            <div align=center><font color="#A0A050"><b><font face="Arial, Helvetica, sans-serif" size="2">4000</font></b></font></div>
          </td>
          <td width=19%> 
            <div align=center><font color="#A0A050"><b><font face="Arial, Helvetica, sans-serif" size="2">600000</font></b></font></div>
          </td>
          <td width=1%>&nbsp;</td>
          <td width=19%> 
            <div align=center><font color="#4B9696"><b><font face="Arial, Helvetica, sans-serif" size="2">16000</font></b></font></div>
          </td>
          <td width=19%> 
            <div align=center><font color="#4B9696"><b><font face="Arial, Helvetica, sans-serif" size="2">800000</font></b></font></div>
          </td>
        </tr>
        </tbody> 
      </table>
      <P>   <P>The labor utilization for the Plant A in the first month is: <P> <CENTER>L<FONT size=+0><SUB>A,1</SUB></FONT> = [(200000/283)] / [(600000/4000)]  = 4.69     </CENTER> <P>Similarly,   <P>         <CENTER>  L<FONT size=+0><SUB>B,3</SUB></FONT> = 53.59/50 = 1.07.  </CENTER>      <P>Upon computing the labor utilization for both plants for each month,   one can present the results by graphing the labor utilization over time  for comparative studies.
<P><A name=rcompindex></A>     <HR>    <H4><FONT color=#dc143c>Composite Index Numbers </FONT></H4>
 Consider the total labor, and material cost for two consecutive years for an industrial plant, as shown in the following   table: 
      <P>      <CENTER>       <TABLE align=center border=0 width=545>          <TBODY>           <TR align=middle> 
   <TD height=22 width=89>&nbsp;</TD>    <TD height=22 width=83>&nbsp;</TD> <TD align=right colSpan=2 height=22>   <DIV align=center><B><FONT face="Arial, Helvetica, sans-serif">Year 2000</FONT></B></DIV>    </TD>  <TD align=right colSpan=2 height=22> <DIV align=center><B><FONT face="Arial, Helvetica, sans-serif">Year 2001</FONT></B></DIV> </TD>   </TR> <TR align=middle>   <TD bgColor=#cccccc width=89>&nbsp;</TD> <TD bgColor=#cccccc noWrap width=83><FONT  face="Arial, Helvetica, sans-serif"><FONT size=2>Unit Needed </FONT></FONT></TD>     <TD align=middle bgColor=#99cccc noWrap width=77>  <DIV align=center><FONT face="Arial, Helvetica, sans-serif" size=2>Unit Cost</FONT></DIV> </TD> <TD align=middle bgColor=#99cccc noWrap width=88>  <DIV align=center><FONT face="Arial, Helvetica, sans-serif"  size=2>Total</FONT></DIV></TD><TD align=middle bgColor=#cc99cc noWrap width=98> <DIV align=center><FONT face="Arial, Helvetica, sans-serif" size=2>Unit Cost</FONT></DIV> </TD> <TD align=middle bgColor=#cc99cc noWrap width=84>  <DIV align=center><FONT face="Arial, Helvetica, sans-serif"  size=2>Total</FONT></DIV></TD> </TR> <TR align=middle>  <TD width=89>  <DIV align=left><FONT face="Arial, Helvetica, sans-serif"         size=2>Labor</FONT></DIV>   </TD> <TD width=83><FONT face="Arial, Helvetica, sans-serif"       size=2>20</FONT></TD> <TD width=77>  <DIV align=center><FONT face="Arial, Helvetica, sans-serif"         size=2>10</FONT></DIV> </TD> <TD width=88> <DIV align=center><FONT face="Arial, Helvetica, sans-serif"  size=2>200</FONT></DIV> </TD> <TD width=98><FONT face="Arial, Helvetica, sans-serif" 
     size=2>11</FONT></TD> <TD width=84><FONT face="Arial, Helvetica, sans-serif" 
      size=2>220</FONT></TD>  </TR> <TR align=middle>  <TD width=89>  <DIV align=left><FONT face="Arial, Helvetica, sans-serif"  size=2>Almunium</FONT></DIV> </TD> <TD width=83><FONT face="Arial, Helvetica, sans-serif"  size=2>02</FONT></TD> <TD width=77>  <DIV align=center><FONT face="Arial, Helvetica, sans-serif"  size=2>100</FONT></DIV>      </TD>     <TD width=88>  <DIV align=center><FONT face="Arial, Helvetica, sans-serif"   size=2>200</FONT></DIV>  </TD> <TD width=98><FONT face="Arial, Helvetica, sans-serif" 
  size=2>110</FONT></TD><TD width=84><FONT face="Arial, Helvetica, sans-serif"  size=2>220</FONT></TD>
  </TR>          <TR align=middle>  <TD width=89> <DIV align=left><FONT face="Arial, Helvetica, sans-serif" 
        size=2>Electricity</FONT></DIV> </TD> <TD width=83><FONT face="Arial, Helvetica, sans-serif" 
      size=2>02</FONT></TD> <TD width=77>  <DIV align=center><FONT face="Arial, Helvetica, sans-serif" 
        size=2>50</FONT></DIV></TD> <TD width=88> <DIV align=center><FONT face="Arial, Helvetica, sans-serif"         size=2>100</FONT></DIV> </TD><TD width=98><FONT face="Arial, Helvetica, sans-serif" 
      size=2>60</FONT></TD> <TD width=84><FONT face="Arial, Helvetica, sans-serif"  size=2>120</FONT></TD>
  </TR> <TR align=middle> <TD bgColor=#cccccc width=89>  <DIV align=left><FONT face="Arial, Helvetica, sans-serif"  size=2><B>Total</B></FONT></DIV>  </TD><TD bgColor=#cccccc width=83>  <DIV align=center><FONT face="Arial, Helvetica, sans-serif"  size=2></FONT></DIV>  </TD> <TD bgColor=#99cccc width=77> 
<DIV align=center><FONT face="Arial, Helvetica, sans-serif" size=2><B></B></FONT></DIV> </TD> <TD bgColor=#99cccc width=88> <DIV align=center><FONT face="Arial, Helvetica, sans-serif"  size=2><B>500</B></FONT></DIV> </TD> <TD bgColor=#cc99cc width=98> <DIV align=center><FONT face="Arial, Helvetica, sans-serif"  size=2><B></B></FONT></DIV> </TD><TD bgColor=#cc99cc width=84> 
 <DIV align=center><FONT face="Arial, Helvetica, sans-serif" size=2><B>560</B></FONT></DIV></TD> </TR>
          </TBODY> </TABLE>   </CENTER>
      <P>From the information given in the above table, the index for the two consecutive years are 500/500 = 1, and 560/500 = 1.12, respectively. <P><B>Further Readings:</B><BR> <FONT face="Bookman Old Style" size=-2>Watson C., P. Billingsley, D. Croft,  and D. Huntsberger, <I>Statistics for Management and Economics</I>, Allyn   &amp; Bacon, Inc., 1993. </FONT>       <P>  


<P><A name=rvarinqual></A>     <HR>    <H4><FONT color=#dc143c>Variation Index as a Quality Indicator</FONT></H4>
 A commonly  used index of variation measure and comparison for nominal and ordinal data is called the index of dispersion:  <P>    <CENTER>
        D = k (N<FONT size=+0><SUP>2</SUP></FONT> - <FONT 
  face=symbol>S</FONT>f<FONT size=+0><SUB>i</SUB></FONT><FONT 
  size=+0><SUP>2</SUP></FONT>)/[N<FONT size=+0><SUP>2</SUP></FONT>(k-1)] 
      </CENTER>
    <P>where k is the number of categories, f<FONT><SUB>i</SUB></FONT> is the number of ratings in each category,  and N is the total number of rating. D is a number between zero and 1 depending if all ratings fall into one category, or if ratings were equally divided among the k categories. <P><B>An Application:</B> Consider the following data with N = 100 participants,   k = 5 categories, f<FONT size=+0><SUB>1</SUB></FONT> = 25, f<FONT size=+0><SUB>2</SUB></FONT>  = 42, and so on. 
    <P>     <P>    <table width="220" border="0" align="center">
  <tr bgcolor=#E8E8FF valign=bottom> 
    <td bordercolor=#333333 height=2 width=50%> 
      <div align=center><font color="#5400A8"><b><font size="2" face="Arial, Helvetica, sans-serif">Category</font></b></font></div>
    </td>
    <td bordercolor=#333333 height=2 width=50%> 
      <div align=center><font color="#5400A8"><b><font size="2" face="Arial, Helvetica, sans-serif">Frequency</font></b></font></div>
    </td>
  </tr>
  <tr bordercolor=#000000> 
    <td height=14 width=50%> 
      <div align=center><font size="2" face="Arial, Helvetica, sans-serif">A</font></div>
    </td>
    <td height=14 width=50%> 
      <div align=center><font size="2" face="Arial, Helvetica, sans-serif">25</font></div>
    </td>
  <tr bordercolor=#000000> 
    <td height=14 width=50%> 
      <div align=center><font size="2" face="Arial, Helvetica, sans-serif">B</font></div>
    </td>
    <td height=14 width=50%> 
      <div align=center><font size="2" face="Arial, Helvetica, sans-serif">42</font></div>
    </td>
  <tr bordercolor=#000000> 
    <td height=14 width=50%> 
      <div align=center><font size="2" face="Arial, Helvetica, sans-serif">C</font></div>
    </td>
    <td height=14 width=50%> 
      <div align=center><font size="2" face="Arial, Helvetica, sans-serif">8</font></div>
  <tr bordercolor=#000000> 
    <td height=14 width=50%> 
      <div align=center><font size="2" face="Arial, Helvetica, sans-serif">D</font></div>
    </td>
    <td height=14 width=50%> 
      <div align=center><font size="2" face="Arial, Helvetica, sans-serif">13</font></div>
    </td>
  <tr bordercolor=#000000> 
    <td height=14 width=50%> 
      <div align=center><font size="2" face="Arial, Helvetica, sans-serif">E</font></div>
    </td>
    <td height=14 width=50%> 
      <div align=center><font size="2" face="Arial, Helvetica, sans-serif">12</font></div>
    </td>
  </tr>
</table>  </CENTER>      <P> 
      <P>Therefore the dispersion index is: D = 5 (100<FONT><SUP>2</SUP></FONT> - 2766)/[100<FONT><SUP>2</SUP></FONT>(4)] = 0.904,  indicating a good spread of scores across the categories. 

<P><A name=rlaborfoun></A>     <HR>    <H4><FONT color=#dc143c>Labor Force Unemployment Index</FONT></H4>
 Is a given  city an economically depressed area? The degree of unemployment among labor (L) force is considered to be a proper indicator of economic depression.  To construct the unemployment index, each person is classified both with  respect to membership in the labor force and the degree of unemployment in fractional value, ranging from 0 to 1. The fraction that indicates the portion of labor that is idle is:  <P> 
        <CENTER>   L = <FONT face=symbol>S</FONT>[U<FONT 
  size=+0><SUB>i</SUB></FONT>P<FONT size=+0><SUB>i</SUB></FONT>] / <FONT 
  face=symbol>S</FONT>P<FONT size=+0><SUB>i</SUB></FONT>,&nbsp; &nbsp; the sums are over all i = 1, 2,, n.         </CENTER>
 <P>where P<FONT size=+0><SUB>i</SUB></FONT> is the proportion of a full workweek for each resident of the area held or sought employment and  n is the total number of residents in the area. U<FONT size=+0><SUB>i</SUB></FONT> is the proportion of P<FONT size=+0><SUB>i</SUB></FONT> for which each 
resident of the area unemployed. For example, a person seeking two days  of work per week (5 days) and employed for only one-half day would be  identified with P<FONT size=+0><SUB>i</SUB></FONT> = 2/5 = 0.4, and U<FONT   size=+0><SUB>i</SUB></FONT> = 1.5/2 = 0.75. The resulting multiplication U<FONT size=+0><SUB>i</SUB></FONT>P<FONT size=+0><SUB>i</SUB></FONT>  = 0.3 would be the portion of a full workweek for which the person was   unemployed.  <P>Now the question is What value of L constitutes an economic depressed  area. The answer belongs to the decision-maker to decide. <P>

<P><A name=rseasonindex></A>     <HR>    <H4><FONT color=#dc143c>Seasonal Index and Deseasonalizing Data</FONT></H4>
<p>
Seasonal index represents the extent of seasonal influence for a particular segment of the year.  The calculation involves a comparison of the expected values of that period to the grand mean.
<p>
We need to get an estimate of the seasonal index for each month, or other periods such as quarter, week, etc, depending on the data availability.  Seasonality is a pattern that repeats for each period.  For example annual seasonal pattern has a cycle that is 12 periods long, if the periods are months, or 4 periods long if the periods are quartets.
<p>
A seasonal index is how much the average for that particular period tends to be above (or below) the grand average. Therefore, to get an accurate estimate for it, we compute the average of the first period of the cycle, and the second period, etc, and divide each by the overall average.  The formula for computing seasonal factors is: 
<p>
<center>
S<FONT><SUB>i</SUB></FONT> = D<FONT><SUB>i</SUB></FONT>/D,       
<p>
</center>
where:<br>
S<FONT><SUB>i</SUB></FONT> = the seasonal index for i<FONT><SUP>th</SUP></FONT> period,<br>
D<FONT><SUB>i</SUB></FONT> = the average values of  i<FONT><SUP>th</SUP></FONT> period,<br>
D = grand avrage,<br>
i = the i<FONT><SUP>th</SUP></FONT> seasonal period of the cycle.<br>
<p>
A seasonal index of 1.00 for a particular month indicates that the expected value of that month is 1/12 of the overall average. A seasonal index of 1.25 indicates that the expected value for that month is 25% greater than 1/12 of the overall average. A seasonal index of 80 indicates that the expected value for that month is 20% less than 1/12 of the overall average.
<p>
<font   color="#DC143C"><b>Deseasonalizing Process:</b></font> Deseasonalizing the data, also called Seasonal Adjustment  is the process of removing recurrent and periodic variations over a short time frame (e.g.,  weeks, quarters, months). Therefore, season variations are regularly repeating movements in series values that can be tied to recurring events.  The Deseasonalized data is obtained by simply dividing each time series observation by the corresponding seasonal index.
<p>
Almost all time series published by the government are already deseasonalized using the seasonal index to unmasking the underlying trends in the data, which could have been caused by the seasonality factor.
<p>
<font   color="#DC143C"><b>A Numerical Application:</b></font>  The following table provides monthly sales ($000) at a college bookstore. 
<p>
<p>
<center><table width="630" border="0" align="center">  <tr>  <td bgcolor="#808080" height=""> 
      <table border="0" align="center" cellpadding="3" width="630" cellspacing="1"> <tr bgcolor="#FFFFFF"> 
          <td width="8%" height="20">  <p align="right"><font face="Arial, Helvetica, sans-serif" size="2" color="#990033"><b>M</b></font></p> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#990033"><b>T</b> </font></div> </td><td width="7%" height="20" valign="top"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#990033"><b>Jan</b></font></div>
          </td> <td width="7%" height="20" valign="top">  <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#990033"><b>Feb</b></font></div> </td><td width="7%" height="20" valign="top"> 
 <div align="center"><font face="Arial, Helvetica, sans-serif" size="2"color="#990033"><b>Mar</b></font></div> </td> <td width="7%" height="20" valign="top"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#990033"><b>Apr</b></font></div></td><td width="7%" height="20" valign="top"> 
 <div align="center"><font face="Arial, Helvetica, sans-serif" size="2"color="#990033"><b>May</b></font></div>
  </td><td width="7%" height="20" valign="top"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#990033"><b>Jun</b></font></div> </td><td width="7%" height="20" valign="top"> 
<div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#990033"><b>Jul</b></font></div>
 </td> <td width="7%" height="20" valign="top"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#990033"><b>Aug</b></font></div></td><td width="7%" height="20" valign="top"> 
 <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#990033"><b>Sep</b></font></div>
 </td><td width="7%" height="20" valign="top"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#990033"><b>Oct</b></font></div></td><td width="7%" height="20" valign="top"> 
 <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#990033"><b>Nov</b></font></div>
  </td><td width="7%" height="20" valign="top"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#990033"><b>Dec</b></font></div></td><td width="8%" height="20" valign="top"> 
<div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#990033"><b>Total</b></font></div></td></tr><tr bgcolor="#FFFFFF"> <td width="8%"> 
 <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#990033">1</font></div>
 </td> <td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">196</font></div>
 </td> <td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">188</font></div>
 </td> <td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">192</font></div>
 </td> <td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">164</font></div>
 </td><td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">140</font></div>
 </td><td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">120</font></div>
 </td><td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">112</font></div>
 </td> <td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">140</font></div>
 </td><td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">160</font></div>
 </td> <td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">168</font></div>
 </td> <td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">192</font></div>
 </td> <td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">200</font></div>
 </td> <td width="8%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">1972</font></div>
 </td> </tr><tr bgcolor="#FFFFFF"> <td width="8%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#990033">2</font></div> </td> <td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">200</font></div> </td> <td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">188</font></div> </td> <td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">192</font></div> </td> <td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">164</font></div> </td><td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">140</font></div> </td> <td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">122</font></div> </td> <td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">132</font></div> </td> <td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">144</font></div> </td> <td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">176</font></div> </td><td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">168</font></div> </td><td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">196</font></div></td> <td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">194</font></div> </td> <td width="8%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">2016</font></div> </td> </tr> <tr bgcolor="#FFFFFF"> <td width="8%"> 
 <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#990033">3</font></div>
 </td> <td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">196</font></div>
 </td><td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">212</font></div>
</td> <td width="7%">  <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">202</font></div>
 </td> <td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">180</font></div>
 </td> <td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">150</font></div>
 </td> <td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">140</font></div>
 </td> <td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">156</font></div>
 </td> <td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">144</font></div>
 </td> <td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">164</font></div>
 </td> <td width="7%">  <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">186</font></div>
 </td> <td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">200</font></div>
 </td> <td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">230</font></div>
 </td> <td width="8%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">2160</font></div>
</td> </tr> <tr bgcolor="#FFFFFF"> <td width="8%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2" color="#990033">4</font></div> </td><td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">242</font></div> </td> <td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">240</font></div> </td> <td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">196</font></div> </td> <td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">220</font></div> </td> <td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">200</font></div> </td> <td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">192</font></div></td><td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">176</font></div> </td><td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">184</font></div></td><td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">204</font></div></td><td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">228</font></div></td><td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">250</font></div></td><td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">260</font></div></td><td width="8%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">2592</font></div></td></tr><tr bgcolor="#FFFFFF"> <td width="8%"><font face="Arial, Helvetica, sans-serif" size="2" color="#990033"><b>Mean:</b></font></td><td align=middle height=19 width=7% valign="bottom" bordercolor="#999966"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">208.6</font></div> </td> <td align=middle height=19 width="7%" valign="bottom" bordercolor="#999966"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">207.0</font></div> </td> <td align=middle height=19 width="7%" valign="bottom" bordercolor="#999966">  <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">192.6</font></div>  </td> <td align=middle height=19 width=7% valign="bottom" bordercolor="#999966">  <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">182.0</font></div>  </td> <td align=middle height=19 width=7% valign="bottom" bordercolor="#999966"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">157.6</font></div> </td> <td align=middle height=19 width="7%" valign="bottom" bordercolor="#999966">  <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">143.6</font></div> </td> <td align=middle height=19 width="7%" valign="bottom" bordercolor="#999966">  <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">144.0</font></div> </td> <td align=middle height=19 width="7%" valign="bottom" bordercolor="#999966"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">153.0</font></div> </td> <td align=middle height=19 width="7%" valign="bottom" bordercolor="#999966"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">177.6</font></div> </td> <td align=middle height=19 width="7%" valign="bottom" bordercolor="#999966"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">187.6</font></div> </td> <td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">209.6</font></div> </td> <td width="7%">  <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">221.0</font></div> </td> <td width="8%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">2185</font></div> </td> </tr> <tr bgcolor="#FFFFFF"> <td width="8%"><font face="Arial, Helvetica, sans-serif" size="2" color="#990033"><b>Index:</b></font></td> <td align=middle height=19 width=7% valign="bottom" bordercolor="#999966"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">1.14</font></div></td> <td align=middle height=19 width="7%" valign="bottom" bordercolor="#999966"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">1.14</font></div>
 </td> <td align=middle height=19 width="7%" valign="bottom" bordercolor="#999966"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">1.06</font></div></td>
          <td align=middle height=19 width=7% valign="bottom" bordercolor="#999966"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">1.00</font></div></td> <td align=middle height=19 width=7% valign="bottom" bordercolor="#999966"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">0.87</font></div></td> <td align=middle height=19 width="7%" valign="bottom" bordercolor="#999966">  <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">0.79</font></div>
 </td> <td align=middle height=19 width="7%" valign="bottom" bordercolor="#999966"> 
 <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">0.79</font></div></td>
          <td align=middle height=19 width="7%" valign="bottom" bordercolor="#999966"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">0.84</font></div> </td>
          <td align=middle height=19 width="7%" valign="bottom" bordercolor="#999966"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">0.97</font></div> </td>
          <td align=middle height=19 width="7%" valign="bottom" bordercolor="#999966"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">1.03</font></div> </td> <td width="7%">  <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">1.15</font></div>  </td> <td width="7%"> <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">1.22</font></div> </td> <td width="8%">  <div align="center"><font face="Arial, Helvetica, sans-serif" size="2">12</font></div> </td> </tr> </table></td></tr></table><p></center>
<p>
The sales show a seasonal pattern, with the greatest number when the college is in session and decrease during the summer months.  For example, for January the index is: 
<p>
<center>
S(Jan) = D(Jan)/D =208.6/181.84 = 1.14,
<p></center>
where D(Jan) is the mean of all four January month, and D is the grand mean of all past four years sales.
<p>
You might like to use the <a href="otherapplets/SeasonalTools.htm" target="new">Seasonal Index</a> JavaScript to check your hand computation. As always you must first use  <a href="graph/TimeSeriesPlot.htm" target="new">Plot of the Time Series</a> as a tool for the initial characterization process.  
<p>
For testing seasonality based on seasonal index, you may like to use 
<a href="otherapplets/TestSeason.htm"  target="new">Test for Seasonality</A> JavaScript.
<p>
For modeling the time series having both the seasonality and trend components, visit the <a href="http://ubmail.ubalt.edu/~harsham/stat-data/opre330Forecast.htm" target="new">Business Forecasting</a> site.
<P>
<P><A name=rstatindex></A>     <HR>    <H4><FONT color=#dc143c>Statistical Technique and Index Numbers</FONT></H4>

 <P>One must <FONT color=#dc143c>be careful in applying or generalizing any statistical technique  to the index numbers</FONT>. For example, the correlation of rates raises  the potential problem. Specifically, let X, Y, and X be three independent variables, so that pair-wise correlations are zero; however, the ratios   X/Y, and Z/Y will be correlated due to the common denominator. <p>Let I = X<FONT><SUB>1</SUB></FONT>/X<FONT><SUB>2</SUB></FONT> where X<FONT><SUB>1</SUB></FONT>, and X<FONT><SUB>2</SUB></FONT> are dependent variables with correlation r , having mean and coefficient of variation m<FONT><SUB>1</SUB></FONT>, c<FONT><SUB>1</SUB></FONT> and m<FONT><SUB>2</SUB></FONT>, c<FONT><SUB>2</SUB></FONT>, respectively; then,
<p>
<center>

Mean of I = m<FONT><SUB>1</SUB></FONT> (1-r<font face=symbol>&#180;</font>c<FONT><SUB>1</SUB></FONT><font face=symbol>&#180;</font>c<FONT><SUB>2</SUB></FONT> + c<FONT><SUB>2</SUB></FONT><FONT><SUP>2</SUP></FONT>)/m<FONT><SUB>2</SUB></FONT>,
<p>Standard Deviation of I = m<FONT><SUB>1</SUB></FONT>(c<FONT><SUB>1</SUB></FONT><FONT><SUP>2</SUP></FONT> - 2 r<font face=symbol>&#180;</font>c<FONT><SUB>1</SUB></FONT><font face=symbol>&#180;</font>c<FONT><SUB>2</SUB></FONT> + c<FONT><SUB>2</SUB></FONT><FONT><SUP>2</SUP></FONT>) <FONT><SUP>½
</SUP></FONT> /m<FONT><SUB>2</SUB></FONT>
<p>
<p>
</center>
<p>
<A name="rtopicaljava">
<HR>
</A>
<H4><FONT color=#dc143c>A Classification of the JavaScript by Application Areas</FONT></H4>

<dd>
This section is a part of the JavaScript <a href="otherapplets/scientificCal.htm" target= "new">E-labs</a> learning technologies for decision making.  The following is a classification of statistical JavaScript by their application areas:
</BLOCKQUOTE>
</font>
<p><center>
    <font size=4 color="#DC143C"><b>MENU</b></font>
  </center>
<center>
<p>
    <table width="600">
      <tr valign=TOP> 
        <td nowrap width="230" align="right" height="63">
          </font> 
          <table width="40%" border="0"> 
            <tr>
              <td nowrap>

<font color="#006699"><b>  
1. Summarizing Data</b></font><br>
<ul>
<li><a href="otherapplets/MultiVariate.htm"  target="new">Bivariate Sampling Statistics</a>
<li><a href="otherapplets/Trend.htm" target ="new">Detective Testing for Trend & Autocrrelation</a>
<li><a href="otherapplets/Descriptive.htm"  target="new">Descriptive Statistics</a>
<li><a href="otherapplets/Outlier.htm" target= "new">Determination of the Outliers</a>
<li><a href="otherapplets/ECDF.htm" target ="new">Empirical Distribution Function</a>
<li><a href="histograming/topframe.html"  target="new">Histogram</a>
<li><a href="otherapplets/Fluctuation.htm" target= "new"> Residuals Random Fluctuations Testing</a>
<li><a href="otherapplets/SeasonalTools.htm" target="new">Seasonal Index</a>
<li><a href="otherapplets/ThreeMeans.htm" target= "new">The Three Means</a>
</ul>
<font color="#006699"><b>  
2. Computational probability </b></font><br>
<ul>
<li><A href="otherapplets/Ustat.htm"  target=new>Comparing Two Random Variables</A>
<li><a href="Matrix/Mat4.htm"  target="new">Markov Chains Calculator</a>
<li><a href="otherapplets/multinomial.htm"  target="new">Multinomial Distributions</a>
<li><a href="otherapplets/pvalues.htm" target="new">P-values for the Popular Distributions</a>
</ul>
<font color="#006699"><b>  
3. Requirements for most tests & estimations</b></font><br>
<ul>
<li><a href="otherapplets/Outlier.htm" target= "new">Removal of the Outliers</a>
<li><a href="otherapplets/SampleSize.htm"  target="new">Sample Size Determination</a>
<li><a href="otherapplets/SubjTest.htm" target= "new">Subjectivity in Hypothesis Testing </a>
<li><a href="histograming/topframe.html"  target="new">Test for Homogeneity of Population</a>
<li><a href="otherapplets/Normality.htm"  target="new">Test for Normality</a>
<li><a href="otherapplets/Randomness.htm"  target="new">Test for Randomness</a>
</ul>
<font color="#006699"><b>  
4. One population & one variable</b></font><br>
<ul>
<li><a href="otherapplets/ConfIntPro.htm" target= "new">Binomial Exact Confidence Intervals</a>
<li><a href="otherapplets/CountTest.htm" target ="new"> Compatibility of Multi-Counts </a>
<li><a href="otherapplets/goodness.htm" target="new">Goodness-of-Fit for Discrete Variables</a>
<li><a href="otherapplets/RevisMean.htm" target= "new">Revising the Mean and the Variance</a>
<li><a href="otherapplets/LilliExpon.htm"  target="new">Testing  the Exponential Distribution</a>
<li><a href="otherapplets/MeanTest.htm"  target="new">Testing  the Mean</a>
</ul>


<p>
</td>
</tr>
</table>
</td>
<td width="97" height="63">&nbsp;</td>
<td nowrap width="232" height="63"> 
<table width="40%" border="0">
<tr>
<td nowrap>

<ul>

<li><a href="otherapplets/MediansTest.htm"  target="new">Testing the Medians</a>
<li><a href="otherapplets/ConfIntPro.htm" target= "new">Testing the Percentage</a>
<li><a href="otherapplets/PoissonTest.htm" target= "new"> Testing the Poisson Process</a>
<li><a href="otherapplets/variationtest.htm"  target="new">Testing the Variance</a>
<li><a href="otherapplets/Uniform.htm"  target="new">Test for Uniform Distribution</a>
</ul>

<font color="#006699"><b>  
5. One population & two or more variables</b></font><br>
<ul>
<li><a href="otherapplets/Paired.htm"  target="new">The Before-and-After Test for Means and Variances</a>
<li><A href="otherapplets/PairedProp.htm"  target=new>The Before-and-After Test for Proportions</A>
<li><a href="otherapplets/Catego.htm" target="new">Chi-square Test for Crosstable Relationship</a>
<li><a href="otherapplets/MultRgression.htm" target="new">Multiple Regressions</a>
<li><a href="otherapplets/PolynoReg.htm" target="new">Polynomial Regressions</a>
<li><a href="otherapplets/QuadReg.htm" target="new">Quadratic Regression</a>
<li><a href="otherapplets/Regression.htm" target ="new"> Simple Regression with Diagnostic Tools</a>
<li><a href="otherapplets/correlation.htm" target= "new"> Testing the Population Correlation Coefficient</a>
</ul>
<font color="#006699"><b>  
6. Two or three populations & one variable</b></font><br>
<ul>
<li><a href="otherapplets/ANOVADep.htm" target= "new">ANOVA for Dependent Populations</a>
<li><a href="otherapplets/ANOVA.htm" target ="new">ANOVA: Testing Equality of the Means</a>
<li><a href="otherapplets/ks.htm" target ="new">K-S Test for Equality of Two Populations</a>
<li><a href="otherapplets/TwoPopTest.htm"  target="new">Two Populations Testing Means & Variances</a>
</ul>
<font color="#006699"><b>  
7. Several populations & one or more variables</b></font><br>
<ul>
<li><a href="otherapplets/ANOCOV.htm" target= "new">Analysis of Covariance</a>
<li><A href="otherapplets/SeveralMeans.htm"  target=new>ANOVA for Condensed Data Sets</A>
<li><a href="otherapplets/CountTest.htm" target ="new"> Compatibility of Multi-Counts </a>
<li><a href="otherapplets/BartletTest.htm" target ="new"> Equality of Multi-variances: The Bartlett's Test</a>
<li><a href="otherapplets/HomoCrosstable.htm" target="new">Identical Populations Test for Crosstable Data</a>
<li><A href="otherapplets/InaccracyAssessmet.htm"  target=new>Subjective Assessment of Estimates</A>
<li><a href="otherapplets/ProporTest.htm"  target="new">Testing the Proportions</a>
<li><a href="otherapplets/MultiCorr.htm" target= "new">Testing  Several Correlation Coefficients</a>
<li><A href="otherapplets/ANOVATwo.htm"  target=new>Two-Way ANOVA Test</A>
<li><A href="otherapplets/ANOVA2Rep.htm"  target=new>Two-Way ANOVA with Replications</A>
</ul>
<p>
</td>
</tr>
</table>
</td>
</table>
</center>
</font><BLOCKQUOTE>

      <HR>      <P>         <CENTER>          <FONT color=#00700f><B><I>A selection of:</I></B></FONT>
        </CENTER>
      <P>| <A href="http://www.academicinfo.com/" target=new>Academic Info</A>|
 <A href="http://www.aacu-edu.org/" target=new>Association of American Colleges and Universities</A>|
  <A href="http://bubl.ac.uk/" target=new>BUBL Catalogue</A>|   <A href="http://www.blacknetuk.com/education/busmeta.html"   target=new>Business (Indexes) </A>| <A href="http://www.bized.ac.uk/"   target=new>Business and Economics (Biz/ed)</A>| <A 
  href="http://www.studyweb.com/" target=new>Business &amp; Finance</A>| <A   href="http://mathforum.org/" target=new>Business &amp; Industrial</A>| <A   href="http://businessnation.com/library" target=new>Business Nation</A>| <A   href="http://www.dartmouth.edu/~chance" target=new>Chance</A>|<A href="http://dirs.educationworld.net/"  target=new>Education World</A>| 
<A href="http://users.telenet.be/educypedia"  target=new>Educypedia</A>| 
<a href="http://econltsn.ilrt.bris.ac.uk/welcome.htm" target="new">Economics LTSN</a>|<P>|<A href="http://www.mic.ki.se/Epidem.html" target=new>Epidemiology and Biostatistics</A>| <A href="http://www.emtech.net/"   target=new>Emerging Technologies</A>|
 <A   href="http://www.satd.uma.es/matap/svera/links/matnet07.html"   target=new>Estadística</A>| 
 <A   href="http://www.paralegals.org/"   target=new>Federation of Paralegal Associations</A>| 
<A 
  href="http://www.puc-rio.br/marco.ind/ot_links.html" target=new>Financial and  Economic Links</A>| <A href="http://www.ifors.org/" target=new>IFORS</A>| 
<A   href="http://www.niss.org/"   target=new>Institute of Statistical Sciences</A>| 

<A  href="http://www-sci.lib.uci.edu/HSG/RefFinance.html" target=new>International Business</A>| <A href="http://www.mi.mun.ca/~drehner/stats" target=new>Marine 
 Institute</A>| <A href="http://www.mgmtguru.com/Mgmt.htm"   target=new>Management</A>|<A href="http://www.managers.org.uk/external/mgt-menu.html"   target=new>Management Sources</A>
|<A 
  href="http://www.library.usyd.edu.au/subjects/mathematics/specialised.html"   target=new>Mathematics and Statistics</A>| <A href="http://mathforum.org/"   target=new>MathForum</A>| <A href="http://ltsn.mathstore.ac.uk/index.shtml"   target=new>Maths, Stats &amp; OR Network</A>| <A 
  href="http://www.mhhe.com/business/opsci/bstat/univres.mhtml"   target=new>McGraw-Hill</A>| <A href="http://www.merlot.org/"   target=new>Merlot</A>| <A href="http://www.needs.org/" target=new>NEEDS</A>| <A href="http://firstsearch.oclc.org/FSIP" target=new>NetFirst</A>| <A 
  href="http://nrich.maths.org/" target=new>NRICH</A>| 
<a href="http://open-site.org/Science/Mathematics" target="new" >Open Encyclopedia</a>|<A   href="http://www.nemspk.cz/patol/index.htm" target=new>Physician's Quiklinks</A>| 
 <A href="http://www.stat.ucl.ac.be/ISpersonnel/lecoutre/stats"  target=new>Ressources en Statistique</A>|
 <A href="http://www.blacknet.co.uk/education/subsci.html" target=new>Science Gateway</A>| <P><FONT color=#00700f><B>Search Engines Directory:</B></FONT><BR> |<A   href="http://www.altavista.com/sites/dir/search?pg=dir&tp=Library/Sciences"   target=new> AltaVista</A>| <A href="http://search.aol.com/"   target=new>AOL</A>| <A href="http://www.excite.com/" target=new>Excite</A>|   <A   href="http://dir.hotbot.lycos.com/Science/Math" target=new>HotBot</A>| <A 
  href="http://www.looksmart.com/eus317836/eus317914/eus328800/r?l&"   target=new>Looksmart</A>| <A href="http://dir.lycos.com/Science/Math"   target=new>Lycos</A>| <A href="http://search.msn.com/default.asp" 
  target=new>MSN</A>| <A href="http://home.netscape.com/"   target=new>Netscape</A>| <A href="http://dmoz.org/"   target=new>OpenDirectory</A>|  <A   href="http://www.scientopica.com/sci" target=new>Scientopica</A>| <A   href="http://www.webcrawler.com/education/science_and_nature" 
  target=new>Webcrawler</A>| <A   href="http://www.yahoo.com.au/Science/Mathematics" target=new>Yahoo</A>| <P>|<A href="http://scout.cs.wisc.edu/" target=new>Scout Report</A>| <A 
  href="http://www.smallbusinessportal.co.uk/ntools.htm" target=new>Small Business</A>| 
        <A href="http://www.sosig.ac.uk/" target=new>Social Science</A>|

 <A href="http://www.marketing.strath.ac.uk/dcd/Statistical_Data"  target=new>Statistical Data</A>| 
<A href="http://www.e-stat.org/copss/teaching.htm"  target=new>Statistical Societies</A>| 
 <A    href="http://www.execpc.com/~helberg/statframes.html" target=new>Statistics on the Web</A>| <A 
  href="http://www.anu.edu.au/nceph/surfstat/surfstat-home/surfstat.html"   target=new>SurfStat</A>|
 <A href="http://www.executivelibrary.com/"   target=new>Wall Street</A>|
 <A href="http://www.virtuallrc.com/"   target=new>Virtual Learning</A>| <A href="http://vlib.org/" 
  target=new>Virtual Library</A>| <A href="http://netec.mcc.ac.uk/WebEc"   target=new>WebEc</A>| <A href="http://wnt.cc.utexas.edu/~wlh"   target=new>World Lecture Hall</A>|  <P>Additional useful sites may be found by clicking on the following search engine:  <CENTER> <A  href="http://www.alltheweb.com/search?advanced=1&cat=web=type=alltq=ejsact=ml=anyaics=iso-8859-1lcs=iso-8859-1iwf%5Bn%5D=3mwf%5B0%5D%5Br%5D=mwf%5B0%5D%5Bq%5D=http%3A%2F%2Fubmail.ubalt.edu%2F%7Eharsham%2FBusiness-stat%2Fbwf%5B0%5D%5Bw%5D=link.all%3Aewf%5B1%5D%5Br%5D=%2B0wf%5B1%5D%5Bq%5D=lwf%5B1%5D%5Bw%5D=Dwf%5B2%5D%5Br%5D=-%wf%5B2%5D%5Bq%5D=pwf%5B2%5D%5Bw%5D==dincl=fdexcl=Dlimip=Dage=msize%5Bp%5D=%3C%size%5Bv%5D=Bsize%5Bx%5D=0ahits=25lnooc=off" 
  target=new>AllTheWeb</A>  </CENTER>
      <P> 
      <HR><P><FONT color=#00700f>The Copyright Statement:</FONT> The fair use, according to the 1996 <A href="http://www.adec.edu/admin/papers/fair10-17.html"   target=new>Fair Use Guidelines for Educational Multimedia</A>, of materials presented on this Web site is permitted for non-commercial and classroom 
        purposes only. <BR>This site may be mirrored intact (including these notices), on any server 
        with public access.  <P>Kindly <A href="mailto:harsham@ubmail.ubalt.edu">e-mail</A> me your comments,  suggestions, and concerns. Thank you.  <DIV align=right><CITE><A href="http://ubmail.ubalt.edu/~harsham/index.html"  target=new>Professor Hossein Arsham</A>&nbsp; &nbsp;</CITE></DIV> <HR> <P>This site was launched on 1/18/1994, and its intellectual materials have 
  been thoroughly revised on a yearly basis. The current version is the  9<FONT 
  size=+0><SUP>th</SUP></FONT> Edition. All external links are checked once a  month. 
      <P><A href="http://ubmail.ubalt.edu/~harsham/index.html">Back to Dr. Arsham's Home Page</A> 
      <P>       <HR>      EOF: © 1994-2004       <HR>    </DIV>  </BLOCKQUOTE></FONT>
</BODY></HTML>

