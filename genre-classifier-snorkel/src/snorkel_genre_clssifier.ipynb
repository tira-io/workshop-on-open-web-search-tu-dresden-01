{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tira.third_party_integrations import ir_datasets, get_output_directory\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from snorkel.labeling import LabelingFunction, PandasLFApplier, LFAnalysis\n",
    "from snorkel.labeling.model import MajorityLabelVoter\n",
    "import numpy as np\n",
    "import spacy\n",
    "from snorkel_genre_classifier import get_tokens_types\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_post_data(html_file):\n",
    "    with open(html_file, 'r') as f:\n",
    "        html_text = f.read()\n",
    "        soup = BeautifulSoup(html_text, 'html.parser')\n",
    "        body = soup.text\n",
    "        return body\n",
    "def extract_posts(content_directory):\n",
    "    body_list = []\n",
    "    for filename in os.listdir(content_directory):\n",
    "        if filename.endswith('.html'):\n",
    "            body = extract_post_data(os.path.join(content_directory, filename))\n",
    "            \n",
    "            body_list.append(body)\n",
    "    return body_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_with_count(tokens):\n",
    "    tokens_with_count = Counter(tokens)\n",
    "    return tokens_with_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' The Java Language Environment Java is Object Oriented --> CHAPTER 3 My Object All Sublime I Will Achieve in Time Gilbert and Sullivan-- The Mikado To stay abreast of modern software development practices, Java is object oriented from the ground up . The point of designing an object-oriented language is not simply to jump on the latest programming fad. The object-oriented paradigm meshes well with the needs of client-server and distributed software. Benefits of object technology are rapidly becoming realized as more organizations move their applications to the distributed client-server model. Unfortunately, \"object oriented\" remains misunderstood, over-marketed as the silver bullet that will solve all our software ills, or takes on the trappings of a religion. The cynic\\'s view of object-oriented programming is that it\\'s just a new way to organize your source code. While there may be some merit to this view, it doesn\\'t tell the whole story, because you can achieve results with object-oriented programming techniques that you can\\'t with procedural techniques. An important characteristic that distinguishes objects from ordinary procedures or functions is that an object can have a lifetime greater than that of the object that created it. This aspect of objects is subtle and mostly overlooked. In the distributed client-server world, you now have the potential for objects to be created in one place, passed around networks, and stored elsewhere, possibly in databases, to be retrieved for future work. As an object-oriented language, Java draws on the best concepts and features of previous object-oriented languages, primarily Eiffel, SmallTalk, Objective C, and C++. Java goes beyond C++ in both extending the object model and removing the major complexities of C++. With the exception of its primitive data types, everything in Java is an object, and even the primitive types can be encapsulated within objects if the need arises. CONTENTS | PREV | NEXT INDEX --> Please send any comments or corrections to jdk-comments@java.sun.com Copyright © 1997 Sun Microsystems, Inc. All Rights Reserved. Company Info | About SDN | Press | Contact Us | Employment How to Buy | Licensing | Terms of Use | Privacy | Trademarks Copyright 1994-2004 Sun Microsystems, Inc. A Sun Developer Network Site Unless otherwise licensed, code in all technical manuals herein (including articles, FAQs, samples) is provided under this License . Content Feeds \\n\\t\\n\\n-->\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe Java Language Environment\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ndevelopers.sun.com\\n\\xa0\\n»\\xa0search tips\\xa0\\xa0|\\xa0\\xa0Search:\\xa0\\n\\n\\xa0\\n\\nin Developers\\' Site\\nin Sun.com\\n\\n\\xa0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\n\\n\\nDevelopers Home > Products & Technologies > Java Technology > Reference > White Papers > The Java Language Environment >\\n\\n\\n\\n\\nProfile and Registration | \\nWhy Register?\\n\\n\\n\\n\\n\\n\\xa0\\n\\n\\n\\nWhite Paper\\nThe Java Language Environment\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\nPrintable Page\\n\\n\\n\\n\\n\\n\\n\\n\\n CONTENTS | PREV\\n| NEXT \\nThe Java Language Environment\\n\\n\\n\\n\\nJava is Object Oriented\\n\\n\\n\\n\\n-->\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCHAPTER \\n3 \\n\\n\\n\\n\\n\\n\\n\\n\\nMy Object All SublimeI Will Achieve in Time\\n\\nGilbert and Sullivan--The Mikado\\n\\n\\n\\nTo stay abreast of modern software development practices, Java is object oriented from the ground up. The point of designing an object-oriented language is not simply to jump on the latest programming fad. The object-oriented paradigm meshes well with the needs of client-server and distributed software. Benefits of object technology are rapidly becoming realized as more organizations move their applications to the distributed client-server model.\\n\\nUnfortunately, \"object oriented\" remains misunderstood, over-marketed as the silver bullet that will solve all our software ills, or takes on the trappings of a religion. The cynic\\'s view of object-oriented programming is that it\\'s just a new way to organize your source code. While there may be some merit to this view, it doesn\\'t tell the whole story, because you can achieve results with object-oriented programming techniques that you can\\'t with procedural techniques.\\n\\nAn important characteristic that distinguishes objects from ordinary procedures or functions is that an object can have a lifetime greater than that of the object that created it. This aspect of objects is subtle and mostly overlooked. In the distributed client-server world, you now have the potential for objects to be created in one place, passed around networks, and stored elsewhere, possibly in databases, to be retrieved for future work.\\n\\nAs an object-oriented language, Java draws on the best concepts and features of previous object-oriented languages, primarily Eiffel, SmallTalk, Objective C, and C++. Java goes beyond C++ in both extending the object model and removing the major complexities of C++. With the exception of its primitive data types, everything in Java is an object, and even the primitive types can be encapsulated within objects if the need arises.\\n\\n\\n\\n\\n\\n\\n CONTENTS | PREV\\n| NEXT \\n\\n\\n\\n\\nPlease send any comments or corrections to\\njdk-comments@java.sun.com\\nCopyright © 1997 Sun Microsystems, Inc. All Rights Reserved.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCompany Info \\xa0|\\xa0\\nAbout SDN \\xa0|\\xa0\\nPress \\xa0|\\xa0\\nContact Us \\xa0|\\xa0\\nEmployment\\nHow to Buy \\xa0|\\xa0\\nLicensing \\xa0|\\xa0\\nTerms of Use \\xa0|\\xa0\\nPrivacy \\xa0|\\xa0\\nTrademarks\\n\\xa0\\n\\xa0\\nCopyright 1994-2004 Sun Microsystems, Inc.\\n\\n\\nA Sun Developer Network Site\\n\\n\\nUnless otherwise licensed, code in all technical manuals herein (including articles, FAQs, samples) is provided under this License.\\n\\xa0\\n\\xa0Content Feeds\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '\\n Spelling Reform\\n\\n\\n\\n  HISTORY OF SPELLING REFORM\\n\\n    This is a history about spelling reform efforts in the U.S. and\\nBritain since the mid-1870s.  Some of this information comes from the\\nresearch of Kenneth Ives, who has published it in a book called \"Written\\nDialects\"; some passages, noted as such, are taken intact from his work.\\nBack to \\nSpelling Reform Page\\n\\n\\n    In 1876, the American Philological Association adopted 11 new\\nspellings, and began promoting their use:\\n\\t    ar   catalog   definit   gard   giv   hav\\n\\t\\tinfinit   liv   tho   thru   wisht*\\n(*see note at end)\\n\\n    Then, as Ken Ives notes, \"Also in 1876, an `International\\nConvention for the Amendment of English Orthography\\' was held in\\nPhiladelphia, during the Centennial Exposition.  This developed into\\nthe Spelling Reform Association.\"\\n\\n    In 1879, the British Spelling Reform Association was founded.  In\\n1886, the American Philological Association (which had earlier proposed\\n11 new spellings) came out with a list of 3500 spellings.\\n\\n    In 1898, the (American) National Education Association began\\npromoting a list of 12 spellings.  They were:\\n       tho  altho  thru  thruout  thoro  thoroly\\n\\t  thorofare  program  prolog  catalog  pedagog  decalog*\\n(*see note at end)\\n \\n\\n    The Simplified Spelling Board was founded in the U.S. in 1906, and\\nhad a list of 300-plus spellings.  One of the founding members was\\nAndrew Carnegie, who donated more than $250,000 over the next several\\nyears.  The Simplified Spelling Society was founded in the U.K. in 1908,\\nas a \"sister\" organization.  (Some more on the Simplified Spelling\\nSociety, which is still operating, a number of paragraphs down.)\\n\\n\\n    U.S. President Theodore Roosevelt also promoted simpler spellings.\\nInitially, he ordered the Government Printing Office to use the\\nSimplified Spelling Board\\'s 300 or so proposed spellings.  This order\\nwas issued on August 27, 1906 (while the U.S. Congress was in recess).\\nThere was resistance from the Government Printing Office and others who\\nwere to carry it out, and when Congress readjourned that fall, they set\\nto revoke Roosevelt\\'s order.  From Ken Ives\\' documentation (his source\\nfor this is \"Our Times,\" Volume 3, by Mark Sullivan, Scribner, 1937), we\\nfind:\\n\\n\\t   Congress ... voted, 142 to 24, that \"no money\\n\\t   appropriated in this act shall be used (for)\\n\\t   printing documents ... unless same shall conform\\n\\t   to the orthography ... in ... generally accepted\\n\\t   dictionaries.\"\\n\\n    Thus, it ended up that simplified spellings were used only in\\nwritten items coming from the White House itself, and at that, only 12\\nwere used.  (I don\\'t know if these were the same 12 that the NEA was\\npromoting.)\\n\\n    The National Education Association continued promoting their list\\nuntil 1921.  The Simplified Spelling Board had a fair amount of\\nactivity until about 1920, and this had been aided by the donations from\\nAndrew Carnegie.  However, Carnegie did not provide any money in his\\nwill for the Spelling Board.\\n\\n    Continuing from Ken Ives\\' research:\\n\\n\\t   With the end of Carnegie funds in 1920, the\\n\\t   Simplified Spelling Board became inactive, and\\n\\t   the Spelling Reform Association was reactivated,\\n\\t   by many of the same people.  It aimed at a more\\n\\t   thorogoing reform.  In 1930, the SRA published\\n\\t   its phonemic alphabet.\\n\\n    A few continued to carry the torch for the Simplified Spelling\\nBoard, in name at least.  The remaining Simplified Spelling Board and\\nthe Spelling Reform Association were merged in 1946, and now there is a\\ngroup with a different name and an additional aim.  An organization\\ntoday called the American Literacy Council, a group as concerned with\\nthe teaching of reading and writing as it is with spelling reform,\\nessentially is the outgrowth of the Spelling Reform Association and\\nthe Simplified Spelling Board.  \\nThe American Literacy Council has a Web site\\n .\\n\\n\\n    This next part, concerning the \"Chicago Tribune,\" was written by\\nKen Ives:\\n    As early as the 1870s, the Chicago Tribune began using reformed\\nspellings.  Joseph Medill, editor and owner, was a member of the Council\\nof the Spelling Reform Association.  In 1880 the Chicago Spelling Reform\\nAssociation met at the Sherman House and read letters approving the\\nTribune\\'s efforts.\\n\\n    About 50 years later, under Medill\\'s grandson, Robert H. McCormick,\\nand editor James O\\'Donnell Bennett, the Tribune began a new effort.\\nThis \"practical test of spelling reform\" started in January 1934, and\\ncontinued for 41 years, with various changes.\\n\\n    An unsystematic list of 80 respelled words was introduced in four\\neditorials over a two month period, and used thereafter in the paper,\\nwhich had the largest circulation in Chicago.  On January 28,\\n\"advertisment, catalog,\" and seven more \"-gue\" words were among those\\nshortened.  The February 11 list included \"agast, ameba, burocrat,\\ncrum, missil, subpena.\"  On February 25, \"bazar, hemloc, herse, intern,\\nrime, sherif, staf,\" were among those introduced.  On March 11 an\\neditorial reported that \"short spelling wins votes of readers 3 to 1.\"\\nOn March 18, the final list included \"glamor, harth, iland, jaz, tarif,\\ntrafic.\"  An editorial that day, \"Why dictionary makers avoid simpler\\nspellings\" claimed that they dare not pioneer, \"prejudice and\\ncompetition prevent it.\"\\n\\n    On September 24, 1939, the list was reduced to 40, but \"tho, altho,\\nthru, thoro,\" were added.  Addition of \"frate, frater\" came on September\\n24, 1945.  Changing \"ph\" not at the start of a word to \"f\" came on July\\n3, 1949, with \"autograf, telegraf, philosofy, photograf, sofomore.\"\\n\\n\\t\\t   (end of passage from Ken Ives)\\n\\n\\n    In the decades following this, the \"Chicago Tribune\" removed more\\nwords from this list.  By the end of the 1960s, \"tarif,\" \"sodder,\"\\n\"clew,\" and \"frate\" were among those dropped.  They used \"thru\" and\\n\"tho\" until 1975, when they basically stopped using simplified\\nspellings.  The newspaper continued to use the \"-log\" for \"-logue\"\\nspellings for a while after that, but then went back to the \"-logue\"\\nforms.\\n\\n\\n    The Simplified Spelling Board\\'s list had, as stated, about 300\\nwords, and one U.S. dictionary maker, for a number of years, listed\\nthese alongside the conventional spellings.  \"Funk & Wagnalls\"\\ndictionaries, at least the larger volumes, did this for at least a few\\ndecades until sometime in the 1950s.  In a \"Funk & Wagnalls\" unabridged\\nfrom 1945 that I\\'ve seen, entries read such as:\\nrough  adj. (ruf) having the texture\\nruf       of coarse or ....\\ndebt  n. (det) a state of owing money\\ndet       or other ....\\n\\n    Thus, \"ruf\" was listed in boldface flush with the margin directly \\nbelow \"rough\"; \"det\" was equally aligned with \"debt,\" etc.\\n\\n\\n    Writer George Bernard Shaw also expressed support for changing\\nEnglish spelling.  In his will, Shaw provided for a \"contest\" to design\\na new, \"phonetic\" (meaning based on the speech of England\\'s late King\\nGeorge V) alphabet for English.  The contest was held during 1958.  The\\nalphabet chosen, which is referred to as the \"Shavian\" alphabet, has 48\\ncharacters, which are different looking from Roman letters; the\\ndesigner\\'s name is Kingsley Read.\\n\\n\\n    The U.K. group, the Simplified Spelling Society, has been operating\\nsince 1908.  They have promoted a few phonetic schemes over the years.\\nIn the 1960s, some British schools agreed to use an idea conceived by\\none of their members, called the \"Initial Teaching Alphabet.\"  (The\\nperson behind it was James Pitman; this was a compromise made with the\\nMinister of Education after an earlier bill had been withdrawn from\\nParliament.)  Basically, children were taught to read and write first\\nusing a totally phonetic system, then later shifted to conventional\\nspelling.  This method was also used in a few schools in the U.S. at\\nthe time.  (I don\\'t know if anyone still uses the Initial Teaching\\nAlphabet.)\\n\\n    At present, the Simplified Spelling Society is officially a forum\\nfor discussing the problems of spelling and different solutions.  They\\naren\\'t officially promoting just one particular scheme now, but there is\\na scheme at the forefront of their work called \"Cut Spelling.\"  This\\nplan calls for removing certain letters from words.\\n\\nThe Society has a Web site.\\n\\n\\n    Better Education thru Simplified Spelling, founded in the U.S. in\\n1978, has been trying to get reform started by encouraging people to use\\n\"tho,\" \"thru,\" and \"hav.\"  (There was talk of dropping \"hav,\" and making\\na \"first stage\" with \"tho\" and \"thru\" and a second one with \"lite\" and\\n\"nite.\")  This group is not an outgrowth of any earlier one, but they\\nhave ties with the Simplified Spelling Society and the American Literacy\\nCouncil.\\n\\n\\n    One item to note is that Charles Darwin and Lord Tennyson gave\\nsupport to the British Spelling Reform Association founded in 1879.  In\\nthe U.S., Mark Twain, in addition to Theodore Roosevelt and Andrew\\nCarnegie, voiced support for the Simplified Spelling Board founded in\\n1906.\\n\\n\\n\\n    This last section looks first at items before the 1870s -- Noah\\nWebster\\'s proposals, at least later ones (Webster\\'s earlier ideas\\ncalled for more spelling reforms) -- and changes since then.\\n\\n    From Ken Ives, we find that \"Webster\\'s plan for reforming English\\nspelling centered on 10 main classes of words\":\\n\\n      1. \"-our\" to \"-or\"\\n      2. \"-re\" to \"-er\"\\n      3. dropping final \"k\" in \"publick,\" etc.\\n      4. changing \"-ence\" to \"-ense\" in \"defence,\" etc.\\n      5. use single \"l\" in inflected forms, e.g. \"traveled\"\\n      6. use double \"l\" in words like \"fulfill\"\\n      7. use \"-or\" for \"-er\" where done so in\\n\\t\\tLatin, e.g. \"instructor,\" \"visitor\"\\n      8. drop final \"e\" to give: ax, determin, definit, infinit,\\n\\t\\tenvelop, medicin, opposit, famin, (others)\\n      9. use single \"f\" at end of words like \"pontif,\" \"plaintif\"\\n     10. change \"-ise\" to \"-ize\" wherever this can be traced\\n\\t     back to Latin and Greek (where a \"z\"/zeta *was* used\\n\\t\\tin the spellings) or a more recent coining which\\n\\t\\t  uses the suffix \"-ize\"  (from Greek \"-izein\")\\n\\n    The U.S. Government Printing Office adopted almost all of the\\nwords in categories 1 thru 7 and category 10 in 1864, and these forms --\\ncolor, center, offense, traveled, organize, etc. -- have been the ones\\nused in all U.S. government documents since.  Many other Americans were\\nalready using these spellings by that time.\\n\\n    Items in category 8 have generally not become the accepted forms in\\nAmerican English, and the closest case would be a word like \"ax/axe,\"\\nwhere the two spellings are equal variants in American usage.\\n\\n\\n    The words promoted by the Simplified Spelling Board beginning in\\n1906 were noted by a set of \"rules,\" each for a certain type of change.\\n\\n    Some rules simply reaffirmed the changes which Webster had set down\\nin his dictionary and which had been adopted by the U.S. Government in\\n1864.  One called for writing \"-or\" instead of \"-our,\" thus \"color,\"\\n\"harbor.\"  Another covered using \"-er\" for \"-re\" as in \"center\" and\\n\"fiber.\"  These spellings were already the preferred forms in many U.S.\\npublications by 1906, but a few Americans were still putting \"centre\"\\netc. into print.\\n\\n    Among the additional rules, a couple called for removing silent\\n\"b\\'s,\" thus \"det,\" \"dout,\" \"lam,\" etc.  A couple more changed some final\\n\"-rr\" and \"-ll\" to single consonants, giving \"bur,\" \"pur,\" \"distil.\"\\n\\n    One of the items concerned respelling \"ough\" when pronounced as\\nlong \"o\" or as the \"oo\" of \"room.\"  There was a section for respelling\\n\"ph\" with \"f,\" while a couple of other rules called for dropping final\\nletters such as \"-ue\" or \"-me.\"\\n\\n    As of 1906, \"phantasy\" was the more common spelling, with \"fantasy\"\\na variant.  \"Fantasy\" then became the more common, standard spelling in\\nthe U.S. and Britain.  \"Programme\" was the preferred, dominant spelling\\nin the U.S. as well as other English-speaking countries at the turn of\\nthe century.  \"Program,\" one of the spellings promoted by the (U.S.)\\nNational Education Association and others at the time, went on to become\\nthe standard U.S. spelling by the middle of the 20th century.  (Further\\n\"program\" is standard in all English-speaking countries for the computer\\nsense.)  \"Catalog,\" another word promoted in these movements, has become\\nthe preferred form in U.S. publishing over the past few decades.\\n\\n    Of the other words promoted by the (U.S.) National Education\\nAssociation, \"thru,\" \"tho,\" \"altho,\" \"prolog,\" and \"decalog/Decalog\" are\\nlisted in American English dictionaries as acceptable variants, and\\n\"thoro\" and \"pedagog\" can sometimes be found listed as informal or\\nvariant forms.\\n\\n\\n        ================================================\\n\\n\\nOther sources (all published in the U.S.):  H.L. Mencken, \"The American\\n   Language\" (Alfred A. Knopf, 1977; there are many printings of this),\\n   pages 479-497;  David Grambs, \"Death By Spelling\" (Harper & Row,\\n   1989), pages 55-59;  \"Merriam-Webster\\'s Dictionary of English Usage\"\\n   (1989), pages 864-866, 906;  Also, a doctoral thesis from Columbia\\n   University: Abraham Tauber, \"Spelling Reform in the United States\"\\n   (1958).\\n\\nSee too the entry for \"spelling reform\" in the \"Oxford Companion to the\\n   English Language.\"\\n\\n\\n        ================================================\\n\\n\\n*Notes Regarding the 1876 and 1898 Lists of Words:\\n\\n    I have found two slightly different lists of what the 12 words were\\nthat the U.S. National Education Association began promoting in 1898,\\nand have also found more than one set given as being what the American\\nPhilological Association adopted in 1876.  I checked this against a few\\nmore sources beyond those just noted, and for the record, here are the\\ndifferences:\\n\\n    Per H.L. Mencken\\'s \"The American Language,\" \"Compton\\'s Encyclopedia\\nOnline,\" and David Grambs\\' \"Death By Spelling,\" the 12 words which the\\n(American) National Education Association selected and began promoting\\nin 1898 were:\\n       tho  altho  thru  thruout  thoro  thoroly\\n\\t  thorofare  program  prolog  catalog  pedagog  decalog\\n\\n    Per Ken Ives\\' \"Written Dialects\" and Abraham Tauber\\'s \"Spelling\\nReform in the United States,\" the 12 words were:\\n       tho  altho  thru  thruout  thoro  thorofare\\n\\t  program  prolog  catalog  pedagog  decalog  demagog\\n    Thus, the first list contains \"thoroly\" but doesn\\'t have \"demagog\";\\nthis second list has \"demagog,\" but not \"thoroly.\"  Additionally, Tauber\\nshows \"Decalog\" with a capital \"d.\"\\n\\n    Further, to the second list of words, we have this from \"The\\nGreatest Good Fortune,\" a biography of Andrew Carnegie written by Simon\\nGoodenough (Macdonalds, Edinburgh, 1985):\\n       \"Fifty distinguished Americans were approached who would\\n\\tagree to adopt the simplified spelling of several\\n\\tcommonly used words, altho, catalog, decalog, demagog,\\n\\tpedagog, prolog, tho, thoro, thorofare, thru, and\\n\\tthruout.\"\\n\\n\\n    The organization currently known as the National Education\\nAssociation in the U.S. was called the National Educational Association\\nin 1898.  This organization was founded in 1857 as the National Teachers\\nAssociation, became the National Educational Association in 1870, then\\nthe National Education Association in 1906.\\n\\n\\n    Per H.L. Mencken and David Grambs, the American Philological\\nAssociation adopted 11 words in 1876.  These words were:\\n\\t    ar   catalog   definit   gard   giv   hav\\n\\t\\tinfinit   liv   tho   thru   wisht\\n\\n    Per Ken Ives, 10 words were adopted by the APA at that time:\\n\\t    ar   catalog   definit   gard   giv   hav\\n\\t\\t    liv   tho   thru   wisht\\n\\n    Per Abraham Tauber, the APA chose 11 words, and they were:\\n\\t    ar   catalog   definit   gard   giv   hav\\n\\t        infinit   liv   tho   thru   wich\\n\\n\\n    Also, if you read H.L. Mencken\\'s \"The American Language\" you will\\nfind that January 28, 1935 is given as the date that the \"Chicago\\nTribune\" made the first in its series of announcements of simplified\\nspellings, while the source I note above gives it as January 28, 1934.\\nI have a copy of this first announcement which I made from microfilm of\\nthe \"Chicago Tribune,\" and it is indeed 1934 and not 1935.\\nby Cornell Kimball\\n \\n',\n",
       " '\\n\\n\\n\\nConstraint Guide - Constraint Hierarchy Solvers\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGuide to Constraint\\n         Programming\\n\\n\\n©\\n         Roman Barták, 1998\\n\\n\\n\\n\\nContents\\n\\n\\nPrev\\n\\n\\nUp\\n\\n\\nNext\\n\\n\\n\\n\\nOver-Constrained Problems\\nAlgorithms for Solving\\nConstraint Hierarchies\\n\\n\\n\\n[Simple]\\n[DeltaStar]\\n[DeltaBlue]\\n[SkyBlue]\\n[Indigo]\\n[Houria]\\n[IHCS] [Projection]\\nA important aspect of\\nconstraint hierarchies is that there are efficient satisfaction\\nalgorithms proposed to solve them. In this section we overview some\\nof the most popular algorithms for solving constraint hierarchies. We\\ncan categorize these algorithms into the following two general\\napproaches:\\n\\nthe refining\\n   method\\n   and\\nthe local\\n   propagation.\\n\\n\\n\\n\\nRefining Algorithms\\nThe refining algorithms\\nfirst satisfy the constraints on the strongest level of the\\nhierarchy, and then, the constraints on the weaker levels\\nsuccessively. It is a straightforward method for solving constraint\\nhierarchies as it follows directly the definition of the solution of\\nconstraint hierarchy, in particular, the property of respecting the\\nhierarchy. Consequently, the refining algorithms can be used to solve\\nall constraint hierarchies, i.e., all type of constraints, using\\narbitrary comparator.\\nThe disadvantage of refining\\nmethod is that it has to recompute the solution from scratch\\neverytime a constraint is added or retracted.\\n\\n\\n\\nSimple\\n            Algorithm\\n\\n\\n\\n\\nM. Wilson, A.\\n            Borning: Hierarchical Constraint Logic Programming, TR\\n            93-01-02a, Department of Computer Science and\\n            Engineering, University of Washington, May\\n            1993\\n\\n\\n\\n\\nThe simple algorithm for\\n   solving constraint hierarchies performs a recursive search for\\n   answers representing locally-predicate-better solutions. It uses\\n   underlying \"flat\" constraint solver to solve individual\\n   constraints, i.e., to test consistnency of the system of\\n   constraints. The solution is accumulated as a set Answer of\\n   satisfiable constraints. Alternative solutions can be found by\\n   choosing constraints from Level in a different order.\\n\\n\\n\\nSimple Algorithm for Solving Constraint Hierarchies\\n            \\n            \\n\\n\\n\\n\\n\\nprocedure Solve(H: constraint hierarchy)\\n  Answer <- empty;\\n  Untried <-H;\\n  while Untried is not empty do\\n    s <- the strongest level of the constraints in Untried;\\n    Level <- delete constraints with level s from Untried;\\n    while Level is not empty do\\n      c <- delete a constraint from Level; % different order = alternative solutions\\n      if c is compatible with Answer then  % call to flat constraint solver\\n        Answer <- Answer + c;\\n      endif\\n    endwhile\\n  endwhile\\n  return Answer;\\nend Solve\\n\\n\\n\\n\\nYou can find implementation\\n   of extended version of the simple constraint hierarchy solver for\\n   various predicate comparators (even global ones) at my\\n   Projects\\n   pages.\\n\\xa0\\n\\n\\n\\nDeltaStar\\n\\n\\n\\n\\nM. Wilson, A.\\n            Borning: Hierarchical Constraint Logic Programming, TR\\n            93-01-02a, Department of Computer Science and\\n            Engineering, University of Washington, May\\n            1993\\n\\n\\n\\n\\nWhile the simple algorithm\\n   described above can solve constraint hierarchies using predicate\\n   comparators only, the DeltaStar algorithms is able to use metric\\n   comparators as well. Again, it uses underlying flat constraint\\n   solver to solve constraints, but now it requires the flat\\n   constraint solver to provide the procedure\\nfilter(S:Solution, C:Set of constraints) -> Solution,\\nthat given an existing\\n   solution S returns the subset of S that minimizes the error in\\n   satisfying the set of constraints C (the implementation of this\\n   routine effectively defines the comparator). In addition, the flat\\n   solver should provide other entries for efficiently determing if a\\n   new constraint is compatible with a current solution, and for\\n   quickly adding a constraint to a current solution, given a\\n   guarantee that the constraint is compatible.\\n\\n\\n\\nDeltaStar\\n            \\n            \\n\\n\\n\\n\\n\\nprocedure DeltaStar(H: constraint hierarchy)\\n  i <- 1;\\n  Solution <- solution of required constraints from H\\n  while not unique Solution and i<number of levels do\\n    Solution <- filter(Solution,Hi);    % Hi is i-th level in H\\n    i++;\\n  endwhile\\n  return Solution;\\nend DeltaStar\\n\\n\\n\\n\\nDeltaStar uses Simplex\\n   algorithm as the underlying flat constraint solver, and therefore\\n   it requires some transformation techniques to be applied to the\\n   constraints before they can be solved using Simplex.\\nThe following figure\\n   illustrates the idea behind the DeltaStar Algorithm.\\n\\n\\n\\n\\nLocal Propagation Algorithms\\nThe local propagation\\nalgorithms gradually solve constraint hierarchies by repeatedly\\nselecting uniquely satisfiable constraints. In this technique, a\\nsingle constraint is used to determine the value for a variable. Once\\nthis variable\\'s value is known, the system may be able to use another\\nconstraint to find a value for another variable, and so forth. This\\nstraightforward execution phase is paid off by a foregoing planning\\nphase that chooses the order of constraints to satisfy.\\nTo support local propagation,\\nthe object representing a constraint includes one or more pieces of\\ncode (methods). Method is a function whose arguments are input\\nvariables and that caculates a value for an output variable(s) that\\nwill satisfy the constraint (for example, constraint\\nA+B=C,\\nin general, includes three methods C<-A+B,\\nA<-C-B, B<-C-A).\\nLocal propagation algorithms select one (or none) method for each\\nconstraint and determine the appropriate order of methods to solve\\nthe constraint hierarchy.\\nThe advantage of this approach\\nis that when a variable is repeatedly updated, e.g., by user\\noperation, it can easily evaluate only the necessary constraints to\\nget a new solution.\\nLocal propagation is also\\nrestricted in some ways:\\n\\nmost local propagation\\n   algorithms solve equality constraints only,\\nthey use locally-predicate\\n   better comparator or its variant only,\\nthey are not able to solve\\n   \"cycles\" of constraints (without additional gadgets),\\nthey cannot find multiple\\n   solutions due to uniqueness.\\n\\n\\xa0\\n\\n\\n\\nDeltaBlue\\n\\n\\n\\n\\nM. Sannella, B.\\n            Freeman-Benson, J. Maloney, A. Borning: Multi-way versus\\n            One-way Constraints in User Interfaces: Experience with\\n            the DeltaBlue Algorithm, TR 92-07-05\\n\\n\\n\\n\\nDeltaBlue is a typical\\n   representative of local propagation algorithms. It solves\\n   multi-way (allows more methods for a constraint) constraints with\\n   one-output variable. DeltaBlue stores the current solution in the\\n   form of a solution graph, which describes how to recompute values\\n   for variables in order to satisfy all the satisfiable constraints.\\n   The following figure shows such solution graphs: each node in the\\n   graph represents a variable, the arcs represent constraints,\\n   labeled with their strengths. Arrows on the arcs show which\\n   methods are used, while dotted arcs indicate constraints that are\\n   unsatisfied.\\n\\nDeltaBlue supports separate\\n   planning and execution stages. Given a constraint graph, the\\n   algorithm can be used to find a plan for re-satisfying the\\n   constraints. During the planning stage, DeltaBlue constructs\\n   incrementally the solution graph by adding constraints to the\\n   graph. The key idea behind DeltaBlue is to associate extra\\n   information, walkabout strength, with the constrained\\n   variables so that the solution graph can be updated incrementally\\n   when a constraint is added or removed without examining, on the\\n   average, more than a small fraction of the entire constraint\\n   hierarchy. Walkabout strength is the weaker of the stregth of the\\n   constraint currently determining the variable and the weakest\\n   walkabout strength among all other potential output of this\\n   constraint (if the variable is not detemined by any constraint,\\n   then the walkabout strength is weakest).\\n\\n\\n\\nDeltaBlue\\n            \\n            \\n\\n\\n\\n\\n\\nprocedure AddConstraint(c: constraint)\\n  select the potential output variable V of c with the weakest walkabout strength\\n  if walkabout strength of V is weaker than strength of c then\\n    c\\'<- the constraint currently determing the variable V;\\n    make c\\' unsatisfied;\\n    select the method determing V in c;\\n    recompute walkabout strengths of downstream variables;\\n    AddConstraint(c\\');\\n  endif\\nend AddConstraint\\n\\n\\n\\n\\nThe following example\\n   animates the process of adding a strong constraint to the\\n   constraint graph. Five variables are linked in a chain by required\\n   equality constraints, and a weak constraint has been added to the\\n   rightmost variable. Nodes are labeled by walkabout\\n   stregths.\\n\\nDeltaBlue has two\\n   limitations: cycles of constraints are prohibited, and the\\n   procedures (methods) used to satisfy a constraint can only have a\\n   single output.\\n\\xa0\\n\\n\\n\\nSkyBlue\\n\\n\\n\\n\\nM. Sannella: The\\n            SkyBlue Constraint Solver, TR 92-07-02, Department of\\n            Computer Science and Engineering, University of\\n            Washington, February 1993\\n\\n\\n\\n\\nSkyBlue is a successor to\\n   the DetlaBlue algorithm, which relaxes the restrictions of\\n   DeltaBlue, i.e., allowing cycles of constraints to be constructed\\n   (although SkyBlue may not be able to satisfy all of the\\n   constraints in a cycle) and supporting multi-output\\n   methods.\\nSkyBlue utilizes the same\\n   ideas as the DeltaBlue algorithm. Again, it incrementally\\n   constructs the constraint network (now called a method graph) and\\n   uses the generalized notion of walkabout strength to recompute\\n   only the small fraction of the graph after adding or removing a\\n   constraint.\\n\\xa0\\n\\n\\n\\nIndigo\\n\\n\\n\\n\\nA. Borning, R.\\n            Anderson, B. Freeman-Benson: The Indigo Algorithm, TR\\n            96-05-01, Department of Computer Science and Engineering,\\n            University of Washington, July 1996\\n\\n\\n\\n\\nIndigo is an efficient local\\n   propagation algorithm for satisfying acyclic constraint\\n   hierarchies, including inequality constraints, using\\n   locally-metric-better comparator. The key idea in Indigo is that\\n   it propagates lower and upper bounds on variables (i.e.\\n   intervals), rather than specific values. The constraints are\\n   processed from strongest to weakest, tightening the bounds on\\n   variables.\\nIn contrast to DeltaBlue and\\n   SkyBlue, in Indigo, each constraint has a collection of bounds\\n   propagation methods. Thus, the A+B=C constraint has three bounds\\n   propagation methods, which tighten the bounds on A, B, and C\\n   respectively. If we have previously tightened the bounds on say C,\\n   when we process the constraint A+B=C we may then need to tighten\\n   the bounds on both A and B. This is a sharp contrast to the\\n   behaviour of standard local propagation algorithms, in which to\\n   satisfy a constraint a single method is executed (and hence a\\n   single variable changed). Also, tightening the bounds on\\n   constraint\\'s variables may cause the bounds on other variables to\\n   be tightened, rippling out to further variables. To implement\\n   this, the algorithm keeps a queue of constraints to be\\n   checked.\\n\\n\\n\\nIndigo\\n            \\n            \\n\\n\\n\\n\\n\\nprocedure Indigo(H: constraint hierarchy)\\n  all_constraints <- list of constraints in H, strongest first;\\n  all_variables <- empty;\\n  active_constraints <- empty;\\n  for each v in all_variables do\\n    initialize v.bounds to unbounded;\\n  endfor\\n               \\n  for current_constraint in all_constraints do\\n    tigh_variables <- empty;\\n    queue <- empty;\\n    queue <- queue + current_constraint;\\n    while queue not empty do\\n      cn <- queue.front;\\n      tighten_bounds(cn,queue,tight_variables,active_constraints);\\n      check_constraint(cn,active_constraints);\\n      queue.dequeue;\\n    endwhile\\n  endfor\\nend Indigo\\n\\n\\n\\n\\nThe following tables\\n   illustrate the process of bounds propagation in Indigo:\\n\\n\\n\\nConstraint\\n            Hierarchy:\\n\\n\\n\\n\\nc1: required a>=10 \\nc2: required b>=20 \\nc3: required a+b=c \\nc4: required c+25=d\\nc5:   strong d<=100\\n\\n\\nc6: medium a=50 \\nc7:   weak a=5  \\nc8:   weak b=5  \\nc9:   weak c=100\\nc10:  weak d=200\\n\\n\\n\\n\\n\\n\\n\\naction\\n\\n\\na\\n\\n\\nb\\n\\n\\nc\\n\\n\\nd\\n\\n\\nnote\\n\\n\\n\\n\\n\\xa0\\n\\n\\n(-inf,inf)\\n\\n\\n(-inf,inf)\\n\\n\\n(-inf,inf)\\n\\n\\n(-inf,inf)\\n\\n\\ninitial\\n            bounds\\n\\n\\n\\n\\nadd\\n            c1\\n\\n\\n[10,inf)\\n\\n\\n(-inf,inf)\\n\\n\\n(-inf,inf)\\n\\n\\n(-inf,inf)\\n\\n\\n\\xa0\\n\\n\\n\\n\\nadd\\n            c2\\n\\n\\n[10,inf)\\n\\n\\n[20,inf)\\n\\n\\n(-inf,inf)\\n\\n\\n(-inf,inf)\\n\\n\\n\\xa0\\n\\n\\n\\n\\nadd\\n            c3\\n\\n\\n[10,inf)\\n\\n\\n[20,inf)\\n\\n\\n[30,inf)\\n\\n\\n(-inf,inf)\\n\\n\\n\\xa0\\n\\n\\n\\n\\nadd\\n            c4\\n\\n\\n[10,inf)\\n\\n\\n[20,inf)\\n\\n\\n[30,inf)\\n\\n\\n[55,inf)\\n\\n\\n\\xa0\\n\\n\\n\\n\\nadd\\n            c5\\n\\n\\n[10,inf)\\n\\n\\n[20,inf)\\n\\n\\n[30,inf)\\n\\n\\n[55,100]\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\xa0\\n\\n\\n[10,inf)\\n\\n\\n[20,inf)\\n\\n\\n[30,75]\\n\\n\\n[55,100]\\n\\n\\npropagate bounds\\n            using c4\\n\\n\\n\\n\\n\\xa0\\n\\n\\n[10,55]\\n\\n\\n[20,65]\\n\\n\\n[30,75]\\n\\n\\n[55,100]\\n\\n\\npropagate bounds\\n            using c3\\n\\n\\n\\n\\nadd\\n            c6\\n\\n\\n[50,50]\\n\\n\\n[20,65]\\n\\n\\n[30,75]\\n\\n\\n[55,100]\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\xa0\\n\\n\\n[50,50]\\n\\n\\n[20,25]\\n\\n\\n[70,75]\\n\\n\\n[55,100]\\n\\n\\npropagate bounds\\n            using c3\\n\\n\\n\\n\\n\\xa0\\n\\n\\n[50,50]\\n\\n\\n[20,25]\\n\\n\\n[70,75]\\n\\n\\n[95,100]\\n\\n\\npropagate bounds\\n            using c4\\n\\n\\n\\n\\nadd\\n            c7\\n\\n\\n[50,50]\\n\\n\\n[20,25]\\n\\n\\n[70,75]\\n\\n\\n[95,100]\\n\\n\\nc7 is\\n            unsatisfied\\n\\n\\n\\n\\nadd\\n            c8\\n\\n\\n[50,50]\\n\\n\\n[20,20]\\n\\n\\n[70,75]\\n\\n\\n[95,100]\\n\\n\\nc8 is unsatisfied\\n            but its error is minimized\\n\\n\\n\\n\\n\\xa0\\n\\n\\n[50,50]\\n\\n\\n[20,20]\\n\\n\\n[70,70]\\n\\n\\n[95,100]\\n\\n\\npropagate bounds\\n            using c3\\n\\n\\n\\n\\n\\xa0\\n\\n\\n[50,50]\\n\\n\\n[20,20]\\n\\n\\n[70,70]\\n\\n\\n[95,95]\\n\\n\\npropagate bounds\\n            using c4\\n\\n\\n\\n\\nadd\\n            c9\\n\\n\\n[50,50]\\n\\n\\n[20,20]\\n\\n\\n[70,70]\\n\\n\\n[95,95]\\n\\n\\nc9 is\\n            unsatisfied\\n\\n\\n\\n\\nadd\\n            c10\\n\\n\\n[50,50]\\n\\n\\n[20,20]\\n\\n\\n[70,70]\\n\\n\\n[95,95]\\n\\n\\nc10 is\\n            unsatisfied\\n\\n\\n\\n\\n\\xa0\\n\\n\\n\\nHouria\\n            III\\n\\n\\n\\n\\nM. Bouzoubaa, B.\\n            Neveu, G. Hasle: Houria III: Solver for Hierarchical\\n            System, Planning of Lexicographic Weight Sum Better Graph\\n            for Functional Constraints, in: 5th INFORMS Computer\\n            Science Technical Section Conference on Computer Science\\n            and Operations Research, Dallas, January\\n            1996\\n\\n\\n\\n\\nHouria III is an incremental\\n   local propagation algorithm for solving constraint hierarchies\\n   using globally-better comparators. Similarly to SkyBlue, the\\n   Houria III algorithm constructs method graphs that are used to\\n   propagate values through constraints. But, Houria III constructs\\n   \"all\" possible method graphs and selects those graphs that satisfy\\n   best the constraints. It starts with graphs for required\\n   constraints and than it adds incrementally soft constraints to\\n   these graphs. To decrease number of maintained graphs, Houria III\\n   uses only those partial graphs that can become the solution graphs\\n   after adding the constraint.\\n\\n\\n\\nHouria III\\n            \\n            \\n\\n\\n\\n\\n\\nprocedure Houria(H: constraint hierarchy)\\n  graphs <- all method graphs for required constraints in H;\\n  queue <- soft (non-required) constraints in H\\n  while queue not empty do\\n    c <- delete the strongest constraint from queue;\\n    graphs <- add c to graphs that can become solution graphs;\\n  endwhile\\nend Houria\\n\\n\\n\\n\\nThe following figures\\n   animate the process of solving constraint hierarchy using the\\n   Houria III algorithm.\\n\\n\\n\\nConstraint\\n            Hierarchy:\\n\\n\\n\\n\\nrequired A+B=C      \\n  strong C=5   (0.5)\\n  strong A=3   (0.8)\\n  strong B=3   (0.8)\\n\\n\\n\\n\\n1) alternative method graphs\\n   for required constraint (squares = constraints, circles =\\n   variables)\\n\\n2) add methods for the\\n   constraint strong\\n   C=5; numbers indicate\\n   the order of graphs usign weighted-sum-better comparator, dotted\\n   arcs indicate unsatisfied constraint\\n\\n3) add methods for the\\n   constraint strong A=3\\n\\n4) add methods for the\\n   constraint strong B=3\\n\\n\\xa0\\n\\n\\n\\nOthers\\n\\n\\n\\n\\n            Incremental Hierarchical Constraint Solver\\n            (IHCS)\\n\\n\\n\\n\\nF. Menezes, P.\\n            Barahoma, P. Codognet: An Incremental Hierarchical\\n            Constraint Solver, in: Proceedings of PPCP93, pp.\\n            190-199, Newport, 1993\\n\\n\\n\\n\\nIncremental Hierarchical\\n   Constraint Solver (IHCS) is an incremental algorithm for solving\\n   constraint hierarchies over finite domains using\\n   localy-predicate-better comparator. It is based on idea of\\n   transforming initial configuration corresponding to the constraint\\n   hierarchy to the solution configuration. A configuration of\\n   the hierarchy H is a triple AS•RS•US (AS - active store,\\n   RS - relaxed store, US - unexplored store) such that the union of\\n   AS, RS and US is equal to H.\\nThe algorithm starts with\\n   the configuration 0•0•H and succussively moves\\n   constraints from US (initially H) to AS (initially empty) using so\\n   called forward rule. As soon as any conflict appears\\n   (consistency\\n   techniques are used\\n   to detect conflicts among constraints), the backward rule is\\n   called to change the configuration by switching some constraints\\n   between sets AS, RS and US. The algorithm stops as soon as the\\n   configuration is AS•RS•0.\\n\\n\\n\\nIHCS\\n            \\n            \\n\\n\\n\\n\\n\\nprocedure IHCS(H: constraint hierarchy)\\n  AS•RS•US <- 0•0•H;\\n  while US not empty do\\n    apply forward rule to AS•RS•US, i.e., move c from US to AS\\n    if conflict in AS then\\n      apply backward rule to AS•RS•US;\\n    endif\\n  endwhile\\nend IHCS\\n\\n\\n\\n\\nThe following tables\\n   illustrate the process of applying forward and backward rules to\\n   solve the constraint hierarchy (the initial domains of variables X\\n   and Y are [1..10]):\\n\\n\\n\\nConstraint\\n            Hierarchy:\\n\\n\\n\\n\\nc1: strong X+Y=15\\nc2: strong 3X-Y<5\\nc3:   weak X>Y+1 \\nc4:   weak X<7   \\n\\n\\n\\n\\n\\n\\n\\naction\\n\\n\\nconfiguration\\n\\n\\nD(X)\\n\\n\\nD(Y)\\n\\n\\nrule\\n\\n\\n\\n\\nadd\\n            c1\\n\\n\\n{        }•{     }•{c1}\\n\\n\\n1..10\\n\\n\\n1..10\\n\\n\\nfw\\n\\n\\n\\n\\n\\xa0\\n\\n\\n{c1      }•{     }•{  }\\n\\n\\n5..10\\n\\n\\n5..10\\n\\n\\n\\xa0\\n\\n\\n\\n\\nadd\\n            c2\\n\\n\\n{c1      }•{     }•{c2}\\n\\n\\n5..10\\n\\n\\n5..10\\n\\n\\nfw\\n\\n\\n\\n\\n\\xa0\\n\\n\\n{c1,c2   }•{     }•{  }\\n\\n\\n-\\n\\n\\n-\\n\\n\\nbw\\n\\n\\n\\n\\n\\xa0\\n\\n\\n{c1      }•{c2   }•{  }\\n\\n\\n5..10\\n\\n\\n5..10\\n\\n\\n\\xa0\\n\\n\\n\\n\\nadd\\n            c3\\n\\n\\n{c1      }•{c2   }•{c3}\\n\\n\\n5..10\\n\\n\\n5..10\\n\\n\\nfw\\n\\n\\n\\n\\n\\xa0\\n\\n\\n{c1,c3   }•{c2   }•{  }\\n\\n\\n7..10\\n\\n\\n5..8\\n\\n\\n\\xa0\\n\\n\\n\\n\\nadd\\n            c4\\n\\n\\n{c1,c3   }•{c2   }•{c4}\\n\\n\\n7..10\\n\\n\\n5..8\\n\\n\\nfw\\n\\n\\n\\n\\n\\xa0\\n\\n\\n{c1,c3,c4}•{c2   }•{  }\\n\\n\\n-\\n\\n\\n-\\n\\n\\nbw\\n\\n\\n\\n\\n\\xa0\\n\\n\\n{c1,c3   }•{c2,c4}•{  }\\n\\n\\n7..10\\n\\n\\n5..8\\n\\n\\n\\xa0\\n\\n\\n\\n\\xa0\\n\\n\\n\\nProjection\\n            Algorithm\\n\\n\\n\\n\\nW. Harvey, P.J.\\n            Stuckey, A. Borning: Compiling Constraint Solving using\\n            Projection, in: Proceedings of CP97, Austria,\\n            October/November 1997\\n\\n\\n\\n\\nThe Indigo algorithm is able\\n   to solve acyclic equality and disequality constraints but as soon\\n   as the cycles appear, Indigo fails. Therefore another algorithm\\n   based on projection was proposed to solve arbitrary sets of linear\\n   equality and disequality constraints using locally-error-better\\n   comparator. This algorithm successively eliminates variables using\\n   either Gaussian or Fourier elimination.\\nFirst, for each variable x\\n   in vars (C), the set C of constraints is partitioned in the\\n   following way:\\n\\nC(0,x): constraints in C\\n      that do not contain x,\\nC(=,x): equations in C\\n      containing x,\\nC(+,x): inequalities in\\n      C that are equivalent to an inequality of the form\\n      x<=e,\\nC(-,x): inequalities in\\n      C that are equivalent to an inequality of the form\\n      e<=x.\\n\\nThen, the projection\\n   algorithm shown bellow eliminates a variable x from the constraint\\n   set C.\\n\\n\\n\\nProjection Algorithm\\n            \\n            \\n\\n\\n\\n\\n\\nprocedure project(C: set of constraints, x: variable)\\n  if exists c in C(=,x) where c is x=e then\\n    D <- C-{c} with every occurence of x replaced by e;\\n  else\\n    D <- C(0,x);\\n    foreach c in C(+,x) where c is x<=e+ do\\n      foreach c in C(-,x) where c is e-<=x do\\n        D <- D union {e-<=e+};\\n      endfor\\n    endfor\\n  endif\\n  return D;\\nend project\\n\\n\\n\\n\\nFourier elimination steps in\\n   the projection algorithm tend to produce a large number of\\n   redundant constraints which can be detected and removed from the\\n   constraint store. The following example shows the process of\\n   elimination of variables in the order xl, xr, xm (note that the\\n   reduntant constraints are removed). In the second stage, the\\n   particular solution is constructed by assigning values to the\\n   variables.\\n\\n\\n\\nExample:\\n\\n\\n\\n\\n      2xm = xl+xr   \\n   xl+10 <= xr      \\nxl,xm,xr <= 100     \\n       0 <= xl,xm,xr\\n\\n\\n   xm+5 <= xr \\n2xm-100 <=xr  \\n     xr <= 2xm\\n  xm,xr <= 100\\n\\n\\n 5 <= xm\\nxm <= 95\\n\\n\\n5 <= 95\\n\\n\\n\\n\\n------->\\n<-------\\n\\n\\n------->\\n<-------\\n\\n\\n---->\\n<----\\n\\n\\n--->\\n---<\\n\\n\\n\\n\\nxl=2xm-xr,\\ni.e. xl=30\\n\\n\\nxr in [55..100]\\nsay xr=70\\n\\n\\nxm in [5..95]\\nsay xm=50\\n\\n\\nconstraints are satisfiable\\n\\n\\n\\n\\nNow, the projection\\n   algorithm can be used to solve hierarchy of equality and\\n   disequality constraints. The algorithm assumes that the only\\n   constraints at non-required levels are in the form x=b, where x is\\n   a variable and b is a constant. This is not a problem as each\\n   non-required constraint e?b@pref (e is a linear expression, ? is\\n   =, <= or >=) can be rewritten as e?ve@required &\\n   ve=b@pref (ve is a new variable).\\nThe solver applies the\\n   projection algorithm into the set of required constraints and it\\n   eliminates successively the variables in the order defined by the\\n   constraint hierarchy (the variables appearing in the weak\\n   constraints only are eliminated first). In the second stage, the\\n   solver selects a value for the variable v from the interval\\n   computed by the projection algorithm in such a way that the value\\n   is closest to the constant b from the strongest non-required\\n   constraint x=b.\\n\\xa0\\n[Simple]\\n[DeltaStar]\\n[DeltaBlue]\\n[SkyBlue]\\n[Indigo]\\n[Houria]\\n[IHCS] [Projection]\\n\\n\\n\\nContents\\n\\n\\nPrev\\n\\n\\nUp\\n\\n\\nNext\\n\\n\\n\\n\\nDesigned and\\n         maintained by Roman\\n         Barták\\n\\n\\n\\n\\n\\n\\n',\n",
       " '\\n\\n\\n\\n\\nRapide -- Overview\\n\\n\\n\\n\\n\\nThe Stanford Rapide Project\\n\\n\\n\\n\\n\\n\\n\\nOverview Of The Rapide Prototyping Project\\nThe Rapide Language effort focuses on developing a new technology\\nfor building large-scale, distributed multi-language systems. This\\ntechnology is based upon a new generation of computer languages,\\ncalled Executable Architecture Definition Languages (EADLs), and an\\ninnovative toolset supporting the use of EADLs in evolutionary\\ndevelopment and rigorous analysis of large-scale systems.\\n\\nThe present Rapide-1.0 effort leverages off of Stanford\\'s Rapide-0.2\\neffort, and on our prior work on applying formal methods to Ada and\\nVHDL.\\n\\nRapide is designed to support component-based development of large,\\nmulti-language systems by utilizing architecture definitions as the\\ndevelopment framework. Rapide allows gradual refinement of\\narchitectures into products, and supports testing and maintenance\\nbased on automated comparison with formal standard architectures.\\nRapide adopts a new event-based execution model of distributed,\\ntime-sensitive systems -- the \"timed poset model.\" Posets provide the\\nmost detailed formal basis to date for constructing early life cycle\\nprototyping tools, and later life cycle tools for correctness and\\nperformance analysis of distributed time-sensitive systems.\\n\\n\\nObjectives\\n\\nRapide focuses on introducing new elements into industrial simulation\\ntechnology as incremental extensions of current tools and methods.\\n\\nSpecification Capabilities For EADLs\\n\\nThe Rapide specification language is planned to allow specification of\\nboth constraints on concurrent patterns of events (posets) and\\nreal-time constraints.  The associated support tools will provide\\nautomated formal analysis (both proof theoretic and simulation\\nchecking) of highly detailed properties of large-scale distributed\\nsystem architectures containing both hardware and software components.\\nWe plan to utilize formal constraints in developing new techniques of\\nperformance analysis of complex prototype systems.  \\n\\nEADL-Based Formal Reference Architectures For Industrial Systems\\n\\nWe plan to define formal reference architectures for particular\\nindustrial systems in Rapide (e.g., X/Open DTP and other industry\\nstandards), analyze them formally for various critical properties,\\n(e.g., consistency, liveness, durability and real-time constraints),\\nand propose them as standard formal architecture definitions.  \\n\\nConsistency Analysis Tools To Compare Architectures\\n\\nRapide support tools will include comparative simulation tools to\\nsupport consistency analysis between architectures. These tools will\\nenable an actual system to be executed and tested for conformance with\\na formal reference architecture, thus providing a new technology for\\nindustrial standardization.\\n\\nArchitectural Frameworks For Distributed, Multi-Lingual Systems.\\n\\nA Rapide toolset is being developed to support the use of executable\\narchitectures as frameworks for composing industrial-size systems that\\ncontain components in different languages (e.g VHDL, Ada, C++, C,\\nRapide) and can be executed on multiple workstations.\\n\\nFormal Methods For EADLs\\n\\nBy defining formal mappings of other architecture formalisms (e.g.,\\nHoneywell\\'s MetaH, VHDL, Verilog, LOTOS) into Rapide, we will develop\\ngeneral techniques for defining formal semantics for domain specific\\nEADLs and a proof theory for component-based architectures in many\\nEADLs.\\n\\n\\nRapide Language\\nRapide is a language framework consisting of (i) a type language, (ii)\\nan executable architecture definition language, (iii) a specification\\nlanguage, and (iv) a concurrent reactive programming language.  The\\narchitecture definition, specification and reactive programming\\nlanguages also share a pattern sublanguage that provides expressions\\nfor reacting to and constraining event-based computations.  While\\nthese languages satisfy certain compatibility requirements (e.g., they\\nhave the same visibility, scoping and naming rules, and underlying\\nevent-based execution model), the Rapide language framework is\\ndesigned to permit alternative programming or specification languages\\nto be used in conjunction with the type language.\\n\\nThe type language is based on a single general interface type\\nconstruct together with inheritance derivations for building new\\ninterfaces from existing ones.  The form of \"derivation\" provided\\nresembles inheritance mechanisms in object-oriented languages.  All\\nstandard kinds of types (e.g., arrays, classes, task types) are\\nparticular instances of interface types.  The type language is used to\\ndefine interfaces of components (both hardware and software) of a\\nsystem.\\n\\n\\nThe architecture definition language provides executable\\nfeatures for composing systems out of component interfaces by defining\\ntheir synchronization and communication interconnections in terms of\\npatterns of events. These features provide powerful new constructs for\\ndefining very large, possibly dynamic, architectures.  Moreover, event\\npattern constructs are used to define mappings between architectures.\\nThese EADL mappings are implementable to provide new tools for\\nautomatically comparing system architectures at differing abstraction\\nlevels in hierarchically designed systems, and for monitoring actual\\nsystems for conformance with industry standard architectures.\\n\\n\\nThe constraint language provides constructs for abstract\\nspecification of the behavior of a distributed system, including\\ntiming requirements.  It is a constraint-based language, based on the\\ntimed poset model, that supplies constructs for defining patterns of\\nevents that are required (or forbidden) in the computation of a\\ndistributed system.  Component interfaces and architectures may be\\ngiven detailed abstract specifications using the specification\\nlanguage.\\n\\n\\nThe executable language is a concurrent reactive programming\\nlanguage.  It uses types, objects, and expressions of the type\\nlanguage, and provides module and control structures.  Its principal\\nconstructs are independent (or concurrent) reactive processes that\\nactivate when patterns of events occur during execution.  These\\npattern-triggered processes are used (i) to define architecture\\nconnections between components, and (ii) to construct behaviors of\\ncomponents by rule-based, reactive programming techniques.  The\\nexecutable language also provides standard kinds of Algol-like control\\nstructures, subprograms, exception raising and handling constructs,\\nand timing features.  Components may be assigned implementations\\n(modules) using either the Rapide reactive programming language, or\\nlanguages such as Ada, C++, or VHDL.\\n\\n\\n\\n7/29/97/lp\\n\\n\\n\\n',\n",
       " '\\n\\nSURPRISE 96\\nFUZZY  LOGIC  and  ITS USES\\n\\nARTICLE 2 \\n\\n\\n\\n\\n\\nFuzzy Logic  Introduction\\n\\n\\nFuzzy logic starts with and builds on a set of user-supplied human \\nlanguage rules. The fuzzy systems \\tconvert these rules to their\\n mathematical equivalents. This simplifies the job of the system \\ndesigner and the computer, and results in much more accurate \\nrepresentations of the way systems behave in the real world.\\n\\nAdditional benefits of fuzzy logic include its simplicity and its flexibility. Fuzzy logic can handle\\nproblems with imprecise and incomplete data, and it can model nonlinear functions of arbitrary\\ncomplexity. \"If you don\\'t have a good plant model, or if the system is changing, then fuzzy will produce a\\nbetter solution than conventional control techniques,\" says Bob Varley, a Senior Systems Engineer at\\nHarris Corp., an aerospace company in Palm Bay, Florida.\\n\\nYou can create a fuzzy system to match any set of input-output data. The Fuzzy Logic Toolbox makes\\nthis particularly easy by supplying adaptive techniques such as adaptive neuro-fuzzy inference systems\\n(ANFIS) and fuzzy subtractive clustering. \\n\\nFuzzy logic models, called fuzzy inference systems, consist of a number of conditional \"if-then\" rules.\\nFor the designer who understands the system, these rules are easy to write, and as many rules as\\nnecessary can be supplied to describe the system adequately (although typically only a moderate\\nnumber of rules are needed). \\n\\nIn fuzzy logic, unlike standard conditional logic, the truth of any statement is a matter of degree. (How\\ncold is it? How high should we set the heat?) We are familiar with inference rules of the form p -> q (p\\nimplies q). With fuzzy logic, it\\'s possible to say (.5* p ) -> (.5 * q). For example, for the rule if (weather\\nis cold) then (heat is on), both variables, cold and on, map to ranges of values. Fuzzy inference systems\\nrely on membership functions to explain to the computer how to calculate the correct value between 0\\nand 1. The degree to which any fuzzy statement is true is denoted by a value between 0 and 1. \\n\\nNot only do the rule-based approach and flexible membership function scheme make fuzzy systems\\nstraightforward to create, but they also simplify the design of systems and ensure that you can easily\\nupdate and maintain the system over time. \\n\\n\\nFuzzy Sets\\n\\nFuzzy Set Theory was formalised by Professor Lofti Zadeh at the University of California in 1965. What\\nZadeh proposed is very much a paradigm shift that first gained acceptance in the Far East and its\\nsuccessful application has ensured its adoption around the world. \\n\\nA paradigm is a set of rules and regulations which defines boundaries and tells us what to do to be\\nsuccessful in solving problems within these boundaries. For example the use of transistors instead of\\nvacuum tubes is a paradigm shift - likewise the development of Fuzzy Set Theory from conventional\\nbivalent set theory is a paradigm shift. \\n\\nBivalent Set Theory can be somewhat limiting if we wish to describe a \\'humanistic\\' problem\\nmathematically. For example, Fig 1 below illustrates bivalent sets to characterise the temperature of a\\nroom. \\n\\n\\n\\n\\nThe most obvious limiting feature of bivalent sets that can be seen clearly from the diagram is that they\\nare mutually exclusive - it is not possible to have membership of more than one set ( opinion would\\nwidely vary as to whether 50 degrees Fahrenheit is \\'cold\\' or \\'cool\\' hence the expert knowledge we need\\nto define our system is mathematically at odds with the humanistic world). Clearly, it is not accurate to\\ndefine a transiton from a quantity such as \\'warm\\' to \\'hot\\' by the application of one degree Fahrenheit of\\nheat. In the real world a smooth (unnoticeable) drift from warm to hot would occur.\\n\\nThis natural phenomenon can be described more accurately by Fuzzy Set Theory. Fig.2 below shows\\nhow fuzzy sets quantifying the same information can describe this natural drift.\\n\\n Fuzzy Set Operations\\nDefinitions.\\nUniverse of Discourse \\n  The Universe of Discourse is the range of all possible values for an input to a\\n fuzzy system.\\n Fuzzy Set \\n A Fuzzy Set is any set that allows its members to have different grades of\\n membership (membership function) in the interval [0,1].\\n Support \\n  The Support of a fuzzy set F is the crisp set of all points in the Universe of\\n Discourse U such that the membership function of F is non-zero.\\n Crossover point \\n   The Crossover point of a fuzzy set is the element in U at which its membership\\n       function is 0.5.\\n Fuzzy Singleton \\n  A Fuzzy singleton is a fuzzy set whose support is a single point in U with a membership\\n     function of one.\\n\\nFuzzy Set Operations.\\n\\nUnion\\nThe membership function of the  Union of two fuzzy sets A and B with \\nmembership functions and \\n  respectively is defined \\nas the maximum of the two individual membership functions\\n\\n\\nThe Union operation in Fuzzy set theory is the equivalent \\n of the OR operation in Boolean algebra.\\nComplement\\nThe membership function of the Complement of a Fuzzy set A with membership\\n function is defined as\\n\\n\\n\\n\\nThe following rules which are common in classical set theory also apply to Fuzzy set theory.\\nDe Morgans\\n law ,\\n  \\nAssociativity\\n\\nCommutativity\\n\\nDistributivity\\n\\n\\n\\n\\n\\n\\nDESIGN GOALS\\nControl of the environment for large computing systems is often a far greater\\n challenge than for rooms inhabited by people. Not only do the systems \\nthemselves generate heat, but they are often specified by their manufacturers\\n to be maintained in as little as a plus-or-minus 1 degree (Fahrenheit) range. \\nHumidity is also a challenge, causing, for example, corrosion and jamming of\\n associated mechanical systems at high humidity levels and the enhanced \\npossibility of static discharge with low levels. Humidity control is often \\nspecified to be 50% relative humidity, with a maximum swing of plus-or-minus 3% per hour. \\n\\n\\nIn addition, the design of a precision environmental control system also \\nfaces nonlinearities, caused by such system behavior as air flow delay and \\ndead times, uneven airflow distribution patterns, and duct work layouts. \\nUncertainties in system parameters are often present, for example, \\nroom size and shape, location of heat-producing equipment, thermal \\nmass of equipment and walls, and amount and timing of external \\nair introduction. \\n\\nRecognizing these challenges, Liebert undertook the design of a \\ncontrol system requiring (in general terms): \\n\\n Precision temperature and humidity control;\\n Minimization of cycling times (i.e., the opening and closing of\\n the damper and turning on and off of the compressor), thereby increasing \\nreliability and component life, and also resulting in increased energy efficiency;\\n Straightforward and therefore inexpensive control electronics.\\n\\n\\nIn short, Liebert wanted to precisely control with simple hardware\\n a nonlinear system with significant uncertainties. Several traditional\\n linear approaches were considered but proved inadequate. A fuzzy logic \\napproach was investigated and ultimately implemented.  \\nDesign specifics - The LogiCool control system has six fuzzy inputs, \\nthree fuzzy outputs, and 144 principles (rules). It runs on a Motorola \\n6803 microprocessor, and is programmed in C. \\n\\nLogiCool\\'s fuzzy input variables are: e_temperature, \\nthe temperature relative to a setpoint; delta_T/delta_t, \\nthe rate of temperature change; e_humidity, the humidity relative \\nto a setpoint; delta_H/delta_t, the rate of humidity change; \\nand two proprietary variables associated with  the action\\n of the controllers. \\n\\nFuzzy outputs control: 1) amount of cooling, 2) amount of dehumidification, \\nand 3) heat. Outputs can also be treated as fedback input variables, and \\ntime delays are treated as fuzzy outputs as well. Each fuzzy variable is \\nassigned seven membership functions as values, with the traditional \\nLarge_Negative, Medium_Negative, Small_Negative, Near_Zero, Small_ Positive, \\nMedium_Positive, and Large_Positive as labels. Ranges for the values of \\neach variable are proprietary. \\n\\nAn example of a temperature control principle, using the as ...then ... \\n(rather than the if ... then ...) syntax, is: \\n\\n\\nas temperature relative to set point is small_positive and temperature rate of \\nchange is medium_positive then amount of cooling is small_positive; \\n\\nThe Liebert design also incorporates time delays into their principles. \\nThe following demonstrates both this as well as the use of a fuzzy output \\nas a feedback variable. \\n\\nas temperature relative to setpoint is small_negative and \\namount of cooling is small_positive then wait delay to cooling change \\nis medium_positive; \\n\\n\\nA fuzzy OR operator (maximizer) is used as the defuzzification technique, \\navoiding the complicated calculations associated with a centroid approach. \\nLiebert has found that with the large number of principles, a more \\nelaborate approach is unnecessary. Inputs are sampled, the principle-base \\naccessed, and outputs are updated once a second. The \"long\" inter-sample \\ndelay allows the 6803, a simple eight-bit microprocessor, to implement \\nthis rather large fuzzy system. \\nFUZZY LOGIC OBJECTIONS\\n\\nIt would be remarkable if a theory as far-reaching as fuzzy systems\\n did not arouse some objections in the professional community. \\nWhile there have been generic complaints about the \"fuzziness\" of the \\nprocess of assigning values to linguistic terms, perhaps the most cogent \\ncriticisms come from Haack . A formal logician, Haack argues that \\nthere are only two areas in which fuzzy logic could possibly be \\ndemonstrated to be \"needed,\" and then maintains that in each case \\nit can be shown that fuzzy logic is not necessary. \\n\\nThe first area Haack defines is that of the nature of Truth and \\nFalsity: if it could be shown, she maintains, that these are fuzzy \\nvalues and not discrete ones, then a need for fuzzy logic would have \\nbeen demonstrated. The other area she identifies is that of fuzzy \\nsystems\\' utility: if it could be demonstrated that generalizing classic \\nlogic to encompass fuzzy logic would aid in calculations of a given sort,\\n then again a need for fuzzy logic would exist. \\n\\nIn regards to the first statement, Haack argues that True and False are \\ndiscrete terms. For example, \"The sky is blue\" is either true or false; \\nany fuzziness to the statement arises from an imprecise definition of terms, \\nnot out of the nature of Truth. As far as fuzzy systems\\' utility is concerned, \\nshe maintains that no area of data manipulation is made easier through the \\nintroduction of fuzzy calculus; if anything, she says, the calculations \\nbecome more complex. Therefore, she asserts, fuzzy logic is unnecessary. \\n\\nFox  has responded to her objections, indicating that there are three \\nareas in which fuzzy logic can be of benefit: as a \"requisite\" apparatus \\n(to describe real-world relationships which are inherently fuzzy); as \\na \"prescriptive\" apparatus (because some data is fuzzy, and \\ntherefore requires a fuzzy calculus); and as a \"descriptive\" \\napparatus (because some inferencing systems are inherently fuzzy). \\n\\nHis most powerful arguments come, however, from the notion that fuzzy \\nand classic logics need not be seen as competitive, but complementary. \\nHe argues that many of Haack\\'s objections stem from a lack of semantic \\nclarity, and that ultimately fuzzy statements may be translatable into \\nphrases which classical logicians would find palatable. \\n\\nLastly, Fox argues that despite the objections of classical logicians, \\nfuzzy logic has found its way into the world of practical applications, \\nand has proved very successful there. He maintains, pragmatically, that \\nthis is sufficient reason for continuing to develop the field. \\nREFERENCES\\n\\n[1]  Daniel Mcneil and Paul Freiberger \" Fuzzy Logic\" \\n\\n[2]  http://www.ortech-engr.com/fuzzy/reservoir.html\\n\\n[3]  http://www.quadralay.com/www/Fuzzy/FAQ/FAQ00.html \\n\\n[4]  http://www.fll.uni.linz.ac.af/pdhome.html \\n\\n[5]  http://soft.amcac.ac.jp/index-e.html \\n\\n[6]  http://www.abo.fi/~rfuller/nfs.html \\n\\n[7]  L.A.Zadeh,\"Making computer think like people,\\n     IEEE spectrum, 8/1984, pp 26-32\\n[8]  S.Haack, \" Do we need fuzzy logic? \"\\n     Int .Jr nl .of Man-Mach.stud , vol.11, 1979,\\n     pp 437-445\\n\\n',\n",
       " '\\n\\n\\n\\n\\n\\n\\nMark Twain on Simplified Spelling\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nweb hosting\\ndomain names\\nemail addresses\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ntwain-simpl.html\\xa0\\nrev. May, 1999\\n\\n\\n\\n\\nMark\\nTwain on Speling Reform and\\nSimplified\\nSpelling\\n(A promotion\\nof phonographic writing)\\n(to be riten\\nin RES:\\xa0 Restored English Spelling)\\nhttp://www.unifon.org/twain-simpspl.html\\nCadmus\\nTries to Reform of Hieroglyphics\\nhttp://www.twainquotes.com/\\n\\n\\n\\xa0\\n\\n\\nThis\\narticle, written during the autumn of 1899, was about the last writing\\ndone by Samuel Clemens on any impersonal subject.\\xa0 This essay is similar\\nin to one written by G.B.Shaw. One difference\\nis that Shaw always wrote in Pitman shorthand and was dissatisfied with\\nit.\\xa0 He didn\\'t like abbreaviations and he believed the alphabet should\\nbe linear - most shorthands are not. Twain seems to say that he never mastered\\na shorthand but he could see its utility.\\xa0 There is a reference to\\nBurnz\\' shorthand which is a variant of Pitman shorthand.\\nTwain and Shaw were both\\ndissatisfied with the efforts of the simplified spellers. Those who used\\nit were in danger of being viewed as illiterate, uneducated, or nuts.\\xa0\\nTwain observed:\\n\\nA\\nwritten character with which we are not familiar does not offend.\\xa0\\nMind, I myself am a Simplified\\nSpeller; I belong to that unhappy guild that is patiently and hopefully\\ntrying to reform our drunken old alphabet by reducing his whiskey. Well,\\nit will improve him. When they get through and have reformed him all they\\ncan by their system he will be only HALF drunk. Above that condition\\ntheir system can never lift him. There is no competent, and lasting, and\\nreal reform for him but to take away his whiskey entirely, and fill up\\nhis jug with Pitman\\'s wholesome and undiseased alphabet. [see Pitman\\'s\\nFonotypy\\n]\\nOne\\ngreat drawback to Simplified Spelling is, that in print a simplified word\\nlooks\\nso like the very nation! and when you bunch\\na whole squadron of the Simplified together the\\nspectacle is very nearly unendurable.\\nTwain\\ndid not have a specific proposal but was partial to some kind of phonographic\\nalphabet - perhaps something along the lines of Pitman\\'s shorthand.\\xa0\\n\\n\\nsend\\ncomments to:\\nsbett@mailcity.com\\xa0\\xa0\\n\\nPMF-Shavian Notation\\xa0\\xa0\\xa0\\xa0 Pitman\\nShorthand\\n\\n\\n\\xa0\\nI have\\nhad a kindly feeling, a friendly feeling, a cousinly feeling toward Simplified\\nSpelling, from the beginning of the movement three years ago [1896], but\\nnothing more inflamed than that. It seemed to me to merely propose to substitute\\none inadequacy for another; a sort of patching and plugging poor old dental\\nrelics with cement and gold and porcelain paste; what was really needed\\nwas a new set of teeth. That is to say, a new ALPHABET.\\xa0\\n[ See G.B. Shaw on same subject ]\\xa0 [Twain]\\nThe\\nheart of our trouble is with our foolish alphabet. It doesn\\'t know how\\nto spell, and can\\'t be taught. In this it is like all other alphabets except\\none--the phonographic. This is the only competent alphabet in the world.\\nIt can spell and correctly pronounce any word in our language.\\nThat admirable alphabet, that brilliant alphabet,\\nthat inspired alphabet, can be learned in an hour or two. In a week the\\nstudent can learn to write it with some little facility, and to read it\\nwith considerable ease. I know, for I saw it tried in a public school in\\nNevada forty-five years ago, and was so impressed by the incident that\\nit has remained in my memory ever since.\\nI wish we could adopt it in place of our present\\nwritten (and printed) character. I mean SIMPLY the alphabet; simply the\\nconsonants and the vowels -- I don\\'t mean any REDUCTIONS or abbreviations\\nof them, such as the shorthand writer uses in order to get compression\\nand speed. No, I would SPELL EVERY WORD OUT.\\nI will insert the alphabet here as I find it in\\nBurnz\\'s PHONIC SHORTHAND. [Figure 1] It is arranged on the basis of Isaac\\nPitman\\'s PHONOGRAPHY. Isaac Pitman was the originator and father of scientific\\nphonography. It is used throughout the globe. It was a memorable invention.\\nHe made it public seventy- three years ago. The firm of Isaac Pitman &\\nSons, New York, still exists, and they continue the master\\'s work.\\nWhat should we gain?\\nFirst of all, we could spell DEFINITELY--and correctly--any\\nword you please, just by the SOUND of it. We can\\'t do that with our present\\nalphabet. For instance, take a simple, every-day word PHTHISIS. If we tried\\nto spell it by the sound of it, we should make it TYSIS, and be laughed\\nat by every educated person.\\nSecondly, we should gain in REDUCTION OF LABOR\\nin writing.\\nSimplified Spelling makes valuable reductions\\nin the case of several hundred words, but the new spelling must be LEARNED.\\nYou can\\'t spell them by the sound; you must get them out of the book.\\nBut even if we knew the simplified form for every\\nword in the language, the phonographic alphabet would still beat the Simplified\\nSpeller \"hands down\" in the important matter of economy of labor. I will\\nillustrate:\\nPRESENT FORM:\\xa0 through, laugh, highland.\\nSIMPLIFIED FORM:\\xa0 thru, laff, hyland.\\n\\n\\xa0\\n\\n\\n\\nPHONOGRAPHIC FORM:\\nthru\\xa0\\xa0\\nlaf\\xa0\\xa0 hyland [RITE]\\nthhroo\\xa0\\xa0\\nlaf\\xa0\\xa0 hieland [Truespel]\\nthhru\\xa0\\xa0\\nlaff\\xa0\\xa0 hailand [Spanglish]\\nthru\\xa0\\xa0\\nlaf\\xa0\\xa0 hyland [RES]\\nTrU\\xa0\\xa0\\xa0\\nlxf\\xa0\\xa0\\xa0 hIlcnd [Unifon]\\nTrM\\xa0\\xa0 lyf\\xa0\\xa0\\xa0 hFland\\n[Shavian]\\nTrM\\xa0\\nlaf\\xa0 hFland\\nshavian requires a free shaw\\nfont to display\\ndownload here\\n\\n\\n\\n\\nFigure 2\\n\\n\\n\\nTo write the word \"through,\" the pen has to make\\ntwenty-one strokes.\\xa0\\xa0 [7\\nletrz, 21 stro\\'ks]\\nTo write the word \"thru,\" then pen has to make\\ntwelve strokes-- a good saving. [4\\nletrz, 12 stro\\'ks]\\nTo write that same word with the phonographic\\nalphabet, the pen has to make only THREE strokes.\\nTo write the word \"laugh,\" the pen has to make\\nFOURTEEN strokes.\\nTo write \"laff,\" the pen has to make the SAME\\nNUMBER of strokes--no labor is saved to the penman.\\nTo write the same word with the phonographic alphabet,\\nthe pen has to make only THREE strokes.\\nTo write the word \"highland,\" the pen has to make\\ntwenty-two strokes.\\nTo write \"hyland,\" the pen has to make eighteen\\nstrokes.\\nTo write that word with the phonographic alphabet,\\nthe pen has to make only FIVE strokes. [Figure 3]\\nTo write the words \"phonographic alphabet,\" the\\npen has to make fifty-three strokes.\\nTo write \"fonografic alfabet,\" the pen has to\\nmake fifty strokes.\\xa0 To the penman, the saving in labor is insignificant.\\nTo write that word (with vowels) with the phonographic\\nalphabet, the pen has to make only SEVENTEEN strokes.\\n\\xa0\\n\\n\\n\\nPHONOGRAPHIC ALPHABET\\n20\\nletrz\\nfonografic\\nalfabet [RITE]\\n18\\nletrz\\xa0\\xa0 50 strokes\\nfoenugrrafik\\nalfubet [Truespel]\\n20 letrz\\xa0\\xa0 58 strokes\\nfoanagraffic\\nalfabet [Spanglish]\\n20\\nletrz\\xa0\\xa0 58 strokes\\nfoanografic\\nalfabet [RES]\\n18\\nletrz\\xa0\\xa0 50 strokes\\nfOncgrafik alfcbet [Unifon]\\n\\n17 letrz\\nfOncgrafik\\nalfcbet\\xa0 [Unifont]\\xa0\\n17 letrz\\nfOnagrAfik Alfabet\\xa0 [Shavian]\\n17\\nletrz\\nfOnagryfik\\nAlfabet 17\\nstrokes\\n\\n17 strokes\\xa0\\nshavian & unifon require\\na free font to display\\n\\n\\n\\n\\nonly\\n17 strokes with a monoline phonetic shorthand\\nFigure 4\\n\\n\\n\\nWithout the vowels, only THIRTEEN strokes.\\xa0\\nThe vowels are hardly necessary, this time.\\nWe\\nmake five pen-strokes in writing an m. Thus: [Figure 5] a stroke down;\\na stroke up; a second stroke down; a second stroke up; a final stroke down.\\nTotal, five. The phonographic alphabet accomplishes the m with a single\\nstroke--a curve, like a parenthesis that has come home drunk and has fallen\\nface down right at the front door where everybody that goes along will\\nsee him and say, Alas!\\nWhen our written m is not the end of a word, but\\nis otherwise located, it has to be connected with the next letter, and\\nthat requires another pen-stroke, making six in all, before you get rid\\nof that m. But never mind about the connecting strokes--let them go. Without\\ncounting them, the twenty-six letters of our alphabet consumed about eighty\\npen-strokes for their construction--about three pen-strokes per letter.\\nIt is THREE TIMES THE NUMBER required by the phonographic\\nalphabet. It requires but ONE stroke for each letter.\\nMy writing-gait is--well, I don\\'t know what it\\nis, but I will time myself and see. Result: it is twenty-four words per\\nminute. I don\\'t mean composing; I mean COPYING. There isn\\'t any definite\\ncomposing-gait.\\nVery well, my copying-gait is 1,440 words per\\nhour--say 1,500. If I could use the phonographic character with facility\\nI could do the 1,500 in twenty minutes. I could do nine hours\\' copying\\nin three hours; I could do three years\\' copying in one year. Also, if I\\nhad a typewriting machine with the phonographic alphabet on it--oh, the\\nmiracles I could do!\\nI am not pretending to write that character well.\\nI have never had a lesson, and I am copying the letters from the book.\\nBut I can accomplish my desire, at any rate, which is, to make the reader\\nget a good and clear idea of the advantage it would be to us if we could\\ndiscard our present alphabet and put this better one in its place--using\\nit in books, newspapers, with the\\ntypewriter, and with the pen.\\n\\xa0\\n\\xa0\\n\\n\\n\\nMAN\\xa0\\xa0\\xa0\\nDOG\\xa0\\xa0\\xa0 HORSE:\\nmaen dog hors\\n[Spanglish]\\nm@n\\xa0\\xa0\\xa0\\xa0\\ndog\\xa0\\xa0\\xa0 hors\\xa0\\xa0\\xa0 [Unigraf]\\nmAn\\xa0\\xa0\\xa0\\xa0 dog\\xa0\\xa0\\xa0\\xa0\\nhDs\\xa0\\xa0\\xa0 [Shavian]\\n\\nmAn\\xa0\\ndog\\xa0 hDs\\nshavian requires a free\\nshaw font to display\\n\\n\\n\\n\\nFigure 6\\n\\n\\n\\n[Figure 6] -- MAN DOG HORSE. I think it is graceful\\nand would look comely in print. And consider--once more, I beg--what a\\nlabor-saver it is! Ten pen-strokes with the one system to convey those\\nthree words above, and thirty-three by the other!\\xa0 [Figure 6] I mean,\\nin SOME ways, not in all. I suppose I might go so far as to say in most\\nways, and be within the facts, but never mind; let it go at SOME. One of\\nthe ways in which it exercises this birthright is--as I think--continuing\\nto use our laughable alphabet these seventy-three years while there was\\na rational one at hand, to be had for the taking.\\nIt has taken five hundred years to simplify some\\nof Chaucer\\'s rotten spelling--if I may be allowed to use to frank a term\\nas that--and it will take five hundred years more to get our exasperating\\nnew Simplified Corruptions accepted and running smoothly. And we sha\\'n\\'t\\nbe any better off then than we are now; for in that day we shall still\\nhave the privilege the Simplifiers are exercising now: ANYBODY can change\\nthe spelling that wants to.\\nBUT YOU CAN\\'T CHANGE THE PHONOGRAPHIC SPELLING;\\nTHERE ISN\\'T ANY WAY. It will always follow the SOUND. If you want to change\\nthe spelling, you have to change the sound first.\\nMind, I myself am a Simplified Speller; I belong\\nto that unhappy guild that is patiently and hopefully trying to reform\\nour drunken old alphabet by reducing his whiskey. Well, it will improve\\nhim. When they get through and have reformed him all they can by their\\nsystem he will be only HALF drunk. Above that condition their system\\ncan never lift him. There is no competent, and lasting, and real reform\\nfor him but to take away his whiskey entirely, and fill up his jug with\\nPitman\\'s wholesome and undiseased alphabet.\\nOne great drawback to Simplified Spelling is,\\nthat in print a simplified word looks so like the very nation! and when\\nyou bunch a whole squadron of the Simplified together the spectacle is\\nvery nearly unendurable.\\n\\xa0\\n\\xa0\\n\\n\\n\"The\\nda ma ov koars kum when the publik ma be expektd to get rekonsyled to the\\nbezair asspekt of the Simplified Kombynashuns, [see note]\\nbut--if\\nI may be allowed the expression-- is it worth the wasted time?\"\\nhttp://www.unifon.org/twain-simpspl.html\\n\\n\\n\\n[SB]\\xa0\\xa0\\nIn 1899 Ellis\\' forerunner of New Spelling would have been available but\\nthe orthography above is not new spelling.\\xa0 It may be more\\nphonemic than the traditional orthography but from this snippet, it doesn\\'t\\nappear to be very systematic.\\xa0\\xa0 The following are examples of\\nsystematic orthography\\xa0\\n\\n\\nTha dey mey ov cors\\ncom wen the pubblic mey bi expected tu get reckonsaild tu the bizaar asspect\\nav the Simmplifaid Commbineishnz.\\n\\n\\nThu\\ndae mae uv kors kum wen thu publik mae bee iksppektid tue get rekaanssield\\xa0\\ntue thu buzzaar aspekt uv thu Simplified Kaambinnaeshinz.\\xa0\\xa0 [Truespel]\\xa0\\nsystematic and phonemic\\n\\n\\nThe day may\\nov koars kum when the publik may be expectd tu get reconsyld tu the bizaar\\naspect ov the Simplified Combinaytions.\\xa0\\xa0\\n[RES positional spelling]\\n\\n\\nx\\ndei mei a\\'v kors kum wen x pu\\'blik mei bi ekspekta\\'d tu get rekonsyld tu\\nx bizar aspekt ov x simplifyd combinei5nz.\\xa0 Chkt\\nSpel\\n\\n\\nDa dA mA cv kOrs\\nkum wen Dc publik mA bE ekspektcd tU get rekcnsIld tu Da bczor aspekt cv\\nDa simplifId kombinAShanz.\\xa0\\nDa dA mA cv\\nkOrs kum wen Dc publik mA bE ekspektcd tU get rekcnsIld tu Da bczor aspekt\\ncv Da simplifId kombinAShanz.\\xa0contact\\nsteve\\n\\xa0\\n\\n\\n\\nFigure 7\\n\\n\\n\\nTo see our letters put together in ways to which\\nwe are not accustomed offends the eye, and also takes the EXPRESSION out\\nof the words.\\n\\nLa on,\\nMakduf, and damd be he hoo furst krys hold, enuf!\\nIt doesn\\'t thrill you as it used to do. The simplifications\\nhave sucked the thrill all out of it.\\nBut a written character with which we are NOT\\nACQUAINTED does not offend us--Greek, Hebrew, Russian, Arabic, and the\\nothers--they have an interesting look, and we see beauty in them, too.\\nAnd this is true of hieroglyphics, as well. There is something pleasant\\nand engaging about the mathematical signs when we do not understand them.\\nThe mystery hidden in these things has a fascination for us: we can\\'t come\\nacross a printed page of shorthand without being impressed by it and wishing\\nwe could read it.\\n\\nVery well, what I am offering for acceptance\\nand adopting is not shorthand, but longhand, written with the SHORTHAND\\nALPHABET UNREACHED. You can write three times as many words in a minute\\nwith it as you can write with our alphabet. And so, in a way, it IS properly\\na shorthand. It has a pleasant look, too; a beguiling look, an inviting\\nlook. I will write something\\nin it, in my rude and untaught way:\\n\\xa0\\n\\n\\n\\n[Figure 8] Pitman Shorthand example\\nLey on,\\nMacduff, and dammd bi hi hu ferst kraiz hoald, enuff!\\n\\n\\n\\n\\n\\nEven when _I_ do it it comes out prettier than\\nit does in Simplified Spelling. Yes, and in the Simplified it costs one\\nhundred and twenty-three pen-strokes to write it, whereas in the phonographic\\nit costs only twenty-nine.\\n\\xa0\\n\\n\\n\\n[Figure 9]\\n\\n\\n\\xa0is probably [Figure 10]\\n\\n\\n\\nLet us hope so, anyway.\\n\\ntop\\nComments\\n\\n\\n\\xa0\\nDoug wrote:\\nMY REACTION:\\xa0 An attractiv HTML layout but sadly far out from\\nenny current SSS groop\\'s consensus on gradual reform.\\nA WELTER OF SIMPLIFYING AULTERNATIVS, MOAST LOOKING AT FERST GLANCE\\nTU BE AULMOAST CONSCIUSLY ARRAINGED TU SHOW GROTESK DISTORTIONS THAT COMPLICATE\\nMOR THAN THAY SIMPLIFY; TIPICAL OF THE DELIBERAT PARODIES OF REFORM COMMON\\nIN MANESTREEM MEEDIA RIDICULING REFORM SUGGESTIONS.\\xa0\\xa0 FEW OF\\nTHEES CODES ENCURAGE LITERAT ENGLISH UZERS TU EXPECT REFORMS THAT RIME\\nWITH COMMON TRADITIONAL GRAFEEMS.\\xa0\\xa0\\xa0\\xa0 IT ISNT OFENDING\\nTHE EYE THAT TURNS ME OFF SO MUCH AZ OFENDING LOGIC:\\nDA MA for DAY MAY, BEZAIR for what I rime with T.S. BIZ + ARE, ASSPEKT\\nusing SS in a consonant cluster tipe non-existent in T.O., etc.\\nSpanglish may be helpful for thoze seeking bilingual literacy in Spanish\\n(or sum uther language) and English, but for thoze unfamilyar with Spanish\\netc. grafeems it can oanly delay familiarity with T.O.\\xa0\\xa0\\xa0\\xa0\\xa0\\n--\\xa0\\xa0\\xa0 Doug Everingham\\n*This is not Spanglish but a pre-NewSpelling notation endorsed by the\\nsimplified spellers.\\xa0 This is not to say that some Spanglish words\\nare not equally bizaare.\\xa0 The saving grace is that Spanglish is pronunciation\\nguide spelling, not a reform proposal.\\xa0 The Saxon reform is simply\\nto reinstate the updated Saxon alphabet [removing those sounds that are\\nno longer part of the language] and respelling 10% of the traditional words\\nthat cannot be pronounced as they are currently spelled.\\xa0 e.g. ruff\\nfor rough.\\nAfter such a reform there would still be problems with some diphthongs.\\xa0\\nallow could be alou or alo or alau.\\xa0 Digraphs would still have up\\nto three different pronunciations but all the pronunciations would be close\\nand interpretable.\\xa0 break would bethe same as breck.\\xa0 beak would\\nbe beck which might be close enough.\\xa0 Respelling this word as it sounds\\nwould be biek or beek.\\n\\xa0\\n\\xa0\\n\\nThe Shaw Alphabet\\nis another example of a non-Roman script that would meet most of Twain\\'s\\ncriteria for something better than Simplified Spelling (phonemic spelling).\\xa0\\nShavian tries the obscure the relationship to the roman character shapes.\\xa0\\nShaw\\'s will specifically requested a non-roman script for the same reasons\\nexplained here by Twain.\\xa0 If you can\\'t see a relationship, then you\\ncan be offended.\\xa0 The problem is that a notation is supposed to communicate\\nand using the Shaw alphabet is like using a secret code.\\xa0 PMF, is\\nclose enough to the Roman shapes that it can almost be read without a key.\\xa0\\nIt is certainly much easier to learn and recall than Shavian because (1)\\nthe shapes are historical and (2) the shapes are pictographic [< = corner]\\nor mnemonically linked to other shapes [ei = e shape + i shape].\\xa0\\nLike Shavian, similar shapes are used to reference similar sounds. \\nThe\\nshape for the voiced and unvoiced phoneme are usually just 180 degrees\\napart.\\xa0 < is the shape for c/k, > is the shape for g.\\xa0 Check\\nout p-b, t-d, k-g, f-v, s-z, ch-j.\\npmf-shavian.jpg\\n\\n\\n\\nsource page: http://victorian.fortunecity.com/vangogh/555/Spell/twain-simpspl.html\\nshavian page:\\xa0 http://victorian.fortunecity.com/vangogh/555/Spell/shaw-keyboard.html\\n\\n\\n\\n\\n\\n\\n\\nLocal (relative)\\nlinks\\n\\n\\nSpelling\\nLinks\\n\\n\\nSiteMap-index\\n\\n\\nAddress\\n\\n\\n\\n\\nRemote (absolute)\\nlinks\\n\\n\\nSpelling\\nLinks\\n\\n\\nSiteMap-index\\n\\n\\nMap-IPA\\n\\n\\n\\n\\nComments\\nor Problems? Contact\\n\\n\\nSteve\\nBett\\n\\n\\nAlt.\\nnotations\\n\\n\\nSpelfun\\n\\n\\n\\n\\nOriginal\\npage elements:\\xa0 Copyright ©1998 BETA Interactive\\n\\n\\n\\nweb hosting • domain names • web designonline games • online dating  • long distancedigital cameras  • advertising online\\n\\n\\n\\n\\n\\n',\n",
       " '\\n\\n \\nEmbedding R in Other Applications\\n\\n\\n\\nEmbedding R in Other Applications\\n\\nOn Unix, it is possible to compile R as a stand-alone library that can\\nbe linked with or dynamically loaded into other applications. One can\\nthen use the programming interface defined in the Writing R\\nExtensions to evaluate R expressions, call R functions, access the\\nmath routines, provide a well-defined and complete scripting language,\\netc.\\n\\nMotivation\\nThere is little doubt that embedding the R library within\\na C routine that acts as a regular shell command is overkill.\\nTo do this, one could execute R in batch mode with a specified\\nscript that queried the vector commandArgs().\\nIf C code is needed, it can be dynamically loaded into R (possibly with\\nequal or less effort than creating an embedded application).\\nIn this case, the functionality of the application is achieved\\nwithout the application being in control. Instead, R is the\\n\"server\" in the setup.\\n\\n\\nEven graphical interfaces which would appear to need to be in control\\nof an application need not use the embedded R library.  Instead, the\\nregular stand-alone R can be used, again in batch mode, to invoke the R\\ncommands to create the GUI (using either of the Java or TclTk\\npackages, or coding it directly in C using one of the GUI libraries\\nand contending with the event loop!).\\n\\n\\n\\nHowever, the embedded R mechanism is useful in applications that\\nreally must be in control of initialization and execution.  Long\\nrunning servers are natural examples. The Postgres and MySQL servers\\nare examples of such applications.  Dynamic, event-driven processing\\nsystems that are given data at different times and update their\\ncomputations accordingly (e.g. produce new reports and plots,\\ninventory tracking, user signatures, etc.) are other examples.  The\\nApache server is another example of where embedding a statistical\\nenvironment is useful in two regards.  Firstly, the R language can be\\nused to generate pages in the same way that Perl, PHP, etc. are\\nemployed. A simple CGI command that specifies an R expression that\\nuses the RSDBI package to extract values from a database and generate\\na plot (in Postscript, PNG, PDF, SVG, etc.) and return a page\\ncontaining that image can be a simple way to produce high-quality\\ngraphics without the overhead of starting R each time.\\n\\n\\n\\nNot only can servers such as Postgres, MySQL and Apache allow its\\nusers to employ R by embedding it as a module, these systems might\\nalso use the statistical facilities (either native routines or via the\\ninterpreted language) to govern their own behavior. Computing models\\nfor transactions so that Apache can pre-fetch pages for clients or\\nreorganize its own caches to optimize current activity are natural\\nuses of the modelling code in R. Similarly, Postgres can tailor its\\nperformance by incrementally computing statistics about its own\\nbehavior.\\n\\n\\n\\nTest Applications\\nThe initial example that was used to test this setup was embedding R\\nwithin Postgres for use as a procedural language.\\nThis allows (privileged) users to define SQL functions as\\nR functions, expressions, etc.  Additionally, tests were\\ndone by evaluating R expressions within\\n\\n a simple application that dynamically loaded\\n      libR.so\\n ggobi that is linked against\\n      libR.so\\n      and contains a GUI callback.\\n      (This is not a very practical example\\n       as ggobi can be entirely embedded and controlled\\n       from within R.)\\n\\nLinking the R library\\n\\n  $(CC) -L$(R_HOME)/bin -lR\\n\\nInitializing R from within an Application\\n\\nCurrently, the following code will initialize the R engine.\\n\\nvoid initR() {\\n char *argv[] = {\"REmbeddedPostgres\", \"--gui=none\", \"--silent\"};\\n int argc = sizeof(argv)/sizeof(argv[0]);\\n\\n  Rf_initEmbeddedR(argc, argv);\\n}\\n\\n\\nWhen this is called, the environment variables such as R_HOME, R_PROFILE, R_LIBS should be appropriately set.\\nThere are a variety of tools which can help an application read\\nconfiguration details. (For example, the C++ properties library in the\\nOmegahat distribution allows one to read a file containing name:\\nvalue pairs.  )\\n\\n\\n\\nHandling Errors\\n\\nAn application that embeds R must be careful to take care of handling\\nerrors that occur within the R engine appropriately.  In the\\nstand-alone version of R, an error will (after other\\non.error() activities in each evaluation frame) return\\ncontrol to the main input-eval-print loop.  In general, this is not\\nwhat is desired within another application.  Instead, we want to trap\\nsuch R errors and handle them from where the application passed\\ncontrol to the R engine.\\n\\n\\n\\nThis can be done most readily using the C routine R_tryEval to evaluate the S expression.  This\\ndoes exactly what we want by guaranteeing to return to this point in\\nthe calling code whether an error occurred or not in evaluating the\\nexpression.  This routine is similar to eval, taking both the expression to evaluate\\nand an environment in which to perform the evaluation. It takes a\\nthird argument which is the address of an integer. If this is\\nnon-NULL, when the call returns, this contains a flag indicating\\nwhether there was an error or not.\\n\\nExample\\n\\nint\\ncallFoo()\\n{\\n SEXP e, val;\\n int errorOccurred;\\n int result = -1;\\n\\n PROTECT(e = allocVector(LANGSXP, 1));\\n SETCAR(e, Rf_install(\"foo\"));\\n\\n val = R_tryEval(e, R_GlobalEnv, &errorOccurred);\\n\\n if(!errorOccurred) {\\n   PROTECT(val);\\n   result = INTEGER(val)[0];\\n   UNPROTECT(1);\\n } else {\\n   fprintf(stderr, \"An error occurred when calling foo\\\\n\");\\n   fflush(stderr);\\n }\\n\\n    /* Assume we have an INTSXP here. */\\n\\n UNPROTECT(1); /* e */\\n\\nreturn(result);\\n}\\n\\n\\nNote that this will, by default, take care of handling all types of\\nerrors that R would usually handle including signals.  So if the user\\nsends an interrupt to a computation (e.g. using Ctrl-C) while an R\\nexpression is being evaluated, R_tryEval\\nwill return and report an error.  If the host application however\\nchanges the signal mask and/or handlers from R\\'s own ones, of course\\nthis will not necessarily happen. In other words, the host application\\ncan control the signal handling differently.\\n\\n\\n\\n\\n\\nHandling The Event Loop\\n\\nAs we have encountered when integrating other software into R and\\nhandling blocking I/O, software that assumes that it is in control of\\nwaiting for events can be challenging to embed in another application.\\nSo as to not inflict this same problem on others, R should be able to\\nexport the file descriptors on which it is waiting for events and also\\nthe individual callbacks associated with each these file descriptors.\\nIt is inconceivable that we can have a common C-level signature for\\nthe callbacks across different applications (other than those defined\\nby the \"standards\" -- X, Gtk, Tk, etc.).  Many applications that have\\nan event loop do not admit the possibility of different sources of\\nevents, but instead assume there are only e.g.  user events on a GUI\\nand not on stdin or other connections.\\n\\n\\n\\nCompiling the Library\\nThe library is not compiled by default during the installation of\\nR. One can pass the argument -enable-R-shlibs\\nto the configure script and then build, as in\\n\\n <top of R source tree>/configure --enable-R-shlibs\\n  make\\n\\nThe resulting (shared) library is currently installed into the\\ndirectory bin/ within the R distribution.\\n\\nFuture Directions\\nIt would be advantageous to decompose the functionality provided by\\nthe large eval_R_command()\\n\\nvoid\\ninit_R()\\n{\\nextern Rf_initEmbeddedR(int argc, char **argv);\\n  int argc = 1;\\n  char *argv[] = {\"ggobi\"};\\n\\n  Rf_initEmbeddedR(argc, argv);\\n}\\n\\n /*\\n  Calls the equivalent of \\n    x <- integer(10)\\n    for(i in 1:length(x))\\n       x[i] <- 1\\n    print(x)\\n */\\nint\\neval_R_command()\\n{\\n SEXP e;\\n SEXP fun;\\n SEXP arg;\\n int i;\\n void init_R(void);\\n\\n  init_R();\\n\\n    fun = Rf_findFun(Rf_install(\"print\"),  R_GlobalEnv);\\n    PROTECT(fun);\\n    arg = NEW_INTEGER(10);\\n    for(i = 0; i < GET_LENGTH(arg); i++)\\n      INTEGER_DATA(arg)[i]  = i + 1;\\n    PROTECT(arg);\\n\\n    e = allocVector(LANGSXP, 2);\\n    PROTECT(e);\\n    SETCAR(e, fun);\\n    SETCAR(CDR(e), arg);\\n\\n      /* Evaluate the call to the R function.\\n         Ignore the return value.\\n       */\\n    eval(e, R_GlobalEnv);\\n    UNPROTECT(3);   \\n  return(0);\\n}\\n\\nRoutines for the Embedded R\\n\\nThe following are the routines that are now visible from the shared\\nlibrary that directly relate to the embedded version of R and are not\\nin the regular API (i.e. mentioned in Writing R\\nExtensions).\\n\\n \\nRoutineDescription\\nvoid\\n\\t  jump_now(void)Should be overridden by the\\n\\t      application loading libR.so\\n\\t      so as to handle errors in R commands. It usually calls\\n\\t      Rf_resetStack()\\n\\t      and returns control to the application.\\n\\t  \\nint\\n\\t  Rf_resetStack(int resetTopLevel)\\n           Resets the R evaluator after an error so that subsequent\\n\\t    evaluations can proceed appropriately. This should be called with\\n\\t      a non-zero argument from applications that embed R.\\n\\t  \\nint Rf_initEmbeddedR(int argc,\\n\\t  char **argv)Initializes the R environment,\\n\\t  passing the specified strings as if they were from the\\n\\t      command line.  The appropriate environment variables\\n\\t   should be set before calling this routine.\\n\\n\\n\\nDuncan Temple Lang\\n(duncan@research.bell-labs.com)\\n\\n Last modified: Mon Aug 21 22:43:35 EDT 2000\\n\\n\\n\\n',\n",
       " '\\n\\n\\n\\n\\nIntroduction\\n\\n\\n\\nVisualizing Graphs with Java\\n(VGJ)\\n\\xa0\\nCONTENTS\\nIntroduction\\nWelcome\\nAbout This Document\\nLegal Stuff\\nGetting Started\\nRunning VGJ\\nRunning VGJ as an Applet\\nRunning VGJ as an Application\\nInstalling VGJ\\nVGJ Basics\\nAreas of Note\\nCreating a graph\\nCreating Nodes\\nCreating Edges\\nEditing a Graph\\nMoving and Resizing Nodes\\nSelecting Multiple Objects\\nDeleting Edges and Nodes\\nEditing Node Attributes\\nEditing Edge Attributes\\nEditing a Graph\\'s Properties\\nOther Graph Properties\\nSaving and Loading Work\\nGroups\\nAutomatic Layouts\\nTree algorithm\\nSpring embedder\\nDirected graphs\\nMenu Commands\\n\\nIntroduction\\nWelcome\\nVisualizing Graphs with Java (VGJ) is a tool for graph drawing and graph layout. Graphs can be input into VGJ in two ways: loading a textual description in GML (graph modeling language) or drawing a graph using the graph editor. The user can then select an algorithm which will layout the graph in an organized and (hopefully) aesthetically pleasing way. \\n\\xa0VGJ has two major components: the graph editor and the layout algorithms. A full-featured graphical user interface is provided.  The graph editor contains the typical features of tools that support graph drawing. Layout algorithms are being developed and implemented to layout different categories of graphs such as trees, planar graphs, directed and undirected graphs, and series-parallel graphs. For the directed graphs, the drawing method uses a unique graph-grammar decomposition to identify intrinsic substructures (clans) in the graph. The method provides for a two-dimensional analysis of the graph while the typical approach to drawing digraphs uses the single dimension, level, to arrange the nodes. \\nCorresponding to every visual graph is a textual representation in GML. GML was developed by Michael Himsolt at the University of Passau and is an attempt to standardize graph input formats. GML facilitates the attachment of arbitrary information to graphs, nodes, and edges. The user can also convert the VGJ drawn graph to PostScript format. This format can then be printed or saved into a user\\'s local file and used as desired.\\nAbout This Document\\nThis document has five major sections: \\n\\nIntroduction - this section \\nGetting Started - how to download and run VGJ \\nVGJ Basics - a tutorial on how to accomplish basic tasks in VGJ \\nAutomatic Layouts - an overview about the graph layout capabilities of VGJ and the algorithms they are based on \\nMenu Commands - a listing and a brief description of the commands available through the menu bar\\nIn VGJ Basics, a long method for accomplishing a task is first shown. This method is more intuitive and can be accomplished with a single-button mouse, but there are quicker methods. Shorter methods will be described following the first method. The following symbols are used for quick reference.\\n* Indicates a short cut using the right mouse button.\\n** Indicates a short cut using the middle mouse button.\\n*** Indicates a key short cut.\\nLegal Stuff\\nVGJ may be distributed under the terms of the GNU General Public License, Version 2.\\nThis graph drawing project is currently not funded. If you like what we\\'re doing or if you want to use it commercially, please help support the graduate students working on the project. You may hire us to customize the tool to your needs.\\n\\nGetting Started\\nRunning VGJ\\nRunning VGJ as an Applet\\nThe easiest way to run VGJ is to use a Java enabled web browser such as Netscape or Internet Explorer.\\n\\nStart your web browser\\nPoint your browser at the VGJ web page located at:\\nwww.eng.auburn.edu/department/csse/research/research_groups/research/graph_drawing/graph_drawing.html\\nLaunch the applet.  Click on the button labeled\\n\\n\\n\\n\\nStart a Graph Window\\n\\n\\n\\nYour web browser may prohibit file operations, in which case you must run the program as an application and not an applet for file i/o to work.\\nRunning VGJ as an Application\\nTo run VGJ as an application you must first install VGJ (see next section.)  VGJ was developed using Sun\\'s Java Development Kit.  If you are using the same, perform the following steps to run VGJ as an application:\\n\\n\\nGo to a command prompt (how will depend on the operating system being used)\\nChange to the graph_drawing directory.  The command will vary depending on the operating system but should be something similar to:\\ncd graph_drawing\\nExecute the applet\\njava EDU.auburn.VGJ.VGJ\\n\\nIf you are not using JDK, consult your documentation to determine how to run an application.\\nWhen VGJ is run as an application, the first window loaded will contain two buttons.  Start a Graph Window will load the graph editor just like the button on the web page.  Exit will close the VGJ application.\\n\\nInstalling VGJ\\n1. Download the software\\n\\nVGJ is available in two different compressed file formats:\\n\\nVGJ.tar (larger) is available for Unix systems without unzip.\\nVGJ.zip (smaller).\\n\\nBoth files contain the whole project (sources, classes, documentation.)\\n\\n2. Uncompress the files\\n\\nFor VGJ.tar, at a command prompt, type\\n\\ttar xf VGJ.tar\\nFor VGJ.zip, you must use PKZip for Windows, unzip for UNIX, or similar software that supports long file names and can decompress the file. If PKZip for Windows is installed, you can double-click on the file to decompress it.\\nBoth files when decompressed correctly will create a directory called graph_drawing. All needed files will be placed in this directory.\\n\\n\\nVGJ Basics\\nThe following figure shows the VGJ main window.\\n\\nAreas of Note\\n\\nMenu Bar - a standard menu bar where many of VGJ functions can be reached and called. A detailed explanation can be found in a later section.\\nMouse Action - this area is a group of action buttons. Only one item can be selected at a time. The item selected determines what functionality a click will have in the graph window.\\nViewing Offset - the big square represents the entire drawing space while the little square represents the area being displayed in the graph window. The area viewed in the graph window can be moved by using the scrollbars on the graph window or clicking on the little square and dragging it to a new location in the drawing space.\\nScale - these buttons allow the user to zoom in and out in the graph window. Note that the Scale/2 and Scale*2 can be pressed multiple times.\\nViewing Angles - allows you to adjust the direction from which you view your graph. By default, the x-axis is right and left, the y-axis is up and down, and z-axis is forward and back. You can adjust the view by using the buttons below the window to set which axes are parallel to the screen, by clicking anywhere in the viewing angle box, or by clicking and dragging inside the viewing angle box. \\nGraph Window - this is where the graph is displayed and edited.\\nCreating a graph\\nTo create a graph in VGJ, you must first create nodes and then create edges to connect them.\\n\\xa0Creating Nodes\\n\\nSelect Create Nodes from the Mouse Action area of the window\\nLeft-click in the graph window where you want the node to be located\\nRepeat as many times as necessary\\nBy default, a node\\'s label is its assigned number in the graph. This can be changed by editing a node\\'s attributes, or by changing a global setting, as explained in later sections.\\n\\xa0Creating Edges\\n\\nSelect Create Edges from the Mouse Action area of the window\\nLeft-click on the node in which the edge will start\\n(Optional) Hold down the Shift key and click on a clear space in the graph window to create bends in the edge\\nLeft-click on the node in which the edge will end\\n\\xa0\\n** The middle button of a three-button mouse can be used without modifying the selection in the Mouse Action area.\\n\\nMiddle-click on the node in which the edge will start\\n(Optional) Hold down the Shift key and click on a clear space in the graph window to create bends in the edge\\nMiddle-click on the node in which the edge will end\\n\\xa0\\n*** A second alternative is:\\n\\nHold down the Ctrl key\\nClick with any mouse button on the node in which the edge will start. (The Ctrl key can be released.)\\n(Optional) Hold down the Shift key and left-click on a clear space in the graph window to create bends in the edge\\nClick with any mouse button on the node in which the edge will end\\nAgain, it does not matter what is selected in Mouse Action.\\n\\xa0\\nBy default, VGJ draws edges for a directed graph and the order in which nodes are clicked determines the direction of the edge. To make an edge bi-directional, draw two edges between the nodes: one from A to B and one from B to A.\\nA graph can be made non-directional by unselecting Directed on the Properties menu.\\nSelf edges are allowed. They can be created by making the same node be the start and end of the edge. Self edges are initialized with a small triangular path above the node.\\nEditing a Graph\\nAn existing graph can be manipulated in several ways: nodes can be moved to new positions, nodes and edges can be deleted, nodes can be resized, the attributes of nodes and edges can be modified, and graph properties can be set.\\nMoving and Resizing Nodes\\n\\nSelect Select Nodes or Select Nodes or Edges from the Mouse Action area\\nLeft-click on a Node, red clickable objects will appear around the node.\\n\\nTo move a node, hold down the left mouse button with the pointer on the movement bar and drag the node to its new location.\\nTo size a node, hold down the left mouse button with the pointer on one of the sizing bars and drag it until the size is as desired\\n\\n\\nThe bottom bar sizes the node vertically.\\nThe left bar sizes the node horizontally.\\nThe upper right bar sizes the node in both directions proportionally.\\n\\xa0\\n\\n* The right mouse button can be used without modifying the selection in the Mouse Action area.\\n\\nRight-click on a Node, red clickable objects will appear around the node.\\nTo move or size the Node, hold down the right mouse button on the appropriate bar and perform the desired drag.\\n\\xa0\\nSelecting Multiple Objects\\n\\nSelect one of the Select options from the Mouse Action area.\\nHold down the left mouse button with the cursor on the drawing area and drag the mouse in any direction. This will form a box with one corner at the starting position and one at the pointer\\'s current position.\\nMove the pointer around so that the objects to be selected are inside the box.\\nRelease the mouse button.\\nThe objects selected are dependent on the Mouse Action selected.\\n\\n\\n\\nSelect Nodes - all nodes inside the box will be selected\\nSelect Edges - all edges inside the box will be selected\\nSelect Nodes and Edges - everything in the box will be selected\\n\\n\\n* Again, the same functionality is available by using the right mouse button, and it does not matter what Mouse Action is selected. Using this option will select both edges and nodes.\\n\\xa0\\n*** Groups of nodes and edges may also be selected one at a time by holding down the Shift key while selecting.\\n\\nSelect one of the Select options from the mouse area.\\nHold down the Shift key.\\nLeft-click on nodes and/or edges.\\n* The right mouse button may also be used in conjunction with the Shift key regardless of what Mouse Action is selected. This method can also be used to add objects to an existing selection of objects.\\nAll objects in the graph may be selected by choosing Select All from the Edit menu.\\nWhen multiple nodes are selected, moving or sizing one of the selected nodes will apply the same transformations to all selected nodes.\\n\\xa0\\nDeleting Edges and Nodes\\n\\nSelect the object(s) to be deleted.\\nChoose Delete Selected Items from the Edit menu, or press the Delete key. The key command may not work properly in some web browsers.\\nDeleting a node will also delete any edges associated with that node.\\n\\xa0\\nEditing Node Attributes\\n\\nSelect Select Nodes or Select Nodes or Edges from the Mouse Action area\\nDouble-click with the left mouse button on a Node, and the Node\\'s attributes dialog box will appear.\\n* Double-clicking with the right mouse button will also bring up the attribute dialog box regardless of what Mouse Action is selected.\\n\\xa0\\n\\n\\xa0\\nAttributes:\\n\\nPosition - the Cartesian coordinates of the node \\nBounding Box - this determines the size of the node \\nShape - nodes can have a rectangular or oval shape \\nLabel - a text string that identifies the node (use \"\\\\n\" to split lines)\\nLabel Position - there are three choices for label position\\n\\n\\nBelow - label is below the node \\nIn (autosize) - label is inside the node. The node is automatically sized to the minimum size in which the text will fit. \\nCenter - label is inside the node, but autoresizing does not occur.\\n\\n\\nImage - the node shape can be replaced with an image. The image source can be an URL or a file and path name.\\n\\xa0\\nEditing Edge Attributes\\n\\nSelect Select Edges or Select Nodes or Edges from the Mouse Action area\\nDouble-click with the left mouse button on an Edge, and the Edge\\'s attributes dialog box will appear.\\n* Double-clicking with the right mouse button will also bring up the attribute dialog box regardless of what Mouse Action is selected.\\n\\xa0\\n\\n\\xa0\\nAttributes:\\n\\nLabel - a text string that identifies the edge \\nPoints - a series of Cartesian coordinates that determines bends in the edge. The coordinates indicate where the bends will occur.\\n\\xa0\\nEditing a Graph\\'s Properties\\nThere are several properties that can be set that affect the entire graph. The following four properties can be set by clicking on the appropriate choice in the Properties menu and only have two values. Selecting a property once will set the property to the second value. Selecting it again will return it to the original.\\n\\nShow Controls - if there is a checkmark beside this menu choice, the Mouse Action, Viewing Offset, Scale, and Viewing Angles sections on the left side of the GUI are displayed. Otherwise, those sections are hidden and more of the graph window is available for work. *** Pressing the s key will toggle this feature. \\nDirected - if there is a checkmark beside this menu choice, the graph is directed (i.e. edges are arrows and the direction of the edges are important.) Otherwise the graph is undirected - edge direction is unimportant. \\nScale Node Size - if there is a checkmark beside this menu choice, new nodes will maintain the default size.  If not checked, the default size of new nodes will be scaled to correlate with the current graph scale. For example, if the graph scale is 2 then the new node size will be halved.\\nUse Node ID as Default Label - if there is a checkmark beside this menu choice, new nodes will be labeled with their ID number until a new label is entered. Otherwise, new nodes will not be labeled. \\nHigh Quality Display - if there is a checkmark beside this menu choice, an edge attached to a node represented by an image will appear to touch the image. Otherwise, there may appear to be a gap between the image and the edge.\\n\\xa0\\nOther Graph Properties\\n\\nSet New Node Properties - when this item is selected, the node attributes dialog box will appear. Changes in this box will be applied to new nodes when they are created. \\nSet Node Spacing - this setting is used by the tree and CGD layout algorithms to determine the amount of space reserved for nodes in the layout. \\nSet Font - when this item is selected a dialog box will appear. In this box, you can supply the name and the point size of the font to be used for node and edge labels.\\nSaving and Loading Work\\nVGJ stores graphs in a text file using GML format. This text can be edited in VGJ by choosing Edit Text Representation from the edit menu. Files can be saved by using the Save and Save As choices in the File menu. Choosing Save will replace the current file you\\'re working on with the new version. If the graph has not been saved before, choosing Save will function the same as Save As. Choosing Save As will allow you to select a file name and a directory for the file.\\nThe graph can also be saved in Postscript format for printing. To do so, select Postscript Output from the file menu.\\nPlease note that file functions may not be allowed when running VGJ as an applet in a web browser. This is dependent on the web browser\\'s implementation of Java.\\nGroups\\nVGJ provides the capability to represent groups of nodes as a single node. Groups in VGJ are indicated visually by nodes having a double border.\\nAn example:\\n\\tOriginal graph\\n\\n\\xa0\\n\\tTwo nodes grouped together\\n\\n\\xa0\\n\\tTwo nodes and a group grouped together\\n\\n\\xa0\\nThere are two methods to work with groups. One, you can use the Group Control dialog box. To open the box, select Group Control from the Edit menu. Two, the letters in square brackets in this box represent the keys that can be pressed to get the same functionality as clicking on the related button. The dialog box does not have to be opened to use these key commands.\\n\\xa0\\n\\n\\xa0\\n\\nCreate Group [c] - when this button is pressed, selected nodes (see Selecting Multiple Objects above) will be combined into a single node. Note that groups can be nested - that is both nodes and groups can be selected to form a group. Also note that nodes/groups do not have to be connected to be grouped. \\nDestroy Group [d] - when this button is pressed, selected groups will be destroyed. The nodes/groups inside the selected group(s) will be returned to their original position. \\nGroup [g] - when this button is pressed, selected nodes will transform into their groups. Note that it is not necessary for all nodes of a group to be selected for this button to work. For example, suppose nodes A and B are in group 1 and nodes C and D are in group 2. If nodes A and C are selected when the Group button is pressed, groups 1 and 2 will be formed. \\nUngroup [u] - when this button pressed, the nodes/groups in the selected group(s) will be returned to their original position. The group can be reformed later by using the Group button. \\nCancel - closes the dialog box.\\nNote that group nodes can be edited just like non-group nodes. If the group is scaled, all nodes in the group will be scaled proportionally. If the group is moved, all nodes in the group will be moved by the same amount.\\n\\nAutomatic Layouts\\nCurrently VGJ offers three layout algorithms:\\n\\nTree - to layout rooted trees \\nCGD - for directed graphs \\nSpring - for undirected graphs\\nThere is also an algorithm to test a graph for biconnectivity or make it biconnected by adding edges.\\nThe objectives of the layout algorithms are to draw graphs that are aesthetically pleasing and useful for visual analysis. For each algorithm the particular aesthetic criteria is specialized to the category of graph on which it operates. \\nTree algorithm\\nThe tree algorithm implementation is that of Walker. Trees are drawn so that\\n\\nNodes at same level lie on a straight line \\nParents are centered over their children \\nThere is vertical symmetry \\nIsomorphic subtrees are drawn identically \\nGiven the above properties and a minimum horizontal spacing, the tree has the minimum possible width. The algorithm does adjust for different sized nodes.\\nIf the graph is not a tree, a depth-first search will be used to identify a spanning tree, which will be used for layout. If the graph contains cycles, a node which will serve as the root must be selected before the algorithm is run.\\n\\nSpring embedder\\nWe are concerned with drawing undirected graphs with the aim to meet some accepted aesthetic criteria. They should have (1) few edge crossings, (2) straight edges and (3) edges of uniform length. The algorithm of Kamada and Kawai meets the last two criteria by defining \"energy\" between pairs of graph points and minimizing the total energy of the graph. \\n\\xa0\\nThe concept of \"spring embedder\" or \"force-directed\" layout algorithms, is to model a graph as a set of rings (nodes) connected by springs (edges). The nodes are placed in some initial state and the springs move the nodes toward a minimum energy state. This energy is reduced by solving a partial differential equation for each vertex. Each vertex is repositioned in turn until the energy goes below some threshold.\\n\\nDirected graphs \\nCGD, clan-based graph drawing, produces a layout for directed graphs. The goals of the layout are to (1) follow the direction of the arcs so that ancestor nodes always lie above their descendants; (2) balance the nodes from right to left within each level; (3) have few edge crossings; (4) have few edge bends. The node layout is determined by the combination of (1) parsing of the graph into logically cohesive subgraphs and (2) defining layout attributes to apply to the resulting parse tree. The parse is based on a simple graph grammar, and the attributes that are now programmed into CGD produce a layout whose nodes are balanced both vertically and horizontally. Attributes are attached to the parse tree that determine space requirements for graph nodes and assign node locations. The method allows nodes of varying sizes to be incorporated into the graph.\\n\\n\\nMenu Commands\\nFile\\n\\nOpen - loads a text file containing a GML description of a graph into the editor.\\nSave - saves the current graph, replacing the old version on disk. If the graph has not been saved before, functions the same as Save As. Files saved as a text file with a GML description of the text.\\nSave as - requests a file name and a directory for saving the current graph. Files saved as a text file with a GML description of the text.\\nOpen Example Graph - loads an example graph into the graph editor\\nPostscript Output - requests a file name and a directory for saving the current graph for printing. File is saved as a Postscript file.\\nExit This Window - closes VGJ.\\n\\xa0\\n\\nAlgorithms\\n\\nTree\\n\\nTree Down - runs the tree layout algorithm and has the tree span down from the root.\\nTree Up - runs the tree layout algorithm and has the tree span up from the root.\\nTree Left - runs the tree layout algorithm and has the tree span left from the root.\\nTree Right - runs the tree layout algorithm and has the tree span right from the root.\\n\\nRandom - generates a random layout for the graph.\\nCGD\\n\\nCGD - runs a layout algorithm for directed graphs.\\nShow CGD Parse Tree - displays the parse tree from the CGD algorithm in a dialog box.\\n\\nSpring - runs a layout algorithm for directed graphs.\\nBiconnectivity\\n\\nRemove Articulation Points - \\nFind Articulation Points - \\n\\xa0\\n\\n\\n\\nEdit\\n\\nEdit Text Representation (GML) - loads the GML representation of the current graph into a dialog box with text editing capabilities\\nDelete Selected Items - removes selected items from the graph window. If a node is deleted, edges attached to it will also be deleted.\\nSelect All - selects all nodes and edges in the graph.\\nRemove All Edge Bends - forces all edges to be straight lines.\\nRemove All Groups - destroys any groups in the graph.\\nGroup Control - displays the group control dialog box.\\n\\xa0\\n\\nProperties\\n Show Controls - determines whether or not the controls on the left of the GUI will be displayed.\\n\\nDirected - determines whether or not a graph is directed\\nSet New Node Properties - loads the node attribute dialog box. Attributes set here determine the properties of nodes when they are created.\\nSet Node Spacing - brings up a dialog box to set the node spacing properties used in layout algorithms.\\nSet Font - brings up a dialog box to determine the type and size of the font used in the graph.\\nScale Node Size - determines whether or not new nodes will be scaled.\\nUse Node ID as Default Label - determines whether or not the Node ID is used as the default label for new nodes.\\nHigh Quality Display - determines whether or not the High Quality Display is active.\\n\\xa0\\n\\xa0\\n\\n\\n\\n',\n",
       " '\\nDr. Arsham\\'s Statistics Site\\n\\n\\n\\n\\n\\n\\n\\n\\nStatistical Thinking for Managerial Decision Making\\n\\n\\n\\n\\nAsia-Pacific Mirror Site\\nEurope Mirror Site\\nMiddle East Mirror Site\\nSouth America Mirror Site\\nUK Mirror Site USA Site\\n\\n\\n\\n\\n\\nThis Web site is a course in statistics appreciation; i.e., acquiring a   feeling for the statistical way of thinking. It is an introductory course in   statistics that is designed to provide you with the basic concepts and methods of statistical analysis for decision making under uncertainties. Materials in  this Web site are tailored to meet your needs in making good decisions by   fostering statistical thinking. The cardinal objective for this Web site is to   increase the extent to which statistical thinking is merged with managerial   thinking for decision making under uncertainty.\\nProfessor Hossein Arsham\\xa0 \\xa0\\n\\n\\n\\xa0 \\xa0MENU\\n\\nChapter 1: \\xa0\\xa0Towards Statistical Thinking for Decision Making\\nChapter 2: \\xa0\\xa0Descriptive Sampling Data Analysis\\nChapter 3: \\xa0\\xa0Probability for Statistical Inference and Modeling\\nChapter 4: \\xa0\\xa0Necessary Conditions for Statistical Decision Making\\nChapter 5: \\xa0\\xa0Estimators and Their Qualities\\nChapter 6: \\xa0\\xa0Hypothesis Testing: Rejecting a Claim\\nChapter 7: \\xa0\\xa0Hypotheses Testing for Means and Proportions\\nChapter 8: \\xa0\\xa0Tests for Statistical Equality of Two or More Populations\\nChapter 9: \\xa0\\xa0Applications of the Chi-square Statistic\\nChapter 10: \\xa0Regression Modeling and Analysis\\nChapter 11: \\xa0Unified Views of Statistical Decision Technologies\\nChapter 12: \\xa0Visualization of Statistics\\nChapter 13: \\xa0Index Numbers with Applications\\n\\n\\xa0 \\xa0Companion Sites:\\n\\n\\nJavaScript E-labs Learning Objects ,  \\xa0Europe Mirror Site Collection.\\nExcel For Introductory Statistical Analysis,  \\xa0Europe Mirror Site.\\n\\nFrequently Asked Questions: A Statistical Why? List (Word.Doc)\\nStatistical Data Analysis,\\xa0Asia-Pacific Mirror Site,\\xa0Europe Mirror Site. \\nTime Series Analysis and Business Forecasting,\\xa0Europe Mirror Site. \\nComputers and Computational Statistics,\\xa0Europe Mirror Site. \\nQuestionnaire Design and Surveys Sampling,\\xa0Europe Mirror Site. \\nProbabilistic Modeling, \\xa0Europe Mirror Site, \\xa0Versión en Español. \\nSystems Simulation,   \\xa0 Europe Mirror Site, \\xa0South Africa Mirror Site. \\nProbability and Statistics Resources,  \\xa0 Europe Mirror Site.\\n  To search the site, try Edit | Find in page [Ctrl + f]. Enter a word or phrase in the dialogue box, e.g. \"parameter\" or \"probability\". If the first appearance of the word/phrase is not   what you are looking for, try Find Next.\\n\\n\\n\\n\\n\\nTowards Statistical Thinking for Decision Making\\n\\nIntroduction\\nThe Birth of Probability and Statistics\\nStatistical Modeling for Decision-Making under  Uncertainties\\nStatistical Decision-Making Process\\nWhat is Business Statistics?\\nCommon Statistical Terminology with Applications\\n\\n\\n\\n\\nDescriptive Sampling Data Analysis\\n\\nGreek Letters Commonly Used in Statistics\\nType of Data and Levels of Measurement\\nWhy Statistical Sampling?\\nSampling Methods\\nRepresentative of a Sample: Measures of Central Tendency\\nSelecting Among the Mean, Median, and Mode\\nSpecialized Averages: The Geometric & Harmonic Means\\nHistogramming: Checking for Homogeneity of Population\\nHow to Construct a BoxPlot\\nMeasuring the Quality of a Sample\\nSelecting Among the Measures of Dispersion\\nShape of a Distribution Function: The Skewness-Kurtosis Chart\\nA Numerical Example & Discussions\\nThe Two Statistical Representations of a Population\\nEmpirical (i.e., observed) Cumulative Distribution Function\\n\\n\\n\\nProbability for Statistical Inference and Modeling\\n\\nIntroduction\\nProbability, Chance, Likelihood, and Odds\\nHow to Assign Probabilities\\nGeneral Laws of Probability\\nMutually Exclusive versus Independent Events\\nWhat Is so Important About the Normal Distributions?\\nWhat Is a Sampling Distribution?\\nWhat Is The Central Limit Theorem?\\nWhat Is \"Degrees of Freedom\"?\\nApplications of and Conditions for Using Statistical Tables\\n\\nBeta Density Function\\nBinomial Probability Function\\nChi-square Density Function\\nExponential Density Function\\nF-Density Function\\nGamma Density Function\\nLog-normal Density Function\\nMultinomial Probability Function\\nNormal Density Function\\nPoisson Probability Function\\nStudent T-Density Function\\nTriangular Density Function\\nUniform Density Function\\n\\n\\n \\n\\nNecessary Conditions for Statistical Decision Making\\n\\nIntroduction\\nMeasure of Surprise for Outlier Detection\\nHomogeneous Population (Don\\'t mix apples and oranges)\\nTest for Randomness\\nTest for Normality\\n\\n\\n\\n\\nEstimators and Their Qualities\\n\\nIntroduction\\nQualities of a Good Estimator\\nStatistics with Confidence\\nWhat Is the Margin of Error?\\nBias Reduction Techniques: Bootstrapping and Jackknifing\\nPrediction Intervals\\nWhat Is a Standard Error?\\nSample Size Determination\\nRevising the Expected Value and the Variance\\nSubjective Assessment of Several Estimates\\n\\n\\n\\n\\nHypothesis Testing: Rejecting a Claim\\n\\nIntroduction\\nManaging the Producer\\'s or the Consumer\\'s Risk\\nClassical Approach to Testing  Hypotheses\\nThe Meaning and Interpretation of P-values (what the data say)\\nBlending the Classical and the P-value Based Approaches in Test of Hypotheses\\nBonferroni Method for Multiple P-Values Procedure\\nPower of a Test and the Size Effect\\nParametric vs. Non-Parametric vs. Distribution-free Tests\\n\\n\\n \\n\\nHypotheses Testing for Means and Proportions\\n\\nIntroduction\\nSingle Population t-Test\\nTwo Independent Populations\\nWhen Should We Pool Variance Estimates?\\nNon-parametric Multiple Comparison Procedures\\nThe Before-and-After Test\\nANOVA for Normal but Condensed Data Sets\\nANOVA for Dependent Populations\\n\\n\\n \\n\\nTests for Statistical Equality of Two or More Populations\\n\\nIntroduction\\nEquality of Two Normal Populations\\nTesting a Shift in Normal Populations\\nAnalysis of Variance (ANOVA)\\nEquality of Proportions in Several Populations\\nDistribution-free Equality of Two Populations\\nComparison of Two Random Variables\\n\\n\\n\\n\\nApplications of the Chi-square Statistic\\n\\nIntroduction\\nTest for Crosstable Relationship\\nIdentical Populations Test for Crosstable Data\\nTest for Equality of Several Population Proportions\\nTest for Equality of Several Population Medians\\nGoodness-of-Fit Test for Probability Mass Functions\\nCompatibility of Multi-Counts\\nNecessary Conditions in Applying the Above Tests\\nTesting the Variance: Is the Quality that Good?\\nTesting the Equality of Multi-Variances\\nCorrelation Coefficients Testing\\n\\n\\n\\n\\nRegression Modeling and Analysis\\n\\nIntroduction\\nRegression Modeling Selection Process\\nCovariance and Correlation\\nPearson, Spearman, and Point-biserial Correlations\\nCorrelation, and Level of Significance\\nIndependence vs. Correlated\\nHow to Compare Two Correlation Coefficients\\nPlanning, Development, and Maintenance of a Model\\nConditions and the Check-list for Linear Models\\nAnalysis of Covariance: Comparing the Slopes\\nResidential Properties Appraisal Application\\n\\n\\n\\n\\nUnified Views of Statistical Decision Technologies\\n\\nIntroduction\\nHypothesis Testing with Confidence\\nRegression Analysis, ANOVA, and Chi-square Test\\nRegression Analysis, ANOVA, T-test, and Coefficient of Determination\\nRelationships among Distributions and Unification of Statistical \\nTables\\n\\n\\n\\n\\nVisualization of Statistics: Analytic-Geometry & Statistics\\n\\nIntroduction\\nMean and Median\\nGeometric Mean\\nVariance, Covariance, and Correlation Coefficient \\n\\n\\n\\n\\nIndex Numbers with Applications\\n\\nIntroduction\\nThe Geometric Mean\\nRatio Indexes\\nComposite Index Numbers \\nVariation Index as a Quality Indicator\\nLabor Force Unemployment Index\\nSeasonal Index and Deseasonalizing Data\\nStatistical Technique and Index Numbers\\n\\n\\n\\n\\n\\n\\n\\nIntroduction to Statistical Thinking for Decision Making\\nThis site builds up the basic ideas of business statistics systematically and correctly. It is a combination of lectures and computer-based practice, joining theory firmly with practice. It introduces techniques for summarizing and presenting data, estimation, confidence intervals and hypothesis testing. The presentation focuses more on understanding of key concepts and statistical thinking, and less on formulas and calculations, which can now be done on small computers through user-friendly  Statistical JavaScript Applets, etc.   Today\\'s good decisions are driven by data. In all aspects of our lives, and importantly in the business context, an amazing diversity of data is available for inspection and analytical insight.  Business managers and professionals are increasingly required to justify decisions on the basis of data.  They need statistical model-based decision support systems.   \\n\\nStatistical skills enable them to intelligently collect, analyze and interpret data relevant to their decision-making. Statistical concepts and statistical thinking enable them to: \\n  \\nsolve problems in a diversity of contexts.add substance to decisions.   reduce guesswork.  \\n \\nThis Web site is a course in statistics appreciation; i.e., acquiring a feel for the statistical way of thinking.  It hopes to make sound statistical thinking understandable in business terms.  An introductory course in statistics, it is designed to provide you with the basic concepts and methods of statistical analysis for processes and products. Materials in this Web site are tailored to help you make better decisions and to get you thinking statistically. A cardinal objective for this Web site is to embed statistical thinking into managers, who must often decide with little information. \\n  In competitive environment,  business managers must design quality into products, and into \\n  the processes of making the products. They must facilitate a process of  never-ending improvement at all stages of manufacturing and service. This is a  strategy that employs statistical methods, particularly statistically designed experiments, and produces processes that provide high yield and products that seldom fail. Moreover, it facilitates development of robust products that are insensitive to changes in the environment and internal  component variation. Carefully planned statistical studies remove hindrances to high quality and productivity at every stage of production. This saves time and money. It is well recognized that quality must be engineered into products as early as possible in the design process. One must know how to use carefully \\n  planned, cost-effective statistical experiments to improve, optimize and make robust products and processes. Business Statistics is a science assisting you to make business decisions under uncertainties based on some numerical and measurable scales. Decision making processes must be based on data, not on personal opinion nor on belief. The Devil is in the Deviations: Variation is inevitable  in life! Every process, every measurement, every sample has variation. Managers need to understand variation for two key reasons. First,   so that they can lead others to apply statistical thinking in day-to-day  activities and secondly, to apply the concept for the purpose of continuous  improvement. This course will provide you with hands-on experience to promote  the use of statistical thinking and techniques to apply them to make educated  decisions, whenever you encounter variation in business data. You will learn techniques to intelligently assess and manage the risks inherent in decision-making. Therefore, remember that:   Just like weather, if you cannot control something,  you should learn how to measure and analyze it, in order to predict it, \\n  effectively.   If you have taken statistics before, and have a feeling of inability to \\n  grasp concepts, it may be largely due to your former non-statistician instructors  teaching statistics. Their deficiencies lead students to develop phobias for  the sweet science of statistics. In this respect, Professor Herman Chernoff (1996) made the following remark: \\n   \"Since everybody in the world thinks he can teach statistics even  though he does not know any, I shall put myself in the position of teaching  biology even though I do not know any\"\\nInadequate statistical teaching during university education leads even after graduation, to one or a combination of the following scenarios:  \\n\\nIn general, people do not like statistics and therefore they try to avoid it. \\n  There is a pressure to produce scientific papers, however often confronted  with \"I need something quick.\"   At many institutes in the world, there are only a few (mostly 1)  statisticians, if any at all. This means that these people are extremely busy.  As a result, they tend to advise simple and easy to apply techniques, or they will have to do it themselves. \\n  Communication between a statistician and decision-maker can be difficult. One speaks in statistical jargon; the other understands the monetary or utilitarian benefit of using the statistician\\'s recommendations. \\n\\nPlugging numbers into the formulas and crunching them have no value by themselves. You should continue to put effort into the concepts and concentrate on interpreting the results. \\n  Even when you solve a small size problem by hand, I would like you to use the available computer software and Web-based computation to do the dirty work for you. \\n  You must be able to read the logical secret in any formulas not memorize them. For example, in computing the variance, consider its formula. \\n  Instead of memorizing, you should start with some why: i. Why do we square the deviations from the mean. Because, if we add up all deviations, we get always zero value. So, to deal with this problem, we \\n  square the deviations. Why not raise to the power of four (three will not work)? Squaring does the trick; why should we make life more complicated than  it is? Notice also that squaring also magnifies the deviations;  therefore it works to our advantage to measure the quality of the data. ii. Why is there a summation notation in the formula.To add up the squared deviation of each data point to compute the total sum of squared deviations. iii. Why do we divide the sum of squares by n-1. The amount of \\n  deviation should reflect also how large the sample is; so we must bring in the sample size. That is, in general,  larger sample sizes have larger sum of square deviation from the mean.  Why n-1 not n? The reason for n-1 is that when you divide by n-1, the sample\\'s variance provides an estimated variance \\n  much closer to the population variance, than when you divide by n. You note that for large sample size n (say over 30), it really does not matter whether it is divided by n or n-1. The results are almost the same, and they are acceptable. The factor n-1 is what we consider as the \"degrees of freedom\". \\n  This example shows how to question statistical formulas, rather than memorizing them. In fact, when you try to understand the formulas, you do not need to remember them, they are part of your brain connectivity. Clear thinking is always more important than the ability to do arithmetic. \\n  When you look at a statistical formula, the formula should talk to you, as when a musician looks at a piece of musical-notes, he/she hears the music.  computer-assisted learning: The \\n  computer-assisted learning provides you a \"hands-on\" experience which will enhance your understanding of the concepts and techniques covered in this site. \\n  \\nJava, once an esoteric programming language for animating Web pages, is now a full-fledged platform for building JavaScript E-labs\\' learning objects with useful applications. As you used to do experiments in physics labs to learn physics, computer-assisted learning enables you to use any online interactive tool available on the Internet to perform experiments. The purpose is the same;  i.e., to understand statistical concepts by using statistical applets which are entertaining and educating. The appearance of computer software, JavaScript Applets, Statistical   Demonstration Applets, and Online Computation are the most important events in the process of teaching and learning concepts in model-based, statistical decision making courses. These e-lab  Technologies allow you to construct numerical examples to understand the concepts, and to find their significance for  yourself. \\n  \\nUnfortunately, most classroom courses are not learning systems. The way the instructors attempt to help their students acquire skills and knowledge has absolutely nothing to do with the way students actually learn. Many instructors rely on lectures and tests, and memorization. All too often, they rely on \"telling.\" No one remembers much that\\'s taught by telling, and what\\'s told doesn\\'t translate into usable skills. Certainly, we learn by doing, failing, and practicing until we do it right. The computer assisted learning serves this purpose. \\n  A course in appreciation of statistical thinking gives business professionals an edge. Professionals with strong quantitative skills are in demand. This phenomenon will grow as the impetus for data-based decisions strengthens and the amount and availability of data increases. The statistical toolkit can be developed and enhanced at all stages of a career. Decision making process under uncertainty is largely based on application of statistics for probability assessment of uncontrollable events (or factors), as well as \\n  risk assessment of your decision. The main objective for this course is to learn statistical thinking; to \\n  emphasize more on concepts, and less theory and fewer recipes, and finally to foster active learning using  the useful and interesting Web-sites. It is already a known fact that \"Statistical thinking will one day be as \\n  necessary for efficient citizenship as the ability to read and write.\" So, let\\'s be ahead of our time. \\n  Further Readings:Chernoff \\n  H., A Conversation With Herman Chernoff, Statistical Science, Vol. 11, No. 4, 335-350, 1996.Churchman C., The Design of Inquiring Systems, Basic Books, New York, 1971. Early in the book he stated that knowledge could be considered as a collection of information, or as an activity, or as a \\n  potential. He also noted that knowledge resides in the user and not in the collection.Rustagi M., et al. (eds.), Recent Advances in Statistics: Papers in Honor of Herman Chernoff on His Sixtieth Birthday, Academic Press, 1983.\\n\\n\\n\\nThe Birth of Probability and StatisticsThe  original idea of \"statistics\" was the collection of information about and for the \"state\". The word statistics derives directly, not from any classical Greek  or Latin roots, but from the Italian word for state. \\n  The birth of statistics occurred in mid-17th century. A commoner, named John Graunt, who was a native of London, began reviewing a weekly church publication issued by the \\n  local parish clerk that listed the number of births, christenings, and deaths in each parish. These so called Bills of Mortality also listed the causes of death. Graunt who was a shopkeeper organized this data in the form  we call descriptive statistics, which was published as Natural and Political  Observations Made upon the Bills of Mortality. Shortly thereafter he was  elected as a member of Royal Society. Thus, statistics has to borrow some  concepts from sociology, such as the concept of Population.  It has been argued that since statistics usually involves the study of human  behavior, it cannot claim the precision of the physical sciences.   Probability has much longer history. Probability is derived from the verb to probe meaning to \"find out\" what is not too easily accessible or understandable. The word \"proof\" has \\n  the same origin that provides necessary details to understand what is claimed to be true. \\n  Probability originated from the study of games of chance and gambling during the 16th century. Probability theory was a branch of mathematics studied by Blaise Pascal and Pierre de Fermat in the seventeenth century. Currently in 21st century, probabilistic \\n  modeling is used to control the flow of traffic through a highway system, a telephone interchange, or a computer processor; find the genetic makeup of individuals or populations; quality control; insurance; investment; and other sectors of business and industry. \\n  New and ever growing diverse fields of human activities are using statistics; however, it seems that this field itself remains obscure to the public. Professor Bradley Efron expressed this fact nicely: \\n  During the 20th Century statistical thinking and methodology have become the scientific framework for literally dozens of fields including education, agriculture, economics, biology, and medicine, and with increasing influence recently on the hard sciences such as astronomy, geology, and physics. In other words, we have grown from a small obscure field into a big obscure field.Further Readings:Daston \\n  L., Classical Probability in the Enlightenment, Princeton University Press, 1988. The book points out that early Enlightenment thinkers could not face uncertainty. A mechanistic, deterministic machine, was the  Enlightenment view of the world.Gillies D., Philosophical Theories of  Probability, Routledge, 2000. Covers the classical, logical, subjective, frequency, and propensity views.Hacking I., The Emergence of   Probability, Cambridge University Press, London, 1975. A philosophical  study of early ideas about probability, induction and statistical inference.\\nHald A., A History of Probability and Statistics and Their Applications before 1750, Wiley, 2003.           Peters W., Counting for Something: Statistical Principles and  Personalities, Springer, New York, 1987. It teaches the principles of applied economic and social statistics in a historical context. Featured topics include public opinion polls, industrial quality control, factor analysis, Bayesian methods, program evaluation, non-parametric and robust methods, and exploratory data analysis.Porter T., The Rise of  Statistical Thinking, 1820-1900, Princeton University Press, 1986. The   author states that statistics has become known in the twentieth century as the  mathematical tool for analyzing experimental and observational data. Enshrined  by public policy as the only reliable basis for judgments as the efficacy of  medical procedures or the safety of chemicals, and adopted by business \\n  for such uses as industrial quality control, it is evidently among the products of science whose influence on public and private life has been most pervasive. Statistical analysis has also come to be seen in many scientific   disciplines as indispensable for drawing reliable conclusions from empirical (i.e., observed) results. This new field of mathematics found so extensive a domain of  applications. Stigler S., The History of Statistics: The Measurement of  Uncertainty Before 1900, U. of Chicago Press, 1990. It covers the people, \\n  ideas, and events underlying the birth and development of early statistics.Tankard J., The Statistical Pioneers, Schenkman Books, New York, 1984.This work provides the detailed lives and times of \\n  theorists whose work continues to shape much of the modern statistics. \\n\\n\\n\\nStatistical Modeling for Decision-Making under Uncertainties:From Data to the Instrumental KnowledgeIn this diverse world of ours, no two things are exactly the same. A statistician is  interested in both the differences and the \\n  similarities; i.e., both departures and patterns.   The actuarial tables published by insurance companies reflect their statistical analysis of the average life expectancy of men and women at any given age. From these numbers, the insurance companies then calculate the \\n  appropriate premiums for a particular individual to purchase a given amount of insurance. \\n  Exploratory analysis of data makes use of numerical and graphical techniques to study patterns and departures from patterns. The widely used descriptive statistical techniques are: Frequency Distribution;  Histograms; Boxplot;  Scattergrams and Error Bar plots;  and diagnostic plots.     In examining distribution of data, you should be able to detect important characteristics, such as shape, location, variability, and unusual values.   From careful observations of patterns in data, you can generate conjectures about relationships among variables. The notion of how one variable may be associated with another permeates almost all of statistics, from simple comparisons of proportions through linear regression. The difference between association and causation must accompany this conceptual development. Data must be collected according to a well-developed plan if valid information on a conjecture is to be obtained. The plan must identify   important variables related to the conjecture, and specify how they are to be measured. From the data collection plan, a statistical model can be formulated from which inferences \\n  can be drawn.   As an example of statistical modeling with managerial \\n  implications, such as \"what-if\" analysis,   consider regression analysis. Regression analysis is a powerful technique for studying relationship between dependent variables  (i.e., output, performance measure) and independent variables (i.e., inputs, factors, decision variables). Summarizing relationships among the variables by the most appropriate equation (i.e., modeling) allows us to predict or identify the most influential factors and study their impacts on the output for any changes \\n  in their current values. Frequently, for example the marketing managers are faced with the \\n  question, What Sample Size Do I Need? This is an important and common statistical decision, which should be given due consideration, since an inadequate sample size invariably leads to wasted resources. The sample size determination section provides a practical solution to this risky decision. \\n  Statistical models are currently used in various fields of business and science. However, the terminology differs from field to field. For example, the fitting of models to data, called calibration, history matching, and data assimilation, are all synonymous with parameter   estimation. Your organization database contains a wealth of information, yet the  decision technology group members tap a fraction of it. Employees waste time scouring multiple sources for a database. The decision-makers are frustrated because they cannot get business-critical data exactly when they need it.   Therefore, too many decisions are based on guesswork, not facts. Many opportunities are also missed, if they are even noticed at all. \\n  Knowledge is what we know. Information is the communication of knowledge. In every knowledge exchange, there is a sender and a receiver. The sender makes common what is private, does the informing, the communicating. Information can be classified as explicit and tacit forms. The explicit information can \\n  be explained in structured form, while tacit information is inconsistent and fuzzy to explain. \\n  Data is known to be crude information and not knowledge by itself. The sequence from data to knowledge is: from Data to Information, from Information to Facts, and finally, from Facts to \\n  Knowledge. Data becomes information, when it becomes relevant to your decision problem. Information becomes fact, when the data can support it. Facts are what the data reveals. However the decisive instrumental knowledge is expressed together with some statistical degree of \\n  confidence.  Fact becomes knowledge, when it is used in the successful completion of \\n  a decision process. Knowledge needs wisdom. Wisdom is the power to put our time and our knowledge to the proper use.  Once you have a massive amount of facts integrated as knowledge, then your mind will be superhuman in the same sense that mankind with writing is superhuman compared to mankind before writing. The following figure illustrates the statistical thinking process based on data in constructing statistical models for decision making under uncertainties.  \\n \\n\\nThe above figure depicts the fact that as the exactness of a statistical model increases, the level of improvements in decision-making increases. That\\'s why we need Business Statistics. Statistics arose from the need to place knowledge on a systematic evidence base. This required a study of the laws of probability, the development of measures of data properties and relationships, and so on. Statistical inference aims at determining whether any statistical significance can be attached that results after due allowance is made for any random variation as a source of error. Intelligent and critical inferences cannot be made by those who do not understand the purpose, the conditions, and applicability of the various techniques for judging significance.\\nThe purpose of statistical thinking is to get acquainted with the statistical techniques, to be able to execute  procedures using available JavaScript Applets, and to be conscious of the conditions and limitations of various techniques. \\n  \\n\\nStatistical Decision-Making Process\\nUnlike the deterministic decision-making process, such as linear optimization by solving systems of equations and \\n in decision making  under pure uncertainty, the variables are often more numerous and more difficult to measure and control. However, the steps are the same. They are:  \\n\\nSimplification   Building a decision model   Testing the model   Using the model to find the solution:    It is a simplified representation of the actual situation     It need not be complete or exact in all respects     It concentrates on the most essential relationships and ignores the less \\n    essential ones.     It is more easily understood than the empirical (i.e., observed) situation, and hence permits the problem to be  solved more readily with minimum time and effort.     It can be used again and again for similar problems or can be modified. \\n\\nFortunately the probabilistic and statistical methods for analysis and decision making under uncertainty are more numerous and powerful today than ever before. The computer makes possible many practical applications. A few examples of business applications are the \\n  following: \\n An auditor can use random sampling techniques to audit the accounts receivable for clients. \\n  A plant manager can use statistical quality control techniques to assure the quality of his production with a minimum of testing or inspection.  A financial analyst may use regression and correlation to help understand the relationship of a financial ratio to a set of other variables in business.   A market researcher may use test of significace to accept or reject the hypotheses about a group of buyers to which the firm wishes to sell a particular product. A sales manager may use statistical techniques to forecast sales for the \\n  coming year. \\nQuestions Concerning Statistical the Decision-Making Process:\\n Objectives or Hypotheses: What are the objectives of the study or the questions to be answered? What is the population to which the investigators intend to refer their findings?    Statistical Design: Is the study a planned experiment (i.e., primary data), or an analysis of records ( i.e., secondary data)? How is the sample to be selected? Are there possible sources of \\n  selection, which would make the sample atypical or non-representative? If so,  what provision is to be made to deal with this bias? What is the nature of the control group, standard of comparison, or cost? Remember that statistical modeling means reflections before actions.\\n Observations: Are there clear definition of   variables, including classifications, measurements (and/or counting), and the  outcomes? Is the method of classification or of measurement consistent for all  the subjects and relevant to Item No. 1.? Are there possible biased in    measurement (and/or counting) and, if so, what provisions must be made to deal   with them? Are the observations reliable and replicable (to defend your finding)?    Analysis: Are the data sufficient and worthy of statistical analysis? If so, are the necessary conditions of the methods of statistical analysis appropriate to the source and nature of the data? The  analysis must be correctly performed and interpreted.   \\nConclusions: Which conclusions are justifiable  by the findings? Which are not? Are the conclusions relevant to the questions posed in Item No. 1?    Representation of Findings: The finding must be represented clearly, objectively, in sufficient but non-technical terms and detail to enable the decision-maker (e.g., a manager) to understand and judge them for himself? Is the finding internally consistent; i.e., do the numbers  added up properly? Can the different representation be reconciled?   Managerial Summary: When your findings and  recommendation(s) are not clearly put, or framed in an appropriate manner understandable by the decision maker, then the decision maker does not feel  convinced of the findings and  therefore will not implement any of the recommendations. You have wasted the time, money, etc. for nothing.  Further  Readings:Corfield  D., and J. Williamson, Foundations of Bayesianism, Kluwer Academic  Publishers, 2001. Contains Logic, Mathematics, Decision Theory, and Criticisms  of Bayesianism.Lapin L., Statistics for Modern Business Decisions,   Harcourt Brace Jovanovich, 1987.Pratt J., H. Raiffa, and R. Schlaifer,   Introduction to Statistical Decision Theory, The MIT Press,   1994.\\n \\nWhat is Business Statistics?The main objective of Business Statistics is to make inferences  (e.g., prediction, making decisions) about certain characteristics of a population   based on information contained in a random sample from the entire population.  The condition for randomness is essential to make sure the \\n  sample is representative of the population.\\n  Business Statistics is the science of \\x91good\\' decision making in the face of uncertainty and is used in many disciplines, such as financial analysis, econometrics, auditing, production and operations, and marketing research. It provides knowledge \\n  and skills to interpret and use statistical techniques in a variety of business applications. A typical Business Statistics course is intended for business majors, and covers statistical study, descriptive statistics \\n  (collection, description, analysis, and summary of data), probability, and the binomial and normal distributions, test of hypotheses and confidence intervals, linear regression, and correlation. \\n  Statistics is a science of making decisions with respect to the characteristics of a group of persons or objects on the basis of numerical information obtained from a randomly selected sample of the group. \\n  Statisticians refer to this numerical observation as realization of a random sample. However, notice that one cannot see a random sample. A random sample is only a sample of a finite outcomes of a random process.\\nAt the planning stage of a statistical investigation, the question of sample size (n) is critical. For example, sample size for sampling from a finite population of size N, is set at: N½+1, rounded up to the nearest integer. Clearly, a larger sample provides more relevant information, and as a result a more accurate estimation and better statistical judgement regarding test of hypotheses.  \\n  Activities Associated with the General Statistical Thinking\\t\\n  Click on the image to enlarge it and THEN print it \\nThe above figure illustrates the idea of statistical inference from a random sample about the population.  It also provides estimation for the population\\'s  parameters; namely the expected value µx, the standard deviation, and the cumulative \\n  distribution function (cdf) Fx, s and their corresponding sample statistics, mean , sample standard deviation Sx, and empirical (i.e., observed) cumulative distribution function (cdf), respectively. The major task of statistics is to study the characteristics of populations whether these populations are people, objects, or collections of information. For two major reasons, it is often impossible to study an entire population:   The process would be too expensive or too time-consuming.  The process would be destructive.  In either case, we would resort to looking at a sample chosen from the  population and trying to infer information about the entire population by only examining the smaller sample. Very often the numbers which interest us most about the population are the mean m and standard deviation s. Any number -- like the mean or standard deviation -- which is calculated from an entire population, is called a Parameter. If the very same numbers are derived only from the data of a sample, then the resulting numbers are called Statistics. Frequently, Greek letters represent parameters   and Latin letters represent statistics (as shown in the above Figure).   Statistics is a tool that enables us to impose order on the disorganized  cacophony of the real world of modern society. The business world has grown both in size and competition. Corporate executive must take risk in business, hence the need for business statistics.   Business statistics has grown with the art of constructing charts and tables! It is a science of basing decisions on numerical data in the face of  uncertainty.  Business statistics is a scientific approach to decision making under risk. In practicing business statistics, we search for an insight, not the solution. Our search is for the one solution that meets all the business\\'s needs with  the lowest level of risk. Business statistics can take a normal business situation, and with the proper data gathering, analysis, and re-search for a solution, turn it into an opportunity. While business statistics cannot replace the knowledge and experience of  the decision maker, it is a valuable tool that the manager can employ to  assist in the decision making process in order to reduce the inherent risk.  Business Statistics provides justifiable answers to the following concerns  for every consumer and producer:    What is your or your customer\\'s, Expectation of the product/service you sell or that your customer buys? That is, what is a good estimate for m ?   Given the information about your, or your customer\\'s, expectation, what is  the Quality of the product/service you sell or that you customer buys. That is, what is a good estimate for s ?  Given the information about your or your customer\\'s expectation, and the quality of the product/service you sell or you customer buy, how does the product/service  compare with other existing similar types? That is, comparing several m \\'s, and several s \\'s .    Common Statistical Terminology with  Applications Like all profession, also statisticians have their own keywords and phrases to ease a precise communication. However, one must interpret the results of any  decision making in a language that is easy for the decision-maker to understand. Otherwise, he/she does not believe in what you recommend, and therefore does not go into the implementation phase. This lack of communication between statisticians and the managers is the major roadblock for using statistics. Population: A population is any entire collection of people, animals, plants or things on which we may collect data. It is the entire group of  interest, which we wish to describe or about which we wish to draw conclusions. In the above figure the life of the light bulbs manufactured say by GE, is the concerned population.\\n\\nQualitative and Quantitative Variables: Any object or event, which can vary in successive observations either in quantity or quality is called a \"variable.\"  Variables are  classified accordingly as quantitative or qualitative.  A qualitative variable,  unlike a quantitative variable does not vary in magnitude in successive observations. The values of quantitative and qualitative variables are called \"Variates\" and \"Attributes\", respectively.\\n Variable: A characteristic or phenomenon, which may take different values, such as weight, gender since they are different from individual to individual.\\nRandomness: Randomness means unpredictability.  The fascinating fact about inferential statistics is that, although each random observation may not be predictable when taken alone, collectively they follow a predictable pattern called its distribution function.  For example, it is a fact that the distribution of a sample average follows a normal distribution for sample size over 30.   In other words, an extreme value of the sample mean is less likely than an extreme value of a few raw data.Sample: A subset of a population or universe.An Experiment: An experiment is a process  whose outcome is not known in advance with certainty.  Statistical Experiment: An experiment in general is an operation in which one chooses the values of some variables  and measures the values of other variables, as in physics.  A statistical  experiment, in contrast is an operation in which one take a random sample from a population and infers the values of some variables. For example, in a survey, we \"survey\" i.e. \"look at\" the situation without aiming to change it, such as in a survey of political opinions. A random sample from the relevant population provides   information about the voting intentions.   In order to make any generalization about a population, a random sample from the entire population; that is meant to be representative of the  population, is often studied. For each population, there are many possible samples. A sample statistic gives information about a corresponding population parameter. For example, the sample mean for a set of data would give information about the overall population mean m . It is important that the investigator carefully and completely defines the population before collecting the sample, including a description of the   members to be included.  Example: The population for a study of infant health might be all  children born in the U.S.A. in the 1980\\'s. The sample might be all babies born on 7th of May in any of the years.  An experiment is any process or study which results in the collection of data, the outcome of which is unknown. In statistics, the term is usually restricted to situations in which the researcher has control over some of the conditions under which the experiment takes place. Example: Before introducing a new drug treatment to reduce high blood pressure, the manufacturer carries out an experiment to compare the effectiveness of the new drug with that of one currently prescribed. Newly diagnosed subjects are recruited from a group of local general practices. Half of them are chosen at random to receive the new drug, the remainder receives the present one. So, the researcher has control over the subjects recruited and the way in which they are allocated to treatment. Design of experiments is a key tool for increasing the rate of acquiring new knowledge. Knowledge in turn can be used to gain competitive advantage, shorten the product development cycle, and produce new products and processes which will meet and exceed your customer\\'s expectations.  Primary data and Secondary data sets: If the data are from a planned experiment relevant to the objective(s) of the statistical investigation, collected by the analyst, it is called a Primary Data set. However, if some condensed records are given to the analyst, it is called a Secondary Data set.    \\nRandom Variable: A random variable is a real function (yes, it is called \" variable\", but in reality it is a  function) that assigns a numerical value to each simple event. For example, in sampling for quality control an item could be defective or non-defective, therefore, one may assign X=1, and X = 0 for a defective and non-defective item, respectively. You may assign any other two distinct real numbers, as you wish; however, non-negative integer random variables are easy to work with.  Random variables are needed since one cannot do arithmetic operations on words; the random variable enables us to compute statistics, such as average and variance. Any random variable has a distribution of probabilities associated with it.\\n\\nProbability: Probability (i.e., probing for the unknown) is the tool used for anticipating what the distribution of data should look like under a given model. Random phenomena   are not haphazard: they display an order that emerges only in the long run and is described by a distribution. The mathematical description of variation is central to statistics. The   probability required for statistical  inference is not primarily axiomatic or combinatorial, but is oriented  toward describing data distributions. \\nSampling Unit: A unit is a person, animal, plant or thing which is actually studied by a researcher; the basic objects upon which the study or experiment is executed. For example, a  person; a sample of soil; a pot of seedlings; a zip code area; a   doctor\\'s practice.  Parameter: A parameter is an unknown \\n  value, and therefore it has to be estimated. Parameters are used to represent a certain population characteristic. For example, the population mean m is a parameter that is often used to indicate the average  value of a quantity.   Within a population, a parameter is a fixed value that does not vary. Each  sample drawn from the population has its own value of any statistic that is   used to estimate this parameter. For example, the mean of the data in a sample  is used to give information about the overall mean min the population from which that sample was drawn.  Statistic: A statistic is a quantity \\n  that is calculated from a sample of data. It is used to give information about unknown values in the  corresponding population. For example, the average of   the data in a sample is used to give information about the overall average in  the population from which that sample was drawn.   A statistic is a function of an observable random sample. It is therefore an observable random variable. Notice that, while a statistic is a \"function\" of observations, \\n  unfortunately, it is commonly called a random \"variable\" not a function.  It is possible to draw more than one sample from the same population, and the value of a statistic will in general vary from sample to sample. For example, the average value in a sample is a statistic. The average values in more than one sample, drawn from the same population, will not necessarily be equal.  Statistics are often assigned Roman letters (e.g.  and s), whereas the equivalent unknown values in the population (parameters ) are assigned Greek letters (e.g., µ, s).  The word estimate means to esteem, that is giving a value to something. A statistical estimate is an indication of the value of an unknown quantity   based on observed data. More formally, an estimate is the particular value of an estimator that is  obtained from a particular sample of data and used to indicate the value of a  parameter.   Example: Suppose the manager of a shop wanted to know m , the mean expenditure of customers in her shop in the last year. She could calculate the average expenditure of the hundreds (or perhaps thousands) of customers who bought goods in her shop; that is, the population mean m . Instead she could use an estimate of this population mean m by calculating the mean of a representative sample of customers. If this value were found to be $25, then $25 would be her estimate. There are two broad subdivisions of statistics: Descriptive Statistics and Inferential Statistics as described below. Descriptive Statistics: The numerical statistical data should be presented clearly, concisely, and in such a way  that the decision maker can quickly obtain the essential characteristics of  the data in order to incorporate them into decision process. The principal descriptive quantity derived from sample data is the mean (), which is the arithmetic  average of the sample data. It serves as the most reliable single measure of   the value of a typical member of the sample. If the sample contains a few values that are so large or so small that they have an exaggerated effect on  the value of the mean, the sample is more accurately represented by the median -- the value where half the sample values fall below and half above. The quantities most commonly used to measure the dispersion of the values about their mean are the variance s2 and its square root , the standard deviation s. The variance is calculated by determining the mean, subtracting it from each of the sample values (yielding the deviation of the samples), and then averaging the squares of these deviations. The mean and standard deviation of the sample are used as estimates of the corresponding characteristics of the entire group from which the sample was drawn. They do not, in general, completely describe the distribution (Fx) of values within either the sample or the parent group; indeed, different distributions may have the same mean and standard deviation. They do, however, provide a   complete description of the normal distribution, in which positive and negative deviations from the mean are equally common, and small deviations are much more common than large ones. For a normally distributed set of values, a graph showing the dependence of the frequency of the deviations upon their magnitudes is a bell-shaped curve. About 68 percent of the values will differ from the mean by less than the standard deviation, and almost 100 percent will differ by less than three times the standard deviation.  Inferential Statistics: Inferential \\n  statistics is concerned with making inferences from samples about the populations from which they have been drawn. In other words, if we find a difference between two samples, we would like to know, is this a \"real\" \\n  difference (i.e., is it present in the population) or just a \"chance\" difference (i.e. it could just be the result of random sampling error). That\\'s what tests of statistical significance are all about. Any inferred conclusion \\n  from a sample data to the population from which the sample is drawn must be expressed in a probabilistic term. Probability is the language and a measuring tool for uncertainty in our statistical \\n  conclusions.\\nInferential statistics could be used for explaining a phenomenon or checking for validity of a claim.  In these instances, inferential statistics is called Exploratory Data Analysis or Confirmatory Data Analysis, respectively.Statistical Inference: Statistical inference refers to extending your knowledge obtained from a random sample from the entire population to the whole population. This is known in  mathematics as Inductive Reasoning, that is, knowledge of the whole from a particular. Its main application is in hypotheses testing about a given population. Statistical inference guides the \\n  selection of appropriate statistical models. Models and data interact in statistical work.  Inference from data can be thought of as the process of selecting a reasonable model, including a statement in probability language of how confident one can be about the selection.  Normal Distribution Condition: The normal or Gaussian distribution is a continuous symmetric distribution that follows the familiar bell-shaped curve. One of its nice features is that, the mean and variance uniquely and independently determines the distribution. It has been noted empirically that many measurement variables have distributions that are at least approximately normal. Even when a distribution is  non-normal, the distribution of the mean of many independent observations from the same distribution becomes arbitrarily close to a normal distribution, as  the number of observations grows large. Many frequently used statistical tests   make the condition that the data come from a normal distribution.  Estimation and Hypothesis Testing:Inference in statistics are of two types. The first is estimation, which involves the determination, with a possible error due to sampling, of the unknown value of a population characteristic, such as the proportion having a specific attribute or the average value m of some numerical measurement. To express the accuracy of the estimates of population characteristics, one must also compute the standard errors of the estimates.  The second type of inference is hypothesis testing. It involves the definitions of a hypothesis as one set of possible population values and an alternative, a different set. There are many statistical procedures for determining, on the basis of a sample, whether the true population characteristic belongs to the set of values in the hypothesis or the alternative.  Statistical inference is grounded in probability, idealized concepts of the group under study, called the population, and the sample. The statistician may view the population as a set of balls from which the sample is selected at random, that is, in such a way that each ball has the same chance as every other one for inclusion in the  sample. Notice that to be able to estimate  the population parameters, the sample size n must be greater than one. For example, with a sample size of one, the variation (s2) within the sample is 0/1 = 0. An estimate for the variation (s2) within the population would be 0/0, which is  indeterminate quantity, meaning impossible.    \\n  Greek Letters Commonly Used as Statistical NotationsWe use Greek letters as scientific notations in statistics and other scientific fields  to honor the ancient Greek philosophers who invented science and scientific thinking. Before Socrates, in 6th \\n    Century BC, Thales and Pythagoras, amomg others, applied geometrical concepts to arithmetic, and Socrates is the inventor of dialectic reasoning. The revival of scientific thinking (initiated by Newton\\'s work) was valued and hence reappeared almost 2000 years later. \\nGreek Letters Commonly Used as Statistical Notationsalphabetaki-sqredelta munupirhosigmatautheta\\na\\nbc 2dm\\nnp\\nr\\nst\\nq\\nNote: ki-square (ki-sqre, Chi-square), c\\n2, is not the square of anything, its name implies Chi-square (read, ki-square). Ki does not exist in statistics.\\nI\\'m glad that you\\'re overcoming all the confusions that exist in learning statistics.  Type of Data and Levels of MeasurementInformation can be collected in statistics using qualitative or quantitative data. Qualitative data, such as eye color of a group of individuals, is not computable by arithmetic relations. They are labels that advise in which category or class an individual, object, or process fall. They are called categorical variables. Quantitative data sets consist of measures that take numerical values for which descriptions such as means and standard deviations are meaningful. They can be put into an order and further divided into two groups: discrete data or continuous data. Discrete data are countable data and are collected by counting, for example, the number of defective items produced during a day\\'s production. Continuous data are collected by measuring and are expressed on a continuous scale. For example, measuring the height of a person. Among the first activities in statistical analysis is to count or measure: Counting/measurement theory is concerned with the connection between data and reality. A set of data is a representation (i.e., a model) of the reality based on numerical and measurable scales. Data are called \"primary type\" data if the analyst has been involved in collecting the data relevant to his/her investigation. Otherwise, it is called \"secondary type\" data. Data come in the forms of Nominal, Ordinal, Interval, and Ratio (remember the French word NOIR for the color black).   Data can be either continuous or discrete.  \\nLevels of Measurements_________________________________________NominalOrdinalInterval/Ratio\\nRanking?noyesyes\\nNumerical differencenonoyes Both the zero point and the units of measurement are arbitrary on the Interval scale. While the unit of measurement is arbitrary on the Ratio scale, its zero point is a natural attribute. The categorical variable is measured on an ordinal or nominal scale. \\nCounting/measurement theory is concerned with the connection between data and reality. Both statistical theory and counting/measurement theory are necessary to make inferences about reality. Since statisticians live for precision, they prefer Interval/Ratio levels of measurement.\\n\\nFor a good business application of discrete random variables, visit Markov Chain  Calculator and  Large Markov Chain Calculator.\\n Why Statistical Sampling?  Sampling is the selection of part of an aggregate or totality known as population, on the basis of which a decision concerning the population is made. The following are the advantages and/or necessities for sampling in statistical decision making: Cost: Cost is one of the main arguments in favor of sampling, because often a sample can furnish data of sufficient accuracy and at much lower cost than a census. Accuracy: Much better control over data collection errors is possible with sampling than with a census, because a sample is a smaller-scale undertaking. \\nTimeliness: Another advantage of a sample over a census is that the sample produces information faster. This is important for timely decision making.  Amount of Information: More detailed information can be obtained from a sample survey than from a census, because it take less time, is less costly, and allows us to take more care in the  data \\nprocessing stage. Destructive Tests: When a test involves the \\n destruction of an item under study, sampling must be used. Statistical sampling determination can be used to find the optimal sample size within an acceptable cost. Further Reading:\\nThompson S., Sampling, Wiley,  2002. \\nSampling MethodsFrom the food you eat to the television you watch, from political elections to school board actions, much of your life is regulated by the results of sample surveys. A sample is a group of units selected from a larger group (the population). By studying the sample, one hopes to draw valid conclusions about the larger group. A sample is generally selected for study because the population is too large to study in its entirety. The sample should be representative of the general population. This is often best achieved by random sampling. Also, before collecting the sample, it is important that one carefully and completely defines the population, including a description of the members to be included. A common problem in business statistical decision-making arises when we need information about a collection called a population but find that the cost of obtaining the information is prohibitive. For instance, suppose we need to know the average shelf life of current inventory. If the inventory is large, the cost of checking records for each item might be high enough to cancel the benefit of having the information. On the other hand, a hunch about the average shelf life might not be good enough for decision-making purposes. This means we must arrive at a compromise that involves selecting a small number of items and calculating an average shelf life as an estimate of the average shelf life of all items in inventory. This is a compromise, since the measurements for a sample from the inventory will produce only an estimate of the value we want, but at substantial savings. What we would like to know is how \"good\" the estimate is and how much more will it cost to make it \"better\". Information of this type is intimately related to sampling techniques. This section provides a short discussion on the common methods of business statistical sampling. Cluster sampling can be used whenever the population is homogeneous but can be partitioned. In many applications the partitioning is a result of physical distance. For instance, in the insurance industry, there are small \"clusters\" of employees in field offices scattered about the country. In such a case, a random sampling of employee work habits might not required travel to many of the \"clusters\" or field offices in order to get the data. Totally sampling each one of a small number of clusters chosen at random can eliminate much of the cost associated with the data requirements of management. Stratified sampling can be used whenever the population can be partitioned into smaller sub-populations, each of which is homogeneous according to the particular characteristic of interest. If there are k sub-populations and we let Ni denote the size of sub-population i, let N denote the overall population size,  and let n denote the sample size, then we select a stratified sample whenever  we choose: \\nni = n(Ni/N) \\nitems at random from sub-population i, i = 1, 2, . . . . , k. \\nThe estimates is: s = S Wt. t, over t = 1, 2, ..L (strata), and t is SXit/nt. Its variance is: \\nSW2t /(Nt-nt)S2t/[nt(Nt-1)] Population total T is estimated by N. s;  its variance is SN2t(Nt-nt)S2t/[nt(Nt-1)]. Random sampling is probably the most popular sampling method used in decision making today. Many decisions are made, for instance, by choosing a number out of a hat or a numbered bead from a barrel, and both of these methods are attempts to achieve a random choice from a set of items. But true random sampling must be achieved with the aid of a computer or a random number table whose values are generated \\nby computer random number generators. A random sampling of size n is drawn from a population size N.  The unbiased estimate for  variance of  is:\\n\\n\\n Var() = S2(1-n/N)/n, \\n\\n\\nwhere n/N is the sampling fraction. For sampling fraction less than 10% the finite population correction factor (N-n)/(N-1) is almost 1. \\n    The total T is estimated by N ´ , its variance is N2Var().  For 0, 1, (binary) type variables, variation in estimated proportion p  is:  S2 = p(1-p) ´ (1-n/N)/(n-1).\\nFor ratio r = Sxi/Syi= / , the variation for r is: [(N-n)(r2S2x + S2y -2 r Cov(x, y)]/[n(N-1)2]. Determination of sample sizes (n) with regard to binary data: Smallest integer greater than or equal to:\\n \\n[t2 N p(1-p)] / [t2 p(1-p) + a2 (N-1)],\\n\\nwith N being the size of the total number of cases, n being the sample size, a the expected error, t being the value taken  from the t-distribution corresponding to a certain confidence interval, and p being the probability of an event. Cross-Sectional Sampling:Cross-Sectional study the observation of a defined population at a single point in time or time interval. Exposure and outcome are determined simultaneously. \\n\\nWhat is a statistical instrument?  A statistical instrument is any process\\nthat aim at describing a phenomena by using any instrument or device, however the results may be used as a control tool.  Examples of statistical instruments are questionnaire and surveys sampling.\\n\\nWhat is grab sampling technique? The grab sampling technique is to take a relatively small sample over a very short period of time, the result obtained are usually instantaneous.  However, the Passive Sampling is a technique where a sampling device is used for an extended time under similar conditions. Depending on the desirable statistical investigation, the passive sampling may\\nbe a useful alternative or even more appropriate than grab sampling.  However, a passive sampling technique needs to be developed and tested in the field.\\n   Further Reading:Thompson S., Sampling, Wiley, 2002.  Statistical Summaries Representative of a Sample: Measures of Central Tendency SummariesHow do you describe the \"average\" or \"typical\" piece of information in a set of data? Different procedures are used to summarize the most representative information depending of the type of question asked and the nature of the data being summarized.  Measures of location give information about the location of the central tendency within a group of numbers. The measures of location presented in this unit for ungrouped (raw) data are the mean, the median, and the mode.  Mean: The arithmetic mean (or the average, simple mean) is computed by summing all numbers in an array of numbers (xi) and then dividing by the number of observations (n) in the array. Mean =  = S Xi /n, \\xa0 \\xa0 the sum is over all i\\'s. \\nThe mean uses all of the observations, and each observation affects the mean. Even though the mean is sensitive to extreme values; i.e., extremely large or small data can cause the mean to be pulled toward the extreme data; it is still the most widely used measure of location. This is due to the fact that the mean has valuable mathematical properties that make it convenient for use with inferential statistical analysis. For example, the sum of the deviations of the numbers in a set of data from the mean is zero, and the sum of the squared deviations of the numbers in a set of data from the mean is the minimum value.  You might like to use Descriptive Statistics  Applet to compute the mean. Weighted Mean: In some cases, the data in the sample or population should not be weighted equally, rather each value should be weighted according to its importance. Median: The median is the middle value in an ordered array of observations. If there is an even number of observations in the array, the median is the average of the two middle numbers. If there is an odd number of data in the array, the median is the  middle number. The median is often used to summarize the distribution of an outcome. If the distribution is skewed, the median and the interquartile range (IQR) may be better than other measures to indicate where the observed data are concentrated. Generally, the median provides a better measure of location than the mean when there are some extremely large or small observations; i.e., when the data are skewed to the right or to the left. For this reason, median income is used as the measure of location for the U.S. household income. Note that if the median is less than the mean, the data set is skewed to the right. If the median is greater than the mean, the data set is skewed to the left.  For normal population, the sample median is distributed normally with m = the mean, and standard error of the median (p/2)½ times standard error of the mean. The mean has two distinct advantages over the median. It is more stable, and one can compute the mean based of two samples by combining the two means.Mode: The mode is the most frequently occurring value in a set of observations. Why use the mode? The classic example is the shirt/shoe manufacturer who wants to decide what sizes to  introduce. Data may have two modes. In this case, we say the data are bimodal, and sets of observations with more than two modes are referred to as multimodal.  Note that the mode is not a helpful measure of location, because there can be more than one mode or even no mode.  When the mean and the median are known, it is possible to estimate the mode for the unimodal distribution using the other two averages as follows:Mode » 3(median) - 2(mean)This estimate is applicable to both grouped and ungrouped data sets.Whenever, more than one mode exist, then the population from which the  sample came is a mixture of more than one population. However, notice that a Uniform distribution has uncountable number of modes having equal density value; therefore it is considered as a homogeneous population. \\n\\nAlmost all standard statistical analyses are conditioned on the assumption that the population is homogeneous.Notice that Excel has  very limited statistical capability. For example, it displays only one mode, the first one. Unfortunately, this is very misleading. However, you may find out if  there are others by inspection only, as follow: Create a frequency distribution, invoke the menu sequence: Tools, Data analysis, Frequency and follow instructions on the screen. You will see the frequency distribution and then find the  mode visually. Unfortunately, Excel does not draw a Stem and Leaf diagram.  All commercial off-the-shelf software, such as  SAS and  SPSS, display a Stem and Leaf diagram, which is a frequency distribution of a given data set. Selecting Among the Mode, Median, and MeanIt is a common mistake to specify the wrong index for central tenancy.  The first consideration is the type of data, if the variable is categorical,  the mode is the single measure that best describes that data. The second consideration in selecting the index is to ask whether the total  of all observations is of any interest. If the answer is yes, then the mean  is the proper index of central tendency. If the total is of no interest, then depending on whether the histogram is symmetric or skewed one must use either mean or median, respectively. In all cases the histogram must be unimodal. However, notice that, e.g.,  a Uniform distribution has uncountable number of modes having equal density value; therefore it is considered as a homogeneous population. Notice also that: |Mean - Median| £s\\nThe main characteristics of these three statistics are tabulated below:  \\n The Main Characteristics of the Mode, the Median, and the Mean  Fact No.The ModeThe MedianThe Mean   1It is the most frequent value in the distribution; it is the point of greatest density. It is the value of the middle point of the array (not midpoint of range), such that half the item are above and half below it.  It is the value in a given aggregate which would obtain if all the values  were equal.  2 \\n      The value of the mode is established by the predominant frequency, not by the value in the distribution.  The value of the media is fixed by its position in the array and doesn\\'t reflect the individual value.  The sum of deviations on either side of the mean are equal; hence, the algebraic sum of the deviation is equal zero.   3 It is the most probable value, hence the most typical. The aggregate distance between the median point and all the value in the array is less than from any other point. \\n      It reflect the magnitude of every value.  4 A distribution may have 2 or more modes. On the other hand, there is no mode in a rectangular distribution. Each array has one and only one median. \\n      An array has one and only one mean.  5 The mode does nott reflect the degree of modality. It cannot be manipulated algebraically: medians of subgroups cannot be weighted and combined.  Means may be manipulated algebraically: means of subgroups may be combined \\n  when properly weighted.   6 It cannot be manipulated algebraically: modes of subgroups cannot be  combined.  It is stable in that grouping procedures do not affect it appreciably.  It may be calculated even when individual values are unknown, provided the sum of the values and the sample size n are known.  7 It is unstable that it is influenced by grouping procedures. Value must be ordered, and may be grouped, for computation. Values need not be ordered or grouped for this calculation.  8 Values must be ordered and group for its computation. It can be compute when ends are open  It cannot be calculated from a frequency table when ends are open. \\n9  It can be calculated when table ends are open.  It is not applicable to qualitative data. \\n      It is stable in that grouping procedures do not seriously affected it. \\n \\n\\nThe Descriptive Statistics  JavaScript provides a complete set of information about all statistics that you ever need.\\nYou might like to use it to perform some numerical experimentation for validating the above assertions for a deeper understanding.\\n\\n Specialized Averages: The Geometric & Harmonic Means\\nThe Geometric Mean: The geometric mean (G) of n non-negative numerical values is the nth root of the product of the n values. \\nIf some values are very large in magnitude and others are small, then the geometric mean is a better representative of the data than the simple average.  In a \"geometric series\", the most meaningful average is the geometric mean (G). The arithmetic mean is very biased toward the larger numbers in the series.\\n\\nAn Application:  Suppose sales of a certain item increase to 110% in the first year and to 150% of that in the second year.  For simplicity, assume you sold 100 items initially.  Then the number sold in the first year is 110 and the number sold in the second is 150% x 110 = 165.  The arithmetic average of 110% and 150% is 130% so that we would incorrectly estimate that the number sold in the first year is 130 and the number in the second year is 169.  The geometric mean of 110% and 150% is G = (1.65)1/2 so that we would correctly estimate that we would sell 100 (G)2 = 165 items in the second year.The Harmonic Mean:The harmonic mean (H) is another specialized average, which is useful in averaging variables expressed as rate per unit of time, such as mileage per hour, number of units produced per day.  The harmonic mean (H) of n non-zero numerical values x(i) is:  H = n/[S (1/x(i)].\\n\\nAn Application:  Suppose 4 machines in a machine shop are used to produce the same part.  However, each of the four machines takes 2.5, 2.0, 1.5, and 6.0 minutes to make one part, respectively. What is the average rate of speed?\\n\\nThe harmonic means is:  H = 4/[(1/2.5) + (1/2.0) + 1/(1.5) + (1/6.0)] = 2.31 minutes.\\n\\nIf all machines working for one hour, how many parts will be produced?  Since four machines running for one hour represent 240 minutes of operating time, then: 240 / 2.31 = 104 parts will be produced. \\n\\nThe Order Among the Three Means: If all the three means exist, then the Arithmetic Mean is never less than the other two, moreover, the Harmonic Mean is never larger than the other two.\\n\\nYou might like to use  The Other Means JavaScript applet in performing some numerical experimentation for validating the above assertions for a deeper understanding.  \\n\\n Further Reading:\\n\\nLangley R., Practical Statistics Simply Explained, 1970, Dover Press.\\n\\n\\n Histogramming: Checking for Homogeneity of Population\\n  A histogram is a graphical presentation of an estimate for the density (for continuous random variables) or probability mass function (for discrete random variables) \\n  of the population.  The geometric feature of histogram enables us to find out useful information about the data, such as:  The location of the \"center\" of the data. The degree of dispersion. The extend to which its is skewed, that is, it does not fall off systemically on both side of its peak. The degree of peakedness. How steeply it rises and falls. The mode is the most frequently occurring value in a set of observations. Data may have two modes. In this case, we say the data are bimodal, and sets of observations with more than two modes are referred to as multimodal. Whenever, more than one mode exist, then the population from which the sample came is a mixture of more than one population.  Almost all standard statistical analyses are conditioned on the assumption that the population is homogeneous, meaning that its density (for continuous random variables) or probability mass function (for discrete random variables) is unimodal. However, notice that, e.g.,  a Uniform distribution has uncountable number of modes having equal density value; therefore it is considered as a homogeneous population.\\n\\nTo check the unimodality of sampling data, one may use the histogramming  process. Number of Class Intervals in a Histogram: Before \\n we can construct our frequency distribution we must determine how many classes  we should use. This is purely arbitrary, but too few classes or too many classes  will not provide as clear a picture as can be obtained with some more nearly optimum number. An empirical (i.e., observed) relationship, known as Sturge\\'s rule, may be used as a useful guide to determine the optimal number of classes (k) is given by  k = the smallest integer greater than or equal to 1 + 3.332 Log(n) where k is the number of classes, Log is in base 10, and n is the total number of the numerical values which comprise the data set.  Therefore, class width is:   (highest value - lowest value) / (1 + 3.332 Logn)where n is the total number of items in the data set. The following JavaScript produces a histogram based on this rule:Test for Homogeneity of a Population. To have an \"optimum\" you need some measure of quality -- presumably in this case, the \"best\" way to display whatever information is available in the data.  The sample size contributes to this; so the usual guidelines are to use between 5 and 15 classes, with more classes, if you have a larger sample. You should take into account a preference for tidy class widths, preferably a multiple of 5 or 10, because this makes it easier to understand.  Beyond this it becomes a matter of judgement. Try out a range of class widths,  and choose the one that works best. This assumes you have a computer and can generate alternative histograms fairly readily. There are often management issues that come into play as well. For example,  if your data is to be compared to similar data -- such as prior studies, or from other countries -- you are restricted to the intervals used therein. If the histogram is very skewed, then unequal classes should be considered.  Use narrow classes where the class frequencies are high, wide classes where they are low. \\n  The following approaches are common:  Let n be the sample size, then the number of class intervals could be Min {n½, 10 Log(n) }.\\nThe Log is the logarithm in base 10. Thus for 200 observations you would use 14 intervals but for 2000 you would use 33. Alternatively,  Find the range (highest value - lowest value). Divide the range by a reasonable interval size: 2, 3, 5, 10 or a multiple of 10.  Aim for no fewer than 5 intervals and no more than 15.  One of the main applications of histogramming is to Test for Homogeneity of a Population. The unimodality of the histogram is a necessary condition for the homogeneity of population to make any statistical analysis meaningful. However, notice that, e.g.,  a Uniform distribution has uncountable number of modes having equal density value; therefore it is considered as a homogeneous population.Further Reading:\\nEfron B., and R. Tibshirani, An Introduction to the Bootstrap, Chapman & Hall (now the CRC Press), 1994. Contains a tedious test for multimodality that is based on the Gaussian kernel density estimates and then test for multimodality by using the window-size approach.   How to Construct a BoxPlotA BoxPlot is a graphical display that has many characteristics.  It includes the presence of possible outliers. It illustrates the range of data. It shows a measure of dispersion such as the upper quartile, lower quartile and interquartile range (IQR) of the data set as well as the median as a measure of central location, which is useful for comparing sets of data. It also gives an indication of the symmetry or skewness of the distribution. The main reason for the popularity of boxplots is that they offer much of information in a compact way.    Steps to Construct a BoxPlot:  Horizontal lines are drawn at the smallest observation (A), lower quartile. And another  from the upper quartile (D), and the largest observation (E).  Vertical lines to produce the box join these horizontal lines at points (B, and D).  A vertical line is drawn at the median point (C), as shown on the above Figure. \\n\\n\\nFor a deeper understanding, you may like using graph paper, and  Descriptive Sampling Statistics Applet in constructing the BoxPlots for some sets of data; e.g.,  from your textbook. \\n Measuring the Quality of a Sample  Average by itself is not a good indication of quality. You need to know the \\n  variance to make any educated assessment. We are reminded of the dilemma of the six-foot tall statistician who drowned in a stream that had an average depth of three feet. Statistical measures are often used for describing the nature and extent of differences among the information in the distribution. A measure of variability is generally reported together with a measure of central tendency. Statistical measures of variation are numerical values that indicate the variability inherent in a set of data measurements. Note that a small \\n value for a measure of dispersion indicates that the data are concentrated around the mean; therefore, the mean is a good representative of the data set. On the other hand, a large measure of dispersion indicates that the mean is not a good representative of the data set. Also, measures of dispersion can be used when we want to compare the distributions of two or more sets of data. Quality of a data set is measured by its variability: Larger variability indicates lower quality. That is why high variation makes the manager very worried. Your job, as a statistician, is to measure the variation, and if it is too high and unacceptable, then it is the job of the technical staff, such as engineers, to fix the process. Decision situations with complete lack of knowledge, known as the flat uncertainty, have the largest risk. For simplicity, consider the case when there are only two outcomes, one with probability of p. Then, the variation in the outcomes  is p(1-p). This variation is the largest if we set p = 50%. That is, equal chance for each outcome. In such a case, the quality of information is at  its lowest level. Remember, quality of information and variation are inversely related. The larger the variation in the data, the lower  the quality of the data (i.e., information): the Devil is in the Deviations. The four most common measures of variation are the range, variance, standard deviation, and coefficient of variation.Range: The range of a set of observations is the absolute value of the difference between the largest and smallest values in the data set. It measures the size of the smallest contiguous interval  of real numbers that encompasses all of the data values. It is not useful when extreme values are present. It is based solely on two values, not on the entire data set. In addition, it cannot be defined for open-ended distributions such as Normal distribution. Notice that, when dealing with discrete random observations, some authors define the range as: Range = Largest value - Smallest value +  1. \\n  A normal distribution does not have a range. A student said, \"since the tails of a normal density function never touch the x-axis and since for an observation to contribute to forming such a curve, very large positive  and negative values must exist\" Yet such remote values are always possible, but increasingly improbable. This encapsulates the asymptotic behavior of normal density very well.  Therefore, in spite of this behavior, it is useful and applicable to a wide range of decision-making situations. Quartiles: When we order the data, for example in ascending order, we may divide the data into quarters, Q1\\x85Q4, known as quartiles. The first Quartile (Q1) is that value where 25%  of the values are smaller and 75% are larger. The second Quartile (Q2) is that value where 50% of the values are smaller and 50% are larger. The third Quartile (Q3) is that value where 75% of the values are smaller and 25% are larger.  Percentiles: Percentiles have a similar concept and therefore, are related; e.g., the  25th percentile corresponds to the first quartile Q1, etc. The advantage of percentiles is that they may be subdivided into 100 parts. The percentiles and quartiles are most conveniently read from a cumulative distribution function.  \\n Interquartiles Range: The interquartile range (IQR) describes the extent for which the middle 50% of the observations scattered or dispersed. It is the distance between the first and the third quartiles:   IQR = Q3 - Q1, \\nwhich is twice the Quartile Deviation. For data that are skewed, the relative dispersion, similar to the coefficient of variation (C.V.) is given (provided the \\n    denominator is not zero) by the Coefficient of Quartile  Variation: \\n   CQV = (Q3-Q1) / (Q3 + Q1).  Note that almost all statistics that  we have covered up to now can be obtained and understood deeply by graphical method using Empirical (i.e., observed) Cumulative Distribution Function (ECDF) JavaScript. However, the numerical Descriptive Statistics applet provides a complete set of information about all statistics that you ever need. \\n\\nThe Duality between the ECDF and the Histogram: Notice that the empirical (i.e., observed) cumulative distribution function (ECDF) indicates by its height at a particular pointthat  is numerically equal to the area in the corresponding histogram to the left of that point.  Therefore, either or both could be used depending on the intended applications.\\nMean Absolute Deviation (MAD):\\nA simple measure of variability is the mean absolute deviation:\\nMAD = S |(xi -  )| / n. \\n\\n The mean absolute deviation is widely used as a performance measure to assess the quality of the modeling,  such forecasting techniques.  However, MAD does not lend itself to further use in making inference;  moreover, even in the error analysis studies, the variance is preferred since variances of independent (i.e., uncorrelated) errors are additive; however MAD does not have such a nice feature.\\n\\nThe MAD is a simple measure of variability, which unlike range and quartile deviation, takes every item into account, and it is simpler and less affected by extreme deviations.  It is therefore often used in small samples that include extreme values.\\nThe mean absolute deviation theoretically should be measured from the median, since it is at its minimum;  however, it is more convenient to measure the deviations from the mean.   As a numerical example, consider the price (in $) of same item at 5 different stores:  $4.75, $5.00, $4.65, $6.10, and $6.30.  The mean absolute deviation from the mean is $0.67, while from the median is $0.60, which is a better representative of deviation among the prices.\\n\\nVariance: An important measure  of variability is variance. Variance is the average of the squared deviations of each observation in the set from the arithmetic mean of all of the observations. \\nVariance = S (xi -  ) 2 \\n  / (n - 1), \\xa0 \\xa0 where n is at least 2. The variance is a measure of spread or dispersion among values in a data set. Therefore, the greater the variance, the lower the quality.   The variance is not expressed in the same units as the observations.  In other words, the variance is hard to understand because the deviations from the mean are squared, making it too large for logical explanation. This  problem can be solved by working with the square root of the variance,  which is called the standard deviation. Standard Deviation: Both variance and standard deviation provide the same information; one can always be obtained from the other. In other words, the process of computing a standard deviation always involves computing a variance. Since standard deviation is the square root of the variance, it is always expressed in the \\n    same units as the raw data: Standard Deviation = S = (Variance) ½For large data sets (say, more than 30), approximately 68% of the data are contained within one standard deviation of the mean, 95% fall within two standard deviations.  97.7% (or almost 100% ) of the data are contained within within three standard deviations (S) from the mean. You may use Descriptive Statistics Applet to compute the mean, and standard deviation.  \\nThe Mean Square Error (MSE) of an estimate is the variance of the estimate plus the square of its bias; therefore, if an estimate is unbiased, then its MSE is equal to its variance, as it is the case in the ANOVA table. \\n  Coefficient of Variation: Coefficient of Variation (CV) is the absolute relative deviation with respect to size , provided  is not zero, expressed in percentage:  CV =100 |S/| % \\nCV is independent of the unit of measurement. In estimation of a parameter, when its CV is less than 10%, the estimate is assumed acceptable. The inverse of CV; namely, 1/CV is called the Signal-to-noise Ratio. The coefficient of variation is used to represent the relationship of the standard deviation to the mean, telling how representative the mean is of the numbers from which it came. It expresses the standard deviation as a percentage of the mean; i.e., it reflects the variation in a distribution relative to the mean. However, confidence intervals for the coefficient of variation are rarely reported. One of the reasons is that the exact confidence interval for the coefficient of variation is computationally tedious.\\n\\nNote that, for a skewed or grouped data set, the coefficient of quartile variation:\\n\\n\\nVQ = 100(Q3 - Q1)/(Q3 + Q1)%\\n\\n\\nis more useful than the CV.\\n You may use Descriptive Statistics Applet to compute the mean, standard deviation and the coefficient of variation.\\nVariation Ratio for Qualitative Data: Since the mode is the most frequently used measure of central tendency for qualitative variables, variability is measured with reference to the mode. The statistic that describes the variability of quantitative data is the Variation Ratio (VR):\\n VR = 1 -  fm/n,\\nwhere fm is the frequency of the mode, and n is the total number of scores in the distribution.\\n\\n Z Score: how many standard deviations a given point (i.e., observation) is above or below the mean. In other words, a Z score represents the number of standard deviations that an observation (x) is above or below the mean. The larger the Z value, the further away a value will be from the mean. Note that values beyond three standard deviations are very unlikely. Note that if a Z score is negative, the observation (x) is below the mean. If the Z score is positive, the observation (x) is above the mean. The Z score is found as: \\nZ = (x - ) / standard deviation of X The Z score is a measure of the number of standard deviations that an observation is above or below the mean. Since the standard deviation is never negative, a positive Z score indicates that the observation is above the mean, a negative \\n    Z score indicates that the observation is below the mean. Note that Z is a dimensionless value, and therefore is a useful measure by which to compare data values from two different populations, even those measured by different units. Z-Transformation: Applying the \\n    formula z = (X - m) / s will always produce a transformed variable with a mean of zero and a standard deviation of one. However, the shape of the distribution will not be affected by the  transformation. If X is not normal, then the transformed distribution will \\n    not be normal either.  \\nOne of the nice features of the z-transformation is that the resulting distribution of the transformed data has an identical shape but with mean zero, and standard deviation equal to 1. One can generalize this data transformation to have any desirable mean and standard deviation other than 0 and 1, respectively.    Suppose we wish the transformed data to have the mean and standard deviation of M and D, respectively. For example, in the SAT Scores, they are set at M = 500, and D=100.  The following transformation should be applied:Z = (standard Z) ´ D + M Suppose you have two data sets with very different scales (e.g., one has very low values, another very high values). If you wish to compare these two data sets, due to differences in scales, the statistics that you generate are not comparable. It is a good idea to use the Z-transformation of both original data sets and then make any comparison. You have heard the terms z value, z test, z transformation, \\n    and z score. Do all of these terms mean the same thing? Certainly not: The z value  refers to the critical value (a point on the horizontal axes) of the Normal (0, 1) density function, for a given area to the left of that z-value. The z test   refers  to the procedures for testing the equality of mean (s) of one (or two) population(s).  The z score of a given observation x, in a sample of size n, is simply (x - average of the sample) divided by the standard deviation of the sample. One must be careful not to mistake  z scores for the Standard Scores. The z transformation of a set of observations of size n is simply (each observation - average of all observations) divided by the standard deviation among all observations. The aim is to produce a  transformed data set with a mean of zero and a standard deviation of one.  This makes the transformed set dimensionless and manageable with respect to its magnitudes. It is  used also in comparing several data sets that have been measured using different scales of measurements. Pearson coined the term \"standard deviation\" sometime near 1900.   The idea of using squared deviations goes back to Laplace in the early 1800\\'s. Finally, notice again, that the transforming raw scores to z scores do NOT normalize the data.  Computation of Descriptive Statistics for Grouped Data: One of the most common ways to describe a single variable is with a frequency distribution. A histogram is a graphical presentation of an estimate for the frequency distribution of the population. Depending upon the particular variable, all of the data values may be represented, or you may group the values into categories first (e.g., by age). It would usually not be sensible to determine the frequencies for each value. Rather, the values are grouped into ranges, and the frequency is then determined.). Frequency distributions can be depicted in two ways: as a table or as a graph that is often referred to as a histogram or bar chart. The bar chart is often used to show the relationship between two categorical variables. \\n  Grouped data is derived from raw data, and it consists of frequencies (counts of raw values) tabulated with the classes in which they occur. The Class Limits represent the largest (Upper) and lowest (Lower) values which the class will contain. The formulas for the descriptive statistic becomes much simpler for the grouped data, as shown below for Mean, Variance, Standard Deviation, respectively, where (f) is for the frequency  of each class, and n is the total frequency:   \\n  \\nSelecting Among the Quartile Deviation, Mean Absolute Deviation, and Standard Deviation\\nA general guideline for selecting a suitable statistic in describing the dispersion in a population includes consideration of the following factors:\\n\\n\\nThe concept of dispersion required by the problem. Is a single pair of values adequate, such as the two extremes or the two quartiles (range or Q)?  \\n\\nThe type of data available. If they are few in numbers, or contain extreme value, avoid the standard deviation. If they are generally skewed, avoid the mean absolute deviation as well. If they have a gap around the quartile, the quartile deviation should be avoided.\\n\\nThe peculiarity of the dispersion measures themselves. These are summarized under \"The Main Characteristics of the Quartile Deviation, the Mean Absolute Deviation, and the Standard deviation\" below.\\n\\n \\n The Main Characteristics of the Quartile Deviation, the Mean Absolute Deviation, and the Standard Deviation  Fact No.The Quartile DeviationThe Mean Absolute DeviationThe Standard Deviation 1The quartile deviation is also easy to calculate and to understand. However, it is unreliable if there are gaps in the data around the quartiles.The mean absolute deviation has the advantage of giving equal weight to the deviation of every value form the mean or median.The standard deviation is usually more useful and better adapted to further analysis than the mean absolute deviation. 2It depends on only 2 values, which include the middle half of the items. Therefore, it is a more sensitive measure of dispersion than those described above and ordinarily has a smaller sampling error.  It is more reliable as an estimator of the population dispersion than other measures, provided the distribution is normal. 3 It is usually superior to the range as a rough measure of dispersion. It is also easier to compute and to understand and is less affected by extreme values than the standard deviation.   It is the most widely used measure of dispersion and the easiest to handle algebraically.  4 It may be determined in an open-end distribution, or one in which the data may be ranked but not measured quantitatively.Unfortunately, it is difficult to handle algebraically, since minus signs must be ignored in its computation. Compared with the others, it is harder to compute and more difficult to understand. 5 It also useful in badly skewed distributions or those in which other measures of dispersion would be warped by extreme values. Its main application is in modeling accuracy for comparative forecasting techniques. It is generally affected by extreme values that may be due to skewness of data   \\nYou might like to use the Descriptive Sampling Statistics JavaScript applet in performing some numerical experimentation for validating the above assertions for a deeper understanding. \\n\\nShape of a Distribution Function: The Skewness-Kurtosis ChartThe pair of statistical measures, skewness and kurtosis, are measuring tools, which is used in selecting a distribution(s) to fit your data. To make an inference with respect to the population distribution, you may first compute skewness and kurtosis from your random sample from the entire population. Then, locating a point with these coordinates on the widely used skewness-kurtosis chart ,  guess a couple of possible distributions to fit your data. Finally, you might use the goodness-of-fit test to rigorously come up with the best candidate fitting your data. Removing outliers improves  the accuracy of both skewness and kurtosis.  Skewness: Skewness is a measure of the degree to which the sample population deviates from symmetry with the mean at the center. \\n  Skewness = S (xi -  ) 3  / [ (n - 1) S 3 ],\\xa0 \\xa0 n is at least 2. Skewness will take on a value of zero when the distribution is a symmetrical curve. A positive value indicates the observations are clustered more to the left of the mean with most of the extreme values to the right of the mean. A negative skewness indicates clustering to the right. In this case we have:  Mean £ Median £ Mode. The \\n    reverse order holds for the observations with positive skewness.   Kurtosis: Kurtosis is a measure of the relative peakedness of the curve defined by the distribution of the  observations.  Kurtosis = S (xi -  ) 4 \\n      / [ (n - 1) S 4 ], \\xa0 \\xa0 n is at least 2. \\nStandard normal distribution has kurtosis of +3. A kurtosis larger than 3 indicates the distribution is more peaked than the standard normal distribution.  Coefficient of Excess Kurtosis = Kurtosis - 3.\\n    A value of less than 3 for kurtosis indicates that the distribution is flatter than the standard normal distribution. It can be shown that,  Kurtosis - Skewness 2\\xa0is greater than or equal to 1, and\\xa0 \\xa0\\xa0Kurtosis\\xa0is less than or equal to the sample size n .  These inequalities hold for any probability distribution having finite skewness and kurtosis. In the Skewness-Kurtosis Chart, you notice two useful families of distributions, namely the beta and gamma families.  The Beta-Type Density Function: Since the beta density has both a shape and a scale parameter, it describes many random phenomena provided the random variable is between [0, 1]. For example, when both parameters are integer \\n    with random variables the result is the binomial Probability function. Applications: A basic distribution of statistics for variables bounded at both sides; for example x between [0, 1]. The beta density  is useful for both theoretical and  applied problems in many areas. Examples include distribution of proportion of population located between lowest and highest value in sample; distribution of daily per cent yield in a manufacturing process; description of elapsed  times to task completion (PERT). There is also a relationship between the  Beta and Normal distributions. The conventional calculation is that given a PERT Beta with highest value as b, lowest as a, and most likely as m, the equivalent normal distribution has a mean and mode of (a + 4M + b)/6 and a standard deviation of (b - a)/6.  Comments: Uniform, right triangular, and parabolic distributions are special cases. To generate beta, generate two random values from a gamma, g1,  g2. The ratio g1/(g1 +g2) is distributed like a beta distribution.     The beta distribution can also be thought of as the distribution of X1 given     (X1+X2), when X1 and X2 are independent gamma random variables. \\n Gamma-Type Density Function: Some random variables are always non-negative. The density function associated with these random variables often is adequately modeled as the gamma density function. The Gamma-Type Density Function has both a shape and a scale parameter. With both the shape and scale parameters equal to 1, the result is the exponential density function. Chi-square is also a special case of gamma density function with shape parameter equal to 2. Applications: A basic distribution of statistics for variables bounded at one side ; for example x greater than or equal to zero.  The gamma density gives distribution of time required for exactly k independent events to occur, assuming events take place at a constant rate. Used frequently in queuing theory, reliability, and other industrial applications. Examples include distribution of time between re-calibrations of instrument that needs re-calibration after k uses; time between inventory restocking, time to failure for a system with standby components. Comments: Erlangian, Exponential, and Chi-square distributions are special cases. The negative binomial is an analog to gamma distribution with discrete  random variable. What is the distribution of the product of sample observations  \\nfrom the uniform (0, 1) random? Like  many problems with products, this becomes a familiar problem when turned into  a problem about sums. If X is uniform (for simplicity of notation make it U(0,1)), Y=-log(X) is exponentially distributed, so the log of the product  of X1, X2, ... Xn is the sum of Y1, Y2, ... Yn which has a gamma (scaled Chi-square)  distribution. Thus, it is a gamma density with shape parameter n and scale  1. \\n   The Log-normal Density Function: Permits representation of  a random variable whose logarithm follows a normal distribution. The ratio of two log-normally random variables is also log-normal. Applications: Model for a process arising from many small multiplicative  errors. Appropriate when the value of an observed variable is a random proportion  of the previously observed value. Applications: Examples include distribution of sizes from a breakage process; distribution of income size, inheritances and bank deposits; distribution of various biological phenomena; life distribution of some transistor types. \\n\\nThe lognormal distribution is widely used in situations where values are positively skewed (where the distribution has a long right tail; negatively skewed distributions have a long left tail; a normal distribution has no skewness). Examples of data that \"fit\" a lognormal distribution include financial security valuations or real estate property valuations. Financial analysts have observed that the stock prices are usually positively skewed, rather than normally (symmetrically) distributed. Stock prices exhibit this trend because the stock price cannot fall below the lower limit of zero but may increase to any price without limit. Similarly, healthcare costs illustrate positive skewness since unit costs cannot be negative. For example, there can\\'t be negative cost for services in a capitation contract. This distribution accurately describes most healthcare data.\\nIn the case where the data are log-normally distributed, the Geometric Mean acts as a better data descriptor than the mean. The more closely the data  follow a log-normal distribution, the closer the geometric mean is to the median, since the log re-expression produces a symmetrical distribution.   Further Reading:Snell J., Introduction to Probability,  Random House, 1987. Read section 4.2 for a link between beta and F distributions  (with the advantage that tables are easy to find). Tabachnick B., and L. Fidell, Using Multivariate Statistics, HarperCollins,  1996. Has a good discussion on applications and significance tests for skewness  and kurtosis.\\nNumerical Example and DiscussionsA Numerical Example: Given the following, small (n = 4) data set, compute the descriptive statistics: x1 = 1, x2 = 2, x3 = 3, and x4 = 6. \\n   i          xi ( xi-  )( xi -  ) 2\\n  ( xi -  ) 3  ( xi -  )4    1 1 -2   4  -8 162  2 -1    1   -1 1 3             3    0      0    0     0      4   6     3    9   27  \\n 81Sum   12  014    18    98 The mean  is 12 / 4 = 3; the variance is s2     = 14 / 3 = 4.67;  the standard deviation is s = (14/3) 0.5     = 2.16; the skewness is 18 / [3 (2.16) 3 ]     = 0.5952, and finally, the kurtosis is 98 / [3 (2.16) 4] = 1.5.  You might like to use Descriptive Statistics   Applet to check your hand computation.   A Short Discussion on the Descriptive Statistic: Deviations about the mean m of a distribution is  the basis for most of the statistical tests we will learn. Since we are measuring     how much a set of scores is dispersed about the mean m , we are measuring variability. We can calculate     the deviations about the mean m and express it as     variance s2 or standard     deviation s. It is very important to have a firm     grasp of this concept because it will be a central concept throughout your statistics course.  Both variance s2  and standard deviation s measure variability within a distribution. Standard deviation s is a number  that indicates how much on average each of the values in the distribution \\n    deviates from the mean m (or center) of the distribution. Keep in mind that variance s2 measures the same thing as standard deviation s  (dispersion of scores in a distribution). Variance s2, however, is the average squared deviations about the mean m . Thus, variance s2  is the square of the standard deviation s. \\n  The expected value and the variance of the statistic   are m and s2/n,  respectively.   The expected value and variance of statistic S2 are s2 and 2s4 / (n-1), respectively.   and S2 are the best estimators for m and s2. They are Unbiased (you may update your estimate); Efficient (they have the smallest  variation among other estimators); Consistent (increasing sample size provides a better estimate); and Sufficient (you do not need to have the whole data  set; what you need are Sxi and Sxi2 for estimations). Note also that the above variance S2 is justified only in the case where the population distribution tends to be normal, otherwise one may use bootstrapping techniques. In general, it is believed that the pattern of mode, median, and mean go from lower to higher in positive skewed data sets, and just the opposite pattern in negative skewed data sets. However; for example, in the following 23 numbers,  mean = 2.87, median = 3, but the data is positively skewed:    4, 2, 7, 6, 4, 3, 5, 3, 1, 3, 1, 2, 4, 3, 1, 2, 1, 1, 5, 2, 2, 3, 1 and, the following 10 numbers have mean = median = mode = 4, but the data set is  left skewed:   1, 2, 3, 4, 4, 4, 5, 5, 6, 6.Note also,  that most commercial software do not correctly compute skewness and kurtosis. There is no easy way to determine confidence intervals about     a computed skewness or kurtosis value from a small to medium sample. The literature     gives tables based on asymptotic methods for sample sets larger than 100 for     normal distributions only.   You may have noticed that using the above numerical example on some computer     packages such as SPSS, the skewness and the kurtosis are different from what     we have computed. For example, the SPSS output for the skewness is 1.190.     However, for large a sample size n, the results are identical.   Reference and Further Readings: David H., Early sample measures of variability, Statistical Science,  Vol. 13, 1998, 368-377. This article provides a good historical account of  statistical measures.  Groeneveld R., A class of quantile measures for kurtosis, The American Statistician, 325, Nov. 1998.  Lehmann E., Testing Statistical Hypotheses, 1996, Wiley. Exact confidence interval for the coefficient of variation is computationally tedious as shown in this  book.  \\n\\n\\nThe Two Statistical Representations of a Population\\n The following figure depicts a typical relationship between the cumulative distribution function (cdf) and the density (for continuous random variables),       All characteristics of the population are well described by either of these two functions. The figure also illustrates their applications in determining the (lower) percentile measures denoted by P:P = P[ X £ x] = Probability that the random variableX is less than or equal to a given number x, among other useful information. Notice that the probability P is the area  under the density function curve, while numerically equal to the height of cdf curve at point x.   Both functions can be estimated by smoothing the empirical (i.e., observed) cumulative step-function, and smoothing the histogram constructed from a  random sample.\\n\\n\\nEmpirical (i.e., observed) Cumulative Distribution Function\\n The empirical cumulative distribution function (ECDF),  also  known as Ogive (pronounced o-jive), is used to graph cumulative frequency. The ogive is the estimator for the population\\'s cumulative distribution function, which contains all the characteristic of the population.  The empirical distribution is a staircase function with the location of the drops randomly placed. The size of the each stair at each  point depends on the frequency of that point value, and it is equal to the frequency/n where n is the sample size. The sample size is the sum of all frequencies. \\nNote that almost all statistics we have covered up to now can be obtained and understood more deeply  by graph paper using Empirical Distribution Function JavaScript. You may like using this JavaScript in performing some numerical experimentation for a deeper understanding.\\n\\nOther widely used decision model based upon empirical\\n cumulative distribution function (ECDF) as a measuring tool and decision procedure are the ABC Inventory Classification, Single-period Inventory Analysis (The Newsboy Model), and determination of the Best Time to Replace Equipment. For other inventory decisions, visit  the Inventory Control Models site.\\n\\n\\n\\nIntroduction Modeling of a Data Set: Families of parametric distribution models are widely used to summarize a huge data set, to obtain predictions,  assess goodness of fit, to estimate functions of the data not easily derived directly, or to render manageable random effects. The trustworthiness of the results obtained depends on the generality of the distribution family employed. \\n  Inductive Inference: This extension of our knowledge from a particular random sample to the population is called inductive inference. The main function of business statistics is the provision of techniques for making inductive inference and for measuring the degree of uncertainty of such inference. Uncertainty is measured in terms of probability statements, and that is the reason we need to learn the language of uncertainty and its measuring tool called probability.In contrast to the inductive inference, mathematics often uses deductive inference to prove theorems, while in empirical science, such as statistics, inductive inference is used to find new knowledge or to extend our knowledge. Further Readings:Brown B., F. Spears, and L. Levy, The log F: A distribution for all seasons, Computational Statistics, 17(1), 47-58, 2002. Probability, Chance, Likelihood, and OddsThe concept of probability occupies an important place in the decision-making process under uncertainty, whether the problem is one faced in business, in government, in the social sciences, or just in one\\'s own everyday personal life. In very few decision-making situations is perfect information -- all the needed facts -- available. Most decisions are made in the face of uncertainty. Probability enters into the process by playing the role of a substitute for certainty - a substitute for complete knowledge. Probability is especially significant in the area of statistical inference. Here the statistician\\'s prime concern lies in drawing conclusions or making inferences from experiments  which involve uncertainties. The concepts of probability make it possible for  the statistician to generalize from the known (sample) to the unknown (population) and to place a high degree of confidence in these generalizations. Therefore, Probability is one of the most \\n  important tools of statistical inference. Probability has an exact technical meaning -- well, in fact it has several, and there is still debate as to which term ought to be used. However, for most  events for which probability is easily computed; e.g., rolling of a die, the probability of getting a four [::], almost all agree on the actual value (1/6), if not the philosophical interpretation. A probability is always a number between 0 and 1. Zero is not \"quite\" the same thing as impossibility.  It is possible that \"if\" a coin were flipped infinitely many times, it would never show \"tails\", but the probability of an infinite run of heads is 0. One is not \"quite\" the same thing as certainty but close enough.  The word \"chance\" or \"chances\" is often used as an approximate synonym of  \"probability\", either for variety or to save syllables. It would be better practice to leave \"chance\" for informal use, and say \"probability\" if that is what is meant.  One occasionally sees \"likely\" and \"likelihood\"; however, these terms are used casually as synonyms for \"probable\" and \"probability\".   Odds is a probabilistic concept related to probability. It is the ratio of the probability (p) of an event to the probability (1-p) that it does not happen: p/(1-p). It is often expressed as a \\n  ratio, often of whole numbers; e.g., \"odds\" of 1 to 5 in the die example above, but for technical purposes the division may be carried out to yield a positive real number (here 0.2). Odds are a ratio of nonevents to events. If the event rate for a disease is 0.1 (10 per cent), its nonevent rate is 0.9 and therefore its odds are 9:1.  \\n  Another way to compare probabilities and odds is using \"part-whole thinking\" with a binary (dichotomous) split in a group. A probability is often a ratio of a part to a whole; e.g., the ratio of the part [those who survived \\n  5 years after being diagnosed with a disease] to the whole [those who were diagnosed with the disease]. Odds are often a ratio of a part to a part; e.g., the odds against dying are the ratio of the part that succeeded [those who survived 5 years after being diagnosed with a disease] to the part that  \\'failed\\' [those who did not survive 5 years after being diagnosed with a  disease].   Aside from their value in betting, odds allow one to specify a small   probability (near zero) or a large probability (near one) using large whole  numbers (1,000 to 1 or a million to one). Odds magnify small probabilities (or  large probabilities) so as to make the relative differences visible. Consider two probabilities: 0.01 and 0.005. They are both small. An untrained observer might not realize that one is twice as much as the other. But if expressed as odds (99 to 1 versus 199 to 1) it may be easier to compare the two situations by focusing on large whole numbers (199 versus 99) rather than on small ratios or fractions. \\n How to Assign Probabilities?  Probability is an instrument to measure the likelihood of the occurrence of an event. There are five major approaches of assigning probability: Classical Approach, Relative Frequency Approach, Subjective Approach, Anchoring, and the Delphi Technique: Classical Approach: Classical probability is predicated on the condition that the outcomes of an experiment are equally likely to happen. The classical probability utilizes the idea that the lack of knowledge implies that all possibilities are equally likely. The classical probability is applied when the events have the same chance of occurring (called equally likely events), and the sets of events are mutually exclusive and collectively exhaustive. The classical probability is defined as: P(X) = Number of favorable outcomes / Total number of possible outcomes Relative Frequency Approach: Relative probability is based on accumulated historical or experimental data. Frequency-based probability is defined as: P(X) = Number of times an event occurred / Total number of  opportunities for the event to occur. Note that relative probability is based on the ideas that what has happened \\n  in the past will hold. Subjective Approach: The subjective probability is based on personal judgment and experience. For example, medical doctors sometimes assign subjective probability to the length of life expectancy for a person who has cancer.  Anchoring: is the practice of assigning a value obtained from a prior experience and adjusting the value in consideration of current expectations or circumstances The Delphi Technique: It consists of a series of questionnaires. Each series is one \"round\". The responses from the first \"round\" are gathered and become the basis for the questions and feedback of the second \"round\". The process is usually repeated for a predetermined number of \"rounds\" or until the responses are such that a pattern is observed. This process allows expert opinion to be circulated to all members of the group and eliminates the bandwagon effect of majority opinion. Delphi Analysis is used in decision making processes, in particular in forecasting. Several \"experts\" sit together and try to compromise on something upon which they cannot agree.  Further Reading:Delbecq, A., Group Techniques for Program Planning, Scott Foresman, 1975. General Laws of ProbabilityGeneral Law of Addition: When two or more events will happen at the same time, and the events are not mutually exclusive, then:  P (X or Y) = P (X) + P (Y) - P (X and Y) Notice that, the equation P (X or Y) = P (X) + P (Y) - P (X and Y),   contains especial events: An event (X and Y) which is the intersection of  set/events X and Y, and another event (X or Y) which is the union (i.e.,  either/or) of sets X and Y. Although this is very simple, it says relatively little about how event X influences event Y and vice versa. If P (X and Y) is 0, indicating that events X and Y do not intersect (i.e., they are mutually exclusive), then we have P (X or Y) = P (X) + P (Y). On the other hand if P (X and Y) is not 0, then there are interactions between the two events X and Y. Usually it could be a physical interaction between them. This makes the  relationship P (X or Y) = P (X) + P (Y) - P (X and Y) nonlinear because the  P(X and Y) term is subtracted from which influences the result. \\n\\nThe above law is known also as the Inclusion-Exclusion Formula.  It can be extended to more than two events.   For example, for three events A, B, and C, it becomes:\\n \\nP(A or B or C) = P(A) + P(B) + P(C) - P(A and B) - P(A and C) - P(B and C) + P(A and B and C)\\n \\nSpecial Law of Addition: When two or more  events will happen at the same time, and the events are mutually  exclusive, then:  P(X or Y) = P(X) + P(Y) General Law of Multiplication: When two or more events will happen at the same time, and the events are dependent, then the general rule of multiplicative law is used to find the joint probability:   P(X and Y) = P(Y) ´ P(X|Y), where P(X|Y) is a conditional probability.   Multiplicative Law: When two or more  events will happen at the same time, and the events are independent,  then the special rule of multiplication law is used to find the joint  probability:    P(X and Y) = P(X) ´ P(Y)  Conditional Probability Law: A conditional   probability is denoted by P(X|Y). This phrase is read: the probability that X   will occur given that Y is known to have occurred. Conditional probabilities are based on knowledge of one of the variables.   The conditional probability of an event, such as X, occurring given that   another event, such as Y, has occurred is expressed as:  P(X|Y) = P(X and Y) ¸ P(Y), provided P(Y) is not zero. Note that when using the conditional law of   probability, you always divide the joint probability by the probability of the   event after the word given. Thus, to get P(X given  Y), you divide the joint probability of X and Y by the unconditional   probability of Y. In other words, the above equation is used to find the  conditional probability for any two dependent events.   The simplest version of the Bayes\\' Theorem is: \\n   P(X|Y) = P(Y|X) ´ P(X) ¸ P(Y) If two events, such as X and Y, are independent then:   P(X|Y) = P(X),and   P(Y|X) = P(Y) The Bayes\\' Law: \\nP(X|Y) = [ P(X) ´ P(Y|X) ]  ¸  [P(X) ´P(Y|X) + P(not X) ´ P(Y| not X)] Bayes\\' Law provides posterior probability [i.e, P(X|Y)] sharpening the  prior probability [i.e., P(X)] by the availability of accurate and relevant  information in probabilistic terms.   \\nAn Application: Suppose two machines, A and B, produce identical  parts. Machine A has probability 0.1 of producing a defective each time,  whereas Machine B has probability 0.4 of producing a defective. Each machine  produces one part. One of these parts is selected at random, tested, and found \\n  to be defective. What is the probability that it was produced by Machine B?  Probability tree diagrams depict events or sequences of events as branches  of a tree. Tree diagrams are useful for visualizing the conditional  probabilities:   \\nThe probabilities at the end of each branch are the probability that events  leading to that end will happen simultaneously. The above tree diagram  indicates that the probability of a part testing Good is 9/20 + 6/20 = 3/4,   therefore the probability of Bad is 1/4. Thus, P(made by B | it is bad) = (4/20) / (1/4) = 4/5.  Now using the Bayes\\' Law we are able to obtain useful information such as:    P(it is bad | made by B) = 1/4(4/5) / [1/4(4/5) + 3/4(2/5)] = 2/5.    Equivalently, using the above conditional probability, results in:    P(it is bad | made by B) = P(it is bad & made by B)/P(made by B) = \\n  (4/20)/(1/2) = 2/5. \\nYou may like using the Bayes\\' Revised Probability JavaScript.\\n \\nMutually Exclusive versus Independent  EventsMutually Exclusive (ME): Event A and B are ME if both cannot occur simultaneously. That is, P[A and B]   = 0.   Independency (Ind.): Events A and B are  independent if having the information that B already occurred does not change \\n  the probability that A will occur. That is P[A given B occurred] = P[A].  If two events are ME they are also Dependent: P(A given B) = P[A and   B] ¸ P[B], and since P[A and B] = 0 (by ME), then P[A given B] = 0. Similarly, \\n  If two events are Independent then they are also not ME.   If two events are Dependent then they may or may not be ME.   If two events are not ME, then they may or may not be Independent.   The following Figure contains all possibilities. The notations used in this   table are as follows: X means does not imply, question mark ?   means it may or may not imply, while the check mark means it implies.    Notice that the (probabilistic)  pairwise independency and mutual independency for a collection of events   A1,..., An are  two different notions.   \\nWhat Is so Important About the Normal Distributions? The term \"normal\" possibly arose because of the   various attempts made to establish this distribution as the underlying law governing all continuous variables. These attempts were  based on false premises and consequently failed. Nonetheless, the normal   distribution rightly occupies a preeminent place in the field of probability.   In addition to portraying the distribution of many types of natural and   physical phenomena (such as the heights of men, diameters of machined parts,   etc.), it also serves as a convenient approximation of many other   distributions which are less tractable. Most importantly, it describes the   manner in which certain estimators of population characteristics vary from  sample to sample and, thereby, serves as the foundation upon which much statistical  inference from a random sample to population are made.   Normal Distribution (called also Gaussian) curves, which have a bell-shaped  appearance (it is sometimes even referred to as the \"bell-shaped curves\") are   very important in statistical analysis. In any normal distribution is  observations are distributed symmetrically around the mean, 68% of all values  under the curve lie within one standard deviation of the mean and 95% lie \\n  within two standard deviations.   There are many reasons for their popularity. The following are the most \\n  important reasons for its applicability:    \\nOne reason the normal distribution is important is that a wide variety of    naturally occurring random  variables such as heights and weights of all creatures are distributed \\n    evenly around a central value, average, or norm (hence, the name normal distribution).  Although the distributions are only approximately normal, they are usually  quite close.   Whenever there are too many  factors influencing the outcome of a random outcome, then the underlying distribution  is approximately normal. For example, the height of a tree is determined  by the \"sum\" of such factors as rain, soil quality, sunshine, disease, etc.  As Francis Galton wrote in 1889, \"Whenever a large sample of chaotic elements are taken in hand and arranged in the order of their magnitude,  an unsuspected and most beautiful form of regularity proves to have been \\n      latent all along.\"   Almost all statistical tables are limited by  the size of their parameters. However, when these parameters are large enough \\n    one may use normal distribution for calculating the critical values for these  tables. For example, the F-statistic is related to standard normal z-statistic  as follows: F = z2, where F has (d.f.1  = 1, and d.f.2 is the largest available in the F-table).  For more, visit the Relationships among Common Distributions.\\nApproximation of the binomial: For example, the normal distribution  provides a very accurate approximation of the binomial when n is large and  p is close to 1/2. Even if n is small and p is not extremely close to 0 or to 1, the approximation is adequate. In fact, the normal approximation  of the binomial will be satisfactory for most purposes provided that np  > 5 and nq > 5.  Here is how the approximation is made. First, set m = np and s2 = npq. To allow for the fact that the binomial is a discrete distribution, we conventionally use a continuity correction factor of 1/2 unit added to or subtracted from X on the  grounds that the discrete value (x = a) should correspond on a continuous  scale to (a - 1/2) < x < (a + 1/2). Then we compute the value of the standard  normal variable by:   z = [(a - 1/2) - m]/s\\xa0 \\xa0OR\\xa0 \\xa0 z = [(a + 1/2) - m]/s  Now one may used the standard normal table for the numerical values.  An Application: The probability of a defective item coming off a  certain assembly line is p = 0.25. A sample of 400 items is selected  from a large lot of these items. What is the probability 90 or less items are defective?  If the mean and standard deviation of a normal distribution are known, it is easy to convert back and forth from raw scores to percentiles. It has been proven that the underlying distribution is normal if and only if the sample mean is independent of the sample variance, this characterizes the normal distribution.  Therefore many effective transformations can be applied to convert almost any shaped distribution into a normal one.  The most important reason for popularity of normal distribution is the Central Limit Theorem (CLT). The distribution of the sample averages of a large number of independent random variables will be approximately normal regardless of the distributions of the individual random variables. \\n\\nThe Sampling distribution of normal populations provide more information than any other distributions.  For example, the following standard (i.e., having the same unit as the data have) errors are readily available:\\n\\n\\nStandard Error of the Median =  (p/2n)½S. \\n     Standard Error of the Standard Deviation = S/(2n)½.  Therefore, the test statistic for the null hypothesis s = s0, is  Z = (2n)½ (S - s0)/s0.\\n    \\nStandard Error of the Variance =  S2[(2/(n-1)]½. \\n    \\nStandard Error of the Interquartiles Half-Range (Q) = 1.166Q/n½  Standard Error of the Skewness = (6/n)½. \\n\\nStandard Error of the Skewness of Sample Mean = Skewness/n½ Notice that the skewness in sampling distribution of the mean rapidly disappears as n gets larger.   Standard Error of the Kurtosis = (24/n)½ = 2 times the standard error of skewness. \\n  Standard Error of the Correlation (r) =  [(1 - r2)/(n-1)]½.\\n\\nMoreover,Quartile deviation » 2S/3, \\xa0 \\xa0 and,\\n\\xa0 \\xa0Mean absolute deviation » 4S/5.\\n The other reason the normal distributions are so important is that the normality condition is required by almost all kinds of parametric statistical tests. The Central Limit Theorem is a useful tool when you are dealing with a population  with an unknown distribution. Often, you may analyze the mean (or the sum) of a sample of size n. For example instead of analyzing the weights of individual items you may analyze the batch of size n, that is, the packages each containing  n items.  What Is A Sampling Distribution?  A sampling distribution describes probabilities associated with a statistic when a random sample is drawn from the entire population. The sampling distribution is the density (for a continuous statistic, such as an estimated mean), or probability function (for discrete statistic, such as an estimated proportion). Derivation of the sampling distribution is the first step in calculating  a confidence interval or carrying out a hypothesis testing for a parameter. Example: Suppose that x1,.......,xn are a simple random sample from a normally distributed population with expected value m and known variance s2. Then, the sample mean is normally distributed with expected value m and variance s2/n.  The main idea of statistical inference is to take a random sample from the entire particular population and then to use the information from the sample to make inferences about the  particular population characteristics such as the mean m(measure of central tendency), the standard deviation (measure of dispersion, spread)  s or the proportion of units in the population that have a certain characteristic. Sampling saves money, time, and effort. Additionally,  a sample can provide, in some cases, as much or more accuracy than a corresponding study that would attempt to investigate an entire population. Careful collection  of data from a sample will often provide better information than a less careful study that tries to look at everything.  Often, one must also study the behavior of the mean of sample values taken from different specified populations; e.g., for comparison purposes.  \\nBecause a sample examines only part of a population, the sample mean will not exactly equal the corresponding mean of the population m . Thus, an important consideration for those planning and interpreting sampling results is the degree to which sample estimates, such as the sample mean, will agree with the corresponding population characteristic. In practice, only one sample is usually taken. In some cases a small \"pilot sample\" is used to test the data-gathering mechanisms and to get preliminary information for planning the main sampling scheme. However, for purposes of understanding the degree to which sample means will agree with the corresponding  population mean m , it is useful to consider what \\n    would happen if 10, or 50, or 100 separate sampling studies, of the same type,  were conducted. How consistent would the results be across these different studies? If we could see that the results from each of the samples would be nearly the same (and nearly correct!), then we would have confidence in the single sample that will actually be used. On the other hand, seeing that answers from the repeated samples were too variable for the needed accuracy would suggest that a different sampling plan (perhaps with a larger sample size) \\n    should be used. A sampling distribution is used to describe the distribution of outcomes  that one would observe from replication of a particular sampling plan. Know that estimates computed from one sample will be different from estimates  that would be computed from another sample.  Understand that estimates are expected to differ from the population characteristics  (parameters) that we are trying to estimate, but that the properties of sampling distributions allow us to quantify, based on probability, how they will differ.   Understand that different statistics have different sampling distributions  with distribution shape depending on (a) the specific statistic, (b) the sample size, and (c) the parent distribution. \\n Understand the relationship between sample size and the distribution of sample estimates. Understand that increasing the sample size can reduce the variability in a sampling distribution. \\n See that in large samples, many sampling distributions can be approximated with a normal distribution. \\n\\nSampling Distribution of the Mean and the Variance for Normal Populations:  Given the random variable X is distributed normally with mean m and standard deviation s, then for a random sample of size n:\\n\\n\\nThe sampling distribution of [ - m] ´ n½ ¸ s, is the standard normal distribution.\\n\\nThe sampling distribution of [ - m ] ´ n½ ¸ S, is  a t-distribution with parameter d.f. = n-1.\\n\\nThe sampling distribution of [S2(n-1) ¸ s2], is  a c2 distribution with parameter d.f. = n-1.\\n\\nFor two independent samples, the sampling distribution of [S 12 / S22],  is  an F distribution with parameters d.f.1 = n 1-1, and d.f.2= n 2-1.\\n\\n\\n  What Is The Central Limit Theorem?  The central limit theorem (CLT) is a \"limit\" that is \"central\" to statistical  practice. For practical purposes, the main idea of the CLT is that the average (center of data) of a sample of observations drawn from some population is approximately distributed as a normal distribution if certain conditions are met. In theoretical statistics there are several versions of the central limit theorem depending on how these conditions are specified. These are concerned with the types of conditions made about the distribution  of the parent population (population from which the sample is drawn) and the actual sampling procedure. One of the simplest versions of the central limit theorem stated by many textbooks is: if we take a random sample of size (n) from the entire population, then, the sample mean which  is a random variable defined by: S xi / n, has a histogram which converges to a normal distribution shape if n is large enough . Equivalently, the sample mean distribution approaches to normal distribution as the sample size increases.\\n\\nSome students having difficulty reconciling their own understanding of the central limit theorem with some of the textbooks statements.  Some textbooks do not emphasize the on the independent, random samples of fixed-size n (say more than 30).\\n\\nThe shape of the sampling distributions for means - becomes increasingly normal as the sample size n becomes larger.  The increasing sample size is what causes the distribution to become increasingly normal and the independence condition provides the Ön  contraction of the standard deviation.\\n\\nThe CLT for proportion data, such as binary 0, 1, again the sampling distribution-- while becoming increasingly \"bell-shaped\"-- remains confined to the domain [0,1]. This domain represents a dramatic difference from a normal distribution, with has an unbounded domain. However, as n increases without bound, the \"width\" of the bell becomes very small so that the CLT \"still works\".\\nIn applications of the central limit theorem to practical problems in statistical inference, however, we are more interested in how closely the approximate distribution of the sample mean follows a normal distribution  for finite sample size, than in the limiting distribution itself. Sufficiently close agreement with a normal distribution allows us to use normal theory for making inferences about population parameters (such as the mean ) using the sample mean, irrespective of the actual form of the parent population.  It can be shown that, if the parent population has mean m and a finite standard deviation s, then the   sample mean distribution has the same mean m but  with smaller standard deviation which is s divided  by n½. You know by now that, whatever the parent population is, the standardized  variable Z = (X - m  )/s will have a distribution with a mean m  = 0 and standard deviation s =1 under random sampling.  Moreover, if the parent population is normal, then Z is distributed exactly as the standard normal. The central limit theorem states the remarkable result that, even when the parent population is non-normal, the standardized \\n    variable is approximately normal if the sample size is large enough. It is generally not possible to state conditions under which the approximation given by the central limit theorem works and what sample sizes are needed before  the approximation becomes good enough. As a general guideline, statisticians have used the prescription that, if the parent distribution  is symmetric and relatively short-tailed, then the sample mean more closely approximates normality for smaller samples than if the parent population is skewed  or long-tailed. Under certain conditions, in large samples, the sampling distribution of  the sample mean can be approximated by a normal distribution. The sample size  needed for the approximation to be adequate depends strongly on the shape of the parent distribution. Symmetry (or lack thereof) is particularly important. For a symmetric parent distribution, even if very different from the shape  of a normal distribution, an adequate approximation can be obtained with small samples (e.g., 15 or more for the uniform distribution). For symmetric, short-tailed parent distributions, the sample mean more closely approximates normality for smaller  sample sizes than if the parent population is skewed and long-tailed. In some extreme cases (e.g. binomial) sample sizes far exceeding the typical guidelines  (e.g., over 30) are needed for an adequate approximation. For some distributions  without first and second moments (e.g., one is known as the Cauchy distribution), the central limit theorem does not hold.  For some distributions, extremely large (impractical) samples would be required  to approach a normal distribution. In manufacturing, for example, when defects  occur at a rate of less than 100 parts per million, using, a Beta distribution yields an honest Confidence Interval (CI) of total defects in the population. \\n What Is \"Degrees of Freedom\"?  Recall that in estimating the population\\'s variance, we used (n-1) rather than  n, in the denominator. The factor (n-1) is called \"degrees of freedom.\"  Estimation of the Population Variance: Variance in a population is defined as the average of squared deviations from the population mean. If we draw a random sample of n cases from a population where the mean  is known, we can estimate the population variance in an intuitive way. We  sum the deviations of scores from the population mean and divide this sum by n. This estimate is based on n independent pieces of information, and we have n degrees of freedom. Each of the n observations, including the last one, is unconstrained (\\'free\\' to vary).  When we do not know the population\\'s mean, we can still estimate the population  variance; but, now we compute deviations around the sample mean. This introduces  an important constraint because the sum of the deviations around the sample  mean is known to be zero. If we know the value for the first (n-1) deviations, the last one is known. There are only n-1 independent pieces of information  in this estimate of variance.  If you study a system with n parameters xi,  i =1..., n, you can represent it in an  n-dimension space. Any point of this space shall represent a potential state of your system. If your n parameters  could vary independently, then your system would be fully described in a n-dimension hyper-volume (for n over 3). Now, imagine you have one constraint between the parameters  (an equation with your n parameters), then your system would be described  by a (n-1)-dimension hyper-surface (for n over 3). For example, in three dimensional space,  a linear relationship means a plane which is 2-dimensional.  In statistics, your n parameters are your n data. To evaluate variance, you  first need to infer the mean m . So when you evaluate the variance, you have one constraint on your system (which is the expression of the mean), and  it remains only  (n-1) degrees of freedom to your system. Therefore, we divide the sum of squared deviations by n-1, rather than by n, when we have sample data. On average, deviations around the sample mean  are smaller than deviations around the population mean. This is because our  sample mean is always in the middle of our sample scores; in fact, the minimum  possible sum of squared deviations for any sample of numbers is around the mean for that sample of numbers. Thus, if we sum the squared deviations from  the sample mean and divide by n, we have an underestimate of the variance  in the population (which is based on deviations around the population mean). If we divide the sum of squared deviations by n-1 instead of n, our estimate is a bit larger, and it can be shown that this adjustment gives us an unbiased estimate of the population variance. However, for large n,  say, over 30, it does not make too much difference if we divide by n, or   n-1.  Degrees of Freedom in ANOVA: You will see the  key parse \"degrees of freedom\" also appearing in the Analysis of Variance (ANOVA) tables. If I tell you about 4 numbers, but don\\'t say what they are,  the average could be anything. I have 4 degrees of freedom in the data set.   If I tell you 3 of those numbers, and the average, you can guess the fourth  number. The data set, given the average, has 3 degrees of freedom. If I tell  you the average and the standard deviation of the numbers, I have given you  2 pieces of information, and reduced the degrees of freedom from 4 to 2.  You only need to know 2 of the numbers\\' values to guess the other 2.   In an ANOVA table, degree of freedom (df) is the divisor in (Sum of Squared deviations)/df which will  result in an unbiased estimate of the variance of a population.  In general, a degree of freedom d.f. = N - k, where N is the sample size, and k is a small number, equal to  the number of \"constraints\", the number of \"bits of information\" already \"used  up\". As we will see in the ANOVA section, degree of freedom is an additive quantity; total amounts of it can be  \"partitioned\" into various components.  For example, suppose we have a sample of size 13 and calculate its mean,  and then the deviations from the mean; only 12 of the deviations are free  to vary. Once one has found 12 of the deviations, the thirteenth one is determined.    In bivariate correlation or regression situations, k = 2. The calculation  of the sample means of each variable \"uses up\" two bits of information, leaving   N - 2 independent bits of information. In a one-way analysis of variance (ANOVA) with g groups, there are three  ways of using the data to estimate the population variance. If all the data  are pooled, the conventional SST/(n-1) would provide an estimate of the population   variance.  If the treatment groups are considered separately, the sample means can also  be considered as estimates of the population mean, and thus SSb/(g - 1) can   be used as an estimate. The remaining (\"within-group\", \"error\") variance can  be estimated from SSw/(n - g). This example demonstrates the partitioning  of d.f.: d.f. total = n - 1 = d.f.(between) + d.f.(within) = (g - 1) + (n - g).   Therefore, the simple \\'working definition\\' of d.f. is \\x91sample size minus the  number of estimated parameters\\'. A more complete answer would have to explain why  there are situations in which the degrees of freedom is not an integer. After  we said all this, the best explanation, is mathematical in that we use d.f. to obtain an unbiased estimate.  In summary, the concept of degrees of freedom is used for the following two  different purposes:  Parameter(s) of certain distributions, such as F and t-distribution, are called degrees of freedom.    Most importantly, the degrees of freedom are used to obtain unbiased estimates for the population parameters.   Applications of and Conditions for Using Statistical Tables Some widely used applications of the popular statistical tables can be categorized   as follows:   T - Table:  Single Population µ Test.     Two Independent Populations µ\\'s Test. The Before-and-After µ\\'s Test.  Tests Concerning Regression Coefficients .     Test Concerning Correlation.   Conditions for using this table: Test for randomness of the data is needed before using this table. Test  for normality condition of the population distribution is also needed if the sample  size is small, or it may not be possible to invoke the central limit theorem.   Z - Table:  Test for Randomness.  Tests concerning µ for one population or two populations based on their large-size, random sample(s),   (say over 30) to invoke the central limit theorem. This includes test  concerning proportions, with large-size, random sample size n (say over  30) to invoke distribution convergence results. To Compare Two Correlation Coefficients.   Notes: As you know by now, in test of hypotheses  concerning m, and construction of confidence interval  for it, we start with s known, since the critical  value (and the p-value) of the Z-Table distribution can be used. Considering  the more realistic situations, when we don\\'t know s,  the T-Table is used. In both cases, we need to verify the normality condition of the population\\'s distribution; however, if the sample size n is very large, we can in fact switch back to Z-Table by  virtue of the central limit theorem. For perfectly normal populations, the t-distribution  corrects for any errors introduced by estimating s with s when doing inference. Note also that, in hypothesis testing concerning the parameter of binomial and Poisson distributions for large sample sizes, the standard deviation is  known under the null hypotheses. That\\'s why you may use the normal approximations  for both of these distributions. Conditions for using this table: Test for randomness of the data is needed before using this table. Test for normality condition of the population distribution is also needed if the sample size is small, or it may not be possible to invoke the Central Limit Theorem. Chi-square - Table: Test for Cross-table Relationship.     Identical-Populations Test for Crosstable Data.  Test for Equality of Several Population Proportions. Test  for Equality of Several Population Medians. Goodness-of-Fit Test for Probability Mass Functions. Compatibility of Multi-Counts.Correlation-Coefficient  Testing. \\nNecessary  Conditions in Applying the Above Tests. Testing the Variance: Is the Quality that Good?. Testing the Equality of Multi-Variances.  Conditions for using this table: The necessary conditions for using this table for all the above tests, except for the last one, can be found  at Conditions for the Chi-square  Based Tests. The last application requires normality (condition) of the population distribution.  F - Table:  Multi-Means Comparisons: Analysis of Variance (ANOVA). \\n    Tests Concerning Two Variances.  Overall Assessment of Regression Models .  Conditions for using this table: Tests for randomness of the data and normality  (condition) of the populations are needed before using this table for ANOVA.  Same conditions must be satisfied for the residuals in regression analysis.  The following chart summarizes application of statistical tables with respect \\n    to test of hypotheses and construction of confidence intervals for mean mand variance s 2 in one population or the comparison of two or more populations.    Selection of of an Appropriate Statistical TableClick on the image to enlarge it and THEN print it You may like using Online Statistical Computation in performing most of these tests. The P-values for the Popular Distributions Web site provides P-values  useful in major statistical testing. The results are more accurate than those  that can be obtained (by interpolation) from statistical tables of your textbook  are.  Further Reading:Evans M., N. Hastings, and B. Peacock, Statistical Distributions, Wiley, 2000.    Kanji G., 100 Statistical Tests, Sage Publisher, 1995.    \\nBinomial Probability Function  An important class of decision problems under uncertainty involves situations  for which there are only two possible random outcomes. \\n  The binomial probability function gives probability of exact number of \"successes\"  in n independent trials, when probability of success p on single trial is  a constant. Each single trial is called a Bernoulli Trial satisfying the following conditions:   Each trial results in one of two possible, mutually exclusive, outcomes.  One of the possible outcomes is denoted (arbitrarily) as a success, and   the other is denoted a failure.  The probability of a success, denoted by p, remains constant from trial  to trial. The probability of a failure, 1-p, is denoted by q. The trials are independent; that is, the outcome of any particular trial  is not affected by the outcome of any other trial.   The number of ways of getting r successes in n trials is:  P (r successes in n trials) = nCr . pr . (1- p)(n-r) = n! / [r!(n-r)!] . [pr . (1- p)(n-r)].  The mean and variance of random variable r, are np and np(1-p), respectively, where q = 1 - p. The skewness and kurtosis are (2q -1)/ (npq)½, and (1- 6pq)/(npq), respectively.  \\nFrom its skewness, we notice that the distribution is symmetric for p =1/2 and most skewed when p is 0 or 1.\\nIts mode is within interval [(n+1)p -1, (n+1)p], therefore if (n+1) p is not an integer, then the mode is an integer within the interval. However if (n+1)p is an integer, then its probability function has two but adjacent modes:  (n+1)p -1, and (n+1)p. Determination of probabilities for p over 0.5: The binomial tables in some textbooks are limited to deterring the probabilities for values of p up to 0.5. However, these tables can be used for values of p over 0.5. By  recasting a problem in terms of p to 1 -p, and setting r to n-r, then the probability of obtaining r successes in n trials for a given value of p is equal to the probability of obtaining n-r failures in n trials with 1-p. An Application: A large shipment of purchased parts is received at a warehouse, and a sample of 10 parts is checked for quality. The manufacturer\\'s  claim is that at most 5% might be defective. What is the chance that the sample \\n includes one defective?  P (one defective out of ten) = {10! /[(1!)(9!)]}(0.05)1(0.95)9 = 32%. \\n Know that the binomial distribution is to satisfy the five following requirements: \\n (1) each trial can have only two outcomes or its outcomes can be reduced to two  categories which are called pass and fail, (2) there must be a fixed number of  trials, (3) the outcome of each trail must be independent, (4) the probabilities must  remain constant, (5) and the outcome of interest is the number of successes.  Normal approximation for binomial: All binomial tables are limited  in their scope; therefore it is necessary to use standard normal distribution  in computing the binomial probabilities. The following numerical example illustrates  how good the approximation could be. This provides an indication for real applications \\n    when n is beyond the given values in the available binomial tables. Numerical Example: A sample of 20 items are taken randomly from a manufacturing  process with defective probability p = 0.40. What is the probability of obtaining exactly 5 defective? P (5 out of 20) = {20!/[(5!)(15!)]} ´ (0.40)5(0.6)15= 7.5%  Since the mean and standard deviation of distribution are:   m = np = 8, and s = (npq)1/2  = 2.19,  respectively; therefore, the standardized observation for r = 5, by using  the continuity factor (which always enlarges) are:   z1 = [(r-1/2) - m] / s = (4.5 -8)/2.19 = -1.60,     and     z2 = [(r+1/2) - m]  / s = (5.5 -8)/2.19 = -1.14.   Therefore, the approximated P (5 out of 20) is P (z being within interval   -1.60, -1.14). Now, by using the standard normal table, we obtain:   P (5 out of 20) = 0.44520 - 0.37286 = 7.2%   Comments: The approximation for binomial distribution is used frequently in quality control, reliability, survey  sampling, and other industrial problems.    \\nYou might like to use the  Exact Confidence Interval Construction and Test of Hypothesis for Binomial Population , and Binomial Probability Function Applet JavaScript in performing some numerical experimentation for validating the above assertions for a deeper understanding. \\n    Exponential Density FunctionAn important class of decision problems under uncertainty concerns the random durations between events. For example, the the length of time between breakdowns  of a machine not exceeding a certain time interval,  such as the copying machine in your office not breaking down during this week.  Exponential distribution gives distribution of time between independent events  occurring at a constant rate. Its density function is:   f(t) = l exp(-lt),     where l is the average number of events per unit of time, which is a positive number. The mean and the variance of the random variable t (time between events) are 1/ l, and  1/l2, respectively. \\n\\n Applications include probabilistic assessment of the time between  arrivals of patients to the emergency room of a hospital, and time between arrivals of ships  at a particular port.   Comments: Itis a special case of  Gamma distribution.   You might like to use Exponential Density Applet to perform your computations, and  Lilliefors Test for Exponentiality to perform the goodness-of-fit  test.   \\n F-Density Function \\nThe F distribution is the distribution of the ratio of two independent sampling (of size of n1, and n2, respectively) estimates of variance from standard normal distributions.  It is also formed by the ratio of two independent chi-square variables divided by their respective independent degrees of freedom. \\n\\n Its main applications are in testing equality of two independent population variances based on two independent random samples,  ANOVA, and regression analysis.You might like to use F-Density Function to obtain its P-values.\\n\\nChi-square Density Function  The probability density curve of a Chi-square distribution is an asymmetric curve  stretching over the positive side of the line and having a long right tail.   The form of the curve depends on the value of a parameter known as the degree of freedom (d.f.).  The expected value of Chi-square statistic is its d.f., its variance is twice  of its d.f., and its mode is equal to (d.f.- 2).  Chi square Distribution relation to Normal Distribution: The Chi-square distribution is  related to the sampling distribution of the variance when the sample is from a  normal distribution. The sample variance is a sum of squares of standard normal variables N (0, 1). Hence, the of square of  N (0,1) random variable is a Chi-square with 1 d.f.. \\n\\n Notice that the Chi-square is related to F-statistics as follows: F = Chi-square/d.f.1,  where F has (d.f.1 = d.f. of the Chi-square-table, and d.f.2 is the largest available in the F-table) \\n  Similar to Normal random variables, the Chi-square has the additive property. For example, for  two independent Chi-square variables, their sum is also Chi-square with degrees of freedom equal to the sum of the d.f. of the individual d.f.s. Thus the unbiased sample variance for a sample of size n from  N (0,1) is a sum of n-1 Chi-squares,  each with d.f. = 1, hence Chi-square with d.f. = n-1.  The most widely used applications of Chi-square distribution are:  The Chi-square Test for Association which is a non-parametric test; therefore, it can be used  for nominal data too. It is a  test of statistical significance widely used bivariate tabular  association analysis. Typically, the hypothesis is whether or not two populations are different  in some characteristic or aspect of their  behavior based on two random samples. This test procedure is also known as     the Pearson Chi-square test. The Chi-square Goodness-of-Fit Test is used to test if an observed distribution conforms to any particular distribution. Calculation of this goodness-of-fit  test is by comparison of observed data with data expected based on a particular distribution.   You might like to use  Chi-square Density to find its P-values. \\nMultinomial Probability Function A multinomial random  variable is an extended binomial. However, the difference is that in a \\n    multinomial case, there are more than two possible outcomes. There are a fixed  number of independent outcomes, with a given probability for each outcome.  The Expected Value (i.e., averages):  Expected Value = m = SXi ´ Pi, \\xa0 \\xa0 the sum is over all i\\'s.     Expected value is known also as the First Moment,  borrowed from Physics, because it is the point of balance where the data  and the probabilities are the distances and the weights, respectively.  The Variance is:     Variance = s2   = S [Xi2 ´ Pi] - m2, \\xa0 \\xa0 the sum is over all i\\'s.  The variance is not expressed in the same units as the expected value.  So, the variance is hard to understand and to explain as a result of the squared  term in its computation. This can be alleviated by working with the square  root of the variance, which is called the Standard (i.e., having the same unit as the data have) Deviation:   Standard Deviation = s = (Variance) ½  Both variance and standard deviation provide the same information and,  therefore, one can always be obtained from the other. In other words, the  process of computing standard deviation always involves computing the variance.  Since standard deviation is the square root of the variance, it is always  expressed in the same units as the expected value.  For the dynamic process, the Volatility as a measure for risk includes  the time period over which the standard deviation is computed. The Volatility measure is defined as standard deviation divided by the square root \\n of the time duration.  Coefficient of Variation: Coefficient of Variation (CV) is the absolute \\n  relative deviation with respect to size  provided  is not zero, expressed in percentage:    CV =100 |s/| %      \\nNotice that the CV is independent from the expected value measurement.  The coefficient of variation demonstrates the relationship between standard   deviation and expected value, by expressing the risk as a percentage of    the expected value. The inverse of CV (namely 1/CV) is called the Signal-to-Noise \\n      Ratio.  You might like to use Multinomial Applet for checking your computation and performing computer-assisted  experimentation.  An Application: Consider two investment alternatives, Investment  I and Investment II with the characteristics outlined in the following table:  \\n\\n\\n\\n- \\n        Two \\n        Investments - \\n\\n\\n\\nInvestment \\n          I\\n\\n\\n\\nInvestment \\n          II\\n\\n\\n\\n\\nPayoff \\n          %\\n\\n\\nProb.\\n\\n\\n\\nPayoff \\n          %\\n\\n\\nProb.\\n\\n\\n\\n\\n1\\n\\n\\n0.25\\n\\n\\n\\n\\n\\n3\\n\\n\\n0.33\\n\\n\\n\\n\\n7\\n\\n\\n0.50\\n\\n\\n\\n\\n\\n5\\n\\n\\n0.33\\n\\n\\n\\n\\n12\\n\\n\\n0.25\\n\\n\\n\\n\\n\\n8\\n\\n\\n0.34\\n\\n\\n\\nPerformance of Two InvestmentsTo rank these two investments under the Standard Dominance Approach  in Finance, first we must compute the mean and standard deviation and  then analyze the results. Using the Multinomial Applet for calculation, we notice  that the Investment I has mean = 6.75% and standard deviation = 3.9%, while the second investment has mean = 5.36% and standard deviation = 2.06%. First  observe that under the usual mean-variance analysis, these two investments  cannot be ranked. This is because the first investment has the greater mean; it also has the greater standard deviation; therefore, the Standard Dominance  Approach is not a useful tool here. We have to resort to the coefficient of variation (C.V.) as a systematic basis of comparison. The C.V. for Investment  I is 57.74% and for Investment II is 38.43%. Therefore, Investment II has  preference over the Investment  I. Clearly, this approach can be used to rank any number of alternative  investments. Notice that less variation in return on investment implies less risk. You might like to use this Applet in performing some numerical experimentation to:  \\nShow that E[aX + b] = aE(X) + b.\\nShow that V[aX + b] = a2V(X).\\nShow that: E(X2)= V(X) + (E(X))2.\\n\\n  Normal Density Function    In the Descriptive Statistic Section of this Web site, we have been concerned with how empirical  scores are distributed and how best to describe their distribution. We have discussed several different measures, but the mean m will  be the measure that we use to describe the center of the distribution, and  the standard deviation s will be the measure we use  to describe the spread of the distribution. Knowing these two facts gives us ample information to make statements about the probability of observing  a certain value within that distribution. If I know, for example, that the average Intelligence Quotient (I.Q.) score is 100 with a standard deviation of s = 20, then I know that someone with an I.Q. of 140 is very  smart. I know this because 140 deviates from the mean mby twice the average amount as the rest of the scores in the distribution.  Thus, it is unlikely to see a score as extreme as 140 because most of the I.Q. scores are clustered around 100 and  only deviate 20 points from  the mean m . Many applications arise from the central limit theorem (CLT). The CLT states that, average of values of n observations approaches normal distribution, irrespective of the form of original  distribution under quite general conditions. Consequently, normal distribution is an appropriate model for many, but not all, physical phenomena, such as distribution of physical measurements on living organisms, intelligence  test scores, product dimensions, average temperatures, and so on. Know that the Normal distribution is to satisfy seven requirements: (1) the graph should be bell shaped curve; (2) mean, median and mode are all equal; (3) mean, median and mode are located at the center of the distribution; (4) it has only one mode, (5) it is symmetric about mean, (6) it is a continuous function; (6) it never touches x-axis; and (7) the area under curve equals one. Many methods of statistical analysis presume normal distribution. \\n\\nWhen we know the mean and variance of a Normal then it allows us to find probabilities. So,  if, for example, you knew some things about the average height of women in the nation, including the fact that heights are distributed normally, you could measure all the women in your extended family and find the average height. This enables you to determine a probability associated with your result, if the probability of getting your result, given your knowledge of women nationwide, is high.  Then your family\\'s female height cannot be said to be different from average. If that probability is low, then your result is rare (given the knowledge about women nationwide), and you can say your family is different. You have just completed a test of the  hypothesis that the average height of women in your family is different from   the overall average.  \\n\\n\\n\\nThe ratio of two independent observations from the standard normal is distributed as the Cauchy Distribution which  has thicker tails than a normal distribution. It density function is f(x) = 1/[p(1+x2)], for all real value x.\\n\\n    You might like to use Standard Normal Applet instead of using tabular values from your textbook, and the \\nwell-known Lilliefors\\' Test for Normality to assess the goodness-of-fit. \\n     Poisson Probability Function\\n\\n\\nLife is good for only two things, discovering mathematics and teaching mathematics. -- Simeon Poisson\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\n\\n\\n\\nAn important class of decision problems under uncertainty is characterized by the small chance of the occurrence of a particular event, such as an accident. Poisson probability function computes the probability of exactly x independent occurrences during a given period of time, if events take place independently and at a constant rate. Poisson probability function also represent number of occurrences over constant areas or volumes: Poisson probabilities are often used; for example in quality control, software and hardware reliability, insurance claim, number of incoming telephone calls, and queuing theory. \\n    An Application: One of the most useful applications of the Poisson distribution is in the field of queuing theory. In many situations where queues occur it has been shown that the number of people joining the queue in a given time period follows the Poisson model. For example, if the rate of arrivals to an emergency room is l per unit of time period (say 1 hr), then: \\nP ( n arrivals) = ln\\xa0 e-l \\n/ n!The mean and variance of random \\n      variable n are both l . However if the mean and variance of a random variable have equal numerical values, then it is not necessary that \\n      its distribution is a Poisson. Its mode is within interval [l -1, l]. Applications:  P ( 0 arrival) = e-lP ( 1 arrival) = l\\xa0 e-l / 1!P ( 2 arrival) = l2\\xa0 e-l/ 2!\\nand so on. In general:  P ( n+1 arrivals ) = l P ( n arrivals ) / n. Normal approximation for Poisson: All Poisson tables are limited  in their scope; therefore, it is necessary to use standard normal distribution in computing the Poisson probabilities. The following numerical example illustrates how good the approximation could be.  Numerical Example: Emergency patients arrive at a large hospital at the rate of 0.033 per minute. What is the probability of exactly two arrivals during the next 30 minutes? The arrival rate during 30 minutes is l = (30)(0.033) = 1. Therefore, P (2 arrivals) = [12 /(2!)] e-1 = 18% \\nThe mean and standard deviation of distribution are: m = l = 1, and s = l 1/2 = 1, \\nrespectively; therefore, the standardized observation for n = 2, by using the continuity factor (which always enlarges) are: z1 = [(r-1/2) - m] / s = (1.5 -1)/1 = 0.5, and z2 = [(r+1/2) - m] / s = (2.5 -1)/1 = 1.5.  Therefore, the approximated P (2 arrivals) is P (z being within the interval 0.5, 1.5). Now, by using the standard normal table, we obtain:  P (2 arrivals) = 0.43319 - 0.19146 = 24% As you see the approximation is slightly overestimated, therefore the error is on the safe side. For large values of l, say over 20, one may use the Normal approximation to calculate Poisson probabilities. Notice that by taking the square root of a Poisson random variable, the transformed variable is more symmetric. This is a useful \\n transformation in regression analysis of Poisson observations. You might like to use Poisson Probability Function Applet to perform your computation, and Testing Poisson to perform the goodness-of-fit test. \\n\\nFurther Reading:\\nBarbour et al., Poisson Approximation, Oxford University Press, 1992. \\n\\n\\nStudent T-Density FunctionThe t distributions were discovered in 1908 by William Gosset, who was a chemist and a statistician employed by the Guinness brewing company. He considered himself a student still learning statistics, so that is how he signed his papers as pseudonym \"Student\". Or, perhaps he used a pseudonym due to \"trade secret\" restrictions by Guinness. \\nNote that there are different t-distributions; it is a class of distributions. When we speak of a specific t distribution, we have to specify the degrees of freedom. The t density curves are symmetric and bell-shaped like the normal distribution and have their peak at 0. However, the spread is more than that of the standard normal distribution. The larger the degrees of  freedom, the closer the t-density is to the normal density. \\n   \\nThe shape of a t-distribution depends on a parameter called \"degree-of-freedom\". As the degree-of-freedom gets larger, the t-distribution gets closer and closer to the standard normal distribution. For practical purposes, the t-distribution is treated as the standard normal distribution when degree-of-freedom is greater than 30. Suppose we have two independent random variables, one is Z, distributed as the standard normal distribution, while the other  has a Chi-square distribution with (n-1) d.f.; then the random variable:  (n-1)Z / c2 has a t-distribution with (n-1) d.f. For large sample size (say, n over 30), the new random variable has an expected value equal to zero, and its variance is (n-1)/(n-3) which is close to one. Notice that the t- statistic is related to F-statistic as follow: F = t2, where F has (d.f.1 = 1, and d.f.2 = d.f. of the t-table) You might like to use Student t-Density to obtain its P-values. \\n\\n\\nTriangular Density Function\\n  Triangular Density Function\\nThe triangular distribution shows the number of successes when you know the minimum, maximum, and most likely values. For example, you could describe the number of intakes seen per week when past intake data show the minimum, maximum, and most likely number of cases seen. It has a continuous probability distribution.\\n\\nThe parameters for the triangular distribution are Minimum, Maximum, and Likeliest. There are three conditions underlying triangular distribution: \\n\\n\\nThe minimum number of items is fixed. \\nThe maximum number of items is fixed. \\nThe most likely number of items falls between the minimum and maximum values.\\n\\n\\nThese three parameters forming a triangular shaped distribution, which shows that values near the minimum and maximum are less apt to occur than those near the most likely value.\\nFurther Reading:\\n\\n Evans M., Hastings N., and B., Peacock, Triangular Distribution,  Ch. 40 in Statistical Distributions, Wiley, pp. 187-188, 2000.\\n\\n  Uniform Density Function The uniform density function gives the probability that observation will occur within a particular interval [a, b] when probability of occurrence within that interval is directly proportional to interval length. Its mean and variance are:\\nm = (a+b)/2, \\xa0\\xa0\\xa0s2 = (b-a)2/12.\\n\\n Applications: Used to generate random numbers in sampling and Monte Carlo simulation.  Comments: Special case of beta distribution.   You might like to use Goodness-of-Fit Test for Uniform and performing some numerical experimentation for a deeper understanding of the concepts. \\n\\nNotice that any Uniform distribution has uncountable number of modes having equal density value; therefore it is considered as a homogeneous population.\\nFurther Reading:\\n\\nBalakrishnan N., and V. Nevzorov, A Primer on Statistical Distributions, Wiley, 2003.\\n\\n\\n\\nNecessary Conditions for Statistical Decision Making\\nIntroduction to Inferential Data Analysis Necessary Conditions: Do not just learn formulas and number-crunching. Learn about the conditions  under which statistical testing procedures apply. The following conditions are   common to almost all statistical tests: \\n     Any undetected outliers may have major impact and may influence the results of almost all statistical estimation and testing procedures.     Homogeneous population. That is, there is not more than one mode. Perform Test for Homogeneity of a Population  The sample must be random. Perform Test for Randomness.   In addition to the Homogeneity requirement, each population has a normal distribution. \\n    Perform the Lilliefors\\' Test for Normality. Homogeneity of variances. Variation in each population is almost the same  as in the other(s). Perform The Bartlett\\'s Test.  For two populations use the F-test. For 3 or more populations, there is a  practical rule known as the \"Rule of 2\". In this rule, one divides the highest variance of a sample by the lowest variance of the other sample. Given that the sample sizes are almost the same, and the value of this division is less than 2, then the variations of the populations are almost the same. \\n    Notice: This important condition in analysis of variance (ANOVA and the t-test for mean differences) is commonly tested by the Levene test or its modified test known as the Brown-Forsythe test. Interestingly, both  tests rely on the homogeneity of variances condition! \\nThese conditions are crucial, not for the method of computation,  but for the testing using the resultant statistic. Otherwise, we can do ANOVA and regression without any assumptions, and the numbers come   out the same. Simple computations give us least-square fits, partitions of  variance, regression coefficients, and so on.  We do need the above conditions when  test of hypotheses are our main concern. Further Readings:\\nGood Ph., and  J.  Hardin, Common Errors in Statistics, Wiley, 2003.\\nWang  H., Improved confidence estimators for  the usual one-sided confidence intervals for the ratio of two normal variances,  Statistics & Probability Letters, Vol. 59, No.3, 307-315, 2002. \\n\\n  Measure of Surprise for Outlier Detection\\nRobust statistical techniques are needed to cope with any undetected outliers; otherwise they are more likely to invalidate the conditions underlying statistical techniques, and they may seriously distort estimates and produce misleading conclusions in test of hypotheses. A common approach consists of  assuming that contaminating models, different from the one generating the rest of the data, generate the (possible) outliers. \\n\\nBecause of a potentially large variance, outliers could be the outcome of sampling errors or clerical errors such as recording data. Therefore, you must be very careful and cautious. Before declaring an observation \"an outlier,\" find out why and how such observation occurred. It could even be an error at the data entering stage while using any computer package.\\n\\n\\nIn practice, any observation with a standardized value greater than 2.5 in absolute value is a candidate for being an outlier. In such a case, one must first investigate the source of the datum. If there is no doubt about the accuracy or veracity of the observation, then it should be removed, and the model should be refitted. \\n\\nCompute the mean () and standard deviation (S) of the whole sample.\\nSet limits for the mean :  \\n -  k ´ S,\\xa0 \\xa0\\xa0 \\xa0  + k ´ S. A typical value for k is 2.5\\nRemove all sample values outside the limits.      \\nNow, iterate through the algorithm, the sample set may reduce after removing the outliers by applying step 3.\\nIn most cases, we need to iterate through this algorithm several times until all outliers are removed.\\n\\nAn Application: Suppose you ask ten of your classmates to measure a given length X.  The results (in mm) are:\\n\\n\\n46, 48, 38, 45, 47, 58, 44, 45, 43, 44\\n\\n\\nIs 58 an outlier?  Computing the mean and the variance of the ten measurement using the Descriptive Sampling Statistics JavaScript, are 45.8, and 5.1(after the needed adjustment), respectively.  The Z-value for 58 is Z (58) = 2.4.  Since the measurements, in general, follow a normal distribution, therefore,\\n\\n\\nProbability [X as large as 2.4 times standard deviation] = 0.008,\\n\\n\\nobtained by using the Standard Normal  P-value JavaScript, or from the normal table in your textbook.\\n\\nAccording this probability, one expects only .09 of the ten measurements as bad as this one.  This is a very rare event, however, in spite of such small probability, it has occurred, therefore, it might be an outlier.\\n\\nThe next most suspected measurement is 38, is it an outlier? It is a question for you.\\n\\nA Notice:  Outlier detection in the single population setting is not too difficult. Quite often, however, one can argue that the detected outliers are not really outliers, but form a second population . If this is the case, a data separation approach needs to be taken. \\n\\nYou might like to use the Identification of Outliers JavaScript in performing some numerical experimentation for validating and for a deeper understanding of the concepts Further Reading:\\n\\nRothamsted V., V. Barnett, and T. Lewis, Outliers in Statistical Data, Wiley, 1994.\\n\\n  Homogeneous Population  A homogeneous population is a statistical  population which has a unique  mode.\\n\\nNotice that, e.g.,  a Uniform distribution has uncountable number of modes having equal density value; therefore it is considered as a homogeneous population.\\n\\n To determine   if a given population is homogeneous or not, construct the histogram of a random  sample from the entire population. If there is more than one mode, then you  have a mixture of two or more different populations. Know that to perform any statistical testing, \\n  you need to make sure you are dealing with a homogeneous population. One of the main applications of histogramming is to \\nTest for Homogeneity of a Population. The unimodality of the histogram  is a necessary condition for the homogeneity of a population in order to conduct any meaningful statistical analysis. However, notice that, e.g.,  a Uniform distribution has uncountable number of modes having equal density value; therefore it is considered as a homogeneous population.    Test for Randomness: The Runs\\' Test   A basic condition in almost all inferential statistics is that a set of data constitutes a random sample from a given  homogeneous  population.  The condition of  randomness is essential to make sure the sample is truly representitive of the population.  The widely used test for randomness is the Runs test. A \"run\" is a maximal subsequence of like elements. Consider the following sequence (D for Defective items, N for Non-defective \\n items) from a production line: DDDNNDNDNDDD. Number of runs is R = 7, with  n1 = 8, and n2 = 4 which are number of D\\'s and N\\'s.  A sequence is a random sequence if it is neither \"over-mixed\"  nor \"under-mixed\". An example of over-mixed sequence is DDDNDNDNDNDD, with  R = 9 while under-mixed looks like DDDDDDDDNNNN with R = 2. There the above \\n    sequence seems to be a random sequence. The Runs Tests, which is also known as Wald-Wolfowitz Test,  is designed to test the randomness of a given sample at 100(1- a)%  confidence level. To conduct a runs test on a sample, perform the following \\n    steps: Step 1: compute the mean of the sample. \\nStep 2: going through the sample sequence, replace  any observation with +, or - depending on whether it is above or below the mean. Discard any ties. Step 3: compute R, n1, and n2.  Step 4: compute the expected mean and variance of  R, as follows:  a =1 + 2n1n2/(n 1 + n2). \\ns2 = 2n1n2(2n 1n2-n1- n2)/[[n1 + n2)2 (n1 + n2 -1)].  Step 5: Compute z = (R-m)/ s.  Step 6: Conclusion: \\nIf z > Za, \\n    then there might be cyclic, seasonality behavior (under-mixing).  If z < - Za, then there might be a trend. \\nIf z < - Za/2, or z > Za/2, reject the randomness.  Note: This test is valid for cases for which both n1 and n2 are large, say greater than 10. For small sample sizes, special tables must be used. \\nFor example, suppose for a given sample of size 50, we have R = 24, n1 = 14 and n2 = 36. Test for randomness at a = 0.05.  The Plugging these into the above formulas we have a  = 16.95, s = 2.473, and z = -2.0 From Z-table, we  have Z = 1.645. Therefore, there might be a trend, which means that the sample is not random. You may use the following JavaScript to Test for Randomness.  \\nTest for NormalityThe standard test for normality is the Lilliefors\\' statistic. A histogram and normal probability plot will also help you distinguish between a systematic departure from normality when it shows up as a curve. \\nLilliefors\\' Test for Normality:    This test is a special case of the Kolmogorov-Smirnov goodness-of-fit test, developed for testing the normality of population\\'s distribution. When  applying the Lilliefors test,  a comparison is made between the standard normal cumulative distribution function, and a sample cumulative distribution function with standardized random variable. If there is a close agreement between the two cumulative distributions,  the hypothesis that the sample was drawn from population with a normal distribution  function is supported. If, however, there is a discrepancy between the two cumulative distribution functions too great to be attributed to chance alone,  then the hypothesis is rejected. The difference between the two cumulative distribution functions is measured by the statistic D, which is the greatest vertical distance between the two functions.  You might like to use the well-known Lilliefors\\' Test for Normality to assess the goodness-of-fit. \\n Further ReadingsThode T., Testing for Normality, Marcel Dekker, Inc., 2001. Contains the major tests for normality. \\n Introduction to EstimationTo estimate means to esteem (to give value to). An estimator is any quantity calculated from the sample data which is used to give information about an unknown quantity in the population. For example, the sample mean is an estimator of the population mean m.  Results of estimation can be expressed as a single value; known as a point estimate, or a range of values, referred to as a confidence     interval. Whenever we use point estimation, we calculate the margin of error associated with that point estimation. Estimators of population parameters are sometimes distinguished     from the true value by using the symbol \\'hat\\'. For example, true population standard deviation s is estimated from a sample population standard deviation. \\nAgain, the usual estimator of the population mean is  = Sxi / n, where n is the size of  the sample and x1, x2, x3,.......,xn are the values of the sample. If the value of the estimator in a particular sample is found to be 5, then 5 is \\n    the estimate of the population mean µ. \\n\\n Qualities of a Good Estimator\\nA \"Good\" estimator is the one which provides an estimate with the following qualities: \\nUnbiasedness:  An estimate is said to be an unbiased estimate of a given parameter when the expected value  of that estimator can be shown to be equal to the parameter being estimated. For example, the mean of a sample is an unbiased estimate \\n    of the mean of the population from which the sample was drawn. Unbiasedness is a good quality for an estimate, since, in such a case, using weighted average of several estimates provides a better estimate than each one of those estimates. Therefore, unbiasedness allows us to upgrade our estimates. For example, if \\n    your estimates of the population mean µ are say, 10, and 11.2 from two independent samples of sizes 20, and 30 respectively, then a better estimate of the population mean µ based on both samples is [20 (10) + 30 (11.2)] (20 + 30) = 10.75. Consistency: \\n    The standard deviation of an estimate is called the standard error of that  estimate. The larger the standard error the more error in your estimate. The standard deviation of an estimate is a commonly used index of the error entailed in estimating a population parameter based on the information in a random sample of size n from the entire population. \\n  An estimator is said to be \"consistent\" if increasing the sample size produces an estimate with smaller standard error. Therefore, your estimate is \"consistent\" with the sample size. That is, spending more money to obtain a larger sample produces a better estimate. Efficiency: An efficient estimate is one which has the smallest standard error among  all unbiased estimators. The \"best\" estimator is the one which is the closest to  the population parameter being estimated.   \\n\\nThe Concept of Distance for an Estimator  Click on the image to enlarge it  and THEN print it \\n The above figure illustrates the concept of closeness by  means of aiming at the center for unbiased with minimum  variance. Each dart board has several samples:  The first one has all its shots clustered tightly together, but none of them hit the center. The second one has a large spread, but around  the center. The third one is worse than the first two. Only the last one has a tight cluster around the center, therefore has good efficiency.  \\nIf an estimator is unbiased, then its variability will determine its reliability. If an estimator is extremely variable, then the estimates it produces may not on average be as close to the population parameter as a biased estimator with small variance. The following chart depicts the quality of a few popular estimators for the population mean µ:      The widely used estimator of the population mean µ is  = Sxi/n, where n is the size of the sample and x1, x2, x3,......., xn are the values of the sample that have all of the above good properties. Therefore, it is a \"good\" estimator. If you want an estimate of central tendency as a parameter for a test or for comparison, then small sample sizes are unlikely to yield any stable estimate. The mean is sensible in a symmetrical distribution as a measure of central tendency; but, e.g., with ten cases, you will not be able to judge whether you have a symmetrical distribution. However, the mean estimate is useful if you are trying to estimate the population sum, or some other function of the expected value of the distribution. Would the median be a better measure? In some distributions (e.g., shirt size) the mode may be better. BoxPlot will indicate outliers in the data set. If there are outliers, the median is better than the mean as a measure of central tendency.  You might like to use Descriptive Statistics Applet for obtaining \"good\" estimates.   Further Readings Casella G., and R. Berger, Statistical Inference, \\n    Wadsworth Pub. Co., 2001.Lehmann E., and G. Casella, Theory of Point Estimation, Springer Verlag, New York, 1998.  \\n Statistics with Confidence  In practice, a confidence interval is used to express the uncertainty   in a quantity being estimated. There is uncertainty because inferences are based   on a random sample of finite size from the entire population or process of interest.   To judge the statistical procedure we can ask what would happen if we were to   repeat the same study, over and over, getting different data (and thus different   confidence intervals) each time.  In most studies, investigators are usually interested in determining     the size of difference of a measured outcome between groups, rather than a simple indication of whether or not it is statistically significant. Confidence  intervals present a range of values, on the basis of the sample data, in which the  value of such a difference may lie. Know that a confidence interval computed from one sample  will be different from a confidence interval computed from another sample.      Understand the relationship between sample size and width  of confidence interval, moreover, know that sometimes the computed confidence interval does not contain the true value.  Let\\'s say you compute a 95% confidence interval for a mean m . The way  to interpret this is to imagine an infinite number of samples from the same population, 95% of the computed intervals will contain the population mean m , and at most 5% will not. However, it is wrong to state, \"I am 95% confident \\n    that the population mean m falls within the interval.\"  Again, the usual definition of a 95% confidence interval is an interval constructed by a process such that the interval will contain  the true value 95% of the time. This means that \"95%\" is a property of the \\n    process, not the interval. Is the probability of occurrence of the population mean greater in the confidence interval (CI) center and lowest at the boundaries? Does the probability of occurrence of the population mean in a confidence interval vary in a measurable way from the center to the boundaries? In a general sense,  normality condition is assumed, and then the interval between CI limits is represented by a bell shaped t distribution. The expectation (E) of another value is highest at the calculated mean value, and decreases as the values approach the CI limits. \\n\\nTolerance Interval and CI: A good approximation for the single measurement tolerance interval is n½ times confidence interval of the mean.  Statistics with Confidence Click on the image to enlarge it and THEN print it \\n \\n\\n\\nYou need to use Sample Size Determination JavaScript at the design stage of your statistical investigation in decision making with specific subjective requirements.\\n\\nA Note on Multiple Comparison via Individual Intervals: Notice that, if the confidence intervals from two samples do not overlap, there is a statistically significant difference, say at 5%. However, the other way is not true; two confidence intervals can overlap even when there is a significant difference between them. As a numerical example, consider the means of two independent samples. Suppose their values are 10 and 22 with equal standard error of 4.  The 95% confidence interval for the two statistics (using the critical value of 1.96) are:  [2.2, 17.8] and [14.2, 29.8], respectively. As you see they display considerable overlap. However, the z-statistic for the two-population mean is: |22 -10|/(16 + 16)½ = 2.12 which is clearly significant under the same conditions as applied for constructing the confidence intervals. One should examine the confidence interval for the difference explicitly. Even if the confidence intervals are overlapping, it is hard to find the exact  overall confidence level. However, the sum of individual confidence levels can serve as an upper limit. This is evident from the fact that: P(A and B) £ P(A) + P(B). \\n  The Confidence Interval JavaScript demonstrates the precision vs confidence. Further Reading:\\nCohen J., Statistical Power Analysis for \\n    the Behavioral Sciences, L. Erlbaum Associates, 1988.Kraemer H., and S. Thiemann, How Many Subjects? Provides basic sample size tables , explanations, and power analysis.Murphy K., and B. Myors, Statistical Power Analysis, L. Erlbaum Associates, 1998. Provides a simple and general sample size determination for hypothesis  tests.Newcombe R., Interval estimation for the difference between independent proportions: Comparison of eleven methods, Statistics in Medicine, 17, 873-890, 1998.\\n Hahn G. and W. Meeker, Statistical Intervals: A Guide for Practitioners, \\n  Wiley, 1991.Schenker N., and J. Gentleman, On judging the significance of differences by examining the overlap between confidence intervals, The American Statistician, 55(2), 135-139, 2001.\\n     What Is the Margin of Error?\\nEstimation is the process by which sample data are used to indicate \\n  the value of an unknown quantity in a population.  Results of estimation can be expressed as a single value, known as a point estimate; or a range of values, referred to as a confidence \\n    interval.  Whenever we use point estimation, we calculate the margin  of error associated with that point estimate. For example, for the estimation of the population proportion, by the means of sample proportion (p), the margin of error is calculated often as follows: \\n  ±1.96 [p(1-p)/n]1/2 In newspapers and television reports on public opinion polls, the margin of error often appears in a small font at the bottom  of a table or screen. However, reporting the amount of error only, is not informative enough by itself, what is missing is the degree of the confidence in the findings. The more important missing piece of  information is the sample size n; that is, how many people \\n    participated in the survey, 100 or 100000? By now, you know well that the larger the sample size the more accurate is the finding, right?  The reported margin of error is the margin of \"sampling error\".  There are many non-sampling errors that can and do affect the accuracy of polls.  Here we talk about sampling error. The fact that sub-groups might have sampling error larger than the group, one must include the following statement in the report:\\n\\n \"Other sources of error include, but are not limited to, individuals refusing to participate in the  interview and inability to connect with the selected number. Every feasible  effort was made to obtain a response and reduce the error, but the reader (or the viewer) should be aware that some error is inherent in all research.\" \\n If you have a yes/no question in a survey, you probably want to calculate a proportion P of Yes\\'s (or No\\'s). In a simple random sample survey, the variance of p is p(1-p)/n, ignoring the finite population correction,  for large n, say over 30. Now a 95% confidence interval is \\n  p - 1.96 [p(1-p)/n]1/2,\\xa0\\xa0 p + 1.96 \\n      [p(1-p)/n]1/2.      A conservative interval can be calculated, since p(1-p) takes  its maximum value when p = 1/2. Replace 1.96 by 2, put p = 1/2 and you have  a 95% consevative confidence interval of 1/n1/2.  This approximation works well as long as p is not too close to 0 or 1. This useful approximation allows you to calculate approximate 95% confidence intervals.      For continuous random variables, such as the estimation of the   population mean m, the margin of error is  calculated  often as follows: \\n  ±1.96 S/n1/2.   The margin of error can be reduced by one or a combination of  the following strategies: \\n\\n Decreasing the confidence in the estimate -- an undesirable strategy since confidence relates to the chance of drawing the wrong conclusion (i.e., increases  the Type II error). Reducing the standard deviation -- something we cannot do since it is usually a static property of the population.  Increasing the sample size -- this provides more information for a better decision. \\nYou might like to use Descriptive Statistics Applet to check your computations, and Sample Size Determination JavaScript at the design stage of your statistical investigation in decision making with specific subjective requirements.\\n\\nFurther Reading Levy P., and S. Lemeshow, Sampling  of Populations: Methods and Applications, Wiley, 1999.    Bias Reduction Techniques: Bootstrapping and Jackknifing   Some  inferencial  statistical techniques do not require distributional assumptions about the statistics involved. These modern non-parametric methods use large amounts of computation to explore the empirical variability of a statistic, rather than making a priori assumptions about this variability, as is done in the traditional parametric t- and z- tests. \\n\\nBootstrapping: Bootstrapping method is to obtain an estimate by combining estimators to each of many sub-samples of a data set. Often M randomly drawn samples of T observations are drawn from the original data set of size n with replacement, where T is less n. \\n\\nJackknife Estimator: A jackknife estimator creates a series of estimate, from a single data set by generating that statistic repeatedly on the data set leaving one data value out each time. This produces a mean estimate of the parameter and a standard deviation of the estimates of the parameter. \\nMonte Carlo simulation allows for the evaluation of the behavior of a statistic when its mathematical analysis is intractable. Bootstrapping and jackknifing allow inferences to be made from a sample when traditional parametric inference  fails. These techniques are especially useful to deal with statistical problems, such as small sample size, statistics with no well-developed distributional  theory, and parametric inference condition violations. Both are computer intensive. Bootstrapping means you take repeated samples from a sample and then make statements about a population. Bootstrapping entails sampling-with-replacement from a sample. Jackknifing involves systematically doing n steps, of omitting 1 case from a sample at a time, or, more generally, n/k steps of omitting k cases; computations that compare \"included\" vs. \"omitted\" can be used (especially) to reduce the bias of estimation.  Both have applications in reducing bias in estimations. Resampling -- including the bootstrap, permutation, and other non-parametric tests -- is a method for hypothesis testing, confidence limits, and other applied problems in statistics and probability. It involves no formulas or tables.  Following the first publication of the general technique (and the bootstrap) in 1969 by Julian Simon and subsequent independent development by Bradley Efron, resampling has become an alternative approach for testing hypotheses. There are other findings:  \"The bootstrap started out as a good notion in that it presented, in theory, an elegant statistical procedure that was free of distributional conditions. In practice the bootstrap technique doesn\\'t work very well, and the attempts to modify it make it more complicated and more confusing than the parametric procedures that it was meant to replace.\" While resampling techniques may reduce the bias, they achieve this at the expense of increase in variance. The two major concerns are:  The loss in accuracy of the estimate as measured by variance can be very large.   The dimension of the data affects drastically the quality of the samples and therefore the estimates.  Further Readings: Young G., Bootstrap: More than a Stab in the Dark?, Statistical Science, l9, 382-395, 1994. Provides the  pros and cons on the bootstrap methods.Yatracos Y., Assessing the quality of bootstrap samples and of the bootstrap estimates obtained with finite resampling, Statistics and Probability Letters,     59, 281-292, 2002.  \\n Prediction Intervals\\n  In many application of business statistics, such as forecasting, we are interested in construction of a statistical interval for random variable, rather than a parameter of a population distribution.  The Tchebysheff\\'s inequality is often used to put bounds on the probability  that  a proportion of random variable X will be within k > 1 standard deviation of the mean m for any probability distribution. In other words:   P [|X - m| ³ k s] £ 1/k2,\\xa0 \\xa0for any k greater than 1  The symmetric property of Tchebysheff\\'s inequality is useful;  e.g., in constructing control limits in the quality control process. However, the limits are very conservative due to lack of knowledge about the underlying  distribution.  The above bounds can be improved (i.e., becomes tighter) if we have some  knowledge about the population distribution. For example, if the population  is homogeneous; that is, its distribution is unimodal; then,     P [|X - m| ³ k s] £ 1/(2.25k2),\\xa0 \\xa0for any k greater than 1.  The above inequality is known as the Camp-Meidell inequality. \\nNow, let X be a random variable distributed normally with estimated mean  and standard deviation S, then a prediction interval  for the sample mean  with 100(1- a)% confidence level is:     ± ta/2 ´ S ´ (1+1/n)1/2.         This is the range of a random variable  with 100(1- a)% confidence, using t-table. Relaxing the normality \\n  condition for sample-mean prediction interval, requires a large sample size, say n over 30. \\nFurther Readings: Grant E., and R. Leavenworth, Statistical Quality Control, McGraw-Hill, 1996.Ryan T., Statistical Methods for Quality Improvement, John Wiley & Sons, 2000. A very good book for a starter.   What Is a Standard Error?For statistical inference, namely statistical testing and estimation, one needs to estimate the population\\'s parameter(s). Estimation involves the determination, with a possible error due to sampling, of the unknown value of a population parameter, such as the proportion having a specific attribute or the average value m of some numerical measurement. To express the accuracy of the estimates of population characteristics, one must also compute the standard errors of the estimates. These are measures of accuracy that determine the possible errors arising from the fact that the estimates are based on random samples from the entire population, and not on a complete population census. Standard error is a statistic indicating the accuracy of an estimate. That  is, it tells us to assess how different the estimate (such as ) is from the population parameter (such as m).   It is therefore, the standard deviation of a sampling distribution of the estimator such as .  The following is a collection of standard errors for the widely used statistics: \\n Standard Error for the  Mean   is:  S/n½.  \\nAs one expects, the standard error decreases as the sample size increases.  However the standard deviation of the estimate decreases by a factor of n½ not n.  For example, if you wish to reduce the error by 50%, the sample size must be 4 times n, which is expensive.  Therefore, as an alternative to increasing sample size, one may reduce the error by obtaining \"quality\" data that provide a more accurate estimate. For a finite population of size N, the standard error of the sample mean of size n, is:  S ´ [(N -n)/(nN)]½. \\nStandard Error for the Multiplication of Two Independent Means  1 ´ 2 is: {1 S22/n2 + 2 S12/n1}½. \\nStandard Error for Two Dependent Means  1 ± 2 is: {S12/n1 +  S22/n2 + 2 r ´ [(S12/n1)(S22/n2)]½}½.\\nStandard Error for the Proportion P is:  [P(1-P)/n]½\\nStandard Error for P1 ± P2, Two Dependent Proportions is: {[P1 + P2 - (P1-P2)2] / n}½.\\nStandard Error of the Proportion (P) from a finite population is:  [P(1-P)(N -n)/(nN)]½. The last two formulas for finite population are frequently used when we wish to compare a sub-sample of size n with a larger sample of size N, which contains the sub-sample. In such a comparison, it would be wrong to treat the two samples \"as if\" there were two independent samples. For example, in comparing the two means one may use the t-statistic but with the standard error: SN [(N -n)/(nN)]½\\nas its denominator. Similar treatment is needed for proportions. \\n\\n   Standard Error of the Slope (m) in  Linear Regression is  Sres / Sxx½,   where  Sres is the residual\\' standard deviation.Standard Error of the Intercept (b) in  Linear Regression is:   Sres[(Sxx + n ´ 2) /(n ´ Sxx] ½.   Standard Error of the Predicted Value using a Linear Regression is:  Sy(1 - r2)½.\\nThe term (1 - r2)½ is called the coefficient of alienation. Therefore if r = 0, the error of prediction is Sy as expected.\\nStandard Error of  the  Linear Regression is:  Sy (1 -  r2)½. Note that if r = 0, then the standard error reaches its maximum possible value, which is standard deviation in Y.\\n\\n\\n\\nStability of an estimator: An estimator is stable if, by taking two different samples of the same size, they produce two estimates having \"small\" absolute difference.   The stability of an estimator is measured by its reliability:\\n\\n\\nReliability of an estimator = 1 / (its standard error)2\\n\\n\\nThe larger the standard error, the less reliable is the estimate.   Reliability of estimators is often used to select the \"best\" estimator among all unbiased estimators.\\n Sample Size Determination\\nAt the planning stage of a statistical investigation, the question of sample size (n) is critical. This is an important matter NOT to be taken lightly. To take a larger sample than is needed to achieve the desired results is wasteful of resources, whereas very small samples often lead to what are no practical use of making good decisions. The main objective is to obtain both a desirable accuracy and a desirable confidence level with minimum cost. Students sometimes ask me, what fraction of the population do you need for \\n  good estimation? I answer, \"It\\'s irrelevant; accuracy is determined by sample size.\" This answer has to be modified if the sample is a sizable fraction  of the population. The confidence level of conclusions drawn from a set of data depends on the size of the data set. The larger the sample, the higher is the associated confidence. However, larger samples also require more effort and resources. Thus, your goal must be to find the smallest sample size that will provide the desirable confidence. For an item scored 0 or 1, for no or yes, the standard error (SE) of the estimated proportion p, based on your random sample observations, is given by: \\n  SE = [p(1-p)/n]1/2  \\n where p is the proportion obtaining a score of 1, and n is the sample size. This SE is the standard deviation of the range of possible estimate values. The SE is at its maximum when p = 0.5, therefore the worst case scenario occurs when 50% are yes, and 50% are no. Under this extreme condition, the sample size, n, can then be expressed as the largest integer less than or equal to:  n = 0.25/SE2 To have some notion of the sample size, for example for SE to be 0.01 (i.e. 1%), a sample size of 2500 will be needed; 2%, 625; 3%, 278; 4%, 156, 5%, 100. \\nNote, incidentally, that as long as the sample is a small fraction of the total population, the actual size of the population is entirely irrelevant for the purposes of this calculation. Pilot Studies: When the needed estimates for sample size calculation is not available from an existing database, a pilot study is needed for adequate estimation with a given precision. A pilot, or preliminary, sample   must be drawn from the population, and the statistics computed from this sample are used in determination of the sample size. Observations used in the pilot sample may be counted as part of the final sample, so that the computed sample size minus the pilot sample size is the number of observations needed to satisfy \\n  the total sample size requirement.  Sample Size with Acceptable Absolute Precision: The following present the widely used method for determining the sample size \\n  required for estimating a population mean and proportion. Let us suppose we want an interval that extends d unit on either side of the estimator. We can write \\nd = Absolute Precision = (reliability coefficient) ´  (standard error) = Za/2 ´ (S/n1/2)  Suppose, based on a pilot sample of size n, the estimated proportion is p, then  the required sample size with the absolute error size not exceeding d, with 1- a confidence is:  \\n    [t2 n p(1-p)] / [t2 p(1-p) - d2 (n-1)],where t = t a/2 being the value taken from the t-table with parameter d.f. = n = n-1, corresponding to the desired 1- a confidence interval. For large pilot sample sizes (n), say over 30, the simplest sample size determinate is: \\n[(Za/2)2 S2] / d2\\xa0 \\xa0\\xa0 for the \\n  Mean m  [(Za/2)2 p(1-p)] / d2\\xa0 \\xa0\\xa0 for the proportion, \\n  where d is the desirable margin of error (i.e., the absolute error), which is the half-length of the confidence interval with 100(1- a)% confidence interval. Sample Size with Acceptable Type I and Type II Errors:   One may use the following sample size determinate, which is based on the size \\n  of type I and Type II errors:   2(Za/2 + Zb/2)2S2/d2,   where a and b are the desirable type I, and type II errors, respectively. S2 is the variance obtained from the pilot run, and d is the difference between the null and alternative (m0 -ma). Sample Size with Acceptable Relative Precision: You may use the following sample size determinate for a desirable relative error\\nD in %, which requires an estimate of the coefficient of variation (CV in %) from a pilot sample with size over 30:   [(Za/2)2 (C.V.)2] / D2\\xa0 Sample Size Based on the Null and an Alternative: One may use power of the test to determine the sample size. The functional  relation of the power and the sample size is known as the operating characteristic curve. On this curve, as sample size increases, the power function increases rapidly. Let d be such that:   ma = m0 + d\\n   is an alternative to represent departure from the null hypothesis. We wish to be reasonably confident to find evidence against the null, if in fact the particular  alternative holds. That is, the type error b, is the probability of failing to find evidence at least at level of a, when the alternative holds. This implies   Required sample size = (z1 + z2) S2/ d2\\n    Where: z1 = |mean - m0|/ SE, z2 = |mean - ma|/ SE, the mean is the current estimate for m, and S is the current estimate for s. \\n All of the above sample size determinates could also be used for estimating the mean of any unimodal population, with discrete or continuous random variables, provided the pilot run size (n) is larger than (say) 30. In estimating the sample size, when the standard deviation is not known, instead of S2 one may use 1/4 of the range for sample \\n   size over 30 as a \"good\" estimate for the standard deviation. It is a good practice to compare the result with IQR/1.349. One may extend the sample size determination to other useful statistics, such as correlation coefficient (r) based on acceptable Type I and Type II errors: \\n 2 + [(Za/2 + Zb/2( 1- r2) ½)/r] 2\\nprovided r is not equal to  -1, 0, or 1.\\n The aim of applying any one of the above sample size determinates is at improving \\n  your pilot estimates at feasible costs. You might like to use Sample Size Determination JavaScript to check your computations. Further Reading:\\nKish L., Survey Sampling, Wiley, 1995.\\n  Murphy K., and B. Myors, Statistical Power Analysis, L. Erlbaum Associates, 1998. Provides a simple and general sample size determination for hypothesis  tests. \\n Revising the Expected Value and the  Variance\\nAveraging Variances: What is the mean variance  \\n of k variances without regard to differences in their sample sizes? The answer  is simply: \\n      Average of Variances = [SSi2] / k However, what is the variance of all k groups combined?  The answer must consider the sample size ni of the ith group: Combined Group Variance = S ni[Si2 + di2]/N,  \\n where di = meani - grand mean, and N = S ni, for all i = 1, 2, .., k. \\n  Notice that the above formula allows us to split up the total variance into its two component parts. This splitting process permits us to determine the extent to which the overall variation is inflated by the difference between group means. What the variation would be if all groups had the same mean?  ANOVA is a well-known application of this concept where the equality of several means is tested.\\n\\n\\nSubjective Mean and Variance: In many applications, we saw how to make decisions based on objective data; however, an informative decision-maker might be able to combine  his/her subjective input and the two sources of information.\\n\\nApplication: Suppose the following information is available from two independent sources:\\n\\n   Revising the Expected Value and the  Variance\\n Estimate SourceExpected value Variance\\n Sales manager\\nm1 = 110\\ns12 = 100\\n Market survey\\nm2 = 70\\ns22 = 49\\n\\n\\n\\nThe combined expected value is:\\n\\n\\n[m1/s12 + m2/s22 ] / [1/s12 + 1/s22]\\n\\nThe combined variance is:\\n2 / [1/s12 + 1/s22]\\nFor our application, using the above tabular information, the combined estimate of expected sales is 83.15 units with combined variance of 65.77.\\n\\n\\nYou might like to use Revising the Mean and Variance JavaScript in performing some numerical experimentation.  You may apply it  for validating the above example and for a deeper understanding of the concept where more than two sources of information are to be combined.\\n Subjective Assessment of Several Estimates Based on Relative Precision\\nIn many cases, we may wish to compare several estimates of the same parameter. The simplest approach is to measure the closeness among the estimates in an attempt to determine that at least one of the estimates is more than r times the parameter away from the parameter, where r is a subjective, non-negative number  less than one. \\n\\nYou might like to use Subjective Assessment of Estimates JavaScript to isolate any inaccurate estimate.  By repeating the same process you might be able to remove all inaccurate estimates. \\nFurther Reading:\\nTsao H. and T. Wright, On the maximum ratio: A tool for assisting inaccuracy assessment,  The American Statistician, 37(4), 1983.\\n  \\n\\nManaging the Producer\\'s or the Consumer\\'s Risk\\nThe logic behind a statistical test of hypothesis is similar to the following logic. Draw two lines on a paper and determine whether they are of different lengths. You compare them and say, \"Well, certainly they are not equal. Therefore they must be of different lengths.  By rejecting equality, that is, the null hypothesis, you assert that there is a difference. \\n\\nThe power of a statistical test is best explained by the overview of the Type I and Type II errors. The following matrix shows the basic representation of these errors.\\n\\n     As indicated in the above matrix a Type-I error occurs   when, based on your data, you reject the null hypothesis when in fact it is true.   The probability of a type-I error is the level of significance of the test of \\n  hypothesis and is denoted by a . \\n\\nType-I error is often called the producer\\'s risk that consumers reject a good product or service indicated by the null hypothesis. That is, a producer introduces a good product, in doing so, he or she take a risk that consumer will reject it.\\nA type II error occurs when you do not reject the  null hypothesis when it is in fact false. The probability of a type-II  error is denoted by b . The quantity 1 - b is known as the Power of a Test.  A Type-II error can be evaluated for any specific alternative hypotheses stated  in the form \"Not Equal to\" as a competing hypothesis.\\n\\nType-II error is often called the consumer\\'s risk for not rejecting possibly a worthless product or service indicated by the null hypothesis.\\n Students often raise questions, such as what are the \\'right\\' confidence intervals,  and why do most people use the 95% level? The answer is that the decision-maker must consider both the Type I and II errors and work out the best tradeoff. Ideally one wishes to reduce the probability of making these types of error; however, for a fixed sample size, we cannot reduce one type of error without at the same time increasing the probability of another type of error. Nevertheless,  to reduce the probabilities of both types of error simultaneously is to increase  the sample size. That is, by having more information one makes a better decision. \\n\\nThe following example highlights this concept. A electronics firm, Big Z, manufactures and sells a component part to a radio manufacturer, Big Y. Big Z consistently maintain a component part failure rate of 10% per 1000 parts produced. Here Big Z is the producer and Big Y is the consumer. Big Y, for reasons of practicality, will test sample of 10 parts out of lots of 1000. Big Y will adopt one of two rules regarding lot acceptance:\\n\\n\\nRule 1: Accept lots with one or fewer defectives; therefore, a lot has either 0 defective or 1 defective.\\nRule 2: Accept lots with two or fewer defectives; therefore, a lot has either 0,1, or 2 defective(s).\\n\\n\\nOn the basis of the binomial distribution, the P(0 or 1) is 0.7367. This means that, with a defective rate of .10, the Big Y will accept 74% of tested lots and will reject  26% of the lots even though they are good lots. The 26% is the producer\\'s risk or the a  level. This a  level is analogous to a Type I error -- rejecting a true null. Or, in other words, rejecting a good lot. In this example, for illustration purposes, the lot represents a null hypothesis. The rejected lot goes back to the producer; hence, producer\\'s risk. If Big Y is to take rule 2, then the producer\\'s risk decreases. The P(0 or, or 1, or 2) is 0.9298 therefore, Big Y will accept 93% of all tested lots, and  7% will be rejected, even though the lot is acceptable. The primary reason for this is that, although the probability of defective is .10, the Big Y through rule 2 allows for a higher defective acceptance rate. Big Y increases its own risk (consumer\\'s risk), as stated previously.\\nMaking Good Decision: Given that there is   a relevant profit (which could be negative) for the outcome of your decision, and a prior probability (before testing) for the null hypothesis to be true,  the objective is to make a good decision. Let us denote the profits for each  cell in the decision table as $a, $b, $c and $d (column-wise), respectively.  The expectation of profit is [aa + (1-a)b], and + [(1-b)c + bd], depending whether the null is true. \\nNow having a prior (i.e., before testing) subjective probability of p that  the null is true, then the expected profit of your decision is:   Net Profit = [aa + (1-a)b]p + [(1-b)c + bd](1-p) - Sampling cost A good decision makes this profit as large as possible. To this end, we must suitably choose the sample size and all other factors in the above profit \\n  function. Note that, since we are using a subjective probability expressing the strength   of belief assessment of the truthfulness of the null hypothesis, it is called a Bayesian Approach to statistical decision making, which is a standard approach in decision theory.\\nYou might like to use the Subjectivity in Hypothesis Testing JavaScript applet in performing some numerical experimentation for validating the above assertions for a deeper understanding. \\nFurther Reading:\\nCochran W., Planning and Analysis of Observational Studies, Wiley, 1983. \\n Hypothesis Testing: Rejecting a Claim\\nTo perform a hypothesis test, one must be very specific about the test one wishes to perform. The null hypothesis must be clearly stated, and the data must be collected in a repeatable manner. If there is \\nany subjectivity, the results are technically not valid. All of the analyses, including the sample size, significance level, the time, and the budget, must be planned in advance, or else the user runs the risk of \"data diving\". \\nHypothesis testing is mathematical proof by contradiction. \\n  For example, for a Student\\'s t test comparing two groups, we assume that the two  groups come from the same population (same means, standard deviations, and in   general same distributions). Then we do our best  to prove that this   assumption is false. Rejecting H0 means either H0 is false, or a rare event as has occurred. The real question is in statistics not whether a null hypothesis is correct, but  whether it is close enough to be used as an approximation. \\n\\n\\n\\nTest of HypothesesClick on the image to enlarge it and THEN print it\\n \\nIn most statistical tests concerning m, we start by assuming the s2, and the higher moments, such as skewness and kurtosis, are equal. Then, we hypothesize that the a\\'s are equal wich is null hypothesis.  The \"null\" often suggests no difference between group means, or no relationship between quantitative variables, and so on.  Then we test with a calculated t-value. For simplicity, suppose we have a  two-sided test. If the calculated t is close to 0, we say \"it is good\", as we expected.  If the calculated t is far from 0, we say, \"the chance of getting this value  of t, given my assumption that the populations are statistically the same,  is so small that I will not believe the assumption. We will say that the populations  are not equal; specifically the means are not equal.\"   As an example, sketch a normal distribution with mean 1 - 2 and standard deviation s. If the null hypothesis is true,   then the mean is 0. We calculate the \\'t\\' value, as per the equation. We look  up a \"critical\" value of t. The probability of calculating a t value more extreme ( + or - ) than this, given that the null hypothesis is true, is equal or less than the a risk we used in pulling the critical value from the table. Mark the calculated t, and critical t (both sides) on the sketch of the distribution. Now, if the calculated t is more extreme than  the critical value, we say, \"the chance of getting this t, by shear chance,  when the null hypothesis is true, is so small that I would rather say the  null hypothesis is false, and accept the alternative, that the means are not equal.\" When the calculated value is less extreme than the calculated value,  we say, \"I could get this value of t by shear chance. I cannot detect a difference in the means of  the two groups at the a significance level.\" \\n  In this test,  we need (among others) the condition that the population variances  (i.e., treatment impacts on central tendency but not variability) are equal. However, this test is robust to violations of that condition if n\\'s are large and almost the same size. A counter example would be to try a t-test between (11, 12, 13) and (20, 30, 40). The pooled and unpooled tests both give t statistics of 3.10, but the degrees of freedom are different: d.f. = 4  (for pooled) or d.f. about 2 (for unpooled). Consequently the pooled test gives p = .036 and the unpooled p = .088. We  could go down to n = 2 and get something still more extreme. You might like to use Online Statistical Computation, Testing the Mean, and Testing the Variance in performing more of these tests. \\n  \\nYou might need to use Sample Size Determination JavaScript at the design stage of your statistical investigation in decision making with specific subjective requirements.\\n\\n  Classical Approach to Testing  Hypotheses In this treatment there are two parties: One party (or a person) proposes the \\n  null hypothesis (the claim). Another party proposes an alternative hypothesis. A significance level a and a sample size n are agreed upon by both parties. The next step is to compute the relevant statistic based on the null hypothesis and the random sample of size n. Finally, \\n  one determines the rejection region. The conclusion based on this approach is as follows: \\n  If the computed statistic falls within the rejection region, then Reject the null hypothesis; otherwise Do Not Reject the null hypothesis (the claim). \\n  You may ask: How do you determine the critical value (such as z-value) for the rejection interval for one and two-tailed hypotheses?. What is the rule? First, you have to choose a significance level a.  Knowing that the null hypothesis is always in \"equality\" form then, the alternative \\n    hypothesis has one of the three possible forms: \"greater-than\", \"less-than\", or \"not equal to\". The first two forms correspond to a one-tail hypothesis while the last one corresponds to a two-tail hypothesis.  \\n If your alternative is in the form of \"greater-than\", then z is the value  that gives you an area to the right tail of the distribution  that is equal to a.   If your alternative is in the form of \"less-than\", then z is the value \\n    that gives you an area to the left tail of the distribution that is equal to a.  If your alternative is in the form of \"not equal to\",  then there are two z values, one positive and the other negative. The positive z is the value that gives you an a/2 area to the right tail of the distribution. While, the negative z is the value that gives you an a/2 area to the left tail of the distribution.  The above rule can be generalized and implemented for determining the critical value for any test of hypothesis, you must first master reading the statistical tables, because, as you see, not all tables in your textbook are presented in the same format. \\n The Meaning and Interpretation of P-values (what the data  say?) The p-value, which directly depends on a given sample attempts to provide a \\n  measure of the strength of the results of a test for the null hypothesis, in contrast to a simple reject or do not reject in the classical approach to the test of hypotheses. If the null hypothesis is true, and if the chance of random variation is the only reason for sample differences, then the p-value is a quantitative measure to feed into the decision-making process as evidence. The following table provides a reasonable interpretation of p-values:     \\n   P-value  \\nInterpretation P < 0.01  very strong evidence against H0    0.01£ P < 0.05  moderate evidence against H0    0.05 £ P < 0.10 \\n        suggestive evidence against H0  \\n 0.10 £ P  little or no real evidences against H0    This interpretation is widely accepted, and many scientific journals routinely publish papers using this interpretation for the result of a  test of hypothesis. For the fixed-sample size, when the number of realizations is decided in \\n    advance, the distribution of p is uniform, assuming the null hypothesis is true. We  would express this as P(p £ x) = x. That means the criterion of p £ 0.05 achieves a of 0.05.  Understand that the distribution of p-values under null hypothesis H0 is uniform, and thus does not depend on a particular \\n    form of the statistical test. In a statistical hypothesis test, the P value  is the probability of observing a test statistic at least as extreme as the  value actually observed, assuming that the null hypothesis is true. The value  of p is defined with respect to a distribution. Therefore, we could call it \"model-distribution hypothesis\" rather than \"the null hypothesis\".  In short, it simply means that, if the null had been true, the p-value is \\n    the probability against the null in that case. The p-value is determined by the observed value; however, this makes it difficult to even state the inverse of p.  Finally, since the p-values are random variables, one cannot compare several p-values for any statistical conclusions \\n    (nor order them).  This is a common mistake many people do, therefore, the above table is not intended for such a comparison. You might like to use The P-values for the Popular Distributions JavaScript. \\n  Further Readings: Arsham H., Kuiper\\'s P-value as a Measuring Tool and Decision Procedure for the Goodness-of-fit Test, Journal of Applied Statistics, Vol. 15, No.3, 131-135, 1988. Good Ph.., Resembling Methods: A Practical Guide to Data Analysis, \\n    Springer Verlag, 1999.     Blending the Classical and the P-value Based Approaches in Test of Hypotheses\\n  A p-value is a measure of how much evidence you have against the null hypothesis. Notice that the null hypothesis is always in = form, and does not contain any forms of inequalities. The smaller the p-value, the more evidence you have. In this setting, the p-value is based on the hull hypothesis and has nothing to do with an alternative hypothesis and therefore with the rejection region. In recent years, some authors try to use the mixture of the classical and the p-value approaches. It is  based on the critical value obtained from given a,  the computed statistics and the p-value. This is a blend of two different schools of thought. In this setting, some textbooks compare \\n  the p-value with the significance level to make decisions on a given test of hypothesis. The larger the p-value is when compared with a (in one-sided alternative hypothesis, and a/2 for the two sided alternative hypotheses), the  less evidence we have for rejecting the null hypothesis. In such a comparison, if the p-value is less than some threshold (usually 0.05, sometimes a bit larger like 0.1 or a bit smaller like 0.01) then you reject the null hypothesis. The following deal with such a combined approach. Use of P-value and a: In this setting, we must also consider the alternative hypothesis in drawing the rejection region. There is only one p-value to compare with a (or a/2). Know that, for any test of hypothesis, there is only  one p-value. The following outlines the computation of the p-value and the decision process involved in a given test of hypothesis:  P-value for One-sided  Alternative Hypotheses: The p-value is defined as the area under the right tail of distribution, if the rejection region in on the right tail; if the rejection region is on the left  tail, then the p-value is the area under  the left tail (in one-sided alternative  hypotheses).  P-value for Two-sided  Alternative Hypotheses: If the alternative hypothesis is  two-sided (that is, rejection regions are both on the left and on the right tails), then the p-value is the area under the right tail or to the left tail of  the distribution, depending on whether the computed statistic is closer to the right rejection region or left rejection region.     \\n\\nFor symmetric densities (such as t-density), the left and right tails p-values are the same. However, for non-symmetric densities (such as Chi-square) use the smaller of the two. This makes the test more conservative. Notice that,  for a two sided-test alternative hypotheses, the p-value is never greater than 0.5.  \\nAfter finding the p-value as defined here, you compare it with a pre-set a value for one-sided tests, and with a/2 for two sided-test. The larger the p-value is when compared with a (in one-sided alternative hypothesis, and  a/2 for the two sided alternative hypotheses), the less evidence we have for rejecting the null hypothesis.  To avoid looking-up the p-values from the limited statistical tables  given in your textbook, most professional statistical packages such as  SAS and  SPSS provide the two-tailed p-value. Based on where the rejection region is, you must find out what p-value to use.  Some textbooks have many misleading statements about p-value and its applications. For example, in many textbooks you find the authors double the p-value to compare it with a when dealing with the two-sided test of hypotheses. One wonders how they do it in the case when \"their\" p-value exceeds 0.5? Notice that, while it is correct to compare the p-value with a for a one sided tests of hypotheses a,  for two-sided hypotheses, one must compare the p-value with a/2, NOT a with 2 times p-value, as some textbooks advise.  While the decision is the same, there is a clear distinction here and \\n    an important difference, which the careful reader will note. How to set the appropriate a value? You may have wondered why a = 0.05 is so popular in a test of hypothesis. a = 0.05 is \\n    traditional for tests, but is arbitrary in its origins suggested by R.A. Fisher, who suggested it in the spirit of 0.05 being the biggest p-value at which one would think maybe the null hypothesis in a statistical experiment was to be considered false. This was also a tradeoff between \"type I error\" and  \"type II error\"; that we do not want to accept the wrong null hypothesis, but  we do not want to fail to reject the false null hypothesis, either. \\nAs a final note, the average of these two p-values is often called the mid-p value. \\nConversions from two-sided to one-sided probabilities: Let C be the probability for a two-sided confidence interval (CI) constructed for an estimate. The probability (C1) that either the estimate is greater than the lower limit or that it is less than the upper limit can be computed by using:\\n\\n\\nC1 = C/2 + 1/2, \\xa0 \\xa0\\xa0 \\xa0for conversion to one-sided\\n\\n\\nNumerical Example: Suppose you wish to convert a C = 90% two-sided CI into a one-sided, then   C1 = 0.90/2  + 1/2 = 95%.\\n\\nYou might need to use Sample Size Determination JavaScript at the design stage of your statistical investigation in decision making with specific, subjective requirements.\\n\\n Bonferroni Method for Multiple P-Values Procedure\\n  One may combine several t-tests by using the Bonferroni method. It works reasonably well when there are only a few tests, but as the number of comparisons increases above 8, the value of \\'t\\' required to conclude that a difference exists becomes much larger than it really needs to be, and the method becomes over conservative. \\n  One way to make the Bonferroni t-test less conservative is to use the estimate of the population variance computed from within the groups in the analysis  of variance.     t = ( 1 -2 )/ ( s2 / n1 + s2 / n2 )1/2,   where s2 is the     population variance computed within the groups.    Hommel\\'s Multiple P-Values Procedure: This test can be summarized as follows:   Suppose we have n number of P-values: p(i), i =1, .., n, in ascending order corresponding  to independent tests. Let j be the largest integer, such as:  p(n-j+k) > ka/j, \\xa0 \\xa0for all k=1,.., ,j.     If no such j exists, reject all hypotheses; otherwise, reject all hypotheses  with p(i) £ a / j. This provides a strong control of the family-wise error rate at a level. There are other improvements on the Bonferroni adjustment when multiple tests  are independent or positively dependent. However, the Hommel\\'s method is the  most powerful compared with other methods. Further Readings: Hommel G., Bonferroni procedures for logically related hypotheses, Journal of Statistical Planning and Inference, \\n    82, 119-128, 1999.Kost  J., and  M. McDermott, Combining dependent P-values, Statistics and Probability Letters, 60,  183-190, 2002.Wasteful P., and S. Young, Resembling-Based Multiple Testing: Examples and Methods for P-Value Adjustment, Wiley, 1992. Wright S., Adjusted P-values for simultaneous inference, Biometrics,  48, 1005-1013, 1992.    \\nPower of a Test and the Size Effect The power of a test plays the same role in hypothesis testing that Standard Error played in estimation. It is a measuring tool for assessing the accuracy of a test or in comparing two competing test procedures. The power of a test is the probability of  rejecting a false null hypothesis when the null hypothesis is false.   This probability is inversely related to the probability of making a Type  II error, not rejecting the null hypothesis when it is false . Recall that we choose the probability of making a Type I error when  we set a. If we decrease the probability of making a Type I error, then  we increase the probability of making a Type II error. Therefore,  there are basically two errors possible when conducting a statistical analysis; type I error and and type II error: Type I error - (producer\\'s) risk of rejecting the null hypothesis when it is in fact true. Type II error - (consumer\\'s) risk of not rejecting the null hypothesis  when it is in fact false. Power and Alpha (a): Thus, the probability of not rejecting  a true null has the same relationship to Type I errors as the probability of correctly rejecting an untrue null     does to Type II error. Yet, as I mentioned if we decrease the odds of making one type of error we increase the odds of making the other type of error. What is the relationship between Type I and Type II errors? For a fixed sample size, decreasing one type of error increases the size of the other one.  Power and the Size Effect:   Anytime we test whether a sample differs from a population, or whether two samples come from 2 separate populations, there is the condition that each of the populations we are comparing has its own mean and standard deviation (even if we do not know it). The distance between the two population means will affect the power of our test. This is known as the size of treatment, also known as  the effect size, as shown in the following table with the  three popular values for a:     Power as a Function of a and the Size Effect   a   Size Effect0.100.050.01\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 1.0.22\\xa0.13\\xa0.03\\xa0\\xa0\\xa02.0.39\\xa0.26\\xa0 .09\\xa0\\xa0\\xa03.0.59\\xa0.44\\xa0 .20\\xa0\\xa0\\xa0 4.0.76\\xa0.64\\xa0 .37\\xa0\\xa0\\xa0 5.0.89\\xa0.79\\xa0 .57\\xa0\\xa0\\xa06.0.96\\xa0.91\\xa0 .75\\xa0\\xa0\\xa07.0.99\\xa0.97\\xa0 .88\\xa0\\xa0\\xa0\\nPower and the Size of Variance s2: The greater the variance S2, the lower the     power 1-b. Anything that effects the extent to which the two distributions share common values will increase b (the likelihood of making a Type II error)   Power and the Sample Size: The smaller the sample sizes n, the lower the power. Very small n produces  power so low that false hypotheses are accepted.   The following is a list of four factors influencing the power:  \\neffect size (for example, the difference between the means) variance S2 significance level a number of observations, or the sample size n  In practice, the first three factors are often fixed. Only the sample size can be controlled by the statistician and that only within budget constraint. There exists a tradeoff between budget and achievement of desirable accuracy in any analysis.\\n  A Numerical Example: The power of a test is most easily understood by viewing it in the context of a composite test. A composite test requires the specification of a population mean as the alternative hypothesis. For example, using Z-test of hypothesis in the following Figure. The power is developed from specification of an alternative hypothesis such as m = 2.5, and m = 3. The resultant distribution under this alternative shifts to the right 2.5 units with the shaded area representing the power of the test, correctly rejecting a false null.\\n  Power of a TestClick on the image to enlarge  it\\nNot rejecting the null hypothesis when it is false is defined as a Type II     error, and is denoted by the b region. In the above     Figure this region lies to the left of the critical value. In the configuration     shown in this Figure, b falls to the left of the     critical value (and below the statistic\\'s density (or probability) function     under the alternative hypothesis Ha). The     b is also defined as the probability of      not-rejecting a false null hypothesis when it is false, also called a miss. Related to the     value of b is the power of a test. The power is defined     as the probability of rejecting the null hypothesis given that a specific     alternative is true, and is computed as (1- a). \\n  A Short Discussion: Consider testing a simple null versus simple alternative.     In the Neyman-Pearson setup, an upper bound is set for the probability of  a given     Type I error (a), and then it is desirable to find     tests with low probability of type II error (b) given this. The usual justification for this is that \"we  are more concerned about a Type I error, so we set an upper limit on the a that we can tolerate.\" I have seen this sort of reasoning in     elementary texts and also in some advanced ones. It doesn\\'t seem to make any   sense. When the sample size is large, for most standard tests, the ratio b/a tends to 0. If we care more  about Type I error than Type II error, why should this concern dissipate with     increasing sample size?   This is indeed a drawback of the classical theory of testing statistical  hypotheses. A second drawback is that the choice lies between only two test  decisions: reject the null or accept the null. It is worth considering approaches   that overcome these deficiencies. This can be done, for example, by the concept \\n    of profile-tests at a \\'level\\' a. Neither the Type  I nor Type II error rates are considered separately, but they are the ratio  of a correct decision. For example, we accept the alternative hypothesis Ha  and reject the null H0, if an event is observed which is at least a-times greater under Ha than under H0. Conversely, we accept H0 and reject Ha, if \\n    an event is observed which is at least a-times greater under H0 than under Ha. This \\n    is a symmetric concept which is formulated within the classical approach.     Power of Parametric versus Non-parametric Tests: As a general rule, for a given  sample size n, the parametric tests are more powerful than their non-parametric counterparts.  \\nThe primarily  reason for this is that we have emphasized parametric tests.  Moreover, among the parametric tests,  those which use correlation are more powerful, such as \\n the before-and-after test.  This is known as a Variance Reduction Technique used in system simulation to increase the accuracy (i.e., reduce variation) without increasing the sample size. \\nCorrelation Coefficient as a Measuring Tool and Decision Criterion for the Effect Size:  The correlation coefficient could be obtained and used as a measuring tool and decision criteron for the strength of the effect size based on the computed test-statistic for major hypothesis testing.\\n\\n The correlation coefficient r stands as a very useful and accessible index of the magnitude of effect. It is commonly accepted that the small, medium, and large effect sizes correspond to r-values over 0.1, 0.3, and 0.5, respectively.  The following are needed transformation of some major inferential statistics to the r-value:\\n\\n\\nFor the t(df)-statistic: \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0  r = [t2/(t2 + df)]½\\nFor the F(1,df2)-statistic: \\xa0 \\xa0 \\xa0 \\xa0  r = [F/(F + df)]½\\nFor the c2(1)-statistic: \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 r = [c2/n] ½\\nFor the Standard Normal Z: \\xa0 \\xa0  r = (Z2/n)½\\n\\n\\nYou might like to use Sample Size Determination JavaScript at the design stage of your statistical investigation in decision making with specific subjective requirements.\\n\\nFurther Reading: Murphy K., and B. Myors, Statistical Power Analysis, L. Erlbaum Associates, 1998.      \\nParametric vs. Non-Parametric vs. Distribution-free Tests\\n  One must use a statistical technique called non-parametric if it satisfies at least one of the following five types of criteria:    The data entering the analysis are enumerative; that is, counted data represent  the number of observations in each category or cross-category.   The data are measured and/or analyzed using a nominal scale of measurement.  The data are measured and/or analyzed using an ordinal scale of measurement.   The inference does not concern a parameter in the population distribution; for example, the hypothesis that a time-ordered set of observations \\n    exhibits a random pattern.  The probability distribution of the statistic upon which the analysis is  based is not dependent upon specific information or conditions (i.e., assumptions) about the population(s) from which the sample(s) are drawn, but only upon general assumptions, such as a continuous and/or symmetric population distribution. According to these creteria, the distinction of non-parametric is accorded either because of the level of measurement used or required for the analysis, as in types 1 through 3; the type of inference, as in type 4, or the generality of the assumptions made about the population distribution, as in type 5. For example, one may use the Mann-Whitney Rank Test as a non-parametric alternative  to Students T-test when one does not have normally distributed data. Mann-Whitney: To be used with two independent groups (analogous to the independent groups t-test) Wilcoxon: To be used with two related (i.e., matched or repeated) groups (analogous to the related samples t-test)  Kruskall-Wallis: To be used with two or more independent groups (analogous to the single-factor between-subjects ANOVA)  Friedman: To be used with two or more related groups (analogous to  the single-factor within-subjects ANOVA) Non-parametric vs. Distribution-free Tests:\\nNon-parametric tests are those used when some specific conditions for the ordinary tests are violated. \\n  Distribution-free tests are those for which the procedure is valid for all different shape of the population distribution. For example, the Chi-square test concerning the variance of a given population is parametric since this test requires that the population distribution be  normal. The Chi-square test of independence does not assume normality condition, or even that the data are numerical. The Kolmogorov-Smirnov test is a distribution-free test, which is applicable to comparing two populations with any distribution of continuous random variable.\\n\\n\\nThe following section is an interesting non-parametric procedure with various and useful applications.\\n\\nComparison of Two Random Variables: Consider two independent observations X = (x1, x2,\\x85, xr) and Y = (y1, y2,\\x85, ys) for two random variables X and Y respectively. To estimate the reliability function:\\n\\n\\nR = Pr (X > Y)\\n\\n\\nOne may use:\\n\\n\\nThe estimator RS = U/(r ´ s), \\n\\n\\n\\nwhere U is the number of pairs (xi, yj) such that xi > yj, for all i = 1, 2,  ,r, \\xa0and  j = 1, 2,..,s.  \\n\\nThis estimator is an unbiased one with the minimum variance for R. It is important to know that the estimate has an upper limit,  non-negative delta value for its accuracy:\\n\\n\\nPr{R ³ RS - d} ³ max {1- exp(-2nd2), 4nd2/(1-4nd2)}.\\n\\n\\n\\nApplication areas include the insurance ruin problem. Let  random variable Y denote the claims per unit of time and let random variable X denote the return on investment (ROI) for the Insurance Company. Finally, let z denote the constant premium amount collected; then the probability that the insurance company will survive is:\\n\\n\\n R = Pr [X + z > Y}.\\n\\n\\nYou might like to use the   Kolmogorov-Smirnov Test for Two Populations and  Comparing Two Random Variables in checking your computations and performing some numerical experiment for a deeper understanding of these concepts.\\n\\nFurther Readings: \\n\\nArsham H., A generalized confidence region for stress-strength reliability, IEEE Transactions on Reliability, 35(4), 586-589, 1986. \\nConover W., Practical Nonparametric  Statistics, Wiley, 1998.\\nHollander M., and D. Wolfe, Nonparametric Statistical Methods, Wiley,  1999.\\nKotz S., Y.  Lumelskii, and M. Pensky, The Stress-Strength Model and Its Generalizations: Theory and Applications,  Imperial College Press, London, UK, 2003, distributed by World Scientific\\nPublishing.\\n\\n\\n  Hypotheses Testing  Remember that, in the t-tests for differences in means, there is a condition of   equal population variances that must be examined. One way to test for possible   differences in variances is to do an F test. However, the F test is very sensitive   to violations of the normality condition; i.e., if populations appear not to be normal, then the F test  will tend to reject too often the null of no differences in population variances.  You might like to use the following JavaScript to check your computations and to perform some statistical experiments for deeper understanding of these concepts:Testing the Mean.Testing the Variance.  Testing Two Populations. Testing the Difference: The Before-and-After Test. ANOVA. For statistical equality of two populations, you might like to use the Kolmogorov-Smirnov Test.      Single Population t-Test The purpose is to compare the sample mean with the given population mean. The aim is to judge the claimed mean value, based on a set of random observations   of size n. A necessary condition for validity of the result is that the population distribution is normal, if the sample size n is small (say less than 30). The task is to decide whether to accept a null hypothesis:   H0 = m = m0 or to reject the null hypothesis in favor of the alternative hypothesis: Ha: m is significantly  different from m0\\nThe testing framework consists of computing a the t-statistics:    T = [( - m0) n1/2] / S   Where       is the estimated mean and S2 is  the estimated variance based on n random observations. \\n  The above statistic is distributed as a  t-distribution with parameter d.f. = n = (n-1).  If the absolute value of the computed T-statistic is \"too large\" compared with the critical value of the t-table, then one rejects  the claimed value for the population\\'s mean.  This test could also be used for testing similar claims for other unimodal populations including those with discrete random variables, such as proportion, provided there are sufficient observations (say, over 30). \\n\\nYou might like to use Testing the Mean JavaScript in checking your computations. and Sample Size Determination JavaScript at the design stage of your statistical investigation in decision making with specific subjective requirements\\n \\n  The Procedure for Two Populations Independent Means Test \\nClick on the image to enlarge it and THEN print it\\n\\n\\n  You might like to use  JavaScript Testing Two Populations.\\n\\n \\nWhen  Should We Pool Variance Estimates?\\n  We should pool variance estimates only if there is a good reason for doing so, and then (depending on that reason) the conclusions might have to be made explicitly conditional on the validity of the equal-variance model. There are several different good reasons for pooling: (a) to get a single stable estimate from several relatively small samples,     where variance fluctuations seem not to be systematic; or (b) for convenience, when all the variance estimates are near enough to equality; or (c) when there is no choice but to model variance (as in simple linear regression  with no replicated X values), and deviations from the constant-variance model do not seem systematic; or (d) when group sizes are large and nearly equal, so that there is essentially  no difference between the pooled and unpooled estimates of standard errors of pairwise contrasts, and degrees of freedom are nearly asymptotic. Note that this last rationale can fall apart for contrasts other than pairwise ones. One is not really pooling variance in case (d), rather one is merely taking a shortcut in the computation of standard errors of pairwise contrasts. If you calculate the test without the assumption, you have to determine the degrees of freedom (d.f.). The formula works in such a way that d.f. will be less if the larger sample variance is in the group with the smaller number of observations. This is the case in which the two tests will differ considerably. A study of the formula for the d.f. is most enlightening, and one must understand the correspondence between the unfortunate design (having the most observations in the group with little variance) and  the low d.f. and accompanying large t-value. Example: When doing t tests for differences in means of populations  (a classic independent samples case):  \\nFor differences in means that do not make any assumption about equality \\n    of population variances, use the standard error formula:   [S21/n1 + S22/n2]½,    with d.f. = n = n1  or n2 whichever is smaller. \\nWith equal variances, use the statistics:       with parameter d.f. = n = (n1 + n2- 2), n1, \\xa0 \\xa0 for n2 greater than or equal to 1, where the pooled variance  is:  \\n    If total N is less than 50 and one sample is 1/2 the size of the other (or less), and if the smaller sample has a standard deviation at least twice as large \\n    as the other sample, then apply the procedure given in item no. 1, but adjust d.f. parameter of the t-test to the largest integer less than or equal to:     d.f. = n = A/(B +C),   where: A = [S21/n1 + S22/n2]2,  B = [S21/n1]2 / (n1 -1),  C = [S22/n2]2/ (n2 -1) \\n    Otherwise, do not worry about the problem of having an actual a level that is much different than what you have set it  to be.  Statistics with Confidence Section is concerned with the construction of a confidence interval where the equality of variances condition is an important  issue. The last approach, which is very general with conservative results, can be  implemented using Testing Two Populations Applet. \\n     The Procedure for Two Dependent Means Test \\nClick on the image to enlarge it and THEN print it   You might like to use  JavaScript Testing the Difference in Means: The Before-and-After Test and Paired Proportion Test for dependent proportions.\\n\\n\\n Non-parametric Multiple Comparison ProceduresDuncan\\'s multiple-range test: This is one of the many multiple comparison procedures.  It is based on the standardized range statistic by comparing all pairs of means  while controlling the overall Type I error at a desirable level. While it does  not provide interval estimates of the difference between each pair of means,   it does indicate which means are significantly different from the others.  For determining the significant differences between a single control group mean and the other means, one may use the Dunnett\\'s multiple-comparison test. \\n\\nIntroduction to Tests for Statistical Equality of Two or More Populations:  Two random variables X and Y having distribution FX(x) and  FY(y) respectively, are said to be equivalent, or equal in law, or equal in distribution, if and only if they have the same distribution function. That is,  \\n\\n\\nFX(z) = FY(z),  \\xa0\\xa0for all z, \\n\\n\\nThere are different tests depending on the intended applications.  The widely used tests for statistical equality of populations are as follow:\\n\\n\\nEquality of Two Normal Populations:\\n One may use the Z-test and F-test to check the equality of the means, and the equality of variances, respectively. \\n\\n\\nTesting a Shift in Normal Populations: Often we are interested in testing for a given shift in a given population distribution, that is testing if a random variable Y is equal in distribution to another X + c for some constant c.   In other words, the distribution of Y is the distribution of X shifted.  In testing any shift in distribution one needs to test for normality first, and then testing the difference in expected values by applying the two-sided Z-test with the null hypothesis of: \\n\\n\\nH0:\\xa0\\xa0 mY - mX = c.\\n\\n\\nAnalysis of Variance: Analysis of Variance (ANOVA) tests are designed for simultaneous testing of equality of three or more populations.  The preconditions in applying ANOVA are normality of each population\\'s distribution, and the equality of all variances simultaneously (not the pair-wise tests).\\n \\nNotice that ANOVA is an extension of item no. 1 in testing equality of more than two populations.  It can be shown that if one applies ANOVA for testing the equality of two populations based on two independent samples with sizes of n1 and n2 form each population, respectively, then the results of both tests will be identical.   Moreover, the test-statistic obtained by each test are directly related, i.e.,  \\n\\n\\n F a , (1, n1+ n2 - 2) \\xa0= \\xa0 t 2 a/2 , (n1+ n2 - 2)\\n\\n\\n\\nEquality of Proportions in Several Populations:  This test is for discrete random variables. It is one of the many interesting chi-square applications.\\n\\nDistribution-free Equality of Two Populations: Whenever one is interested in testing the equality of two populations with a common continuous random variable, without any reference to the underlying distribution such as normality condition, one may use the distribution-free known as the K-S test.\\n\\nNon-parametric Comparison of Two Random Variables:  Consider two independent observations X = (x1, x2,\\x85, xr) and Y = (y1, y2,\\x85, ys) for two independent populations with random variables X and Y, respectively. Often we are interested in estimating the Pr (X > Y).\\n\\n\\n\\n\\nEquality of Two Normal Populations: The normal or Gaussian distribution is a continuous symmetric distribution that follows the familiar bell-shaped curve. One of its nice features is that, the mean and variance uniquely and independently determines the distribution. \\n\\nTherefore, for testing the statistical equality of two independent normal populations, one must first perform the Lilliefors\\' Test for Normality to assess this condition. Given that both populations are normally distributed, then one must performing two more tests, namely the test for equality of the two means and the test for equality of the two variances.  Both of these tests can be carried out by using the\\nTest of  Hypotheses for Two Populations JavaScript Applets.\\n\\n\\nMulti-Means Comparisons: Analysis of Variance (ANOVA)\\n  The tests we have learned up to this point allow us to test hypotheses that examine the difference between only two means. Analysis of Variance or ANOVA will allow us to test the difference between two or more means. ANOVA does this by examining the ratio of variability between two conditions and variability within each condition. For example, say we give a drug that we believe will improve memory to a group of people and give a placebo to another group of people. We might measure memory performance by the number of words recalled from a list we ask everyone to memorize. A t-test would compare the likelihood of observing \\n  the difference in the mean number of words recalled for each group. An ANOVA test, on the other hand, would compare the variability that we observe between the two conditions to the variability observed within each condition. Recall that we measure variability as the sum of the difference of each score from the mean. When we actually calculate an ANOVA we will use a short-cut formula  Thus, when the variability that we predict  between the two groups is much greater than the variability we don\\'t predict within each group, then we \\n    will conclude that our treatments produce different results. An Illustrative Numerical Example for ANOVAConsider the following (small integers, indeed for illustration  while saving space) random samples from three different populations. With the null hypothesis:H0: µ1 = µ2 = µ3,  and the alternative:Ha: at least two of the means are not equal. At the significance level a = 0.05, the critical value from F-table is  F 0.05, 2, 12 = 3.89.   \\n\\n\\n\\n\\n\\xa0\\n\\nSum\\n\\n\\nMean\\n\\n\\n\\nSample \\n              P1\\n\\n2\\n\\n\\n3\\n\\n\\n1\\n\\n\\n3\\n\\n\\n1\\n\\n\\n10\\n\\n\\n2\\n\\n\\n\\nSample \\n              P2\\n\\n3\\n\\n\\n4\\n\\n\\n3\\n\\n\\n5\\n\\n\\n0\\n\\n\\n15\\n\\n\\n3\\n\\n\\n\\nSample \\n              P3\\n\\n5\\n\\n\\n5\\n\\n\\n5\\n\\n\\n3\\n\\n\\n2\\n\\n\\n20\\n\\n\\n4\\n\\n\\n\\n\\n\\n\\n\\n\\nDemonstrate that, SST=SSB+SSW.That is, the sum of squares total (SST) equals sum of squares between (SSB) the groups plus sum of squares within (SSW) the groups.   Computation of sample SST: With the grand mean = 3, first, start with  taking the difference between each observation and the grand mean, and then     square it for each data point.   \\n\\n\\n\\n\\n\\xa0\\n\\nSum\\n\\n\\n\\nSample \\n              P1\\n\\n1\\n\\n\\n0\\n\\n\\n4\\n\\n\\n0\\n\\n\\n4\\n\\n\\n9\\n\\n\\n\\nSample \\n              P2\\n\\n0\\n\\n\\n1\\n\\n\\n0\\n\\n\\n4\\n\\n\\n9\\n\\n\\n14\\n\\n\\n\\nSample \\n              P3\\n\\n4\\n\\n\\n4\\n\\n\\n4\\n\\n\\n0\\n\\n\\n1\\n\\n\\n13\\n\\n\\n\\n\\n\\nTherefore SST = 36 with d.f = (n-1) = 15-1 = 14 \\n  Computation of sample SSB: Second, let all the data in each sample have the same value as the mean in  that sample. This removes any variation WITHIN. Compute SS differences from the grand mean. \\n   \\n\\n\\n\\n\\n\\xa0\\n\\nSum\\n\\n\\n\\nSample \\n              P1\\n\\n1\\n\\n\\n1\\n\\n\\n1\\n\\n\\n1\\n\\n\\n1\\n\\n\\n5\\n\\n\\n\\nSample \\n              P2\\n\\n0\\n\\n\\n0\\n\\n\\n0\\n\\n\\n0\\n\\n\\n0\\n\\n\\n0\\n\\n\\n\\nSample \\n              P3\\n\\n1\\n\\n\\n1\\n\\n\\n1\\n\\n\\n1\\n\\n\\n1\\n\\n\\n5\\n\\n\\n\\n\\n\\nTherefore SSB = 10, with d.f = (m-1)= 3-1 = 2 for m=3 groups.  Computation of sample SSW:\\nThird, compute the SS difference within each sample using their own sample means. This provides SS deviation WITHIN all samples.    \\n\\n\\n\\n\\n\\xa0\\n\\nSum\\n\\n\\n\\nSample \\n              P1\\n\\n0\\n\\n\\n1\\n\\n\\n1\\n\\n\\n1\\n\\n\\n1\\n\\n\\n4\\n\\n\\n\\nSample \\n              P2\\n\\n0\\n\\n\\n1\\n\\n\\n0\\n\\n\\n4\\n\\n\\n9\\n\\n\\n14\\n\\n\\n\\nSample \\n              P3\\n\\n1\\n\\n\\n1\\n\\n\\n1\\n\\n\\n1\\n\\n\\n4\\n\\n\\n8\\n\\n\\n\\n\\n\\nSSW = 26 with d.f = 3(5-1) = 12. That is, 3 groups times (5 observations in each -1) \\n  Results are: SST = SSB + SSW, and d.fSST  = d.fSSB + d.fSSW, as expected. Now, construct the ANOVA table for this numerical example by plugging the results of your computation in the ANOVA Table.  Note that, the Mean Squares are the Sum of squares divided by their Degrees of Freedom. F-statistics is the ratio of the two Mean Squares.    The ANOVA Table\\n Sources of VariationSum of Squares Degrees of FreedomMean Squares F-Statistic Between Samples10\\n252.30 Within Samples26122.17 \\xa0  Total36\\n14\\xa0\\xa0\\nConclusion: There is not enough evidence to reject the null hypothesis H0.   The Logic behind ANOVA: First, let us try to explain the logic and then illustrate it with a simple example. In performing the ANOVA  test, we are trying to determine if a certain number of population means are equal. To do that, we measure the difference of the sample means and compare that to the variability within the sample observations. That is why the test statistic is the ratio of the between-sample variation (MSB) and the within-sample variation (MSW). If this ratio is close to 1, there is evidence that the population means are equal. Here is a good application for you: Many people believe that men get paid more in the business world than women, simply because they are male. To justify or reject such a claim, you could look at the variation within each group  (one group being women\\'s salaries and the other group being men\\'s salaries) and compare that to the variation between the means of randomly selected samples of each population. If the variation in the women\\'s salaries is much larger than the variation between the men\\'s and women\\'s mean salaries, one could say that because the variation is so large within the women\\'s group that this may not be a gender-related problem.  Now, getting back to our numerical example of the drug treatment to improve memory vs the placebo. We notice that: given the test conclusion and the ANOVA test\\'s conditions, we may conclude that these three     populations are in fact the same population. Therefore, the ANOVA technique could be used as a measuring tool and statistical routine for quality control as described below using our numerical example. \\n  Construction of the Control Chart for the Sample Means: \\n    Under the null hypothesis, the ANOVA concludes that µ1 = µ2 = µ3; that is, we have a \"hypothetical parent population.\" The question is, what is its variance? The estimated variance (i.e., the total mean squares) is 36 / 14 = 2.75. Thus, estimated standard deviation is = 1.60 and estimated standard deviation for the means is 1.6 / 5½  = 0.71. Under the conditions of ANOVA, we can construct a control chart with   the warning limits = 3 ± 2(0.71); the action limits = 3 ± 3(0.71). The following figure depicts the control chart.      You might like to use ANOVA: Testing Equality of Means, or ANOVA for your computations, and then to interpret the results in managerial (not technical) terms.\\nYou might need to use Sample Size Determination JavaScript at the design stage of your statistical investigation in decision making with specific subjective requirements.  \\n\\nThe Procedure for More Than Two Independent Means Test Click on the image to enlarge it and THEN print it \\n\\n ANOVA for Normal but Condensed Data Sets\\n In testing the equality of several means, often the raw data are not available.  In such a case, one must perform the needed analysis based on secondary data using the data summaries; namely, the triple-set: The sample sizes, the sample means, and the sample variances. \\n\\nSuppose one of the samples is of size n having the sample mean ,  and the sample variance S2. Let:\\n\\n\\nyi  =  + (S2/n)½ \\xa0 \\xa0 for all i = 1, 2, \\x85, n-1,\\n\\n\\nand\\n\\n\\nyn = n   -  (n - 1)y1\\n\\n\\nThen, the new random data yi\\'s are surrogate data having the same mean and variance as the original data set.  Therefore, by generating the surrogate data for each sample, one can perform the standard ANOVA test. The results are identical.\\nYou might like to use ANOVA for Condensed Data for your computation and experimentation. \\n\\nThe JavaScript Subjective Assessment of Estimates tests the claim that at least the ratio of one estimate to   the largest estimate is as large as a given claimed value. \\n  Further Reading: Larson D., Analysis of variance with just summary statistics as input, The American Statistician, 46(2), 151-152, 1992.  \\n ANOVA for Dependent Populations\\nPopulations can be dependent in either of the following ways:\\n\\n\\nEvery subject is tested in every experimental condition.  This kind of dependency is called the repeated-measurement design.\\nSubjects under different experimental conditions are related in some manner.  This kind of dependency is called matched-subject designed.\\n\\n\\nAn Application:  Suppose we are interested in studying the effect of alcohol on driving ability.  Ten  subjects are given three different alcohol levels and the number of driving errors are tabulated below:\\n \\n\\n\\xa0\\n\\nMean\\n\\n\\n\\n0 \\n            oz\\n\\n2\\n\\n\\n3\\n\\n\\n1\\n\\n\\n3\\n\\n\\n1\\n\\n\\n4\\n\\n\\n1\\n\\n\\n3\\n\\n\\n2\\n\\n\\n1\\n\\n\\n2.1\\n\\n\\n\\n2 \\n            oz\\n\\n3\\n\\n\\n2\\n\\n\\n1\\n\\n\\n4\\n\\n\\n2\\n\\n\\n3\\n\\n\\n1\\n\\n\\n5\\n\\n\\n1\\n\\n\\n2\\n\\n\\n2.4\\n\\n\\n\\n4 \\n            oz\\n\\n3\\n\\n\\n1\\n\\n\\n2\\n\\n\\n4\\n\\n\\n2\\n\\n\\n5\\n\\n\\n2\\n\\n\\n4\\n\\n\\n3\\n\\n\\n2\\n\\n\\n3.1\\n\\n\\n\\n\\n\\nThe test null hypothesis is:\\n\\nH0: µ1 = µ2 = µ3,  and the alternative:Ha: at least two of the means are not equal.\\n\\n\\nUsing the  ANOVA for Dependent Populations JavaScripts, we obtain the needed information in constructing the following ANOVA table:\\n   The ANOVA Table\\n Sources of VariationSum of Squares Degrees of FreedomMean Squares F-Statistic\\n Subjects\\n31.50\\n93.50-\\n Between\\n5.26\\n22.637.03\\n Within\\n6.70180.37 \\xa0  Total\\n43.46\\n29\\xa0\\xa0\\nConclusion: The p-value is P= 0.006, indicating a strong evidence against the null hypothesis. The means of the populations are not equal.  Here, one may conclude that person who has consumed more than certain level of alcohol commits more driving errors.\\n\\n  The Procedure for More Than Two Dependent Populations Test\\n  Click on the image to enlarge it and THEN print it \\n\\n\\n\\n\\nA \"block design sampling\" implies studying more than two dependent populations.  For testing the equality of means of more than two populations based on block design sampling, you may use Two-Way ANOVA Test JavaScript.  In the case of having block design data with replications, use Two-Way ANOVA with Replications JavaScript to obtain the needed information for constructing the ANOVA tables.\\n\\n\\n Test for Equality of Several Population Proportions\\nThe Chi-square test of homogeneity provides an alternative method for testing the null hypothesis that two population proportions are equal. Moreover, it extend, to several populations similar to the ANOVA test that compares several means.  An Application: Suppose we wish to test the null hypothesis \\n      H0: P1 = P2 = ..... = Pk  That is, all three population proportions are almost identical. The sample data from each of the three populations are given in the following table:      Test for homogeneity of Several Population Proportions Populations Yes  No  Total\\n  Sample I 60 40 100   Sample II57 53110 Sample III\\n48 72120  Total 165 165 330   The Chi-square statistic is 8.95 with d.f. = (3-1)(3-1) = 4. The p-value is equal to 0.062, indicating that there is moderate evidence against the null hypothesis that the three populations are statistically identical.   You might like to use Testing Proportions to perform this test.   \\n\\n  Distribution-free Equality of Two Populations \\nFor statistical equality of two populations, one may use the Kolmogorov-Smirnov Test (K-S Test) for two populations. The K-S test seeks differences between the two population\\'s distribution function based on their two independent random samples. The test rejects the null hypothesis of no difference between the two populations if the difference between the two empirical distribution functions is \"large\".Prior to applying the K-S test it is necessary to arrange each of the two sample observations in a frequency table. The frequency table must have a common classification. Therefore the test is based on the frequency table, which belongs to the family of distribution-free tests.The K-STest process is as follows:Some k number of \"classes\" is selected, each typically covering a different but similar range of values. Some much larger number of independent observations (n1, and n2, both larger than 40) are taken. Each is measured and its frequency is recorded in a class.Based on the frequency table, the empirical cumulative distribution functions  F1i and F2i for two sample populations are constructed,  for i = 1, 2,..., k. The K-S statistic is the largest absolute difference between F1i and F2i; i.e.,\\nK-S statistic = D = Maximum | F1i - F2i |, \\xa0 \\xa0\\xa0 \\xa0   for all i = 1, 2, .., k.The critical values of K-S statistic can be found at Computers and Computational Statistics with ApplicationsAn Application: The daily sales of the two subsidiaries of The PC & Accessories Company are shown in the following table, with n1 = 44, and n2 = 54: Daily Sales at Two Branches Over 6 MonthsSales ($1000)\\xa0Frequency IFrequency II0 - 2\\n111\\n\\n3 - 5\\n73\\n6 - 886\\n9 - 11\\n312\\n12 - 145\\n1215 - 17\\n5\\n1418 - 20\\n56\\nSums\\n4454\\nThe manager of the first  branch is claiming that \"since the daily sales are random phenomena, my overall performance is as good as the other manager\\'s performance.\"   In other words:\\n\\nH0: The daily sales at the two stores are almost the same.\\nHa: The performance of the managers is significantly different.\\nFollowing the above process for this test, the K-S statistic is 0.421 with the p-value of 0.0009, indicating a strong evidence against the null hypothesis.  There is enough evidence that the performance of the manager of the second branch is better.\\n\\n  Introduction to Applications of the Chi-square Statistic The variance is not the only thing for which you use a Chi-square test for. \\n\\nThe most widely used applications of Chi-square distribution are: \\nThe Chi-square Test for Association which is a non-parametric test; therefore, it can be used for nominal data too. It is a test of statistical significance widely used bivariate tabular association analysis. Typically, the hypothesis is whether or not two populations are different in some characteristic or aspect of their behavior based on two random samples. This test procedure is also known as the Pearson Chi-square test. \\nThe Chi-square Goodness-of-Fit Test is used to test if an observed distribution conforms to any particular\\n distribution. Calculation of this goodness-of-fit test is by comparison of observed data with data expected based on a particular distribution. \\nOne of the disadvantages of some of the Chi-square tests is that they do  not permit the calculation of confidence intervals; therefore, determination of the sample size is not readily available. \\nTreatment of Cases with Many Categories: Notice that, although in the following section most of the crosstables have only two categories, it is always possible to convert cases with many categories into similar crosstables. To do so, one must consider all possible pairs of categories and their numerical values while constructing the equivalent \"two-categories\" crosstable.    Test for Crosstable Relationship Crosstables: Often crosstables are used to test relationships among two categorical  types of  data, or independence of two variables, such as cigarette smoking and drug use. If you were to survey 1000 people on whether or not they smoke and whether   or not they use drugs, you would  get one of four answers: (no, no) (no, yes) (yes, no)   (yes, yes)  By compiling the number of people in each category, you can ultimately test  whether drug usage is independent of cigarette smoking by using the Chi-square  distribution (this is approximate, but works well). Again, the methodology  for this is in your textbook. The degrees of freedom is equal to (number of  rows-1)(number of columns -1). That is, these many numbers needed to fill  in the entire body of the crosstable, the rest will be determined by using the given row sums and the column sums values. Do not forget the conditions for the validity of Chi-square test and related expected values greater than 5 in 80% or more of the cells. Otherwise, one could use an \"exact\" test, using either a permutation or resampling approach.  Using Chi-square in a 2x2 table requires the Yates\\'s correction. One first  subtracts 0.5 from the absolute differences between observed and expected  frequencies for each of the three genotypes before squaring, dividing by the expected frequency, and summing. The formula for the Chi-square value in a 2x2 table can be derived from the Normal Theory comparison of the two proportions in the table using the total incidence to produce the standard errors. The rationale of the correction is a better equivalence of the area under the normal curve and the probabilities obtained from the discrete frequencies. In other words,  the simplest correction is to move the cut-off point for the continuous distribution from the observed value of the discrete distribution to midway between that and the next value in the direction of the null hypothesis expectation. Therefore, the correction essentially only applied to one d.f. tests where the \"square root\" of the Chi-square looks like a \"normal/t-test\" and where a direction can be attached to the 0.5 addition. \\n  Chi-square distribution is used as an approximation of the binomial distribution. By applying a continuity correction, we get a better approximation of the binomial distribution for the purposes of calculating tail probabilities.  Given the following 2x2 table, one may compute some relative risk measures:  \\n\\n\\n\\n\\na \\n        \\n\\n\\nb \\n        \\n\\n\\n\\n\\nc \\n        \\n\\n\\nd \\n        \\n\\n\\n\\n\\nThe most usual measures are:  Rate-difference: a/(a+c) - b/(b+d) Rate-ratio: (a/(a+c))/(b/(b+d))\\n    Odds-ratio: ad/bc The rate difference and rate ratio are appropriate when you are contrasting  two groups whose sizes (a+c and b+d) are given. The odds ratio is for when the issue is association rather than difference. The risk-ratio (RR) is the ratio of the proportion (a/(a+b)) to the proportion  (c/(c+d)):      RR = (a / (a + b)) / (c / (c + d)) RR is thus a measure of how much larger the proportion in the first row is compared to the second.  RR value of < 1.00 indicating a \\'negative\\' association [a/(a+b) <  c/(c+d)], 1.00 indicating no association [a/(a+b) = c/(c+d)], and >1.00  indicating a \\'positive\\' association [a/(a+b) > c/(c+d)]. The further from 1.00 the RR is, the stronger the association.    An Application: Suppose a counselor of a school in a small town is interested whether the curriculum chosen by students is related to the occupation  of their parents. It is necessary to record the data as shown in the following  contingency table with two rows (r1, r2) and three columns (c1, c2, c3):     \\nRelationship between occupation of parents and curriculum chosen by high school students\\n   Curriculum Chosen by Students        Parental  Occupation College prepVocational General Totals Professional      12 2 6  66 8  20  Blue collar 20   Totals 18 8 14 \\xa0    \\n   Under the hypothesis that there is no relation, the expected (E) frequency   would be:          Ei, j = (Sri)(Scj)/N      The Observed (O) and Expected (E) frequencies are recorded in the following   table:       Expected frequencies for the data.       \\xa0 College prepVocational General Totals   Professional      O = 12 E = 9 O = 2 E = 4O = 6 E = 7   O = 6\\nE = 9 O = 6E = 4O = 8\\nE = 7    åO = 20åE = 20 \\nBlue collaråO= 20åE =  20 TotalsåO = 18åE = 18 åO =  8 åE = 8  åO = 14åE = 14 \\xa0 \\n    The quantity     c 2 = S [(O - E )2 / E]  is a measure of the degree of deviation between the Observed  and Expected frequencies. If there is no relationship between the row  variable and the column variable  this measure will be very close to zero. Under the hypothesis that there is a relationship between the rows and the columns, this quantity has a Chi-square distribution with parameter equal to number of rows minus 1, multiplied by number of columns minus 1. For this numerical example we have:    c 2 = S [(O - E )2 / E] = 30/7 = 4.3 \\nwith d.f. = (2-1)(3-1) = 2, that has the p-value of 0.14, suggesting little or no real evidences against the null hypothesis. The main question is how large is this measure. The maximum value of this measure is: \\n    c 2max = N(A-1),  where A is the number of rows or columns, whichever is smaller. For our numerical example it is, 40(2-1) = 40. The coefficient of determination which has a range of [0, 1], provides relative strength of relationship, computed as   c 2/c 2max = 4.3/40 = 0.11  \\nTherefore we conclude that the degree of association is only 11% which is fairly weak.   Alternatively, you could also look at the contingency coefficient f statistic, which is:     f = [ c2/(N + c2)]½ = 0.31 \\nThis statistic ranges between 0 and 1 and can be interpreted like the correlation  coefficient. This measure also indicates that the curriculum chosen by students is related to the occupation of their parents. \\n  You might like to use Chi-square Test for Crosstable Relationship in performing this  test, and he P-values for the Popular Distributions Applet to findout the p-values of Chi-square statistic. Further Readings: Agresti A., Categorical Data Analysis, \\n    Wiley, 2002. Fleiss J., Statistical Methods for Rates and Proportions, Wiley, 1981.  \\n  Identical Populations Test for Crosstable DataTest of homogeneity is much like the Test for Crosstable Relationship in \\n    that both deal with the cross-classification of nominal data; that is, r ´ c  tables. The method of computing Chi-square statistic is the same for both  tests, with the same d.f.  The two tests differ, however, in the following respect. The Test for Crosstable Relationship is made on data drawn from a single population (with fixed  total) where one is concerned with whether one set of attributes is independent of another set. The test for homogeneity, on the other hand, is designed  to test the null hypothesis that  two or more random samples are drawn from the same population or from different       populations, according to some criterion of classification applied  to the samples.  The homogeneity test is concerned with the question: Are the samples drawn  form populations that are homogeneous (i.e., the same) with respect to some   criterion of classification? In the crosstable for this test, either the row or the column categories   may represent the populations from which the samples are drawn.  An Application: Suppose a board of directors of a labor union wishes  to survey the opinion of its members regarding a change in its constitution. The following table shows the result of the survey sent to three union locals:     Reactions of A Sample of Three Locals Group Members    Union Local        ReactionABC \\nIn Favor \\n  182210 7 149   5\\n411\\nAgainst No Response    \\xa0 The problem is not to determine whether or not the union members are in favor of the change. The question is to test if there is a significant difference in the proportions of opinion of the three populations\\' members concerning  the proposed change. The Chi-square statistic is 9.58 with d.f. = (3-1)(3-1) = 4. The p-value  is equal to 0.048, indicating that there is moderate evidence against the null hypothesis that the three union locals are the same. \\n    You might like to use Populations Homogeneity Test to perfor this test.   Further Readings: Agresti A., Categorical Data Analysis,  Wiley, 2002.Clark Ch., and L. Schkade, Statistical Analysis for Administrative Decisions,  South-Western Pub., 1979.     Test for Equality of Several Population Medians Generally, the median provides a better measure of location than the mean when there are some extremely large or small observations; i.e., when the data are skewed  to the right or to the left. For this reason, median income is used as the measure of location for the U.S. household income. Suppose we are interested in testing the equality of the medians of k number  of populations with respect to the same continuous random variable. The first step in calculating the test statistic is to compute the common  median of the k samples combined. Then, determine for each group the number  of observations  falling above and below the common median. The resulting frequencies are arranged in a 2 by k crosstable. If the k samples are, in  fact, from populations with the same median, one expects about one half the score in each sample to be above the combined median and about one half  to be below. In the case that some observations are equal to the combined  median, one may drop those few observations, in constructing a 2 by k crosstable.  Under this condition, now the Chi-square statistic may be computed and compared  with the p-value of Chi-square distribution with d.f. = k-1. An illustrative application: Do public and private primary school  teachers differ with respect to their salary?  The data from a random sample are given in the following table (in thousands of dollars per year). \\n   Public PrivatePublic Private  35 29   25  50  26 50 \\n27  37   27 43  45 34   21   22  46  31    2742 33 38  47 26    23 42 46 \\n 25  32 41       The test of hypothesis is: H0: The public and private school  teachers\\' salaries are almost the same.  The median of all data (i.e., combined) is 33.5. Now determine in each group the number of observations  falling above and below the common median  of 33.5. The resulting frequencies are shown in the following table:      Crosstable for the  public and private school teachers\\'    Public  Private  Total   Above median 6 8 14   Below median 10 4 14   Total 16 12 28    The Chi-square statistic based on this table is 2.33. The p-value for   the computed test statistic with d.f. = (2-1)(2-1) = 1 is 0.127, therefore,   we are unable to reject the null hypothesis.     You might like to use Testing Medians to perform this test.       Goodness-of-Fit Test for Probability Mass Functions    There are other tests that might use the Chi-square, such as goodness-of-fit     test for discrete random variables. Therefore, Chi-square is a statistical test that measures \"goodness-of-fit\".     In other words, it measures how much the observed or actual frequencies differ  from the expected or predicted frequencies. Using a Chi-square table will enable you to discover how significant the difference is. A null hypothesis in the context of the Chi-square test is the model that you use to calculate  your expected or predicted values. If the value you get from calculating the  Chi-square statistic is sufficiently high (as compared to the values in the \\n    Chi-square table),  it tells you that your null hypothesis is probably wrong.  Let Y1, Y 2, . . ., Y n be a set of independent  and identically distributed discrete random variables. Assume that the probability  distribution of the Y i\\'s has the probability mass function f o (y). We can divide the set of all possible values of Yi,  i = {1, 2, ..., n}, into m non-overlapping intervals D1, D2,  ...., Dm. Define the probability values p1,    p2 , ..., pm as; \\n    p1 = P(Yi Î D1)\\np2 = P(Yi Î D2) \\n    : pm = P(Yi Î Dm)     Where the symbol Î means, \"an element of\". \\n    Since the union of the mutually exclusive intervals D1, D2,       ...., Dm is the set of all possible values for the Yi\\'s,   (p1 + p2 + .... + pm)   = 1. Define the set of discrete random  variables X1, X2, ...., Xm, where \\n    X1= number of Yi\\'s whose valueÎD1     X2= number of Yi\\'s whose value Î D2 :: Xm= number of Yi\\'s whose value Î Dm and (X1+ X2+ .... + Xm) = n. Then the  set of discrete random variables X1, X2, ...., Xmwill  have a multinomial probability distribution with parameters n and  the set of probabilities {p1, p2,  ..., pm}. If the intervals D1, D2,   ...., Dm are chosen such that npi ³  5 for i = 1, 2, ..., m, then; \\n             C = S (Xi - npi)         2/ npi.                The sum is over i = 1, 2,..., m. The results is distributed as c2\\nm-1.      For the goodness-of-fit sample test, we formulate the null and alternative       hypothesis as     H0 : fY(y) = fo(y) \\n      Ha : fY(y) ¹ fo(y)     At the a level of significance, H0 will be rejected       in favor of Ha if \\n             C = S (Xi - npi)         2/ npi     is greater than c2 m\\nHowever, it is possible that in a goodness-of-fit test, one or more of  the parameters of fo(y) are unknown. Then the probability values  p1, p2, ..., pm  will have to be estimated by assuming that H0 is true and calculating  their estimated values from the sample data. That is, another set of probability  values p\\'1, p\\'2, ..., p\\'mwill need to be computed so that the values (np\\'1,  np\\'2, ..., np\\'m) are the estimated  expected values of the multinomial random variable (X1, X2,  ...., Xm). In this case, the random variable C will still have       a Chi-square distribution, but its degrees of freedom will be reduced. In   particular, if the probability function fo(y) has r  unknown parameters,        C = S (Xi - npi) \\n        2/ npi\\n is distributed as c2 m-1-r. For this goodness-of-fit test, we formulate the null and alternative hypothesis   as  H0: fY(y) = fo(y)      Ha: fY(y) ¹ fo(y)  At the a level of significance, H0  will be rejected in favor of Ha if C is greater than c2 m-1-r.   An Application: A die is thrown 300 times and the following frequencies  are observed. Test the hypothesis that the die is fair at level 0.05.   Under  the null hypothesis that the die is fair, the expected frequencies are all  equal to 300/6 = 50. Both the Observed (O) and Expected (E) frequencies  are recorded in the following table together with the random variable Y that represents the number on each sides of the die:       Goodness-of-fit Test \\n  For Discrete Variables  Y1234\\n5 6 O57 43 5955\\n63 23  E 50 50 50 50\\n50 50 The quantity  \\nc 2 = S [(O - E )2 / E] = 22.04   is a measure of the goodness-of-fit. If there is a reasonably good fit  to the hypothetical distribution, this measure will be very close to zero. Since c 2 n-1, 0.95 = 11.07, we reject the null hypothesis that the die is a fair one.   You might like to  use this JavaScript to perform this test.     For statistical equality of two random  variables characterizing two populations, you might like to use the Kolmogorov-Smirnov Test if you have two independent sets of random  observations, one from each population.    Compatibility of Multi-Counts Test\\nIn some applications, such as quality control, it is necessary to check if the process is under control. This can be done by testing if there are significant differences between number of \"counts\", taken over k equal-periods of times. The counts are supposed to have been obtained under comparable conditions. The null hypothesis is:  H0: There is no significant difference   between number of \"counts\" taken over k equal-periods of times. Under the null hypothesis, the statistic:    S (Ni - N)2/N   has a Chi-square distribution with d.f. = k-1. Where i is the count\\'s number,  Ni is its counts, and N = SNi/k. One may extend this useful test to where the duration of obtaining the  ith count is ti. Then the above test statistic becomes:    S [(Ni - tiN)2/  tiN]     and has a Chi-square distribution with d.f. = k-1, where i is the count\\'s number,  Ni is its counts, and N = SNi/Sti.  You might like to use the Compatibility of Multi-Counts  JavaScript to check your computations,  and to perform some numerical experimentation for a deeper understanding   of the concepts.  \\n Necessary Conditions for the Above Chi-square Based Testing\\n    Like any statistical test procedures, the Chi-square based testing must meet certain necessary conditions to apply; otherwise, any obtained conclusion might be wrong or misleading. This is  true in particular for using the Chi-square-based  test for  cross-tabulated data. Necessary conditions for the Chi-square based tests for crosstable data are:    Expected values greater than 5 in 80% or more of the cells. Moreover, if number of cells is fewer than 5, then all expected values must be greater than 5.   An  Example: Suppose the monthly number of accidents reported in a factory in three eight-hour shifts is 1, 7, and 7, respectively.  Are the working conditions and the exposure to risk similar for all shifts? Clearly, the answer must be, No they are not. However, applying the goodness-of-fit,  at 0.05, under the null hypothesis that there are no differences in the number  of accidents in three shifts, one expects 5, 5, and 5 accidents in each  shift. The Chi-square test statistic is:    c 2 = S [(O - E )2 / E] = 4.8   However, since c 2 n-1, 0.95 = 5.99, there is no reason to  reject that there is no difference, which is a very strange conclusion.  What is wrong with this application?   You might like to use this JavaScript to verify your computation.       Testing the Variance: Is the Quality that Good?    Suppose a population has a normal distribution. The manager is to test a specific \\n    claim made about the quality of the population by way  of testing its variance     s2. Among three possible scenarios, the interesting   case is in testing the following null hypothesis based on a set of n random sample observations: \\n    H0: Variation is about the claimed  value.\\nHa: The variation is more than what \\n      is claimed, indicating the quality is much lower than expected.  Upon computing the estimated variance S2  based on n observations, then the statistic: \\n      c½ = [(n-1) . \\n        s2] / s2  has a Chi-square distribution with degree of freedom n = n - 1. This statistic is then used for testing the above  null hypothesis.  You might like to use Testing the Variance JavaScript to check your computations.     \\n  Testing the Equality of Multi-Variances    The equality of variances across populations is called homogeneity of variances  or \\n    homoscedasticity.  Some statistical tests, such as testing equality of the means by the t-test and ANOVA, assume that the data come from populations that have the same variance, even if the test rejects the null hypothesis of equality of population means.  If this condition of homogeneity of variance is not met, the statistical test results may not     be valid. Heteroscedasticity refers to lack of homogeneity of variances.     Bartlett\\'s Test is used to test if k  samples have equal variances. It compares the Geometric Mean of the group  variances to the arithmetic mean; therefore, it is a Chi-square statistic with (k-1) degrees of freedom, where k is the number of categories in the      independent variable. The test is sensitive to departures from normality.  The sample sizes do not have to be equal but each must be at least 6. Just  like the two population t-test, ANOVA can go wrong when the equality of variances  condition is not met.  The Bartlett test statistic is designed to test for equality of variances  across groups against the alternative that variances are unequal for at  least two groups. Formally,    H0: All variances are almost equal.   The test statistic: \\n       B = {S [(ni -1)LnS2]  S [(ni -1)LnSi2]}/ C   In the above, Si2 is the variance of the ith group, ni is the sample size of the ith group, k is the number of groups, and S2 is the pooled variance. The pooled variance is a  weighted average of the group variances and is defined as:   S2 = {S [(ni \\n        -1)Si2]} / S [(ni -1)], over all i = 1,  2,..,k      \\nand    C = 1 + {S [1/(ni -1)] - 1/ S [1/(ni -1)] }/[3(k+1)].    You might like to use the Equality of Multi-Variances JavaScript tor check your computations,  and to perform some numerical experimentation for a deeper understanding of the concepts.  Rule of 2: For 3 or more populations,   there is a practical rule known as the \"Rule of 2\". According to this rule,  one divides the highest variance of a sample by the lowest variance of the  other sample. Given that the sample sizes are almost the same, and the value  of this division is less than 2, then, the variations of the populations  are almost the same.  Example: Consider the following three random samples from three  populations, P1, P2, P3: \\n\\n \\n   Sample P1Sample P2Sample P3 2517 8  25 2110 201714 182516 13191262114  5 156 221616 252413 10236 N101010Mean16.9019.8011.50  Std.Dev.7.873.523.81 SE Mean2.491.111.20   \\n  The ANOVA Table\\n Sources of VariationSum of Squares Degrees of FreedomMean Squares F-Statistic Between Samples79.40\\n239.704.38 Within Samples244.90279.07 \\xa0  Total324.3029\\xa0\\xa0\\n\\nWith an F = 4.38 and a p-value of .023, we reject the null at a = 0.05. This is not good news, since ANOVA, like the two-sample  t-test, can go wrong when the equality of variances condition is not met.    \\nFurther Readings: Hand D., and C. Taylor, Multivariate  Analysis of Variance and Repeated Measures, Chapman and Hall, 1987.Miller R. Jr, Beyond ANOVA: Basics of Applied Statistics, Wiley,  1986. \\n\\n Correlation Coefficients Testing\\nThe Fisher\\'s Z-transformation is a useful tool in the circumstances in which two or more independent correlation coefficients are to be compared simultaneously.   To perform such a test one may evaluate the Chi-square statistic:\\n\\nc2 = S[(ni - 3).Zi2] - [S(ni - 3).Zi]2 / [S(ni - 3)],\\xa0 \\xa0 the sums are over all i = 1, 2, .., k.\\nWhere the Fisher Z-transformation is Zi = 0.5[Ln(1+ri) - Ln(1-ri)],\\xa0 \\xa0provided | ri | ¹ 1.Under the null hypothesis:H0: All correlation coefficients are almost equal.   \\nThe test statistic c2 has  (k-1) degrees of freedom, where k is the number of  populations.\\n An Application: Consider the following correlation coefficients obtained by random sampling form ten independent populations. \\n\\n\\n\\n\\n\\nPopulation \\n              Pi\\n\\n\\nCorrelation \\n              ri\\n\\n\\nSample \\n              Size ni\\n\\n\\n\\n\\n1\\n\\n\\n0.72\\n\\n\\n67\\n\\n\\n\\n\\n2\\n\\n\\n0.41\\n\\n\\n93\\n\\n\\n\\n\\n3\\n\\n\\n0.57\\n\\n\\n73\\n\\n\\n\\n\\n4\\n\\n\\n0.53\\n\\n\\n98\\n\\n\\n\\n\\n5\\n\\n\\n0.62\\n\\n\\n82\\n\\n\\n\\n6\\n\\n\\n0.21\\n\\n\\n39\\n\\n\\n\\n\\n7\\n\\n\\n0.68\\n\\n\\n91\\n\\n\\n\\n\\n8\\n\\n\\n0.53\\n\\n\\n27\\n\\n\\n\\n\\n9\\n\\n\\n0.49\\n\\n\\n75\\n\\n\\n\\n\\n10\\n\\n\\n0.50\\n\\n\\n49\\n\\n\\n\\n\\n\\n     Using the above formula  c2-statistic = 19.916, that has a p-value of 0.02.  Therefore, there is  moderate evidence against the null hypothesis.In such a case, one may omit a few outliers from the group, then use the Test for Equality of Several Correlation Coefficients applet. Repeat this process until a possible homogeneous sub-group may emerge.\\nYou might need to use Sample Size Determination JavaScript at the design stage of your statistical investigation in decision making with specific subjective requirements.\\n\\n\\n Regression Modeling and Analysis\\n    Many problems in analyzing data involve describing how variables are related. The simplest of all models describing the relationship between two variables is a linear, or straight-line, model. Linear regression is always linear in  the coefficients being estimated, not necessarily linear in the variables.  The simplest method of drawing a linear model is to \"eye-ball\" a line through the data on a plot, but a more elegant, and conventional method is that  of least squares, which finds the line minimizing the sum of the vertical distances between observed points and the fitted line. Realize that fitting the \"best\" line by eye is difficult, especially when there is much residual variability   in the data. Know that there is a simple connection between the numerical coefficients  in the regression equation and the slope and intercept of the regression line.  Know that a single summary statistic, like a correlation coefficient, does not tell the whole story. A scatterplot is an essential complement to examining  the relationship between the two variables. Again, the regression line is a group of estimates for the variable plotted  on the Y-axis. It has a form of y = b + mx, where m is the slope of the line. \\n      The slope is the rise over run. If a line goes up 2 for each 1 it goes over,  then its slope is 2. Formulas and Notations:   = S x(i)/n        This is just the mean of the x values. \\n       = S y(i)/n         This is just the mean of the y values.       Sxx = S(x(i) \\n        - )2 = Sx(i)2 - [ Sx(i) ] 2 / n       Syy = S(y(i)         - )2 = Sy(i)2 - [ Sy(i) ] 2 / n       Sxy = S(x(i)         - )(y(i) - ) = Sx(i).y(i) - [ Sx(i) . Sy(i)] / n       Slope m = Sxy / Sxx Intercept, b =  - m .  y-predicted = yhat = mx + b.Residual = Error = y - yhat.Serrors =  S(y - yhat)2.Standard deviation of residuals = Sres = [Serrors / (n-2)]½.Standard error of the slope (m) = Sres / Sxx½.Standard error of the intercept (b) =  Sres[(Sxx + n. 2) /(n.Sxx] ½.     The regression line goes through a point with coordinates of (mean of x values, mean of y values), known as the mean-mean point.  If you plug each x in the regression equation, then you obtain a predicted value for y. The difference between the predicted y and the observed y is called a residual, or an error term.  Some errors are positive and some are negative. The sum of squares of the errors plus the sum of squares of the estimates add up to the sum of squares of Y. The regression line is the line that minimizes the variance of the errors. The mean error is zero; so, this means that it minimizes the sum of the squares errors.The reason for finding the best fitting line is so that you can make a reasonable prediction of what y will be if x is known (not vise-versa). r2 is the variance of the estimates divided by the variance of Y.   r is the size of the slope of the regression line, in terms of standard deviations. In other words, it is the slope of the regression line if we use the standardized X and Y. It is how many standard deviations of Y you would go up, when you go one standard deviation of X to the right. Coefficient of Determination: Another measure of the closeness of the points to the regression line is the Coefficient  of Determination:      r2 = Syhat yhat / Syy \\nwhich is the amount of the squared deviation in Y, that is explained by the points on the least squares regression line. Homoscedasticity and Heteroscedasticity: \\n      Homoscedasticity (homo = same, skedasis = scattering) is a word used to describe the distribution of data points around the line of best fit. The opposite  term is heteroscedasticity. Briefly, homoscedasticity means that data points are distributed equally about the line of best fit. Therefore, homoscedasticity means constancy of variance over all the levels of factors. Heteroscedasticity means that the data points cluster or clump above and below the line in a non-equal pattern. Standardized Regression Analysis: The scale of measurements used to measure  X and Y has major impact on \\n   the regression equation and correlation coefficient. This impact is more drastic comparing two regression equations having different scales of measurement. To overcome these drawbacks, one must standardize both X and Y prior to constructing the regression and interpreting the results. In such a model, the slope is equal to the correlation coefficient r. Notice that the derivative of function Y with respect to dependent variable X is the correlation coefficient.  Therefore, there is a nice similarity in the meaning of r in statistics and the derivative from calculus, in that its sign and its magnitude reveal the increasing/decreasing and the rate of change, as the derivative of a function do.\\nIn the usual regression modeling the estimated slope and intercept are correlated; therefore, any error in estimating the slope influences the estimate of the intercept. One of the main advantages of using the standardized data is that the intercept is always equal to zero.\\n\\n  Regression when both X and Y are random: Simple linear least-squares regression has among its conditions that the  data for the independent (X) variables are known without error. In fact,  the estimated results are conditioned on whatever errors happened to be present in the independent data sets. When the X-data have an error associated  with them the result is to bias the slope downwards. A procedure known as Deming regression can handle this problem quite well. Biased slope estimates  (due to error in X) can be avoided using Deming regression. \\nIf X and Y are random variables, then the correlation coefficient R is often referred to as the Coefficient of Reliability.\\n\\n\\n The Relationship Between Slope and Correlation Coefficient:  By a little bit of algebraic manipulation, one can show that the coefficient of correlation is related to the slope of the two regression lines:  Y on X, and X on Y, denoted by m yx and mxy, respectively:   R2 = m yx . mxy \\n Lines of regression through the origin:  Often the conditions of a practical problem require that the regression line go through the origin (x = 0, y = 0).  In such a case, the regression line has one parameter only, which is its slope: m =  S (xi ´ yi)/  Sxi2\\n \\nNotice that, for the models with the omission of the intercept, it is generally agreed that R2 should not be defined or even considered.\\n\\nParabola models:  Parabola regressions have three coefficients with a general form: Y = a + bX + cX2,\\nwhere\\n\\nc = { S (xi  - xbar)2×yi - n[S(xi - xbar)\\n2× Syi]} / {n S(xi - xbar)\\n4  - [S(xi - xbar)2] 2}\\n\\nb = [S(xi- xbar) yi]/[ S(xi - xbar)2] - 2×c×xbar \\na = {Syi - [c× S(x i - xbar) 2)}/n  -  (c×xbar×xbar + b×xbar),\\n\\nwhere xbar is the mean of xi\\'s.\\nApplications of quadratic regression include fitting the supply and demand curves in econometrics and fitting the ordering cost and holding cost functions in inventory control for finding the optimal ordering quantity.\\n\\nYou might like to use Quadratic Regression JavaScript to check your hand computation.  For higher degrees than quadratic, you may like to use the \\nPolynomial Regressions JavaScript.\\n\\n\\n\\nMultiple Linear Regression: The objectives in a multiple regression problem are essentially the same as for a simple regression.  While the objectives remain the same, the more predictors we have the calculations and interpretations become more complicated.  With multiple regression, we can use more than one predictor. It is always best, however, to be parsimonious, that is to use as few variables as predictors as necessary to get a reasonably accurate forecast. Multiple regression is best modeled with commercial package such as  SAS and  SPSS. The forecast takes the form: \\n\\n\\nY = b0 + b1X1 + b2X2 + . . .+ bnXn,\\n\\n\\nwhere b0 is the intercept, b1,  b2,  . . . bn are coefficients representing the contribution of the independent variables X1, X2,..., Xn.\\n\\nFor small sample size, you may like to use the Multiple Linear Regression JavaScript.\\n\\n\\nWhat Is Auto-Regression: In time series analysis and forecasting techniques, often linear regression is use to combine present and past values of an observation in order to forecast its future value.  The model is called an autoregressive model.  For details and implementation process visit Autoregressive Modeling JavaScript.\\n\\nWhat Is Logistic Regression: Standard logistic regression is a method for modeling binary data (e.g., does a person smoke or not?, does a person survive a disease, or not?). Polygamous logistic regression is a method for modeling more than two options (e.g., does a person take the bus, drive a car or take the subway? does an office use WordPerfect, Word, or other office-ware?).\\n Why Linear Regression? The study of corn shell (i.e., ear of corn) height versus rainfall has shown to have the following regression curve:\\n  \\nClearly, the relationship is highly nonlinear; however, if we are interested in a \"small\" range (say, for a specific geographical area, like southern region of the state of Maryland) then the condition of linearity might be satisfactory.  A typical application is depicted in the above figure where we are interested in predicting the height of corn in an area with rainfall in the range of [a, b].  Magnifying process of scale for this range allows us to fit a useful linear regression.   If the range is not short enough, then one may sub-divide the range accordingly by applying the same process of fitting a few lines, one for each sub-interval. \\n Structural Changes:  When a regression model has been estimated using the available data set, an additional data set may sometimes become available.  To test if previous model is still valid or the two separate models are equivalent or not, one may use the analysis of covariance testing described on this site.\\nYou might like to use the Regression Analysis JavaScript to check your computations and to perform some numerical experimentation for a deeper understanding of the concepts.   \\nFurther Reading:Chatterjee S., B. Price, and A. Hadi, Regression Analysis by Example,  Wiley, 1999.\\n\\n\\nRegression Modeling Selection Process\\n When you  have more than one regression equation based on data, to select the \"best model\", you should compare:   R-squares: That is, the percentage of variance [in fact, the sum of  squares] in Y accounted for by variance in X captured by the model. When you want to compare models of different sizes (different numbers of  independent variables (p) and/or different sample sizes n), you must use  the Adjusted R-Square, because the usual r-square tends to grow with the  number of independent variables.   r2 a \\n          = 1 - (n - 1)(1 - r2)/(n - p - 1)\\n          Standard deviation of error terms, i.e., observed y-value - predicted y-value for each x.Trends in errors as a function of control variable x. Systematic trends are not uncommon. The T-statistic of individual parameters. The values of the parameters and its content to content underpinnings.  Fdf1 df2 value for overall assessment.  Where df1 (numerator degrees of freedom) is the number of linearly independent  predictors in the assumed model minus the number of linearly independent predictors in the restricted model; i.e., the number of linearly independent restrictions imposed on the assumed model, and df2 (denominator degrees of freedom) is the number of observations minus the number of linearly independent \\n      predictors in the assumed model. The observed F-statistic should exceed not merely the selected critical  value of F-table, but at least four times the critical value.  \\n  Regression Analysis Process  Click on the image to enlarge it and THEN print it    Finally in statistics for business, there exists an opinion that with more  than 4 parameters, one can fit an elephant so that if one attempts to fit  a regression funtion  that depends on many parameters, the result should not be regarded  as very reliable. Further Reading:Draper N., and H. Smith, Applied Regression Analysis, Wiley, 1998.\\n\\n\\n Covariance and Correlation\\n    Suppose that X and Y are two random  variables for the outcome of a random experiment. The covariance of X and Y is defined by   Cov (X, Y) = E{[X - E(X)][Y - E(Y)]} \\n           and, given that the variances are strictly positive, the correlation of X and   Y is defined by    r (X, Y) = Cov(X, Y) / [sd(X) . sd(Y)]      Correlation is a scaled version of covariance; note that the two parameters  always have the same sign (positive, negative, or 0). When the sign is positive,  the variables are said to be positively correlated; when the sign is negative, the variables are said to be negatively correlated; and when it is 0, the variables are said to be uncorrelated.Notice that the correlation between two random variables is often due only to the fact that both variables are correlated with the same third variable. \\n As these terms suggest, covariance and correlation measure a certain kind  behavior in both variables. Correlation is very similar to the derivative of a function that you may have studies in high school.Coefficient of Determination: The square of correlation coefficient r 2 indicates the proportion of the variation in one variable that can be associated with the variance in the other variable. The three typical possibilities are depicted in the following figure:\\n \\nThe proportion of shared variance by two variables for the different values of the coefficient of determination:r2 = 0, r2 = 1, and r2 = 0.25, as shown by the shaded areas in this figure. \\n\\nProperties: The following exercises give some basic properties of  expected values. The main tool that you will need is the fact that expected  value is a linear operation.  You might like to use this Applet in performing some numerical experimentation to:  \\n\\nShow that E[X/Y] ¹ E(X)/E(Y). Show that E[X ´ Y] ¹ E(X) ´ E(Y). Show that [E(X ´\\tY)2] £ E(X2)  ´ E(Y2). Show that [E(X/Y)n] ³ E(Xn)/E(Yn), for any n. Show that Cov(X, Y) = E(XY) - E(X)E(Y). \\nShow that Cov(X, Y) = Cov(Y, X). Show that Cov(X, X) = V(X). Show that: If X and Y are independent random variables, then  Var(XY) = 2 V(X) ´\\tV(Y) + V(X)(E(Y))2 + V(Y)(E(X))2.    \\n\\nPearson, Spearman, and Point-Biserial Correlations\\n    \\nThere are measures that describe the degree to which two variables are linearly related. For the majority of these measures, the correlation is expressed as a coefficient that ranges from 1.00 to -1.00.  A value of 1 is indicating a perfect linear relationship, such that knowing the value of one variable will allow perfect prediction of the value of the related value.  A value of 0 is indicating no predictability by a linear model. With negative values indicating that, when the value of one variable is higher than average, the other is lower than average (and vice versa); and positive values indicating that, when the value of one variable is high, so is the other (and vice versa). \\n\\n Correlation is similar to the derivative you have learned in calculus (a deterministic course). The Pearson\\'s product correlation is an index of the linear relationship between two variables. Formulas and Notations:  = S xi / n. This is just the mean of the x values.   = S yi / n. This is just the mean of the y values.  Sxx = S(xi - )2 = Sxi2 - [ Sxi ] 2 / n.Syy = S(yi - )2 = Syi2 - [ Syi ] 2 / n. Sxx = S(xi - )(yi - ) = S(xi yi) - [ Sxi . Syi ] / n. \\nThe Pearson\\'s correlation is \\n     r = Sxy / (Sxx ´ Syy)0.5   A positive relationship indicates that if an individual value of x is above the  mean of x\\'s, then this individual x is likely to have a y value that is above the mean of y\\'s, and vice versa.  A negative relationship would be an x score above the mean of x and a y score below the mean of y. It is a measure of the relationship between variables and an index of the proportion of individual differences in one variable that can be associated with the individual differences in another variable.  \\n\\nNotice that, the correlation coefficient is the mean of the cross-products of scores. Therefore, if you have three values for  r of .40, .60, and .80, you cannot say that the difference between   r = .40 and  r = .60 is the same as the difference between  r =.60 and  r = .80, or that  r = .80 is twice as large as  r = .40 because the scale of values for the correlation coefficient is not interval or ratio, but ordinal. Therefore, all you can say is that, for example, a correlation coefficient of +.80 indicates a high positive linear relationship and a correlation  coefficient of +.40 indicates a some what lower positive linear relationship. \\n The square of the correlation coefficient equals the proportion of the total variance in Y that can be associated  with the variance in x. It can tell us how much of the total variance of one variable can be associated with the variance of another variable.\\n Note that a correlation coefficient is done on linear correlation. If the data forms a parabola, then a linear correlation of x and y will produce an r-value equal to zero. So one must be careful and look at data.  The standard statistics for hypothesis testing: H0: r = r0,  is the Fisher\\'s normal transformation:  z = 0.5[Ln(1+r) - Ln(1-r)],\\xa0 \\xa0 with mean m = 0.5[Ln(1+ r0) - Ln(1-r0)],\\xa0 \\xa0and standard deviation s =  (n-3)-½.  Having constructed a desirable confidence interval, say [a, b], based on statistic Z, it has to be transformed back to the original scale. That is, the confidence interval is: (e2a -1)/ (e2a +1), \\xa0 \\xa0\\xa0 \\xa0\\n(e2b -1)/ (e2b +1).\\n\\n\\nProvided | r0 | ¹ 1,  and  | r0 | ¹ 1, and n is greater than 3.\\n\\nAlternatively, \\n \\xa0 \\xa0\\xa0 \\xa0{1+ r - (1-r) exp[2za/(n-3)½]} / {1+ r + (1-r) exp[2za/(n-3)½]} , \\xa0 \\xa0and\\n \\n{1+ r - (1-r) exp[-2za/(n-3)½]} / {1+ r + (1-r) exp[-2za/(n-3)½]}\\n\\n\\nYou might like to use this calculator for your needed computation. You may perform Testing the Population Correlation Coefficient .\\n       Spearman rank-order correlation is used as a non-parametric version of Pearson\\'s. It is expressed as:   r  = 1 - (6 S d2) / [n(n2 - 1)],   where d is the difference rank between each X and Y pair. Spearman correlation coefficient can be algebraically derived from the Pearson correlation formula by making use of sums of series. Pearson contains expressions for S X(i), S Y(i),  S X(i)2, and SY(i)2. In the Spearman case, the X(i)\\'s and Y(i)\\' are ranks, and so the sums of the ranks, and the sums of the ranks squared, are entirely determined by  the number of cases (without any ties).   S i = (n+1)n/2, S i2 = n(n+1)(2n+1)/6.\\nThe Spearman formula then is equal to:  [12P - 3n(n+1)2] / [n(n2 - 1)],  where P is the sum of the product of each pair of ranks X(i)Y(i). This reduces to:  r = 1 - (6 S d2) / [n(n2 - 1)],  where d is the difference rank between each x(i) and y(i) pair. An important consequence of this is that if you enter ranks into a Pearson formula, you get precisely the same numerical value as that obtained by entering the ranks into the Spearman formula. This comes as a bit of a shock to those who like to adopt simplistic slogans, such as \"Pearson is for interval data, Spearman is for ranked data\". Spearman doesn\\'t work too well if there are many tied ranks. That\\'s because the formula for calculating the sums of squared ranks no longer holds true. If one has many tied ranks, use the Pearson formula.   One may use this measure as a decision-making tool:      Value of |r|   Interpretation\\n  0.00 - 0.40  Poor 0.41 - 0.75 \\nFair0.76 - 0.85 Good 0.86 - 1.00 \\nExcellentThis interpretation is widely accepted, and many scientific journals routinely publish papers using this interpretation for the estimated result, and even \\nfor the test of hypothesis. Point-Biserial Correlation is used when  one random variable is binary (0, 1) and the other is a continuous random variable;  the strength of relationship is measured by the point-biserial correlation:  \\nr = (X1 - X0)[pq/S2] ½\\nWhere X1and X0 are the means of scores having 1, and 0 values, and p and q are their proportions, respectively. S2 is the sample variance of the continuous random variable. This is a simplified version of the Pearson correlation for the case when one of \\n      the two random variables is a (0, 1) Nominal random variable.\\nNote also that r has the shift-invariant property for any positive scale. That is ax + c, and by + d, have same r as x and y, for any positive a and b. \\n\\n Correlation, and Level of Significance It is intuitive that with very few data points, a high correlation may not be statistically significant. You may see statements such as, \"correlation is significant between x and y at the a = .005 level\"  and \"correlation is significant at the a = .05 level.\" The question is:  how do you determine these numbers? Using the simple correlation r,  the formula for F-statistic is:\\n   F= (n-2) r2 / (1-r2), \\xa0 \\xa0 where n is at least 2.  As you see, F statistic  is monotonic function with respect to both: r2, and the sample size n.  \\n\\nNotice that the test for the statistical significance of a correlation coefficient requires that the two variables be distributed as a bivariate normal.\\n\\n  Independence vs. Correlated\\n    In the sense that it is used in statistics; i.e., as an assumption in applying a statistical test; a random sample from the entire population provides a set of random variables X1,...., Xn, that are identically distributed and mutually independent. Mutually independent is stronger than pairwise independence. The random variables are mutually independent if their joint distribution is equal to  the product of their marginal distributions.  In the case of joint normality, independence is equivalent to zero correlation,   but not in general. Independence will imply zero correlation but not conversely. Not that not all random variables have a first moment, let alone a second moment, and hence there may not be a correlation coefficient. However; if the correlation coefficient of two random variables is not zero then the random variables are not independent. \\n How to Compare Two Correlation Coefficients?\\nGiven that two populations have normal distributions,  we wish to test for the following null hypothesis regarding the equality of correlation coefficients:\\n\\nHo: r 1 = r \\n    2,\\n\\n based on two observed correlation coefficients r1, and r2, obtained from two random sample of size n1 and n2, respectively, provided | r1 | ¹ 1,  and  | r2 | ¹ 1, and  n1,  n2 both are greater than 3.  Under the null hypothesis and normality condition , the test statistic is: \\n        Z = (z1 - z2) / [ 1/(n1-3) + 1/(n2-3) ]½   where:  z1 = 0.5 Ln [ (1+r1)/(1-r1) ],    z2 = 0.5 Ln [ (1+r2)/(1-r2) ],  \\nand n1= sample size associated with r1, and n2 =sample size \\n      associated with r2.   The distribution of the Z-statistic is the standard  Normal(0,1); therefore, you may reject H0 if |Z|> 1.96 at the 95% confidence level. \\n\\nAn Application: Suppose r1 = 0.47, r2 = 0.63 are obtained from two independent random samples of size  n1=103, and n2 = 103, respectively. Therefore, the z1 = 0.510, and z2 = 0.741, with Z-statistics:\\n\\n\\nZ = (0.510 - 0.7)/ [1/(103-3) + 1/(103-3)]½ = -1.63\\n\\n\\nThis result is not within the rejection region of the two-tails critical values at a = 0.05, therefore is not significant. Therefore, there is not sufficient evidence to reject the null hypothesis that the two correlation coefficients are equal\\n\\nClearly, this test can be modified and applied for test of hypothesis regarding population correlation r based on observed r obtained from a random sample of size n:\\n\\n\\n        Z = (zr - zr ) / [1/(n-3) ]½, \\n\\nprovided | r | ¹ 1,  and  | r | ¹ 1, and n is greater than 3.\\n\\nTesting the Equality of Two Dependent Correlations: In testing the hypothesis of no difference between two population correlation coefficients:\\n\\nH0: r (X, Y) = r (X, Z)\\n\\nAgainst the alternative:\\n\\nHa: r (X, Y) ¹ r (X, Z)\\n\\n with a common covariare X, one may use the following test statistics:\\n\\nt = { (rxy - rxz ) [ (n-3)(1 + ryz)]½ ] } / {2(1-rxy2 - rxz2 - ryz2 + 2rxyrxzryz )}½,    \\n\\nwith n - 3 degree of freedom, where n is the tripled-ordered sample size, provided all absolute value of r\\'s are not equal to 1. \\n\\nNumerical example: Suppose n = 87, rxy = 0.631, rxz = 0.428, and ryz = 0.683, then t-statistic is equal to 3.014,  with p-value equal to 0.002, indicating a strong evidence against the null hypothesis.\\n\\n\\nAdjusted R2: \\n\\nIn modeling selection process based of  R2 values, it is often necessary and meaningful to adjust the  R2\\'s for their degrees of freedom. Each Adjusted  R2 is calculated by:\\n\\n\\n1 - [(n - i)(1 - R2)] / (n - p),\\n\\nwhere i is equal to 1 if there is an intercept and 0 otherwise; n is the number of observations used to fit the model; and p is the number of parameters in the model. \\n\\nYou might like to use  the Testing the Population Correlation Coefficient JavaScript in performing some numerical experimentation for validating and a deeper understanding of the concepts. \\n \\nPlanning, Development, and Maintenance of a Linear Model\\nA. Planning:\\nDefine the problem; select response; suggest variables. Are the proposed variables fundamental to the problem, and are they variables? Are they measurable/countable? Can one get a complete set of observations at the same time?  Ordinary regression analysis does not assume that the independent variables are measured without error. However, they are conditioned on whatever errors happened to be present in the independent data set. Is the problem potentially solvable?  Find  correlation matrix and first regression runs (for a subset of data).  Find the basic statistics, correlation matrix.  How difficult is the problem?  Compute the Variance Inflation Factor:\\n\\n\\n\\n VIF = 1/(1 -rij),  for all i, j. \\n\\n\\nFor moderate VIF\\'s, say between  2 and 8, you might be able to come-up with a \\x91good\\' model.     Inspect rij\\'s; one or two must be large. If all are small, perhaps the ranges of the X variables are too small. Establish goal; prepare budget and time table.       a.  The final equation should have Adjusted R2  =  0.8 (say). \\n        b. Coefficient of Variation of say; less than 0.10  c. Number of predictors should not exceed p (say, 3), (for example for  p = 3, we need at least 30 points). Even if all the usual assumptions for a regression model are satisfied, over-fitting can ruin a model\\'s usefulness. The widely used approach is the data reduction method to deal with the cases where the number of potential predictors is large in comparison with the number of observations.  d. All estimated coefficients must be significant at m = 0.05 (say).  e. No pattern in the residuals Are goals and budget acceptable? \\nB. Development of the Model:   Collect date; check the quality of date; plot; try models;  check the regression conditions. \\nConsult experts for criticism.  Plot new variable and examine same fitted model. Also transformed Predictor Variable may be used.   Are goals met?  Have you found \"the best\" model?  C. Validation and Maintenance of the Model:\\n  Are parameters stable over the sample space?   Is there a lack of fit? \\n      Are the coefficients reasonable? Are any obvious variables missing? Is the equation usable for control or for prediction?   Maintenance of the Model.  Need to have control chart to check the model periodically by statistical  techniques.   You might like to use Regression Analysis with Diagnostic Tools in performing regression analysis. \\n   Conditions and the Check-list for Linear Models Almost all models of reality, including regression models, have assumptions that must be verified in order that the model has power to test hypotheses and for it to be able to predict accurately. \\n    The following is the list of basic assumptions (i.e., conditions) and the tools to check these necessary conditions.  Any undetected outliers may have major impact on the regression model. Outliers are a few observations that are not well fitted by the \"best\" available model.  In such case one, must first investigate the source of data, if there is no doubt about the accuracy or veracity of the observation, then it should be removed and the model should be refitted. \\n\\nYou might like to use the Determination of the Outliers JavaScript to perform some numerical experimentation for validating and for a deeper understanding of the concepts\\nThe dependent variable Y is a linear function of the independent variable X.  This can be checked by carefully examining all the points in the scatter diagram, and see if it is possible to bound them all  within two parallel lines. You may also use the Detective Testing for Trend to check this condition.  The distribution of the residual must be normal. You may check this condition by using the Lilliefors Test for Normality.  The residuals should have a mean equal to zero, and a constant standard deviation (i.e., homoskedastic condition). You may check this condition by dividing the residuals data into two or more groups; this approach is known as the Goldfeld-Quandt test. You may use the Stationary Testing Process to check this condition.    The residuals constitute a set of random variables. You may use the Test for Randomness and  Test for Randomness of Fluctuations to check this condition .  Durbin-Watson (D-W) statistic quantifies the serial correlation of least-squares errors in its original form. D-W statistic is defined by:    D-W statistic = S2n (ej \\n          - ej-1)2 / S1n ej2,       where ej is the jth error.  D-W takes values within [0, 4]. For no serial correlation, a value close     to 2 is expected. With positive serial correlation, adjacent deviates  tend to have the same sign, therefore D-W becomes less than 2; whereas, with negative serial correlation, alternating signs of error, D-W takes values larger than 2. For a least-squares fit where the value of D-W is significantly different from 2, the estimates of the variances and covariances of the parameters (i.e., coefficients) can be in error, being either too large or too small. The serial correlation of the deviates arise also time series analysis and forecasting. You may use the Measuring for Accuracy JavaScript to check this condition.  The \"good\" regression equation candidate is further analyzed using a plot of the residuals versus the independent variable(s). If any patterns are seen in the graph; e.g., an indication of non-constant variance; then  there is a need for data transformation. The following are the widely used transformations: \\n  X\\' = 1/X, \\xa0 \\xa0for non-zero X.     X\\' = Ln (X), \\xa0 \\xa0for positive X.\\n    X\\' = Ln(X), Y\\' = Ln (Y), \\xa0 \\xa0for positive X, and Y.     Y\\' = Ln (Y), \\xa0 \\xa0for positive Y.     Y\\' = Ln (Y) - Ln (1-Y), \\xa0 \\xa0for Y positive, less than one.    Y\\' = Ln [Y/(100-Y)], \\xa0 \\xa0 known as the logit transformation,   which is useful for the S-shape functions.  Taking square root of a Poisson random variable, the transformed variable is more symmetric. This is a useful transformation in regression analysis with Poisson observations. It also stabilizes the residual variation.\\n\\nBox-Cox Transformations: The Box-Cox transformation, below, can be applied to a regressor, a combination of regressors, and/or to the dependent variable (y) in a regression. The objective of doing so is usually to make the residuals of the regression more homoskedastic (ie., independently  and identically distributed) and closer to a normal distribution:\\n\\n\\n   (yl - 1) / l\\n\\xa0 \\xa0for a constant l not equal to zero, and log(y) \\xa0 \\xa0for l = 0.\\n\\n\\n You might like to use the Regression Analysis with Diagnostic Tools JavaScript to  check your computations, and to perform some numerical experimentation for a deeper understanding of the concepts. \\n\\n\\n\\n\\n Analysis of Covariance: Comparing the Slopes\\nConsider the following two samples of before-and-after independent treatments.  \\n\\nValues of Covariate X and a Dependent Variable YTreatment-I\\n\\xa0 Treatment-II \\nXY\\xa0XY511\\n21\\n39\\n67\\n15\\n4348\\n78\\n612\\n32\\nWe wish to test the following test of hypothesis on the two means of the dependent variable Y1, and Y2:\\nH0:  The difference between the two means is about a given value M.Ha:  The difference between the two means is quite different than it is claimed.  Since we are dealing with dependent variables, it\\'s natural to investigate the linear regression coefficients of the two samples; namely, the slopes and the intercepts.Suppose we are interested in testing the equality of two slopes. In other words, we wish to determine if two given lines are statistically parallel.  Let m1 represent the regression coefficient for explanatory variable X1 in sample 1 with size n1.  Let m2 represent the regression coefficient for X2 in sample 2 with size n2.    The difference between the two estimated slopes has the following variance:V= Var [m1 - m2] = {Sxx1 ´ Sxx2[(n1 -2)Sres12 + (n2 -2)Sres22] /[(n1 + n2 - 4)(Sxx1 + Sxx2].  \\nThen, the quantity:(m1 - m2) / V½\\nhas a t-distribution with d.f. = n1 + n2 - 4.\\nThis test and its generalization in comparing more than two slopes are called the Analysis of Covariance (ANOCOV).  The ANOCOV test is the same as in the ANOVA test; however there is an additional variable called covariate.   ANOCOV enables us to conduct and to extend the before-and-after test for two different populations. The process is as follows:Find a linear model for (X1, Y1) = (before1, after1), and one for (X2, Y2) = (before2, after2) that fit best.\\nPerform the test of the hypothesis m1 = m2. \\nIf the test result indicates that the slopes are almost equal, then compute the common slope of the two parallel regression lines:Slopepar = (m1Sxx1 + m2Sxx2) / (Sxx1 + Sxx2).\\nThe variance of the residuals is: Sres2 = [Syy1 + Syy2 - (Sxx1 + Sxx2) Slopepar] / ( n1 +  n1 -3).\\n \\nNow, perform the test for the difference between the two the intercepts, which is the vertical difference between the two parallel lines:\\nIntercepts\\' difference =1  -2 - (1 - 2) Slopepar.\\nThe test statistic is:(Intercepts\\' difference) / {Sres  [1/n1 + 1/n2 + (1 - 2)2/(Sxx1 + Sxx2)]½},\\nwhich has a t-distribution with parameter d.f. = n1 +  n1 -3.\\nDepending on the outcome of the last test, one may reject the null hypothesis.\\n\\nFor our numerical example, using the Analysis of Covariance JavaScripts, we obtained the following statistics: \\nSlope 1 = 1.3513514; its standard error = .2587641\\nSlope 2 = 1.4883721; its standard error = 1.0793906\\n\\nThese indicate that there is no evidence against equality of the slopes.  Now, we may test for any differences in the intercepts. Suppose we wish to test the null hypothesis that the vertical distance between the two parallel lines is about 4 units. \\n\\nUsing the second function in the Analysis of Covariance JavaScripts,  we obtained the  statistics: Common Slope = 1.425, Intercept =5.655,  providing a moderate evidence against the null hypothesis.\\nFurther Reading: Wall F., Statistical Data Analysis Handbook, by  McGraw-Hill, New York, 1986.  \\n\\n  \\nResidential Properties Appraisal Application\\nEstimating the market value of large numbers of residential properties is of interest to a number of socio-economic stakeholders, such as mortgage and insurance companies, banks and real-estate agencies, and investment property companies, etc.  It is both a science and an art. It is a science, because it is based on formal, rigorous and proven methods. It is an art because interaction with socio-economic stakeholders and the methods used give rise to all sorts of tradeoffs and compromises that assessors and their organizations must take into account when making decisions on the basis of their experience and skills.\\n\\nThe market value assessment of a set of selected houses involves performing an assessment by a few individual appraisers for each property and then computing an average obtained from the few individuals. \\n\\nIndividual appraisal refers to the process of estimating the exchange value of a house on the basis of a direct comparison between its profiles and the profiles of a set of other comparable properties sold on acceptable conditions. The profile of a property consists of all the relevant attributes of each house, such as the location, size, gross living space, age, one-story, two-story or more, garage, swimming pool, basement, etc.  Data on prices and characteristics of individual houses are available; e.g., from the U.S Bureau of the Census. \\n\\nOften regression analysis is used to determine what characteristics influence the price of the houses.  Thus it is  important to correct the subjective elements in the appraisal value before carrying out the regression analysis.  Coefficients that are not significantly different from zero as indicated by insignificant t-statistics at a 5% level are dropped from the regression model.\\n\\nThere are several practical questions to be answered before the actual data collection can take place.\\n\\nThe first step is to use statistical techniques, such as geographic clustering, to define  homogeneous groupings of houses within an urban area.\\n\\nHow many houses should we look at?   Ideally, one would collect information on as many houses as time and money allow.  It is these practical considerations that make statistics so useful.  Hardly anyone could spend the time, money, and effort needed to look at every house for sale.  It is unrealistic to obtain information on every house of interest, or in statistical terms, on every item in the population.  Thus, we can look only at a sample of houses -- a subset of the population -- and hope that this sample will give us reasonably accurate information about the population.  Let us say we can afford to look at 16 houses.\\n\\nWe would probably choose to select a simple random sample-that is, a sample in which, roughly speaking, every house in the population has equal chance of being included.  Then we would expect to get a reasonably representative sample of houses throughout this selected size range, reflecting prices for the whole neighborhood.  This sample should give us some information about all houses of all sizes within this range, since a simple random sample tends to select as many larger houses as smaller houses, and as many expensive as less expensive ones. \\n\\nSuppose that the 16 houses in our random sample have the sizes and prices shown in the following Table.   If 160 houses are randomly selected, variables Y, X1, and X2 are random variables.  We have no control over them and cannot know what specific values will be selected.  It is chance only that determines them. \\n\\n   \\n- Sizes, Ages, and Prices of Twenty Houses -\\n \\nX1 = Size\\n X2 = Age  Y = Price\\xa0\\n\\xa0  X1 \\n  = Size \\nX2 = Age\\n Y = Price   \\n 1.8   30   32  \\xa0 \\xa0  2.3\\n 30  44  1.0 \\n 33   24 \\xa0\\n\\xa0 1.4   17\\n 27  \\n1.7  25\\n  27  \\xa0 \\xa0 3.3 16\\n50\\n  1.2 \\n12 \\n 25  \\xa0 \\xa0 2.2  22  \\n37 \\n  2.8 \\n12  47\\n\\xa0\\xa0\\n 1.5  29  28 \\n  1.7  1   30  \\xa0 \\xa0 1.1   29  20 \\n  2.5   12   43\\n \\xa0\\xa0\\n 2.0   25   38    3.6   28  \\n52  \\xa0 \\xa0  2.6\\n 2  45  \\n\\n \\nWhat can we tell about the relationship between size and price from our sample?   Reading the data from the above table row-wise, and entering them in the Regression Analysis with Diagnostic Tools JavaScript, we found the following simple regression model:\\n\\n\\nPrice = 9.253 + 12.873(Size)\\n\\n\\nNow consider the problem of estimating the price (Y) of a house from knowing its size (X1) and also its age (X2).  The sizes and prices will be the same as in the simple regression problem.  What we have done is add ages of houses to the existing data.  Note carefully that in real life, one would not first go out and collect data on sizes and prices and then analyze the simple regression problem.  Rather, one would collect all data, which might be pertinent on all twenty houses at the outset.  Then the analysis performed would throw out predictors which turn out not to be needed.\\n\\nThe objectives in a multiple regression problem are essentially the same as for a simple regression.  While the objectives remain the same, the more predictors we have the calculations and interpretations become more complicated.  For large data set one may use  the multiple regression module of any statistical package such as  SAS and  SPSS. Using the \\nMultiple Linear Regression JavaScript, for our numerical example with X1 = Size, X2 = Age, and Y = Price, we obtain the following statistical model:\\n\\n\\n\\nPrice = 9.959 + 12.800(Size) - 0.027(Age)\\n\\n\\nThe regression results suggest that, on average, as the Size of house increases the Price increases. However, the coefficient of the variable Age is significantly small with negative value indicating an inverse relationship. Older houses tend to cost less than newer houses.   Moreover, the correlation between Price and Age is -0.236.  This result indicates that only 6% of variation in price can be accounted by the different in ages of the houses.  This result supports our suspicion that the Age is not a significant predictor of price.  Therefore, the simple regression:\\n\\n\\nPrice = 9.253 + 12.873(Size)\\n\\n\\nNow, the question is: Is this model is good enough to satisfy the usual conditions of the regression analysis.\\n\\n    The following is the list of basic assumptions (i.e., conditions) and the tools to check these necessary conditions.  Any undetected outliers may have major impact on the regression model. Using the Determination of the Outliers JavaScript we found that there is no outlier in the above data set.\\nThe dependent variable Price is a linear function of the independent variable Size.  By carefully examining the scatter diagram we found that the linearity condition is satisfied. The distribution of the residual must be normal. Reading the data from the above table row-wise, and entering them in the  Regression Analysis with Diagnostic Tools JavaScript, we found that the normality condition is also satisfied.  The residuals should have a mean equal to zero, and a constant standard deviation (i.e., homoskedastic condition). By the  Regression Analysis with Diagnostic Tools JavaScript, the results are satisfactory. \\n\\nThe residuals constitute a set of random variables. The persistent non-randomness in the residuals violates the best linear unbiased estimator condition. However, since the numerical statistics corresponding to the residuals obtained by using  Regression Analysis with Diagnostic Tools JavaScript, are not significant, therefore our ordinary least square regression is adequate for our analysis.\\n\\nDurbin-Watson (D-W) statistic quantifies the serial correlation of least-squares errors in its original form. D-W statistic for this model is 1.995, which is good enough in rejecting any serial correlation.\\n\\nMore Useful Statistics for the Model: The standard errors for the Slope and the Intercept are0.881, and 1.916, respectively, which are small enough. The F-statistic is 213.599, which is large enough indicating that the model is good enough overall for prediction purposes. \\n\\n\\n\\nNotice that since the above analysis is performed on a specific set of data, as always, one must be careful in generalizing its findings.\\nFurther Reading:\\n\\nLovell, R., and French, N., Estimated realization price: what do the banks want and what can be realistically provided? Journal of property finance, 6, 7-16, 1995.\\nNewsome, B.A. and Zeitz, J., 1992. Adjusting comparable sales using multiple regression analysis-the need for segmentation, The Appraisal Journal, 8, 129-135.\\n\\n\\n  \\nIntroduction to Integrating Statistical Concepts\\nStatistical thinking for decision-making requires a deeper understanding than merely memorizing each isolated technique.  Understanding involves ever expansion of neural network by means of correct connectivity between concepts.  The aim of this chapter is to  look closely at some of the concepts and techniques that we have learned up to now in a unifying theme. The following case studies,  improve your statistical thinking to see the wholeness and manifoldness of statistical tools.\\n As you will see, although one would hope that all tests give the same results this is not always the case. It all depends on how  informative the data are and to what extend they have been condensed before presenting them  to you  for analysis (while becoming a good statistician). The following sections are illustrations in examining how much useful information they provide and how they may result in opposite conclusions, if one is not careful enough.\\n\\n\\n Hypothesis Testing with Confidence  \\nOne of the main advantages of constructing a confidence interval (CI) is to provide a degree of confidence for the point estimate for the population parameter.  Moreover, one may utilize CI for the test of hypothesis purposes. Suppose you wish to test the following general test of hypothesis:\\n\\nH0: The population parameter is almost equal to a given claimed value,\\n\\nagainst the alternative:\\n\\nHa: The population parameter is not even close to the claimed value.\\n\\nThe process of executing the above test of hypothesis at a level of significance using CI is as follows:\\n\\n\\nIgnore the claimed value in the null hypothesis, for the time being.\\nConstruct a 100(1- a)% confidence interval based on the available data.\\nIf the constructed CI does not contain the claimed value, then there is enough evidence to reject the null hypothesis; otherwise, there is no reason to reject the null hypothesis.\\n\\n\\nYou might like to use the Hypothesis Testing with Confidence  JavaScript applet to  perform some numerical experimentation for validating the above assertions and for a deeper understanding. \\n\\n\\n\\n Regression Analysis, ANOVA, and Chi-square Test \\nThere are close relationships among linear regression, analysis of variance and the Chi-square test.  To illustrate the relationship, consider the following application:\\n Relationship between age and income in a given neighborhood:  A random survey sampling of size 33 individuals in a neighborhood revealed  the following pairs of data. For each pair age is in years and the indicated income is in thousands of dollars:   \\n\\n\\n\\n- \\n        Relation between Age and Income($1000) -\\n\\n\\n\\n\\nAge\\n\\n\\nIncome\\n\\n\\n\\nAge\\n\\n\\nIncome\\n\\n\\n\\nAge\\n\\n\\nIncome\\n\\n\\n\\n\\n20\\n\\n\\n15\\n\\n\\xa0\\n\\n42\\n\\n\\n19\\n\\n\\xa0\\n\\n61\\n\\n\\n13\\n\\n\\n\\n\\n22\\n\\n\\n13\\n\\n\\xa0\\n\\n47\\n\\n\\n17\\n\\n\\xa0\\n\\n62\\n\\n\\n14\\n\\n\\n\\n\\n23\\n\\n\\n17\\n\\n\\xa0\\n\\n53\\n\\n\\n13\\n\\n\\xa0\\n\\n65\\n\\n\\n9\\n\\n\\n\\n\\n28\\n\\n\\n19\\n\\n\\xa0\\n\\n55\\n\\n\\n18\\n\\n\\xa0\\n\\n67\\n\\n\\n7\\n\\n\\n\\n\\n35\\n\\n\\n15\\n\\n\\xa0\\n\\n41\\n\\n\\n21\\n\\n\\xa0\\n\\n72\\n\\n\\n7\\n\\n\\n\\n\\n24\\n\\n\\n21\\n\\n\\xa0\\n\\n53\\n\\n\\n39\\n\\n\\xa0\\n\\n65\\n\\n\\n22\\n\\n\\n\\n\\n26\\n\\n\\n26\\n\\n\\xa0\\n\\n57\\n\\n\\n28\\n\\n\\xa0\\n\\n65\\n\\n\\n24\\n\\n\\n\\n\\n29\\n\\n\\n27\\n\\n\\xa0\\n\\n58\\n\\n\\n22\\n\\n\\xa0\\n\\n69\\n\\n\\n27\\n\\n\\n\\n\\n39\\n\\n\\n31\\n\\n\\xa0\\n\\n58\\n\\n\\n29\\n\\n\\xa0\\n\\n71\\n\\n\\n22\\n\\n\\n\\n\\n31\\n\\n\\n16\\n\\n\\xa0\\n\\n46\\n\\n\\n27\\n\\n\\xa0\\n\\n69\\n\\n\\n9\\n\\n\\n\\n\\n37\\n\\n\\n19\\n\\n\\xa0\\n\\n44\\n\\n\\n35\\n\\n\\xa0\\n\\n62\\n\\n\\n21\\n\\n\\n\\n\\nConstructing a linear regression gives us:   Income = 22.88 - 0.05834 (Age)  This suggests a negative relationship; as people get older, they have lower income, on average. Although slope is small, it cannot be considered as zero, since the t-statistic for it is -0.70, which is significant. \\nNow suppose you have only the following secondary data, where the original data have  been condensed:    \\n\\n\\n- \\n          Relation between Age and Income($1000) -\\n\\n\\n\\nAge \\n            ( 29 - 39 ) \\n\\n\\xa0\\n\\nAge \\n            ( 40 - 59 )\\n\\n\\xa0\\n\\nAge \\n            ( 60 & Over )\\n\\n\\n\\n\\n15\\n\\n\\xa0\\n\\n19\\n\\n\\xa0\\n\\n13\\n\\n\\n\\n\\n13\\n\\n\\xa0\\n\\n17\\n\\n\\xa0\\n\\n14\\n\\n\\n\\n\\n17\\n\\n\\xa0\\n\\n13\\n\\n\\xa0\\n\\n9\\n\\n\\n\\n\\n21\\n\\n\\xa0\\n\\n21\\n\\n\\xa0\\n\\n7\\n\\n\\n\\n\\n15\\n\\n\\xa0\\n\\n39\\n\\n\\xa0\\n\\n21\\n\\n\\n\\n\\n26\\n\\n\\xa0\\n\\n28\\n\\n\\xa0\\n\\n24\\n\\n\\n\\n\\n27\\n\\n\\xa0\\n\\n22\\n\\n\\xa0\\n\\n27\\n\\n\\n\\n\\n31\\n\\n\\xa0\\n\\n26\\n\\n\\xa0\\n\\n22\\n\\n\\n\\n\\n16\\n\\n\\xa0\\n\\n27\\n\\n\\xa0\\n\\n9\\n\\n\\n\\n\\n19\\n\\n\\xa0\\n\\n35\\n\\n\\xa0\\n\\n22\\n\\n\\n\\n\\n19\\n\\n\\xa0\\n\\n18\\n\\n\\xa0\\n\\n7\\n\\n\\n\\n One may use ANOVA in testing that there is no relationship among age and  income. Performing the analysis provides the F-statistic equal to 3.87  which is quite significant; i.e., rejecting the hypothesis of no difference   in population average income for the three age groups.  Now, suppose  more condensed  secondary data are provided as in the following table:    \\xa0\\n  \\nRelation between Age and Income($1000):   Age    Income 20-3940-5960 and over  Up to $20,0007\\xa0  4\\xa0 6\\xa0   $20,000 and over 4\\xa0 7\\xa0 5\\xa0         One may use the Chi-square test for the null hypothesis that age and income are unrelated. The Chi-square statistic is 1.70, which is not significant; therefore there is no reason to believe income and age are related!  But of course, these data are over-condensed, because when all data in the sample were used, there was an observable relationship.\\nRegression Analysis, ANOVA, T-test, and Coefficient of DeterminationThere are very direct relationships among linear regression, analysis of variance,  t-test and the coefficient of determination.  The following  small data set is for illustrating the connections among the above statistical procedures, and therefore relationships among statistical tables: \\n \\n\\n\\n\\n\\n\\nX1\\n\\n\\n4\\n\\n\\n5\\n\\n\\n4\\n\\n\\n6\\n\\n\\n7\\n\\n\\n7\\n\\n\\n8 \\n                \\n\\n\\n9\\n\\n\\n9\\n\\n\\n11\\n\\n\\n\\n\\nX2\\n\\n\\n8\\n\\n\\n6\\n\\n\\n8\\n\\n\\n10\\n\\n\\n10\\n\\n\\n11\\n\\n\\n13 \\n                \\n\\n\\n14\\n\\n\\n14\\n\\n\\n16\\n\\n\\n\\n\\n\\nSuppose we apply the t-test.  The statistic is  t = 3.207, with d.f. = 18.  The p-value is 0.003 indicating a very strong evidence against the null hypothesis.Now, by introducing a dummy variable x with two values, say 0 and 1, representing the two data sets,  respectively, we are able to apply regression analysis:  \\n\\n\\n\\n\\n\\n\\nx\\n\\n\\n0\\n\\n\\n0\\n\\n\\n0\\n\\n\\n0\\n\\n\\n0\\n\\n\\n0\\n\\n\\n0 \\n              \\n\\n\\n0\\n\\n\\n0\\n\\n\\n0\\n\\n\\n\\n\\ny\\n\\n\\n4\\n\\n\\n5\\n\\n\\n4\\n\\n\\n6\\n\\n\\n7\\n\\n\\n7\\n\\n\\n8 \\n              \\n\\n\\n9\\n\\n\\n9\\n\\n\\n11\\n\\n\\n\\n\\n\\n\\n\\nx\\n\\n\\n1\\n\\n\\n1\\n\\n\\n1\\n\\n\\n1\\n\\n\\n1\\n\\n\\n1\\n\\n\\n1 \\n              \\n\\n\\n1\\n\\n\\n1\\n\\n\\n1\\n\\n\\n\\n\\ny\\n\\n\\n8\\n\\n\\n6\\n\\n\\n8\\n\\n\\n10\\n\\n\\n10\\n\\n\\n11\\n\\n\\n13 \\n              \\n\\n\\n14\\n\\n\\n14\\n\\n\\n16\\n\\n\\n\\n\\n\\n\\nAmong other statistics, we obtain a large slope = m = 4 ¹ 0, indicating the rejection of the null hypothesis.  Notice that, the t-statistic for the slope is: t-statistic = slope/(its standard error) = 4/ 1.2472191 =  3.207, which is the t-statistic we obtained from the t-test. In general, the square of t-statistic of the slope is the F-statistic in the ANOVA table;  i.e.,tm2 = F-statistic\\nMoreover, the coefficient of determination r 2 = 0.36, which is always obtainable from the t-test, as follows: \\nr2  = t 2 / (t 2 + d.f.).\\nFor our numerical example, the r 2 is (3.207) 2 / [(3.207) 2 + 18] = 0.36, as expected.\\nNow, applying ANOVA on the two sets of data, we obtain the F-statistic = 10.285, with d.f.1 = 1, and d.f.2 = 18. The F-statistic is not large enough; therefore, one must reject the null hypothesis.  Note that, in general, \\n F a , (1, n) \\xa0= \\xa0 t 2 a/2 , n.For our numerical example, F = t 2 = (3.207) 2 = 10.285, as expected.As expected, by just looking at the data, all three tests indicate strongly that the means of the two sets are quite different. \\n\\n \\nRelationships among Distributions and Unification of Statistical Tables Particular attention must be paid to a first course in statistics. When I first  began studying statistics, it bothered me that there were different tables for  different tests. It took me a while to learn that this is not as haphazard as it appeared. Binomial, Normal, Chi-square, t, and F distributions that you will learn are actually closely connected. A problem with elementary statistical textbooks is that they not only don\\'t provide information of this kind, to permit a useful understanding of the  principles involved, but they usually don\\'t provide these conceptual links.  If you want to understand connections between statistical concepts, then you  should practice making these connections. Learning by doing statistics  lends itself to active rather than passive learning. Statistics is a highly interrelated set of concepts, and to be successful at it, you must learn to make these links conscious in your mind. Students often ask: Why T- table values with d.f. = 1 are much larger compared  with other d.f. values? Some tables are limited. What should I do when the sample size is too large?, How can I become familiar with tables and their differences. Is there any type of integration among tables? Is there any connection  between test of hypotheses and confidence interval under different scenarios? For example, testing with respect to one, two, more than two populations, and so on.  The following Figure demonstrates useful relationships among distributions  and a unification of statistical tables:  A Unification of Common Statistical TablesClick on the image to enlarge it and THEN print itFor example, the following are some nice connections between major tables:   \\n Standard normal z and F-statistics: F = z2, where F has (d.f.1 = 1, and d.f.2 is \\n    the largest available in the F-table) T- statistic and F-statistic: F = t2, where F has (d.f.1 = 1, and d.f.2 = d.f. of the t-table) Chi-square and F-statistics: F = Chi-square/d.f.1, where F has (d.f.1 = d.f. of the Chi-square-table, and d.f.2 is the largest available in the F-table) \\nT-statistic and Chi-square: (Chi-square)½ = t,    where Chi-square has d.f.=1, and t has d.f. = ¥.\\nStandard normal z and T-statistic: z = t, where  t  has d.f. = ¥.\\nStandard normal z and Chi-square: (2 Chi-square)½ - (2d.f.-1)½ = z,    where d.f. is the largest available in the Chi-square table).  Standard normal z, Chi-square, and  T- statistic: z/[Chi-aquare/n)½ = t with d.f. = n. F-statistics and its Inverse: Fa(n1, n2) = 1/F1-a(n2, n1), therefore it is only necessary to tabulate, say the upper tail probabilities. \\nCorrelation coefficient r and T-statistic:   t =  [r(n-2)½]/[1 - r2]½.\\n\\nTransformation of Some Inferential Statistics to the Standard normal Z:\\n\\nFor the t(df):  Z = {df ´ Ln[1 + (t2/df)]}½ ´ {1 - [1/(2df)]}½.\\nFor the F(1,df): Z = {df ´ Ln[1 + (F/df)]}½ ´ {1 - [1/(2df)]}½,\\n\\n where Ln is the natural logarithm. \\n\\n\\nVisit also the Relationships among Common Distributions.\\n\\nYou may like using the statistical tables at the back of your book and/or P-values JavaScript in performing some numerical experimentation for validating the above relationships for a deeper understanding of the concepts. You might need to use a scientific calculator, too.\\n\\nFurther Reading:Kagan. A., What students can learn  from tables of basic distributions, Int. Journal of Mathematical Education in Science and Technology, 30(6), 1999.  \\n   \\nIntroduction to Visualization of Statistics    Most of statistical data processing involves algebraic operations on the  dataset. However, if the dataset contains more than 3 numbers, it is not possible to visualize it by geometric representation, mainly due to human sensory limitation. Geometry has a much longer history than algebra. Ancient Greeks applied geometry to  measure land, and developed the geo-metric models. The analytic-geometry is to find equivalency between algebra and geometry. The aim is a better understanding by visualization in 2-or-3 dimensional space, and to generalize the ideas for higher dimensions by analytic thinking.  Without the loss of generality, and conserving space, the following presentation is in the context of small sample size, allowing us to see statistics in 1, or 2-dimensional space.  \\n\\n  The Mean and The Median \\nSuppose that four people want to get together to play poker. They live on 1st Street, 3rd Street, 7th Street, \\n and 15th Street. They want to select a house that involves the minimum amount of driving for all parties concerned. Let\\'s suppose that they decide to minimize the absolute amount of driving. If they met at 1st Street, the amount of driving would be 0 + 2 + 6 + 14  = 22 blocks. If they met at 3rd Street, the amount of driving would be 2 + 0+ 4 + 12 = 18 blocks. If they met at 7th Street, 6 + 4 + 0 + 8 = 18 blocks. Finally, at 15th Street, 14 + 12 + 8 + 0 = 34 blocks. So the two houses that would minimize the amount of driving would be 3rd or 7th Street.  Actually, if they wanted a neutral site, any place on 4th, 5th, or 6th Street would also work. Note that any value between 3 and 7 could be defined as the median of 1, 3, 7, and 15. So the median is the value that minimizes the absolute distance  to the data points.  Now, the person at 15th is upset at always having to do more driving. So the group agrees to consider a different rule. In deciding to minimize the square of the distance driving, we are using the least square principle.  By squaring,  we give more weight to a single very long commute than to a bunch of shorter commutes. With this rule, the 7th Street house (36 + 16 + 0 + 64 = 116 square blocks) is preferred to the 3rd Street house (4 + 0 + 16 + 144 = 164 square blocks). If you consider any location, and not just the houses themselves, then 9th Street is the location that minimizes the square of the distances driven.  Find the value of x that minimizes: \\n\\n\\n(1 - x)2 + (3 - x)2 +(7 - \\n      x)2 + (15 - x)2. \\n\\nThe value that minimizes the sum of squared values is 6.5, which is also equal to the arithmetic mean of 1, 3, 7, and 15. With calculus, it\\'s easy to show that this holds in general.  Consider a small sample of scores with an even number of cases; for example,  1, 2, 4, 7, 10, and 12. The median is 5.5, the midpoint of the interval  between the scores of 4 and 7. As we discussed above, it is true that the median is a point around which  the sum of absolute deviations is minimized. In this example the sum of absolute deviations is 22. However, it is not a unique point. Any point in the 4 to 7 region will have the same value of 22 for the sum of the absolute deviations. Indeed, medians are tricky. The 50% above -- 50% below is not quite correct.  For example, 1, 1, 1, 1, 1, 1, 8 has no median. The convention says that, \\n the median is 1; however, about 14% of the data lie strictly above it; 100% of the data are greater than or equal to the median.  We will make use of this idea in regression analysis. In an analogous argument, the regression line is a unique line, which minimizes the sum of the squared deviations from it. There is no unique line that minimizes the sum of the absolute deviations from it.\\n\\n  Arithmetic and  Geometric Means\\n\\nArithmetic Mean: Suppose you have two data points x and y, on real number- line axis:\\n\\n\\n\\n\\n\\nThe arithmetic mean (a) is a point such that the following vectorial relation holds: ox - oa = oa - oy.\\n\\nGeometric Mean: Suppose you have two positive data points x and y, on the above  real number- line axis, then the Geometric Mean (g) of these numbers is a point g such that |ox| / |og| = |og| / |oy|, where |ox| means the length of line segment ox, for example.\\n  \\n  Variance, Covariance, and Correlation Coefficient  \\nConsider a data set containing n = 2 observations (5, 1). Upon centralizing  the data, one obtains the vector V1 = (5-3 = 2, 1-3 = -2), as shown in  the following n = 2 dimensional coordinate system:       \\n \\n  Notice that the vector V1 length is:       \\n          |V1| = [(2)2 + (-2)2]½ = 8½    \\n        The variance of V1 is:                  Var(V1) = S Xi2/ n = |V1|2/n = 4            \\n        The standard deviation is:                 |OS1| = |V1| / n½ = 8½ / 2½ = 2.               Now, consider a second observation (2, 4). Similarly, it can be represented by vector V2 = (-1, 1).       The covariance is, \\n            Cov (V1, V2) = the dot product / n = [(2)(-1) + (-2)(1)]/2 = -4/2 =  -2    \\n   Therefore:        n Cov (V1, V2) = the dot product of the two vectors V1, and V2      \\n  Notice that the dot-product is multiplication of the two lengths times  the cosine of the angle between the two vectors. Therefore,           Cov (V1, V2) = |OS1| ´ |OS2| ´  Cos (V1, V2) = (2) (1) Cos(180°) = -2        The correlation coefficient is therefore:        r = Cos (V1, V2)        \\n   This is possibly the simplest proof that the correlation coefficient is always bounded by the interval [-1, 1]. The correlation coefficient for our numerical  example is Cos (V1, V2) = Cos(180°) = -1, as expected from the above figure.  The distance between the two-point data sets V1, and V2 is also a dot-product:           |V1 - V2| = (V1-V2) . (V1-V2) = |V1|2 + |V2|2 - 2 |V1| ´ |V2 |\\n          = n[Var(V1) + VarV2 - 2Cov(V1, V2)]               Now, construct a matrix whose columns are the coordinates of the two vectors  V1 and V2, respectively. Multiplying the transpose of this matrix by itself provides a new symmetric matrix containing n times the variance of V1 and  variance of  V2 as its main diagonal elements (i.e., 8, 2), and n times Cov (V1, V2) as its off diagonal element (i.e., -4).    \\nYou might like to use a graph paper, and a scientific calculator to check the results of these numerical examples and to perform some additional numerical experimentation for a deeper understanding of the concepts. \\nFurther Reading: Wickens T., The Geometry of  Multivariate Statistics, Erlbaum Pub., 1995. \\n  Index Numbers with Applications    When facing a lack of a unit of measure, we often use indicators as surrogates  for direct measurement. For example, the height of a column of mercury is a familiar indicator of temperature. No one presumes that the height of mercury  column constitutes temperature in quite the same sense that length constitutes  the number of centimeters from end to end. However, the height of a column of mercury is a dependable correlate of temperature and thus serves as a useful measure of it. Therefore, and indicator is an accessible and dependable correlate of a dimension of interest; that correlate is used as a measure of that dimension   because direct measurement of the dimension is not possible  or practical. In like manner index numbers serve as surrogate for actual data.     The primary purposes of an index number are to provide a value useful for   comparing magnitudes of aggregates of related variables to each other, and  to measure the changes in these magnitudes over time. Consequently, many   different index numbers have been developed for special use. There are a  number of particularly well-known ones, some of which are announced on public   media every day.  Government agencies often report time series  data in the form of index numbers. For example, the consumer price index  is an important economic indicator. Therefore, it is useful to understand  how index numbers are constructed and how to interpret them. These index  numbers are developed usually starting with base 100 that indicates a change   in magnitude relative to its value at a specified point in time. For example, in determining the cost of living, the Bureau of Labor Statistics   (BLS) first identifies a \"market basket\" of goods and services the typical  consumer buys. Annually, the BLS surveys consumers to determine what they  buy and the overall cost of the goods and services they buy: What, where,  and how much.  The Consumer Price Index (CPI) is used to monitor changes  in the cost of living (i.e. the selected market basket) over time. When  the CPI rises, the typical family has to spend more dollars to maintain  the same standard of living. The goal of the CPI is to measure changes in  the cost of living. It reports the movement of prices, not in dollar amounts,  but with an index number.    \\n The Geometric MeanThe Geometric Means are used extensively by the U.S. Bureau of Labor Statistics, \"Geomeans\" as they call them, in the computation of the U.S. Consumer Price Index.  The geomeans are also used in price indexes   Ratio Index Numbers The following provides the computational procedures with applications for some  Index numbers, including  the Ratio Index, and Composite Index numbers.  Suppose we are interested in  the labor utilization of two manufacturing plants A and B with the unit  outputs and man/hours, as shown in the following table, together with the  national standard over the last three months:  \\n\\n\\n\\n\\n\\nPlant \\n              Type - A\\n\\n\\n\\nPlant \\n              Type - B \\n\\n\\n\\n\\nMonths\\n\\n\\xa0\\n\\nUnit \\n              Output\\n\\n\\nMan \\n              Hours\\n\\n\\xa0\\n\\nUnit \\n              Output\\n\\n\\nMan \\n              Hours\\n\\n\\n\\n\\n1\\n\\n\\xa0\\n\\n0283\\n\\n\\n200000\\n\\n\\xa0\\n\\n11315\\n\\n\\n680000\\n\\n\\n\\n\\n2\\n\\n\\xa0\\n\\n0760\\n\\n\\n300000\\n\\n\\xa0\\n\\n12470\\n\\n\\n720000\\n\\n\\n\\n\\n3\\n\\n\\xa0\\n\\n1195\\n\\n\\n530000\\n\\n\\xa0\\n\\n13395\\n\\n\\n750000\\n\\n\\n\\n\\nStandard\\n\\n\\xa0\\n\\n4000\\n\\n\\n600000\\n\\n\\xa0\\n\\n16000\\n\\n\\n800000\\n\\n\\n\\n\\n The labor utilization for the Plant A in the first month is:  LA,1 = [(200000/283)] / [(600000/4000)]  = 4.69      Similarly,      LB,3 = 53.59/50 = 1.07.   Upon computing the labor utilization for both plants for each month,   one can present the results by graphing the labor utilization over time  for comparative studies.\\n  Composite Index Numbers \\n Consider the total labor, and material cost for two consecutive years for an industrial plant, as shown in the following   table: \\n          \\n\\xa0 \\xa0  Year 2000   Year 2001    \\xa0 Unit Needed   Unit Cost   Total Unit Cost   Total    Labor  20  10   200  11 220    Almunium  02  100   200  110220\\n   Electricity  02  50  100 60 120\\n   Total    \\n   500    \\n560 \\n  \\nFrom the information given in the above table, the index for the two consecutive years are 500/500 = 1, and 560/500 = 1.12, respectively. Further Readings: Watson C., P. Billingsley, D. Croft,  and D. Huntsberger, Statistics for Management and Economics, Allyn   & Bacon, Inc., 1993.  \\n  Variation Index as a Quality Indicator\\n A commonly  used index of variation measure and comparison for nominal and ordinal data is called the index of dispersion:   \\n        D = k (N2 - Sfi2)/[N2(k-1)] \\n      \\nwhere k is the number of categories, fi is the number of ratings in each category,  and N is the total number of rating. D is a number between zero and 1 depending if all ratings fall into one category, or if ratings were equally divided among the k categories. An Application: Consider the following data with N = 100 participants,   k = 5 categories, f1 = 25, f2  = 42, and so on. \\n      \\n\\n\\nCategory\\n\\n\\nFrequency\\n\\n\\n\\n\\nA\\n\\n\\n25\\n\\n\\n\\nB\\n\\n\\n42\\n\\n\\n\\nC\\n\\n\\n8\\n\\n\\nD\\n\\n\\n13\\n\\n\\n\\nE\\n\\n\\n12\\n\\n\\n  \\nTherefore the dispersion index is: D = 5 (1002 - 2766)/[1002(4)] = 0.904,  indicating a good spread of scores across the categories. \\n\\n  Labor Force Unemployment Index\\n Is a given  city an economically depressed area? The degree of unemployment among labor (L) force is considered to be a proper indicator of economic depression.  To construct the unemployment index, each person is classified both with  respect to membership in the labor force and the degree of unemployment in fractional value, ranging from 0 to 1. The fraction that indicates the portion of labor that is idle is:  \\n   L = S[UiPi] / SPi,\\xa0 \\xa0 the sums are over all i = 1, 2,\\x85, n.         \\nwhere Pi is the proportion of a full workweek for each resident of the area held or sought employment and  n is the total number of residents in the area. Ui is the proportion of Pi for which each \\nresident of the area unemployed. For example, a person seeking two days  of work per week (5 days) and employed for only one-half day would be  identified with Pi = 2/5 = 0.4, and Ui = 1.5/2 = 0.75. The resulting multiplication UiPi  = 0.3 would be the portion of a full workweek for which the person was   unemployed.  Now the question is What value of L constitutes an economic depressed  area. The answer belongs to the decision-maker to decide. \\n  Seasonal Index and Deseasonalizing Data\\n\\nSeasonal index represents the extent of seasonal influence for a particular segment of the year.  The calculation involves a comparison of the expected values of that period to the grand mean.\\n\\nWe need to get an estimate of the seasonal index for each month, or other periods such as quarter, week, etc, depending on the data availability.  Seasonality is a pattern that repeats for each period.  For example annual seasonal pattern has a cycle that is 12 periods long, if the periods are months, or 4 periods long if the periods are quartets.\\n\\nA seasonal index is how much the average for that particular period tends to be above (or below) the grand average. Therefore, to get an accurate estimate for it, we compute the average of the first period of the cycle, and the second period, etc, and divide each by the overall average.  The formula for computing seasonal factors is: \\n\\n\\nSi = Di/D,       \\n\\n\\nwhere:\\nSi = the seasonal index for ith period,\\nDi = the average values of  ith period,\\nD = grand avrage,\\ni = the ith seasonal period of the cycle.\\n\\nA seasonal index of 1.00 for a particular month indicates that the expected value of that month is 1/12 of the overall average. A seasonal index of 1.25 indicates that the expected value for that month is 25% greater than 1/12 of the overall average. A seasonal index of 80 indicates that the expected value for that month is 20% less than 1/12 of the overall average.\\n\\nDeseasonalizing Process: Deseasonalizing the data, also called Seasonal Adjustment  is the process of removing recurrent and periodic variations over a short time frame (e.g.,  weeks, quarters, months). Therefore, season variations are regularly repeating movements in series values that can be tied to recurring events.  The Deseasonalized data is obtained by simply dividing each time series observation by the corresponding seasonal index.\\n\\nAlmost all time series published by the government are already deseasonalized using the seasonal index to unmasking the underlying trends in the data, which could have been caused by the seasonality factor.\\n\\nA Numerical Application:  The following table provides monthly sales ($000) at a college bookstore. \\n\\n\\n  \\n \\n M T   Jan\\n  Feb \\nMar   Apr\\nMay\\n Jun \\nJul\\n  Aug\\nSep\\n Oct\\nNov\\n Dec\\nTotal \\n1\\n  196\\n  188\\n  192\\n  164\\n 140\\n 120\\n 112\\n  140\\n 160\\n  168\\n  192\\n  200\\n  1972\\n   2   200   188   192   164  140   122   132   144   176  168  196  194   2016    \\n3\\n  196\\n 212\\n  202\\n  180\\n  150\\n  140\\n  156\\n  144\\n  164\\n  186\\n  200\\n  230\\n  2160\\n    4  242   240   196   220   200   192 176  184 204 228 250 260 2592 Mean: 208.6   207.0   192.6   182.0   157.6   143.6   144.0   153.0   177.6   187.6   209.6   221.0   2185    Index:  1.14  1.14\\n  1.06\\n 1.00  0.87  0.79\\n \\n0.79\\n 0.84 \\n 0.97 \\n 1.03   1.15   1.22   12   \\n\\nThe sales show a seasonal pattern, with the greatest number when the college is in session and decrease during the summer months.  For example, for January the index is: \\n\\n\\nS(Jan) = D(Jan)/D =208.6/181.84 = 1.14,\\n\\nwhere D(Jan) is the mean of all four January month, and D is the grand mean of all past four years sales.\\n\\nYou might like to use the Seasonal Index JavaScript to check your hand computation. As always you must first use  Plot of the Time Series as a tool for the initial characterization process.  \\n\\nFor testing seasonality based on seasonal index, you may like to use \\nTest for Seasonality JavaScript.\\n\\nFor modeling the time series having both the seasonality and trend components, visit the Business Forecasting site.\\n\\n  Statistical Technique and Index Numbers\\nOne must be careful in applying or generalizing any statistical technique  to the index numbers. For example, the correlation of rates raises  the potential problem. Specifically, let X, Y, and X be three independent variables, so that pair-wise correlations are zero; however, the ratios   X/Y, and Z/Y will be correlated due to the common denominator. Let I = X1/X2 where X1, and X2 are dependent variables with correlation r , having mean and coefficient of variation m1, c1 and m2, c2, respectively; then,\\n\\n\\n\\nMean of I = m1 (1-r´c1´c2 + c22)/m2,\\nStandard Deviation of I = m1(c12 - 2 r´c1´c2 + c22) ½\\n /m2\\n\\n\\n\\n\\n\\n\\n\\nA Classification of the JavaScript by Application Areas\\n\\nThis section is a part of the JavaScript E-labs learning technologies for decision making.  The following is a classification of statistical JavaScript by their application areas:\\n\\n\\n\\nMENU\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n1. Summarizing Data\\n\\nBivariate Sampling Statistics\\nDetective Testing for Trend & Autocrrelation\\nDescriptive Statistics\\nDetermination of the Outliers\\nEmpirical Distribution Function\\nHistogram\\n Residuals Random Fluctuations Testing\\nSeasonal Index\\nThe Three Means\\n\\n  \\n2. Computational probability \\n\\nComparing Two Random Variables\\nMarkov Chains Calculator\\nMultinomial Distributions\\nP-values for the Popular Distributions\\n\\n  \\n3. Requirements for most tests & estimations\\n\\nRemoval of the Outliers\\nSample Size Determination\\nSubjectivity in Hypothesis Testing \\nTest for Homogeneity of Population\\nTest for Normality\\nTest for Randomness\\n\\n  \\n4. One population & one variable\\n\\nBinomial Exact Confidence Intervals\\n Compatibility of Multi-Counts \\nGoodness-of-Fit for Discrete Variables\\nRevising the Mean and the Variance\\nTesting  the Exponential Distribution\\nTesting  the Mean\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\nTesting the Medians\\nTesting the Percentage\\n Testing the Poisson Process\\nTesting the Variance\\nTest for Uniform Distribution\\n\\n  \\n5. One population & two or more variables\\n\\nThe Before-and-After Test for Means and Variances\\nThe Before-and-After Test for Proportions\\nChi-square Test for Crosstable Relationship\\nMultiple Regressions\\nPolynomial Regressions\\nQuadratic Regression\\n Simple Regression with Diagnostic Tools\\n Testing the Population Correlation Coefficient\\n\\n  \\n6. Two or three populations & one variable\\n\\nANOVA for Dependent Populations\\nANOVA: Testing Equality of the Means\\nK-S Test for Equality of Two Populations\\nTwo Populations Testing Means & Variances\\n\\n  \\n7. Several populations & one or more variables\\n\\nAnalysis of Covariance\\nANOVA for Condensed Data Sets\\n Compatibility of Multi-Counts \\n Equality of Multi-variances: The Bartlett\\'s Test\\nIdentical Populations Test for Crosstable Data\\nSubjective Assessment of Estimates\\nTesting the Proportions\\nTesting  Several Correlation Coefficients\\nTwo-Way ANOVA Test\\nTwo-Way ANOVA with Replications\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n   A selection of:\\n\\n| Academic Info|\\n Association of American Colleges and Universities|\\n  BUBL Catalogue|   Business (Indexes) | Business and Economics (Biz/ed)| Business & Finance| Business & Industrial| Business Nation| Chance|Education World| \\nEducypedia| \\nEconomics LTSN||Epidemiology and Biostatistics| Emerging Technologies|\\n Estadística| \\n Federation of Paralegal Associations| \\nFinancial and  Economic Links| IFORS| \\nInstitute of Statistical Sciences| \\n\\nInternational Business| Marine \\n Institute| Management|Management Sources\\n|Mathematics and Statistics| MathForum| Maths, Stats & OR Network| McGraw-Hill| Merlot| NEEDS| NetFirst| NRICH| \\nOpen Encyclopedia|Physician\\'s Quiklinks| \\n Ressources en Statistique|\\n Science Gateway| Search Engines Directory: | AltaVista| AOL| Excite|   HotBot| Looksmart| Lycos| MSN| Netscape| OpenDirectory|  Scientopica| Webcrawler| Yahoo| |Scout Report| Small Business| \\n        Social Science|\\n\\n Statistical Data| \\nStatistical Societies| \\n Statistics on the Web| SurfStat|\\n Wall Street|\\n Virtual Learning| Virtual Library| WebEc| World Lecture Hall|  Additional useful sites may be found by clicking on the following search engine:   AllTheWeb \\n\\nThe Copyright Statement: The fair use, according to the 1996 Fair Use Guidelines for Educational Multimedia, of materials presented on this Web site is permitted for non-commercial and classroom \\n        purposes only. This site may be mirrored intact (including these notices), on any server \\n        with public access.  Kindly e-mail me your comments,  suggestions, and concerns. Thank you.  Professor Hossein Arsham\\xa0 \\xa0  This site was launched on 1/18/1994, and its intellectual materials have \\n  been thoroughly revised on a yearly basis. The current version is the  9th Edition. All external links are checked once a  month. \\n      Back to Dr. Arsham\\'s Home Page\\n       EOF: © 1994-2004         \\n\\n',\n",
       " '\\n\\n The Strange Case of Emily X \\n\\n\\nSearch MAA OnlineMAA Home\\n\\n\\n\\n\\n   Devlin\\'s\\nAngle \\nSeptember 2000 \\n The Strange Case of Emily X \\n\\nReaders in their fifties and older will probably be able to \\nidentify the person I am calling \"Emily X.\" In the 1960s, \\nEmily was a brilliant and vivacious mathematics student at \\nthe University of California at Berkeley. Both her parents \\nwere physicians at the nearby Oakland Children\\'s Hospital, \\nand she had a younger brother, Todd, then still in high \\nschool. Her mathematical ability was so great that as a \\nsophomore she was already taking graduate classes in \\naddition to her undergraduate courses, and her professors \\nhad begun steering her towards graduate school. Her future \\nseemed assured.\\n\\nThen, in her junior year, Emily disappeared. There was no \\nnote, no message to her friends. Her parents and younger \\nbrother were devastated. Days turned into weeks and \\nmonths, and eventually the newspapers dropped the story. \\nA year went by, then two, then three, and still there was no \\nclue as to her whereabouts. Most people who knew her \\nfeared the worst -- that she had been abducted and \\nmurdered. \\n\\nThen, almost five years to the day after she vanished, Emily \\nreappeared. She simply walked into her former home, \\npoured a glass of milk, made a peanut butter and jelly \\nsandwich, switched on the television, and sat down to wait \\nfor her parents to come home from work. \\n\\nShe looked fit and healthy, she showed no signs of any \\nphysical abuse, and she was outwardly happy. But she had \\nabsolutely no recollection of having been away, or indeed \\nof the preceding five years. She did not know that President \\nKennedy had been shot. She had never heard of The \\nBeatles. She thought it was still 1962. \\n\\nThe only apparent change was one that only her former \\nmathematics professors could spot. Emily\\'s mathematical \\nability had increased dramatically. Five years earlier, she \\nhad been a very promising sophomore. Now she was on a \\npar with the members of Berkeley\\'s world-famous \\nmathematics faculty. \\n\\nHow could this have happened? It soon became apparent \\nthat she had not attended any other university. Even if she \\nhad studied in a foreign country, as some commentators \\nsuggested, it could not account for her total inability to \\nremember anything but the mathematics she had learned. \\n\\n\\nIt took Emily about six months of intense effort to fill in the \\nmissing years. During this time, she resumed her studies. \\nOr rather, she worked alongside her former teachers as \\nthey tackled major unsolved problems of mathematics. \\n\\nThen, after she had \"caught up\" with the world, two major \\nevents occurred that again changed her life. \\n\\nFirst, she woke up one morning and found she had \\napparently lost all her mathematical ability. She could still \\ndo arithmetic, and she could carry out the symbolic \\nmanipulations of elementary algebra as well as any bright \\nhigh school student. But she could not follow even the \\nsimplest mathematical proof -- it was as if she did not \\nunderstand what a proof was. \\n\\nSecond, that night, she experienced the first of many mental \\nflashbacks that would eventually allow her to piece together \\nher experiences during what she called her \"lost years.\" \\n\\n\\nThus far, everything is well documented. The doubts occur \\nwhen Emily tells of her life during her five \"lost years.\" Her \\ndescriptions of life in \"a cold place, with snow everywhere, \\nand a sky of shining silver\" (from her autobiography, My \\nLost Years) are so detailed, and so logically consistent, \\nthat it is hard to imagine that they are fabricated. Yet she \\ndescribes a world most of us would dismiss as science \\nfiction. \\n\\nThough Emily eventually recovered sufficiently to live an \\noutwardly normal life, she never married or formed any \\nclose personal relationships. This is particularly significant, \\ngiven that her descriptions of her lost years are filled with \\nhighly detailed accounts of friendships, love affairs, \\nweddings, and marriages. Only much later, when she had \\nretrieved the more painful memories from her lost years, did \\nit become clear what had led to this interest. \\n\\nSome of the interpersonal relationships in Emily\\'s other \\nworld are straightforward enough, others seem strange. For \\ninstance, in one section she describes (in extreme detail) \\nher \"other world\" friends Janet and Eric, a married couple, \\nand how they fell in love with, and then married, a young \\nman called Paul. Three-way marriages were apparently \\nquite normal in Emily\\'s other world. In her own words \\n[My Lost Years, p.193]:\\n\\n\"The law does not prevent marriages of any size, but \\nfinancial concerns do. In a three-way marriage, for instance, \\nthe new partner has exactly the same rights as the first two \\npartners. When Paul married Janet and Eric, he had the \\nsame rights as they did. It would have been just the same if \\nEric had married Paul first and then they had married Janet. \\nBeing the first in a marriage offers no legal advantages. \\nCouples definitely give something up if they marry a third \\nperson. Of course, they gain a lot as well. But most decide \\nnot to.\"\\n\\nElsewhere, Emily describes her own \"empty marriage.\" The \\nterm turns out to have a different meaning in Emily\\'s other \\nworld than it has in the all-too-familiar world of failed \\nmarriages. An \"empty marriage,\" Emily explains, was a \\nmarriage ceremony in which a person married a state-\\nappointed individual called a \"tolen.\" Though legally valid, \\nthe ceremony does not affect either party\\'s legal status. It\\'s \\nonly purpose is to enable the individual to experience a \\nmarriage ceremony. \\n\\nMany readers will remember Emily\\'s appearance on The \\nTonight Show with Johnny Carson. Carson recounts the \\ninterview in his own autobiography [Here\\'s Johnny, \\np.207]:\\n\\n\"What about divorce?\" I asked at one point in the \\nconversation. \\n\\n\"Oh yes, of course there\\'s divorce,\" Emily replied. \\n\\n\"Maybe you were living in Nevada!\" I quipped. She did not \\nrespond to my humor, so I asked another question: \"Tell me, \\nhow do you go about getting a divorce in your world? Is it \\neasy?\" \\n\\n\"No, it\\'s not easy at all,\" Emily replied. \"Divorce is just a \\nspecial kind of marriage. You have to find your spouse\\'s \\nnullifier. Then you go through a normal marriage ceremony \\nwith the nullifier. Then you are no longer married.\" \\n\\nI remarked that this didn\\'t sound difficult at all, and joked \\nthat Elizabeth Taylor did this all the time, but Emily ignored \\nmy attempt at humor. She went on to explain the process  \\nstep-by-step, as if she were telling a child how to boil an \\negg. \"Everyone has a nullifier,\" she said, \"but you only have \\none of them, so the difficult thing is to find him. It can take a \\nlot of time. Maybe you\\'ll never find him. That can happen. Of \\ncourse, there are agencies that specialize in locating \\nnullifiers . . .\" \\n\\nBy now the producer was waving furiously at me to liven \\nthings up, and I had to prevent the interview from turning \\ninto a lecture. \"I\\'ll bet they are expensive,\" I broke in again. \\n\\n\\nUnfortunately, once again Emily did not pick up on my \\nattempt to inject a bit of humor. She simply confirmed, in a \\nvery matter-of-fact way, that divorces were both expensive \\nand difficult to arrange. The producer was getting \\ndesperate. I tried again: \\n\\n\"Then I guess you were not living in Nevada after all!\" \\n\\nThis time it worked. She picked up on my intention. \"No, I\\'m \\nsure it wasn\\'t Nevada,\" she replied with a laugh. It was a \\ngreat laugh. \\n\\nAt that point, the show came to an end. Emily\\'s final laugh \\nleft a positive impression on the audience. She could have \\ncome across sounding like a freak, and she almost did, but \\nin the end she didn\\'t. That last remark saved her. And the \\nshow. \\n\\nLooking back, it was one of my favorite shows. Emily was a \\nfascinating person. I don\\'t have any record of the rest of our \\nconversation, but it was probably as long as I have spent \\nwith a guest after we had gone off the air.\\n\\n\\nBut if Emily had not spent her lost years in Nevada, where \\nhad she been? Many dismiss Emily\\'s story as the delusions \\nof a schizophrenic. But they cannot explain the fact that for \\nfive years not a single person saw her or heard from her, \\nand that during that same period, she went -- untutored by \\nany human being -- from being a bright mathematics \\nundergraduate to a world class research mathematician. \\nYou can\\'t fake mathematics. \\n\\nOr can you? For anyone who is unable to figure out the\\nsignificance of the Emily X story, I offer my own explanation \\nin my new book\\n\\nThe Math Gene: How Mathematical Thinking\\nEvolved and Why Numbers Are Like Gossip\\n(published in the USA by Basic Books), from which this\\nmonth\\'s column is extracted.\\n\\n\\nDevlin\\'s Angle is updated at the beginning of each month.\\n\\n\\nKeith Devlin\\n(\\ndevlin@stmarys-ca.edu) is Dean of Science at\\nSaint Mary\\'s College of California, in Moraga,\\nCalifornia, and a Senior Researcher at Stanford\\nUniversity.\\n\\n\\n\\n\\nCopyright ©2003 The Mathematical Association of AmericaPlease send comments, suggestions, or corrections about this page to webmaster@maa.org.\\n\\n',\n",
       " ' An instantaneous introduction to CGI scripts and HTML forms, Academic Computing Services, University of Kansas and tags installed here by insert-targets-with-anchors.pl. They are not necessary. --> An instantaneous introduction to CGI scripts and HTML forms World Wide Web (WWW) browsers display hypertext documents written in the Hypertext Markup Language (HTML). Web browsers can also display \"HTML forms\" that allow users to enter data. By using forms browsers can collect as well as display infomation. When information is collected by a browser it is sent to a HyperText Transfer Protocol (HTTP) server specified in the HTML form, and that server starts a program, also specified in the HTML form, that can process the collected information. Such programs are known as \"Common Gateway Interface\" programs, or CGI scripts. This document is available in two formats: Standard HTML (this document), and Expandable HTML (with \"twist down\" knobs) . Table of Contents Introduction The Canonical Browser-Server Interaction Executing \"scripts\" Executing a Script via an HTML Form HTML Tags Related to Forms Mode An Example Form What a Post Query Looks Like What the Server Does A Custom Events Database Possible Input Tag Data Types The SELECT Tag The TEXTAREA Tag Some Forms Don\\'t Really Exist Using the GET Method HTML Forms as an Interface to Databases Sources of Additional Information Introduction This document describes the Common Gateway Interface in some detail. It focuses on the ways in which a form, a client browser, a server, and the HTTP protocol work together. To understand this complex interaction, you must first understand how a client and a server work together to deliver a \"normal\" HTML document. This is the \"canonical\" Web activity; the \"usual\" Web function. Then you need to understand how scripts are executed in the Web environment without mediating forms. Once these two processes are clear, you have the foundation to understand the interaction bewteen HTML forms and the scripts that process the data from those forms. For a different approach to this same topic consult its companion piece Building blocks for CGI scripts in Perl , which provides Perl code for many common CGI tasks, making script creation fairly simple. The Canonical Browser-Server Interaction During a \"normal\" document exchange a WWW client (Netscape, Mosaic, Lynx, etc.) requests a document from a WWW server and displays that document on a user display device. If that document contains a link to another document, and the user activates that link, the WWW client will then fetch and display the linked document. The following diagram shows a WWW client running on a desktop system, Computer A, interacting with two servers: An HTTP server running on Computer B and an HTTP server running on Computer C. The client running on Computer A gets a document, stored in a file named docu1.html, from the HTTP server running on Computer B. This document contains a link to another document, stored in a file named docu2.html on Computer C. The Uniform Resource Locator (URL) for that link might look something like: http://ComputerC.domain/docu2.html If the user activates that link, the client retrieves the file from the HTTP server running on Computer C and displays it on the monitor connected to Computer A. The HyperText Transfer Protocol defines communication between the client and an HTTP server. The following example shows what an HTTP exchange between a Lynx client and an HTTP server running on Computer C might look like as the client fetches docu2.html. The client sends the following text to server: GET /docu2.html HTTP/1.0 Accept: www/source Accept: text/html Accept: image/gif User-Agent: Lynx/2.2 libwww/2.14 From: montulli@www.ku.edu * a blank line * The \"GET\" request indicates which file the client wants and announces that it is using HTTP version 1.0 to communicate. The client also lists the Multipurpose Internet Mail Extension (MIME) types it will accept in return, and identifies itself as a Lynx client. (The \"Accept:\" list has been truncated for brevity.) The client also identifies its user in the \"From:\" field. Finally, the client sends a blank line indicating it has completed its request. The server then responds by sending: HTTP/1.0 200 OK Date: Wednesday, 02-Feb-94 23:04:12 GMT Server: NCSA/1.1 MIME-version: 1.0 Last-modified: Monday, 15-Nov-93 23:33:16 GMT Content-type: text/html Content-length: 2345 * a blank line * . . . . . .etc. In this message the server agrees to use HTTP version 1.0 for communication and sends the status 200 indicating it has successfully processed the client\\'s request. It then sends the date and identifies itself as an NCSA HTTP server. It also indicates it is using MIME version 1.0 to describe the information it is sending, and includes the MIME-type of the information about to be sent in the \"Content-type:\" header. Finally, it sends the number of characters it is going to send, followed by a blank line and the data itself. Things to note here: Client and server headers are RFC 822 compliant mail headers. A Client may send any number of Accept: headers and the server is expected to convert the data into a form the client can accept. Executing \"scripts\" An HTTP URL may identify a file that contains a program or script rather than an HTML document. That program may be executed when a user activates the link containing the URL. The diagram below shows an hypertext document on Computer B with a link to a file on Computer C that holds the CGI program that will be executed if a user activates the link. This link is a \"normal\" http: link, but the file is stored in such a way that the HTTP server on Computer C can tell that the file contains a program that is to be run, rather than a document that is to be sent to the client as usual. When the program runs, it prepares an HTML document on the fly, and sends that document to the client, which displays the document as it would any other HTML document. Such programs are sometimes called HTTP scripts or \"Common Gateway Interface\" (CGI) scripts. Note that CGI scripts may be written in scripting languages (like Perl, TCL, etc.) or in any other programming language (like C, Pascal, Basic). On some HTTP servers these CGI programs are stored in a directory called cgi-bin, and so they are also sometimes called \"cgi-bin scripts.\" Here is a simple AppleScript program that can be run by a MacHTTP server when it receives a request for the file containing the script. When it runs, this program builds an HTML document containing the current time and returns the document to the WWW client that requested it. set crlf to (ASCII character 13) & (ASCII character 10) set header to \"HTTP/1.0 200 OK\" & crlf - & \"Server: MacHTTP\" & crlf set header to header & \"MIME-Version: 1.0\" - & crlf & \"Content-type: text/html\" set header to header & crlf & crlf - & \" Server Script \" set body to \" The time is: \" - & (current date) & \" \" return header & body The program is stored in a file named \"date\", in a folder called \"scripts\". When a user activates a link that points to this script, the Web client will generate an HTTP request that might look like: GET /scripts/date HTTP/1.0 Accept: www/source Accept: text/html Accept: image/gif User-Agent: Lynx/2.2 libwww/2.14 From: montulli@www.ku.edu * a blank line * When the script runs it will generate an HTTP response that might look like: HTTP/1.0 200 OK\" Server: MacHTTP\" MIME-Version: 1.0 Content-type: text/html * blank line * Server Script The time is: September 15, 1994 3:15 pm This looks just like any HTTP response from an HTTP server returning a normal HTML document. It just happens to have been generated on the fly. Executing a Script via an HTML Form The ability to process fill-out forms within the Web required modifications to HTML, Web clients, and Web servers (and eventually to HTTP, as well). A set of tags was added to HTML to direct a WWW client to display a form to be filled out by a user and then forward the collected data to an HTTP server specified in the form. Servers were modified so that they could then start the CGI program specified in the form and pass the collected data to that program, which could, in turn, prepare a response (possibly by consulting a pre-existing database) and return a WWW document to the user. The following diagram shows the various components of the process. In this diagram, the Web client running on Computer A acquires a form from some Web server running on Computer B. It displays the form, the user enters data, and the client sends the entered information to the HTTP server running on Computer C. There, the data is handed off to a CGI program which prepares a document and sends it to the client on Computer A. The client then displays that document. HTML Tags Related to Forms Mode The tags added to HTML to allow for HTML forms are: <FORM>. . . </FORM> Define an input form. Attributes: ACTION, METHOD, ENCTYPE <INPUT> Define an input field. Attributes: NAME, TYPE, VALUE, CHECKED, SIZE, MAXLENGTH <SELECT> . . . </SELECT> Define a selection list. Attributes: NAME, MULTIPLE, SIZE <OPTION> Define a selection list selection (within a SELECT). Attribute: SELECTED <TEXTAREA> . . . </TEXTAREA> Define a text input window. Attribute: NAME, ROWS, COLS An Example Form This section presents a simple form and shows how it can be represented using the HTML forms facility, filled out by a user, passed to a server, and generate a reply. The form asks for information about using the World Wide Web. This is a practice form. Please help us to improve the World Wide Web by filling in the following questionaire: Your organization? _________________________________ Commercial? ( ) How many users? ____________________ Which browsers do you use? 1. Cello ( ) 2. Lynx ( ) 3. X Mosaic ( ) 4. Others ___________________________________ A contact point for your site: __________________________________________ Many thanks on behalf of the WWW central support team. Submit Reset Here is an HTML document that defines the Example Form just presented (courtesy of Dave Raggett, Hewlett-Packard, but modified to reflect the current implementation of HTML) You can select this link to see what this form looks like from your browser This is a practice form. Please help us to improve the World Wide Web by filling in the following questionaire: Your organization? Commercial? How many users? Which browsers do you use? Cello Lynx X Mosaic Others A contact point for your site: Many thanks on behalf of the WWW central support team. When this document gets filled out by the user, it might look something like this from Lynx: This is a practice form. Please help us to improve the World Wide Web by filling in the following questionaire: Your organization? Academic Computing Services____ Commercial? ( ) How many users? 10000______________ Which browsers do you use? 1. Cello (*) 2. Lynx (*) 3. X Mosaic (*) 4. Others Mac Mosaic, Win Mosaic____________________ A contact point for your site: Michael Grobe grobe@kuhub.cc.ku.edu___ Many thanks on behalf of the WWW central support team. Submit Reset What a Post Query Looks Like When the form is \"submitted\" as filled out above, the following information is sent to www.ku.edu by the client: POST /cgi-bin/post-query HTTP/1.0 Accept: www/source Accept: text/html Accept: video/mpeg Accept: image/jpeg Accept: image/x-tiff Accept: image/x-rgb Accept: image/x-xbm Accept: image/gif Accept: application/postscript User-Agent: Lynx/2.2 libwww/2.14 From: grobe@www.ku.edu Content-type: application/x-www-form-urlencoded Content-length: 150 * a blank line * org=Academic%20Computing%20Services &users=10000 &browsers=lynx &browsers=cello &browsers=mosaic &others=MacMosaic%2C%20WinMosaic &contact=Michael%20Grobe%20grobe@kuhub.cc.ku.edu This query is a \"POST\" query addressed for the program residing in the file at \"/cgi-bin/post-query\". Post-query is a script that simply echoes the values it receives. Once again the client lists the MIME-types it is capable of accepting, and identifies itself and the version of the WWW library it is using. Finally, it indicates the MIME-type it has used to encode the data it is sending, the number of character included, and the list of variables and their values it has collected from the user. The MIME-type application/x-www-form-urlencoded means that the variable name-value pairs will be encoded the same way a URL is encoded. In particular, any special characters, including puctuation characters, will be encoded as %nn where nn is the ASCII value for the character in hexidecimal. What the Server Does The server takes the incoming data and passes it to the program post-query, which uses it to construct a file to return to the client. The reply may be HTML, an image file, or any other kind of document, though returning an HTML document is most common. The script\\'s response to the example query is an HTML document that lists the variable values it received. The HTML looks like: Content-type: text/html * a blank line * Query Results You submitted the following name/value pairs: org = Academic Computing Services users = 10000 browsers = cello browsers = lynx browsers = xmosaic others = Mac Mosaic, Win Mosaic contact = Michael Grobe grobe@kuhub.cc.ku.edu Which looks like this on the Lynx user\\'s screen: QUERY RESULTS You submitted the following name/value pairs: * org = Academic Computing Services * users = 10000 * browsers = cello * browsers = lynx * browsers = xmosaic * others = Mac Mosaic, Win Mosaic * contact = Michael Grobe grobe@kuhub.cc.ku.edu Post-query is written in C and can be inspected by activating this link . Scripts can written in other languages, and frequently are written in whatever language a particular server interacts with most gracefully: a version in AppleScript for MacHTTP, and a version in VisualBasic for WinHTTP a version in Perl . Note that all three programs are short; each is about one page long. Of course they all call some subroutines that are not shown, but these subroutines are not large and are available with the servers each program was designed to work with, or from some other net source. For more information see below . A Custom Events Database The KU Events database is accessed via an HTML form that looks like this from Lynx: UNIVERSITY OF KANSAS EVENTS DATABASE Search for events Beginning search date: January__, 27, 1993 Ending search date: May______, 1_, 1994 (*)Academic field (*)Museum & gallery (*)Academic year (*)Music (*)Athletic (*)Other cultural (*)Parties (*)Ceremonies & recognitions (*)Recreational (*)Club & group meeting (*)Theatre (*)Conferences & workshops (*)Film (*)Special academic matters (*)Holidays, etc (*)Service & charitable (*)Lecture (*)Training events (*)Local & area (*)University governance & structure Search for events Reset to default values (Form submit button) Use right-arrow or to submit form. Arrow keys: Up and Down to move. Right to follow a link; Left to go back. H)elp O)ptions P)rint G)o M)ain screen Q)uit /=search [delete]=history list To see the Event Calendar from your browser click here . The following query is being sent to the Event Calendar. It\\'s very similar to the one generated by the simple example, but somewhat longer due to the complexity of the event form. POST /cgi-bin/events-form HTTP/1.0 Accept: www/source Accept: text/html Accept: video/mpeg Accept: image/jpeg Accept: image/x-tiff Accept: image/x-rgb Accept: image/x-xbm Accept: image/gif Accept: application/postscript User-Agent: Lynx/2.2 libwww/2.14 From: montulli@www.ku.edu Content-type: application/x-www-form-urlencoded Content-length: 681 start_month=January &start_day=27 &start_year=1993 &end_month=May &end_day=1 &end_year=1994 &event_type=Academic%20field &event_type=Museum%20%26%20gallery &event_type=Academic%20year &event_type=Music &event_type=Athletic &event_type=Other%20cultural &event_type=Parties &event_type=Ceremonies%20%26%20recognitions &event_type=Recreational &event_type=Club%20%26%20group%20meetings &event_type=Theatre &event_type=Conferences%20%26%20workshops &event_type=Film &event_type=Special%20academic%20matters &event_type=Holidays%2C%20etc &event_type=Service%20%26%20charitable &event_type=Lecture &event_type=Training%20events &event_type=Local%20%26%20area &event_type=University%20governance%20%26%20structure Possible Input Tag Data Types The input tag currently supports the following data types (depending somewhat on which client you are using): TEXT For entering a single line of text. The SIZE attribute can be used to specify the visible width of the field. The MAX attribute can be used to specify the maximum number of characters that can be typed into the field. CHECKBOX For Boolean variables, or for variables which can take multiple values at the same time. When a box is checked, the value specified in its VALUE attribute is assigned to the variable specified in its NAME attribute. If several checkbox fields each specify the same variable NAME, they can be used to assign multiple values to the named variable, since each checkbox field may have a VALUE attribute. RADIO For variables which can take only a single value from a set of alternatives. If several radio buttons have the same NAME, selecting one of the buttons will cause any already selected button in the group to be deselected. SUBMIT Selecting this link or pressing this button submits the form. RESET Selecting this link or pressing this button resets the form\\'s fields to their initial values as specified by their VALUE attributes. HIDDEN For passing state information from one form to the next or from one script to the next. An input field of type HIDDEN will not appear on the form, but the value specified in the \"VALUE\" attribute will be passed along with the other values when the form is submitted. IMAGE For displaying an image map within a form and returning the coordinates of a mouse click within the image. The SELECT Tag The RADIO and CHECKBOX fields can be used to specify multiple choice forms in which every alternative is visible as part of the form. An alternative is to use the SELECT element which produces a pull down list. Every alternative is specified in an OPTION element. Cello Lynx X Mosaic Mac Mosaic Win Mosaic Line Mode Some other The next example shows how Lynx would render a select list used with the Web info form presented earlier. Click here to see how the select list would be rendered by your browser. This is a practice form. Please help us to improve the World Wide Web by filling in the following questionaire: Your organization? ___________________________________________ Commercial? ( ) How many users? ____________________ Which browser do you use most often? [Cello_____] A contact point for your site: __________________________________________ Many thanks on behalf of the WWW central support team. Submit Reset (Option list) Hit return and use arrow keys and return to select option Arrow keys: Up and Down to move. Right to follow a link; Left to go back. H)elp O)ptions P)rint G)o M)ain screen Q)uit /=search [delete]=history list When you move to the question about browsers, you see a window open showing the options. It will look something like this in Lynx: This is a practice form. Please help us to improve the World Wide Web by filling in the following questionaire: Your organization? ___________________________________________ Commercial? ( ) How many users? ____________________ ************** Which browsers do you use most often? * Cello * * Lynx * A contact point for your site: * X Mosaic * ______________________________________* Mac Mosaic * * Win Mosaic * Many thanks on behalf of the WWW centr* Line Mode *m. * Some other * Submit Reset ************** (Option list) Hit return and use arrow keys and return to select option Arrow keys: Up and Down to move. Right to follow a link; Left to go back. H)elp O)ptions P)rint G)o M)ain screen Q)uit /=search [delete]=history list You can then use the up- and down-arrow keys to select an option which will be set when you press enter to leave the pull down menu. If you include the MULTIPLE attribute in the <SELECT> tag, the user should be able to select more than one optional value from the list. Click here to see how multiple selects works with your browser. Lynx will render a multiple select as a set of checkboxes, rather than as pull-down menu. The TEXTAREA Tag When you need to let users enter more than one line of text, you should use the TEXTAREA element: Academic Computing Services The University of Kansas Lawrence, Kansas 66045 The text between the <TEXTAREA> and </TEXTAREA> tags is used to initialize the text area variable value. This </TEXTAREA> tag is always required even if the field is initially blank. The ROWS and COLS attributes determine the visible dimension of the field in characters. Click here to see how text areas are rendered by your browser. Some Forms Don\\'t Really Exist Some forms don\\'t really exist as HTML documents; they are produced by programs (CGI scripts). Once they are filled out, the information provided by the user may then be sent to another program for processing. For example, the link to the KU events database is: http://www.ku.edu/events_form/events-form-get which generates the event query form for the user to fill out. There is no free-standing HTML document containing the event query form. Note also that the program that generates the form and the program that processes the form may be the same program. For example, the event query form generated by events-form-get contains the <form> tag: which points to the program stored in /events_form/events-form, which is actually the same program as in /events_form/events-form-get. In fact, the two files are really the same file with two different names (a UNIX symbolic link). events-form-get is accessed with a standard HTTP GET method, since any http: URL within an HTML anchor is accessed by using an HTTP GET method, as described earlier. However, events-form will be accessed by using the POST method as specified in the <form> tag. Since the program can tell which method is being used it can act accordingly. That is, it will send the event query form when accessed with a GET method and will process form data when accessed with the POST method. Using the GET Method Form data may be sent to scripts for processing by using the GET method as well as the POST method. For example, the first form example above could have been encoded as If a GET method is used, an HTTP request from the client would look something like: GET /cgi-bin/post-query?org=Academic%20Computing%20Services &users=10000&browsers=lynx&browsers=cello&browsers=mosaic &others=MacMosaic%2C%20WinMosaic &contact=Michael%20Grobe%20grobe@kuhub.cc.ku.edu HTTP/1.0 Accept: www/source Accept: text/html Accept: video/mpeg Accept: image/jpeg Accept: image/x-tiff Accept: image/x-rgb Accept: image/x-xbm Accept: image/gif Accept: application/postscript User-Agent: Lynx/2.2 libwww/2.14 From: grobe@www.ku.edu * a blank line * This request is very similar to the POST request except that the values of the form variables are sent as part of the URL. That is, the variable values are appended to the URL following a question mark (?), and special characters are escaped just as special characters in URLs must be escaped to assure interoperability. Hence the MIME type designation: application/x-www-form-urlencoded. The server script can unpack the data shipped in the URL somewhat as it can unpack data sent via the POST method. The ability to append data to an arbitrary URL makes it possible to construct HTML anchors that send data to server scripts when they are activated. This allows document creators to prepare \"canned queries\" within their documents, something that is not possible with the POST method. For example, the anchor below could generate a list of activites related to athletic events at the University of Kansas when activated if the event script could recognize GET queries with extended URLs (which it does not). Click here for a list of Kansas Athletic events Here the user does not have to interface with a form, because the document creator has already decided what information must be sent to the form script. In general, GET should probably be used when a URL access will not change the state of a database (by, for example, adding or deleting information) and POST should be used when an access WILL cause a change. However, due to bugs in some server software you might not be able to use a GET method if the query is too long. HTML Forms as an Interface to Databases One of the forces behind the development of the Common Gateway Interface was the desire to integrate databases with the Web. There are several alternative approaches , and the CGI is one of the most widely used. There are several advantages to the CGI approach: One client can serve as a front end for multiple databases One database can talk to multiple clients, each with its native platform interface characteristics. Changing the database query model does not require changing all clients in the field--only the form documents accessed by clients And, of course, there are some difficulties: The interface does not support an exhaustive set of data types The forms interface is form oriented rather than field oriented, so that it is not as robust as it could be: The forms interface does not support client-side range checking for data values. This disadvantage has been attenuated with the advent of JavaScript, which may be used to write scripts that execute on the client and perform data validation before sending the data to the server, and The forms interface requires the user to press a submit button for any server involvement. This disadvantage has also been attenuated with the advent of JavaScript. Navigation among various input fields can be awkward on some platforms CGI is built over HTTP which is a \"stateless\" protocol. That is, the connection between the client and the server is broken as soon as the server responds. Implementing \"statefulness\" in this environment is awkward, complex, and can be wasteful of computing resources. Sources of Additional Information For an introduction to CGI scripting and HTML forms organized around writing Perl scripts see \"Building Blocks for CGI Scripts in Perl\" at http://www.ku.edu/~acs/docs/other/cgi-with-perl.shtml Information describing the NCSA implementation of NCSA httpd and its forms interface is provided at: http://hoohoo.ncsa.uiuc.edu/cgi/overview.html Information about the Windows implementation of the NCSA server and its forms interface is available from: http://www.city.net/win-httpd/ For a brief description of the tags used to implement forms in HTML see http://www.ku.edu/~acs/docs/other/HTML_quick.shtml Additional Information about writing CGI scripts in Perl is available at http://www.yahoo.com/Computers/World_Wide_Web/Programming/Perl_Scripts/ Michael Grobe Academic Computing Services The University of Kansas grobe@kuhub.cc.ku.edu First version: Sometime in 1994 Current version: July 22, 1998 Updates and additions made by: Hasan Naseer June 1998 Academic Computing Services at the University of Kansas ACS Main ACS Help Search ACS KU Main Search KU The current URL is http://www.ku.edu/~acs/docs/other/forms-intro.shtml. This file was last modified Thursday, 15-Mar-2001 15:50:14 CST. Questions about Academic Computing Services to question@ku.edu Problems, comments about this Web site to acsweb@ku.edu . June 1998 Academic Computing Services at the University of Kansas ACS Main ACS Help Search ACS KU Main Search KU The current URL is http://www.ku.edu/~acs/docs/other/forms-intro.shtml. This file was last modified Thursday, 15-Mar-2001 15:50:14 CST. Questions about Academic Computing Services to question@ku.edu Problems, comments about this Web site to acsweb@ku.edu . =\"below.shtml\"--> \\n\\t\\n\\n-->\\n\\n\\n\\n\\n\\n\\n\\n\">\\n\\n\\n\\nAn instantaneous introduction to CGI scripts and HTML forms, Academic\\nComputing Services,\\nUniversity of Kansas\\n\\n\\n\\n\\n\\n\\nAn instantaneous introduction to CGI scripts and HTML forms\\n\\nWorld Wide Web (WWW) browsers display hypertext documents written in\\nthe Hypertext Markup Language (HTML).\\nWeb browsers can also display \"HTML forms\" that allow users to enter data.\\nBy using forms browsers can collect as well as display infomation.\\n\\nWhen information is collected by a browser it is sent to a\\nHyperText Transfer Protocol (HTTP) server\\nspecified in the HTML form, and that server starts a program, also specified\\nin the HTML form, that can process the collected information.\\nSuch programs are known as \"Common Gateway Interface\" programs, or \\nCGI scripts.\\n\\nThis document is available in two formats:\\n\\n\\nStandard HTML (this document), and\\nExpandable HTML (with \"twist down\" knobs).\\n\\n\\nTable of Contents\\n\\n\\n\\nIntroduction\\n\\n\\nThe Canonical Browser-Server Interaction\\n\\n\\nExecuting \"scripts\"\\n\\n\\nExecuting a Script via an HTML Form\\n\\n\\nHTML Tags Related to Forms Mode\\n\\n\\nAn Example Form\\n\\n\\nWhat a Post Query Looks Like\\n\\n\\nWhat the Server Does\\n\\n\\nA Custom Events Database\\n\\n\\nPossible Input Tag Data Types\\n\\n\\nThe SELECT Tag\\n\\n\\nThe TEXTAREA Tag\\n\\n\\nSome Forms Don\\'t Really Exist\\n\\n\\nUsing the GET Method\\n\\n\\nHTML Forms as an Interface to Databases\\n\\n\\nSources of Additional Information\\n\\n\\n\\n\\nIntroduction\\nThis document describes the Common Gateway Interface in some detail.\\nIt focuses on the ways in which a form, a client browser, \\na server, and the HTTP protocol work together.\\n\\nTo understand this complex interaction, you must first\\nunderstand how a client and a server work together to deliver\\na \"normal\" HTML document.  This is the \"canonical\" Web activity; the\\n\"usual\" Web function.\\nThen you need to understand how scripts are executed in the Web\\nenvironment without mediating forms.  Once these two processes are\\nclear, you have the foundation to understand the interaction\\nbewteen HTML forms and the scripts that process the data from those\\nforms.\\n\\n\\nFor a different approach to this same topic consult its companion piece\\n\\nBuilding blocks for CGI scripts in Perl, which provides Perl\\ncode for many common CGI tasks, making script creation fairly simple.  \\n\\n\\nThe Canonical Browser-Server Interaction\\n\\nDuring a \"normal\" document exchange a WWW client \\n(Netscape, Mosaic, Lynx, etc.) requests a document from a WWW server and\\ndisplays that document on a user display device.  \\nIf that document contains a link to another document, and \\nthe user activates that link, the WWW client will then fetch and \\ndisplay the linked document.\\n\\nThe following diagram shows a WWW client running on a desktop \\nsystem, Computer A, interacting with two servers:  An HTTP\\nserver running on Computer B and an HTTP server running on Computer C.\\n\\n\\n\\nThe client running on Computer A gets a document, stored in a file\\nnamed docu1.html, from the HTTP server running on Computer B.  \\nThis document contains a link to another document, stored in a\\nfile named docu2.html on Computer C.\\nThe Uniform Resource Locator (URL) for that link might look something like:\\n\\nhttp://ComputerC.domain/docu2.html\\n\\nIf the user activates that link, the client retrieves the file from \\nthe HTTP server running on Computer C and displays it on the \\nmonitor connected to Computer A.\\n\\nThe HyperText Transfer Protocol defines communication \\nbetween the client and an HTTP server.  The following example\\nshows what an HTTP\\nexchange between a Lynx client and an HTTP server running\\non Computer C might look like as the client fetches docu2.html.\\n\\nThe client sends the following text to server:\\n\\nGET /docu2.html HTTP/1.0\\nAccept: www/source\\nAccept: text/html\\nAccept: image/gif\\nUser-Agent:  Lynx/2.2  libwww/2.14\\nFrom:  montulli@www.ku.edu\\n     * a blank line *\\n\\nThe \"GET\" request indicates which file the client wants and\\nannounces that it is using HTTP version 1.0 to communicate.\\nThe client also lists the  Multipurpose Internet Mail Extension (MIME)\\ntypes it will accept in return, and identifies \\nitself as a Lynx client.  (The \"Accept:\" list has been truncated for \\nbrevity.)\\nThe client also identifies its user in the \"From:\" field.\\n\\nFinally, the client sends a blank line indicating it has completed \\nits request.\\n\\nThe server then responds by sending:\\n\\nHTTP/1.0 200 OK\\nDate: Wednesday, 02-Feb-94 23:04:12 GMT\\nServer: NCSA/1.1\\nMIME-version: 1.0\\nLast-modified: Monday, 15-Nov-93 23:33:16 GMT\\nContent-type: text/html\\nContent-length: 2345\\n     * a blank line *\\n . . .  . . .etc.\\n\\nIn this message the server agrees to use HTTP version 1.0 for \\ncommunication and sends the status 200 indicating it has \\nsuccessfully processed the client\\'s request.\\nIt then sends the date and identifies itself as an NCSA HTTP \\nserver.  It also indicates it is using MIME version 1.0 to describe \\nthe information it is sending, and includes the MIME-type of the \\ninformation about to be sent in the \"Content-type:\" header.\\nFinally, it sends the number of characters it is going to send, \\nfollowed by a blank line and the data itself.\\n\\nThings to note here:\\n\\nClient and server headers are RFC 822 compliant mail headers.  \\nA Client may send any number of Accept: headers and the \\nserver is expected to convert the data into a form the \\nclient can accept.\\n\\n\\nExecuting \"scripts\"\\n\\nAn HTTP URL may identify a file that contains a program or script\\nrather than an HTML document.  That program may be executed when \\na user activates the link containing the URL.\\n\\nThe diagram below shows an hypertext\\ndocument on Computer B with a link to a file on\\nComputer C that holds the CGI program that will be executed \\nif a user activates the link.\\nThis link is a \"normal\" http: link, but the file is stored in such\\na way that the HTTP server on Computer C can tell that the file\\ncontains a program that is to be run, rather than a document\\nthat is to be sent to the client as usual.  \\n\\n\\nWhen the program runs, it prepares an HTML document on the fly, and\\nsends that document to the client, which displays the document as it would any\\nother HTML document.\\n\\n\\n\\nSuch programs are sometimes called HTTP scripts \\nor \"Common Gateway Interface\" (CGI) scripts. \\nNote that CGI scripts may be written in scripting languages (like Perl,\\nTCL, etc.) or in any other programming language (like C, Pascal, Basic).\\n\\nOn some HTTP servers these CGI programs are stored \\nin a directory called cgi-bin, and so they are also \\nsometimes called \"cgi-bin scripts.\"\\n\\n\\nHere is a simple AppleScript program that can be run by a MacHTTP \\nserver when it receives a request for the file containing the script.  \\nWhen it runs, this program builds an HTML document containing the \\ncurrent time and returns the document to the WWW client \\nthat requested it.\\n\\nset crlf to (ASCII character 13) & (ASCII character 10)\\n\\nset header to \"HTTP/1.0 200 OK\" & crlf    -\\n& \"Server: MacHTTP\" & crlf \\nset header to header & \"MIME-Version: 1.0\"     -\\n& crlf & \"Content-type: text/html\" \\nset header to header & crlf & crlf     -\\n& \"Server Script\"\\n\\nset body to \"The time is:\"      -\\n& (current date) & \"\"\\n\\nreturn header & body\\n\\n\\nThe program is stored in a file named \"date\", in a folder\\ncalled \"scripts\".  When a user activates a link that points\\nto this script, the Web client will generate an HTTP\\nrequest that might look like:\\n\\n\\nGET /scripts/date HTTP/1.0\\nAccept: www/source\\nAccept: text/html\\nAccept: image/gif\\nUser-Agent:  Lynx/2.2  libwww/2.14\\nFrom:  montulli@www.ku.edu\\n     * a blank line *\\n\\nWhen the script runs it will generate an HTTP response that\\nmight look like:\\n\\n\\nHTTP/1.0 200 OK\"\\nServer: MacHTTP\"\\nMIME-Version: 1.0\\nContent-type: text/html\\n* blank line *\\nServer Script\\nThe time is:\\nSeptember 15, 1994 3:15 pm \\n\\n\\nThis looks just like any HTTP response from an HTTP server returning\\na normal HTML document.  It just happens to have been generated on the\\nfly.\\n\\nExecuting a Script via an HTML Form\\nThe ability to process fill-out forms within the Web required modifications\\nto HTML, Web clients, and Web servers (and eventually to HTTP, as well).\\nA set of tags was added to HTML to direct a WWW \\nclient to display a form to be filled out by a user and then \\nforward the collected data to an HTTP server specified in the \\nform.\\n\\nServers were modified so that they could then \\nstart the CGI program specified in the form and pass the collected \\ndata to that program, which could, in turn, \\nprepare a response (possibly by consulting a pre-existing database) \\nand return a WWW document to the user.\\n\\nThe following diagram shows the various components of the process.\\n\\n\\n\\nIn this diagram, the Web client running on Computer A acquires \\na form from some Web server running on Computer B.  It displays the\\nform, the user enters data, and the client sends the entered information\\nto the HTTP server running on Computer C.  There, the data is handed off\\nto a CGI program which prepares a document and sends it to\\nthe client on Computer A.  The client then displays that document.\\n\\n\\nHTML Tags Related to Forms Mode\\n\\nThe tags added to HTML to allow for HTML forms are:\\n\\n\\n<FORM>. . . </FORM>\\nDefine an input form.\\nAttributes:  ACTION, METHOD, ENCTYPE\\n\\n\\n<INPUT>\\nDefine an input field.\\nAttributes:  NAME, TYPE, VALUE, CHECKED, SIZE, MAXLENGTH\\n\\n\\n<SELECT> . . . </SELECT>\\n\\tDefine a selection list.\\n\\tAttributes:  NAME, MULTIPLE, SIZE\\n\\n\\n<OPTION>\\n\\tDefine a selection list selection (within a \\n\\tSELECT).\\tAttribute:  SELECTED\\n\\n\\n<TEXTAREA> . . . </TEXTAREA>\\nDefine a text input window.\\nAttribute:  NAME, ROWS, COLS\\n\\n\\nAn Example Form\\n\\nThis section presents a simple form and shows how it can be \\nrepresented using the HTML forms facility, filled out by a user, \\npassed to a server, and generate a reply.  The form asks for \\ninformation about using the World Wide Web.\\n\\n                                    This is a practice form.\\n\\n   Please help us to improve the World Wide Web by filling in the \\n   following questionaire:\\n\\n   Your organization?  _________________________________\\n\\n   Commercial? ( ) How many users? ____________________\\n\\n   Which browsers do you use?\\n    1. Cello ( )\\n    2. Lynx ( )\\n    3. X Mosaic ( )\\n    4. Others ___________________________________\\n\\n\\n\\n   A contact point for your site:\\n   __________________________________________\\n\\n   Many thanks on behalf of the WWW central support team.\\n\\n   Submit Reset\\n\\n\\n\\nHere is an HTML document that defines the Example \\nForm just presented (courtesy of Dave Raggett, Hewlett-Packard, but \\nmodified to reflect the current implementation of HTML)\\n\\nYou can select this link to see\\n\\nwhat this form looks like from your browser\\n\\n\\n\\nThis is a practice form.\\n\\n\\n\\n\\nPlease help us to improve the World Wide Web by filling in \\nthe following questionaire:\\n\\n   Your organization? \\nCommercial? \\n\\n           How many users? \\nWhich browsers do you use?\\n\\n   \\nCello \\nLynx \\nX Mosaic \\nOthers \\n\\n\\nA contact point for your site: \\nMany thanks on behalf of the WWW central support team.\\n\\n     \\n\\n\\n\\n\\n\\nWhen this document gets filled out by the user, it might look \\nsomething like this from Lynx:\\n\\n\\n                                   This is a practice form.\\n\\n   Please help us to improve the World Wide Web by filling in the \\n   following questionaire:\\n\\n   Your organization? Academic Computing Services____\\n\\n   Commercial? ( ) How many users? 10000______________\\n\\n   Which browsers do you use?\\n    1. Cello (*)\\n    2. Lynx (*)\\n    3. X Mosaic (*)\\n    4. Others\\n       Mac Mosaic, Win Mosaic____________________\\n\\n\\n   A contact point for your site:\\n   Michael Grobe grobe@kuhub.cc.ku.edu___\\n\\n   Many thanks on behalf of the WWW central support team.\\n\\n   Submit Reset\\n\\n\\n\\nWhat a Post Query Looks Like\\n\\nWhen the form is \"submitted\" as filled out above, the following \\ninformation is sent to www.ku.edu by the client:\\n\\n\\nPOST /cgi-bin/post-query HTTP/1.0\\nAccept: www/source\\nAccept: text/html\\nAccept: video/mpeg\\nAccept: image/jpeg\\nAccept: image/x-tiff\\nAccept: image/x-rgb\\nAccept: image/x-xbm\\nAccept: image/gif\\nAccept: application/postscript\\nUser-Agent:  Lynx/2.2  libwww/2.14\\nFrom:  grobe@www.ku.edu\\nContent-type: application/x-www-form-urlencoded\\nContent-length: 150\\n     * a blank line *\\norg=Academic%20Computing%20Services\\n&users=10000\\n&browsers=lynx\\n&browsers=cello\\n&browsers=mosaic\\n&others=MacMosaic%2C%20WinMosaic\\n&contact=Michael%20Grobe%20grobe@kuhub.cc.ku.edu\\n\\n\\nThis query is a \"POST\" query addressed for the program residing \\nin the file at \"/cgi-bin/post-query\".\\nPost-query is a script that simply echoes the values it receives.\\nOnce again the client lists the MIME-types it is capable of \\naccepting, and identifies itself and the version of the WWW\\nlibrary it is using.\\nFinally, it indicates the MIME-type it has used to encode the \\ndata it is sending, the number of character included, and the\\nlist of variables and their values it has collected from the \\nuser.\\n\\nThe MIME-type application/x-www-form-urlencoded means that the\\nvariable name-value pairs will be encoded the same way a URL is\\nencoded.  In particular, any special characters, including puctuation\\ncharacters, will be encoded as %nn where nn\\nis the ASCII value for the character in hexidecimal.\\n\\n\\nWhat the Server Does\\n\\nThe server takes the incoming data and passes it to the  \\nprogram post-query, which uses it to construct a file to \\nreturn to the client.  \\n\\nThe reply may be HTML, an image file, or any other kind of \\ndocument, though returning an HTML document is most \\ncommon.\\n\\nThe script\\'s response to the example query is an HTML \\ndocument that lists the variable values it received.  The HTML \\nlooks like:\\n\\n\\nContent-type: text/html\\n     * a blank line *\\nQuery Results\\nYou submitted the following name/value pairs:\\n\\norg = Academic Computing Services\\nusers = 10000\\nbrowsers = cello\\nbrowsers = lynx\\nbrowsers = xmosaic\\nothers = Mac Mosaic, Win Mosaic\\ncontact = Michael Grobe grobe@kuhub.cc.ku.edu\\n\\n\\n\\nWhich looks like this on the Lynx user\\'s screen:\\n\\n\\n\\n                                 QUERY RESULTS\\n\\n   You submitted the following name/value pairs:\\n\\n     * org = Academic Computing Services\\n     * users = 10000\\n     * browsers = cello\\n     * browsers = lynx\\n     * browsers = xmosaic\\n     * others = Mac Mosaic, Win Mosaic\\n     * contact = Michael Grobe grobe@kuhub.cc.ku.edu\\n\\n\\nPost-query is written in C and can be inspected by activating \\nthis link.\\nScripts can written in other languages, and frequently are written in \\nwhatever language a particular server interacts with most gracefully:\\n\\n\\na version in AppleScript for MacHTTP,\\n and\\na version in VisualBasic for WinHTTP\\na version in Perl\\n.\\n\\n\\nNote that all three programs are short; each is about one page long.\\nOf course they all call some subroutines that are not shown, but these\\nsubroutines are not large and are available with the servers \\neach program was designed to work with, or from some other net source.\\nFor more information see below.\\n\\n\\nA Custom Events Database\\n\\nThe KU Events database is accessed via an HTML form that looks \\nlike this from Lynx:\\n\\n\\n          UNIVERSITY OF KANSAS EVENTS DATABASE\\n\\n   Search for events\\n\\n   Beginning search date: January__, 27, 1993\\n      Ending search date: May______, 1_, 1994\\n\\n(*)Academic field    (*)Museum & gallery\\n(*)Academic year     (*)Music\\n(*)Athletic          (*)Other cultural\\n(*)Parties           (*)Ceremonies & recognitions\\n(*)Recreational      (*)Club & group meeting\\n(*)Theatre           (*)Conferences & workshops\\n(*)Film              (*)Special academic matters\\n(*)Holidays, etc     (*)Service & charitable\\n(*)Lecture           (*)Training events\\n(*)Local & area      (*)University governance & structure\\n\\n     Search for events  Reset to default values\\n\\n(Form submit button)   Use right-arrow or  to submit form.\\n  Arrow keys: Up and Down to move. Right to follow a link; Left to go back.\\n H)elp O)ptions P)rint G)o M)ain screen Q)uit /=search [delete]=history list\\n\\n\\n\\nTo see the Event Calendar from your browser click\\n\\nhere.\\n\\n\\n\\nThe following query is being sent to the Event Calendar.  It\\'s very \\nsimilar to the one generated by the simple example, but somewhat \\nlonger due to the complexity of the event form.\\n\\n\\nPOST /cgi-bin/events-form HTTP/1.0\\nAccept: www/source\\nAccept: text/html\\nAccept: video/mpeg\\nAccept: image/jpeg\\nAccept: image/x-tiff\\nAccept: image/x-rgb\\nAccept: image/x-xbm\\nAccept: image/gif\\nAccept: application/postscript\\nUser-Agent:  Lynx/2.2  libwww/2.14\\nFrom:  montulli@www.ku.edu\\nContent-type: application/x-www-form-urlencoded\\nContent-length: 681\\n\\nstart_month=January\\n&start_day=27\\n&start_year=1993\\n&end_month=May\\n&end_day=1\\n&end_year=1994\\n&event_type=Academic%20field\\n&event_type=Museum%20%26%20gallery\\n&event_type=Academic%20year\\n&event_type=Music\\n&event_type=Athletic\\n&event_type=Other%20cultural\\n&event_type=Parties\\n&event_type=Ceremonies%20%26%20recognitions\\n&event_type=Recreational\\n&event_type=Club%20%26%20group%20meetings\\n&event_type=Theatre\\n&event_type=Conferences%20%26%20workshops\\n&event_type=Film\\n&event_type=Special%20academic%20matters\\n&event_type=Holidays%2C%20etc\\n&event_type=Service%20%26%20charitable\\n&event_type=Lecture\\n&event_type=Training%20events\\n&event_type=Local%20%26%20area\\n&event_type=University%20governance%20%26%20structure\\n\\n\\nPossible Input Tag Data Types \\n\\nThe input tag currently supports the following data types \\n(depending somewhat on which client you are using):\\n\\n\\nTEXT\\n\\tFor entering a single line of text.  The SIZE attribute can be \\nused to specify the visible width of the field.  The MAX \\nattribute can be used to specify the maximum number of \\ncharacters that can be typed into the field. \\n\\nCHECKBOX \\n\\tFor Boolean variables, or for variables which can take multiple \\nvalues at the same time.  When a box is checked, the value \\nspecified in its VALUE attribute is assigned to the variable \\nspecified in its NAME attribute.  If several checkbox fields \\neach specify the same variable NAME, they can be used to \\nassign multiple values to the named variable, since each \\ncheckbox field may have a VALUE attribute.\\n\\nRADIO \\n\\tFor variables which can take only a single value from a set \\nof alternatives.  If several radio buttons have the same \\nNAME, selecting one of the buttons will cause any already \\nselected button in the group to be deselected.\\n\\nSUBMIT\\n\\tSelecting this link or pressing this button submits the form.\\n\\nRESET \\n\\tSelecting this link or pressing this button resets the form\\'s \\nfields to their initial values as specified by their VALUE \\nattributes.\\n\\nHIDDEN\\n    For passing state information from one form to the next or from\\none script to the next. \\nAn input field of type HIDDEN will not appear on the form, but the value\\nspecified in the \"VALUE\" attribute will be passed along with the other\\nvalues when the form is submitted. \\n\\nIMAGE\\n\\tFor displaying an image map within a form and returning the coordinates of a mouse click within the image.\\n\\n\\n\\nThe SELECT Tag\\n\\nThe RADIO and CHECKBOX fields can be used to specify multiple \\nchoice forms in which every alternative is visible as part of \\nthe form. An alternative is to use the SELECT element which \\nproduces a pull down list.  Every alternative is specified in an \\nOPTION element.\\n\\n\\n Cello\\n        Lynx\\n        X Mosaic\\n        Mac Mosaic\\n        Win Mosaic\\n        Line Mode\\n        Some other\\n\\n    \\n\\nThe next example shows how Lynx would render a select list used with\\nthe Web info form presented earlier.\\nClick here to see\\n\\nhow the select list would be rendered by your browser.\\n\\n                                         This is a practice form.\\n\\n   Please help us to improve the World Wide Web by filling in the\\n   following questionaire:\\n\\n   Your organization? ___________________________________________\\n\\n   Commercial? ( ) How many users? ____________________\\n\\n   Which browser do you use most often? [Cello_____]\\n\\n   A contact point for your site:\\n   __________________________________________\\n\\n   Many thanks on behalf of the WWW central support team.\\n\\n   Submit Reset\\n\\n\\n\\n(Option list)  Hit return and use arrow keys and return to select option\\n  Arrow keys: Up and Down to move. Right to follow a link; Left to go back.\\n H)elp O)ptions P)rint G)o M)ain screen Q)uit /=search [delete]=history list\\n\\n\\nWhen you move to the question about browsers, you \\nsee a window open showing the options.  It will look \\nsomething like this in Lynx:\\n\\n\\n\\n                                          This is a practice form.\\n\\n   Please help us to improve the World Wide Web by filling in the\\n   following questionaire:\\n\\n   Your organization? ___________________________________________\\n\\n   Commercial? ( ) How many users? ____________________\\n                                         **************\\n   Which browsers do you use most often? * Cello      *\\n                                         * Lynx       *\\n   A contact point for your site:        * X Mosaic   *\\n   ______________________________________* Mac Mosaic *\\n                                         * Win Mosaic *\\n   Many thanks on behalf of the WWW centr* Line Mode  *m.\\n                                         * Some other *\\n   Submit Reset                          **************\\n\\n\\n\\n\\n(Option list)  Hit return and use arrow keys and return to select option\\n  Arrow keys: Up and Down to move. Right to follow a link; Left to go back.\\n H)elp O)ptions P)rint G)o M)ain screen Q)uit /=search [delete]=history list\\n\\n\\n\\nYou can then use the up- and down-arrow keys to select an option which\\nwill be set when you press enter to leave the pull down menu.\\n\\nIf you include the MULTIPLE attribute in the <SELECT> tag, the \\nuser should be able to select more than one optional value \\nfrom the list.\\nClick here to see \\n\\nhow multiple selects works with your browser.\\n\\nLynx will render a multiple select as a set of checkboxes, rather than as\\npull-down menu.\\n\\n\\nThe TEXTAREA Tag\\n\\nWhen you need to let users enter more than one line of text, \\nyou should use the TEXTAREA element:\\n\\n\\n           Academic Computing Services\\n           The University of Kansas\\n           Lawrence, Kansas 66045\\n       \\n\\n\\nThe  text between the <TEXTAREA> and \\n</TEXTAREA> tags is used to initialize the text area \\nvariable value.  \\n\\nThis </TEXTAREA> tag is always required even if the field \\nis initially blank.  \\n\\nThe ROWS and COLS attributes determine the visible \\ndimension of the field in characters.\\nClick here to see \\n\\nhow text areas are rendered by your browser.\\n\\nSome Forms Don\\'t Really Exist\\n\\nSome forms don\\'t really exist as HTML documents; they are \\nproduced by programs (CGI scripts).  Once they are filled out,\\nthe information provided by the user may then be sent to another\\nprogram for processing.\\n\\nFor example, the link to the KU events database is:\\n\\n  http://www.ku.edu/events_form/events-form-get\\n\\n\\nwhich generates the event query form for the user to fill \\nout.  There is no  free-standing HTML document containing the \\nevent query form. \\n\\nNote also that the program that generates the form and the program\\nthat processes the form may be the same program.\\nFor example, the event query form generated by events-form-get\\ncontains the <form> tag:\\n\\n\\n\\n\\nwhich points to the program stored in /events_form/events-form,\\nwhich is actually the same program as in /events_form/events-form-get.  \\nIn fact, the two files are\\nreally the same file with two different names (a UNIX symbolic link).\\n\\nevents-form-get is accessed with a standard HTTP GET method, since\\nany http: URL within an HTML anchor is accessed by using an HTTP GET \\nmethod, as described earlier.\\n\\nHowever, events-form will be accessed by using the POST method \\nas specified in the <form> tag.\\nSince the program can tell which method is being used it can \\nact accordingly.\\nThat is, it will send the event query form when accessed with a GET\\nmethod and will process form data when accessed with the POST method.\\n\\n\\nUsing the GET Method\\n\\nForm data may be sent to scripts for processing by using the GET method \\nas well as the POST method.  For example, the first form example above could\\nhave been encoded as\\n\\n\\n\\n\\n\\nIf a GET method is used, an HTTP request from the client would look\\nsomething like:\\n\\nGET /cgi-bin/post-query?org=Academic%20Computing%20Services\\n&users=10000&browsers=lynx&browsers=cello&browsers=mosaic\\n&others=MacMosaic%2C%20WinMosaic\\n&contact=Michael%20Grobe%20grobe@kuhub.cc.ku.edu HTTP/1.0\\nAccept: www/source\\nAccept: text/html\\nAccept: video/mpeg\\nAccept: image/jpeg\\nAccept: image/x-tiff\\nAccept: image/x-rgb\\nAccept: image/x-xbm\\nAccept: image/gif\\nAccept: application/postscript\\nUser-Agent:  Lynx/2.2  libwww/2.14\\nFrom:  grobe@www.ku.edu\\n     * a blank line *\\n\\n\\nThis request is very similar to the POST request except that the \\nvalues of the form variables are sent as part of the URL.  \\nThat is, the variable values are appended to the URL following\\na question mark (?), and special characters are escaped just \\nas special characters in URLs must be escaped to assure interoperability.\\nHence the MIME type designation: application/x-www-form-urlencoded.\\n\\nThe server script can unpack the data shipped in the URL somewhat as\\nit can unpack data sent via the POST method.\\n\\nThe ability to append data to an arbitrary URL \\nmakes it possible to construct HTML anchors that send data to\\nserver scripts when they are activated.  This allows document\\ncreators to prepare \"canned queries\" within their documents, something\\nthat is not possible with the POST method.\\n\\nFor example, the anchor below could generate a list of activites related\\nto athletic events at the University of Kansas when activated if the\\nevent script could recognize GET queries  with extended URLs\\n(which it does not).\\n\\n\\nClick here for a list of\\nKansas Athletic events\\n\\n\\n\\nHere the user does not have to interface with a form, because the\\ndocument creator has already decided what information must be sent\\nto the form script.\\n\\nIn general, GET should probably be used when a URL access will not change\\nthe state of a database (by, for example, adding or deleting information)\\nand POST should be used when an access WILL cause a change.\\nHowever, due to bugs in some server software you might not be able\\nto use a GET method if the query is too long.\\n\\n\\nHTML Forms as an Interface to Databases\\n\\nOne of the forces behind the development of the Common Gateway Interface was\\nthe desire to integrate databases with the Web.  \\nThere are several \\n\\nalternative approaches, and the CGI is one of the\\nmost widely used.\\nThere are several advantages to the CGI approach:\\n\\nOne client can serve as a front end for multiple databases \\nOne database can talk to multiple clients, each with its native \\nplatform interface characteristics.  \\nChanging the database query model does not require\\nchanging all clients in the field--only the form documents \\naccessed by clients\\n\\n\\nAnd, of course, there are some difficulties:\\n\\nThe interface does not support an exhaustive set of data \\ntypes\\nThe forms interface is form oriented rather than field \\noriented, so that it is not as robust as it could be:\\n\\nThe forms interface does not support client-side range checking for \\ndata values. This disadvantage has been attenuated with\\nthe advent of JavaScript, which may be used to write scripts that\\nexecute on the client and perform data validation before sending \\nthe data to the server, and\\n\\nThe forms interface requires the user to press a submit button \\nfor any server involvement.  This disadvantage has also been \\nattenuated with the advent of JavaScript.\\n\\nNavigation among various input fields can be awkward on \\nsome platforms\\nCGI is built over HTTP which is a \"stateless\" protocol.  That is,\\nthe connection between the client and the server is broken as soon\\nas the server responds. Implementing \"statefulness\" in this environment\\nis awkward, complex, and can be wasteful of computing resources.\\n\\n\\n \\nSources of Additional Information\\n\\nFor an introduction to CGI scripting and HTML forms organized around\\nwriting Perl scripts see \"Building Blocks for CGI Scripts in Perl\" at\\n\\nhttp://www.ku.edu/~acs/docs/other/cgi-with-perl.shtml\\nInformation describing the NCSA implementation of NCSA \\nhttpd and its forms interface is provided at:\\n\\n\\thttp://hoohoo.ncsa.uiuc.edu/cgi/overview.html\\nInformation about the Windows implementation of the \\nNCSA server and its forms interface is available from: \\n\\n\\thttp://www.city.net/win-httpd/\\nFor a brief description of the tags used to implement forms in HTML see\\n\\nhttp://www.ku.edu/~acs/docs/other/HTML_quick.shtml\\nAdditional Information about writing CGI scripts in Perl is available at\\n\\nhttp://www.yahoo.com/Computers/World_Wide_Web/Programming/Perl_Scripts/\\n\\n\\n\\n\\n\\n\\nMichael Grobe\\nAcademic Computing Services\\nThe University of Kansas\\ngrobe@kuhub.cc.ku.edu\\nFirst version: Sometime in 1994\\nCurrent version: July 22, 1998\\nUpdates and additions made by:\\nHasan Naseer\\nJune 1998\\n\\n\\n\\n\\n\\n\\n\\nAcademic \\n          Computing Services at the University of Kansas\\n\\n\\n\\nACS \\n        Main\\nACS \\n        Help\\n Search \\n        ACS\\n KU \\n        Main \\n Search \\n        KU \\n\\n\\n\\n\\n\\n The current URL is http://www.ku.edu/~acs/docs/other/forms-intro.shtml. \\n  \\nThis file was last modified Thursday, 15-Mar-2001 15:50:14 CST.\\n  Questions about Academic Computing Services to  \\n  question@ku.edu\\n  Problems, comments about this Web site to  acsweb@ku.edu. \\n  \\n\\n\\n\\n\\nJune 1998\\n\\n\\n\\n\\n\\n\\n\\nAcademic \\n          Computing Services at the University of Kansas\\n\\n\\n\\nACS \\n        Main\\nACS \\n        Help\\n Search \\n        ACS\\n KU \\n        Main \\n Search \\n        KU \\n\\n\\n\\n\\n\\n The current URL is http://www.ku.edu/~acs/docs/other/forms-intro.shtml. \\n  \\nThis file was last modified Thursday, 15-Mar-2001 15:50:14 CST.\\n  Questions about Academic Computing Services to  \\n  question@ku.edu\\n  Problems, comments about this Web site to  acsweb@ku.edu. \\n  \\n\\n\\n\\n=\"below.shtml\"-->\\n\\n',\n",
       " '\\n\\nIFA Services: Statistics, Sign Test\\n\\n\\n\\nReturn to Statistics\\n\\nThe Sign Test \\nExample:\\nn+ 12, n- 3, \\n   p <= 0.0352\\n\\n \\nn+ \\n\\n\\nn- \\n\\n\\n\\nCharacteristics:\\nCrudest and most insensitive test. It is also the most convincing and\\neasiest to apply. The level of significance can almost be estimated\\nwithout the help of a calculator or table. If the Sign-test indicates a\\nsignificant difference, and another test does not, you should seriously\\nrethink whether the other test is valid.\\nH0:\\nThe median value of the distribution is m (generally m = 0), \\nvalues larger (+) and smaller (-) than the median are equally likely. \\nWhen matched pairs are used, the probability of observing (A, B) is\\nequal to that of observing (B, A) and the value of A-B has median value 0.\\n\\nAssumptions:\\nNone other than H0.\\n\\nScale:\\nOrdinal, but sometimes nominal when it is used as a binomial test\\n\\nProcedure:\\nLabel observations as +/-, positive/negative, greater/smaller, left/right, \\nblack/white, male/female, or whatever dichotomy you want to use and count the \\nnumber of times each label is observed. n+ is the number of times a \"+\" is \\nobserved and n- the number of times a \"-\" is observed. \\nIgnore all zero or equal observations.\\n\\nLevel of Significance:\\nn+ and n- are \\n\\nbinomial distributed with p = q = 1/2\\nand N = (n+) + (n-).\\nIf k is the smaller of (n+) and (n-) then:\\np <= 2 * Sum (i=0 to k) {N!/(i!*(N-i)!)}/4\\n(with k! = k*(k-1)*(k-2)*...*1 is the factorial of k and 0! = 1)\\n\\nApproximation:\\nIf (n+) + (n-) = N > 25, then \\nZ = (| n+ - n- | - 1)/sqrt( N ) \\ncan be approximated with a \\n\\nStandard Normal distribution. In our example, we calculate the exact \\nprobabilities upto N = 100.\\nFor N > 30, the \\n\\nStudent t-test can be used. However, the \\n\\nWilcoxon Matched-Pairs Signed-Ranks test is a better alternative.\\n\\nRemarks:\\nThe Sign-test answers the question: How often?, whereas other\\ntests answer the question How much?. It must be kept in mind that\\nthese two questions might have different answers.\\nWhen the problem concerns the sizes of the differences, \\nthe \\nWilcoxon Matched-Pairs Signed-Ranks Test should be preferred. This test \\nis also distribution free, but it is much more sensitive than the Sign-test. \\nFinally, the Sign-test is an almost ideal quick-and-dirty test that \\ncan be used to browse datasets or to check the results of other tests \\n(e.g., as in: \"All six subjects show an increase, so why does your test\\ninsist that p > 0.05?\"). \\n\\n\\n\\nReturn to: Statistics\\n\\n',\n",
       " '\\n\\n\\n\\nILU Reference Manual - ILU Concepts\\n\\n\\n[Previous]\\xa0\\xa0\\xa0 [Next]\\n\\n(with contributions from Doug Cutting, Frank Halasz, and Denis Seversen) \\n(typeset 25 November 1997) \\nCopyright (C) 1993--1997 Xerox Corporation\\nAll Rights Reserved. \\n@defcodeindex ft @defcodeindex vt @defcodeindex et @defcodeindex mt @defcodeindex tt\\n@defcodeindex dt \\nThis document describes version 2.0alpha12 of the Inter-Language Unification (ILU)\\nsystem. \\nLots of people contributed significant amounts of code to the ILU system, including\\n(alphabetically): Joachim Achtzehnter, Judy Anderson, Antony Courtney, Doug Cutting, Mark\\nDavidson, Ken Fishkin, Frank Halasz, Scott Hassan, Rob Head, Chris Jacobi, Bill Janssen,\\nDan Larner, Martin von Loewis, Bill Nell, Ansgar Rademacher, Dennis Seversen, Bridget\\nSpitznagel, Mike Spreitzer, and Farrell Wymore. \\nMany others have contributed in other ways, including our reviewers, alpha and beta\\ntesters, and regular users. The list includes (but is not limited to): Shridhar Acharya,\\nJoachim Achtzehnter, Judy Anderson, Maria Perez Ayo, Mike Beasley, Erik Bennett, Dan\\nBrotsky, David Brownell, Bruce Cameron, George Carrette, Philip Chou, Daniel W. Connolly,\\nAntony Courtney, Doug Cutting, Mark Davidson, Jim Davis, Larry Edelstein, Paul Everitt,\\nJosef Fink, Jeanette Figueroa, James Flagg, Steve Freeman, Mark Friedman, Jim Gettys,\\nGabriel Sanchez Gutierrez, Jun Hamano, Bruno Haible, Scott W. Hassan, Carl Hauser, Rob\\nHead, Andrew Herbert, Angie Hinrichs, Ben Hurwitz, Roberto Invernici, Christian Jacobi,\\nSwen Johnson, Gabor Karsai, Nobu Katayama, Dan `Bud\\' Keith, Sangkyun Kim, Ted Kim, Don\\nKimber, Steve Kirsch, Dan Larner, Carsten Malischewski, Larry Masinter, Fernando D. Mato\\nMira, Fazal Majid, Steven D. Majewski, Michael McIlrath, Scott Minneman, Masashige\\nMizuyama, Curtis McKelvey, Chet Murthy, Farshad Nayeri, Bill Nell, Les Niles, T. Owen\\nO\\'Malley, Annrai O\\'Toole, Andreas Paepcke, Jan Pedersen, Karin Petersen, Steve Putz,\\nGeorge Robertson, Joerg Schreck, Ian Smith, Bridget Spitznagel, Peter Swain, Marvin\\nTheimer, Lindsay Todd, P. B. Tune, Bill Tutt, Kevin Tyson, Bill van Melle, Guido van\\nRossum, Brent Welch, Jody Winston, Rick Yardumian. \\nILU Concepts\\n    \\nWhat ILU Does\\nILU is primarily about interfaces between units of program structure; we call these\\nunits modules. The notion is that each module enscapsulates some logical part of a\\nprogram, that has high `cohesiveness\\' internally, and low `coupling\\' to other parts of the\\nprogram. ILU provides you with a way of writing down an object-oriented interface\\nto the module; that is, a set of object types and other types, constants, and exceptions\\nthat another module would use to communicate with it. This interface can then be processed\\nby various ILU tools to implement that communication. \\nILU allows many different binding relationships between modules. The modules can be\\nparts of one program instance, all written in the same language; they can be parts written\\nin different languages, sharing runtime support in one memory image; they can be parts\\nrunning in different program instances on different machines (on different sides of the\\nplanet). A module could even be a distributed system implemented by many program instances\\non many machines. A particular module might be part of several different program instances\\nat the same time. ILU does all the translating and communicating necessary to use all\\nthese kinds of modules in a single program. It optimizes calls across module interfaces to\\ninvolve only as much mechanism as necessary for the calling and called modules to\\ninteract. In particular, when the two modules are in the same memory image and use the\\nsame data representations, the calls are direct local procedure calls -- no stubs or other\\nRPC mechanisms are involved. The notion of a `module\\' should not be confused with the\\nindependent concept of a program instance; by which we mean the combination of code\\nand data running in one memory image. A UNIX process is (modulo the possibilities\\nintroduced by the ability, in some UNIX sytems, to share memory between processes) an\\nexample of a program instance. \\nBecause ILU standardizes many of the issues involved in providing proper inter-module\\nindependence, such as memory management and error detection and recovery strategies, it\\ncan be used to build language-independent class libraries, collections of re-usable\\nobject definitions and implementations. Because one of the design goals of ILU was to use\\nexisting standards for various pieces, rather than inventing anything new, ILU can be used\\nto implement ONC RPC or Xerox Courier services, or clients for existing ONC RPC or Xerox\\nCourier services. ILU also includes an implementation of the Object Management Group\\'s\\nCORBA Internet Inter-Orb Protocol (IIOP), and can be used to write CORBA services\\nor clients, as well. \\nHow ILU Works\\nThe approach used by ILU is one common to standard RPC systems such as Sun\\'s ONC RPC,\\nXerox\\'s Courier, and most implementations of OMG\\'s CORBA. An interface is described once\\nin some `language-neutral\\' interface specification language. Types and exceptions are\\ndescribed; exported functionality is specified by defining methods on object types.\\nTools are then run against the interface description to produce stubs for\\nparticular programming languages; these stubs can bind to, call, and be called from stubs\\ngenerated from the same interface description for a different programming language. The\\nstub code is then linked with the application code, some language-specific code containing\\nany necessary ILU support for that programming language, and the ILU kernel library,\\nwhich is code written in ANSI C. The following diagram illustrates the process: \\n\\nSeveral modules may be linked together, for a standalone use. ILU stubs are generated\\nin such a way that applications which link a caller and callee written in the same\\nlanguage directly together suffer no calling overhead. This makes ILU useful for defining\\ninterfaces between modules even in programs that do not use RPC. \\nDifferent modules of the program may be written in different programming languages.\\nThese can either be linked together in the same address space, if the runtimes of the\\ndifferent languages allow that, or they can be used to make separate network servers and\\nclients. In the case of a network service, the memory layout for the program would be\\nsomething like \\n\\nCore ILU Concepts\\n \\nObjects\\nILU is object-oriented. By this, we mean that object types serve as the primary\\nencapsulation mechanism in ILU. All functionality is exported from a module as methods\\nthat can be invoked on an instance of some object type, rather than as simple procedures.\\nThe object instance provides the context within which methods are executed. The object\\ntype system provides subtyping (`inheritance\\' of interfaces (ILU does not address object\\nimplementation)), to aid in structuring of interfaces. \\nWith respect to a particular ILU object instance, a module is called the server  if it implements the methods of that object, or a client  if it calls, but does not implement, the methods of that object. One\\nmodule can thus be a client of one object, and the server of another. An ILU object can be\\npassed as a parameter to or result of a method call, and can be (in) the parameter to an\\nexception. An object may be passed from its server to a client, from a client to its\\nserver, or between two clients, in any of the above three kinds of position. Unlike some\\nRPC systems, there can be multiple ILU objects of the same type, even on one machine, even\\nwithin one program instance. \\nFor a given ILU object, there will, in general, be multiple language-specific\\nobjects; each is an \"object\" in one of the programming languages used in the\\nsystem. One language-specific object, designated the true object, \\nactually provides the implementation of the ILU object; it is thus part of the server\\nmodule. The true object\\'s methods are written by the programmer, not generated by ILU. The\\nother language-specific objects are surrogate objects;  their\\nmethods are actually RPC stubs (generated by ILU) that call on the true object. A\\nsurrogate object is used by a client module when the server module is in a different\\nprogram instance or uses different data representations. \\nKernel Servers\\nEach instance in an ILU address space is associated with a kernel server, a\\nconstruct which manages a group of objects. Kernel servers are found in both client and\\nserver modules. Each kernel server has a server ID, a universally unique string ID.\\nThe server ID makes up part of the object ID of the instances supported by the server.\\nSome kernel servers contain only surrogate instances, and are called surrogate servers;\\nothers contain both surrogate and true instances, and are called true servers.   \\nKernel servers serve as the locus of communication between two address spaces. A true\\nserver may have a number of ports associated with it; a port is a mechanism by\\nwhich other address spaces can interact with objects in this address space. Other address\\nspaces use the port by creating a surrogate server which mirrors the true server, and\\nopening a connection from the surrogate server to the true server. Calls from\\nsurrogate instances on true instances are carried along this connection. A true server may\\nhave multiple ports, each of which may provide connectability via different RPC protocols\\nor transport mechanisms. \\nSubtyping (interface inheritance)\\nThe object model specified here provides for multiple interface inheritance. It is\\nintended that the subtype provide all the methods described by its supertypes, plus\\npossibly other methods described directly in the subtype description. It is expected that\\nin languages which support multiple-inheritance object models, that an ILU inheritance\\ntree will be reflected in the language-specific inheritance tree. In a single-inheritance\\nlanguage, or a non-object-oriented one, an ILU-specific multiple-(interface-)inheritance\\nobject system must be embedded. \\nSubtype Relationships\\nIn the ILU type system, the only subtyping questions that arise are between two object\\ntypes. This is because ILU employs only those OOP features common to all languages\\nsupported. \\nSubtyping in ILU is based on structure and name; we include the names in the structure,\\nand thus need only talk about structure. An object type declaration of the form defined\\nlater constructs a structure of the form \\n\\n(OBJTYPE\\n     SINGLETON: singleton-protocol-info\\n     OPTIONAL: Boolean\\n     COLLECTIBLE: Boolean\\n     AUTHENTICATION: authentication-type\\n     SUPERTYPES: supertype-structure, ...\\n     METHODS: method-structure, ...\\n     LEVEL-BRANDS: (interface-name, interface-brand,\\n                type-name, type-brand))\\n\\nStructure A is a subtype of structure B iff either (1) A and B are equal structures, or\\n(2) one member of A\\'s supertype-structures is a subtype of B. \\nNote that the level-brands include the interface name and (optional) brand, as well as\\nthe name and (optional) brand of the type being declared. Thus, two declarations of\\nsubtypes of the same type normally create distinct subtypes, because they would declare\\ntypes of different names, or in interfaces with different names. When the interface name\\nand the type name are the same, this does not cause a distinction, although other\\nstructural differences might. If the programmer wants to indicate that there\\'s a semantic\\ndistinction, even though it doesn\\'t otherwise show up in the structure, s/he can use\\ndifferent interface brands and/or different type brands. These distinctions can be made\\nbetween declarations in different files, or between successive versions of a declaration\\nin a file that\\'s been edited. \\n \\nSingleton Object Types\\nMany existing RPC protocols and servers do not have the notion of multiple instances of\\na type co-existing at the same server, so cannot use the instance discrimination\\ninformation passed in ILU procedure calls. To support the use of these protocols and\\nservers, we introduce the notion of a singleton object type, of which there is only\\none instance (of each singleton type) at a kernel server. Note that because a single\\naddress space may support multiple kernel servers, this means that in a single address\\nspace, there may be multiple instances of the same singleton type. When a method is being\\ncalled on an instance of a singleton type, no instance discrimination information is\\npassed. Singleton types may not be subclassed. \\nInstantiation\\nTo use (e.g., call the methods of) an ILU object, a client must first obtain a\\nlanguage-specific object for that ILU object. This can be done in one of two ways: (1) the\\nclient can call on a language-specific object of a different ILU object to return the\\nobject in question (or receive the object in a call made on the client, or in the\\nparameter of an exception caught and handled by the client); or (2) certain standard\\nfacilities can be used to acquire a language-specific object given either addressing or\\nnaming information about the ILU object. The addressing information is called a string\\nbinding handle  (SBH), and the ILU runtime library includes a\\nprocedure to acquire a language-specific object given a string binding handle for an ILU\\nobject (in strongly-typed languages, this procedure is typed to return an object of the\\nbase type common to all ILU objects in that language). \\nEvery creation of a surrogate instance implies communication with the server module,\\nand binding of the surrogate instance to the true instance. ILU may attempt to perform\\nthis communication when it is actually necessary, rather than immediately on surrogate\\ninstance creation. \\nThe process of creating an instance may bootstrapped via a name service, such as\\nthe PARC Name-and-Maintenance-Server (NMS), which allows servers to register\\ninstances on a net-wide basis. A server registers a mapping from naming information to a\\nstring binding handle. The client-side stubs for an interface include a procedure that\\ntakes naming information, looks up the corresponding string binding handle in the name\\nservice, and calls the above-mentioned library routine to map the SBH to a\\nlanguage-specific object. Alternatively, a client can do those steps itself, using an ILU\\nruntime library procedure to acquire a language-specific object for the name service. \\nString Binding Handle\\nIn ILU, there is a string-based representation for a reference to an object. That\\nrepresentation consists of a single string, called a string binding handle. ILU\\nuses string binding handles when marshalling object references for RPC. ILU also allows\\napplications to interconvert between objects and string binding handles. This is necessary\\nwhen dealing with name services, and useful in other circumstances. \\nA string binding handle contains several different pieces of information: \\n\\n\\nThe server ID, a string which identifies the particular kernel server that\\n    the object belongs to; any program can separate its objects into one or more groups, each\\n    group associated with a different kernel server. Two objects from the same kernel server\\n    are called sibling objects. \\nThe instance handle, identifies which object on the particular kernel server is\\n    being specified. \\nThe most specific type ID (also called the MSTID), a type fingerprint for\\n    the most specific type of the object. \\nThe contact info, specifies one or more of the ways by which a client of the\\n    object can communicate with it. \\n\\nThe server ID, instance handle, and MSTID may each contain any ASCII character other\\nthan NUL. They are composed into the string binding handle according the the IETF rules\\nfor URLs, but the precise form of the URL is not specified here. (In versions of ILU\\nbefore 2.0, string binding handles had a completely different syntax.) \\nThe pair (server ID, instance handle) are also known\\nas the object ID (or OID) of the object, because together they form a\\nuniversally unique ID for the object. \\nThe contact info part contains one or more contact info sequences, each\\ndescribing one particular way of communicating with the object\\'s kernel server. Each\\ncontact info sequence consists of a series of fields. The first field is known as the protocol\\ninfo, and names a particular RPC protocol, and any parameters that might influence the\\nway in which this protocol would be used. Each of the succeeding fields specifies transport\\ninfo, which defines a way of transforming or communicating data, and any parameters\\nwhich might influence that transport method. There may be many sequences of contact info\\nin any one string binding handle (but ILU currently ignores all but the first). \\nSiblings\\nSome ILU object instances may have implementation dependencies on private communication\\nwith other instances. For example, imagine an object type time-share-system,\\nwhich provides the method ListUsers(), which returns a list of\\n\"user\" instances. Imagine that time-share-system also provides the\\nmethod SetUserPriority(u : user, priority : integer). We would like to be\\nable to provide some assurance that the user instance used as a parameter to SetUserPriority\\nis an instance returned from a call to ListUsers on the same instance of a time-share-system,\\nbecause the way in which SetUserPriority is implemented relies on the user\\nbeing a user of that particular time-share-system. \\nThe ILU model provides the notion of a sibling object.  Two\\ninstances are siblings if their methods are handled by the same kernel server. Instances\\nthat are non-discriminator parameters to methods may be specified in ISL as having to be\\nsiblings of the discriminator. \\n  \\nObject Tables (or, Just-in-Time Objects)\\nTrue objects may either be created explicitly, or upon arrival of calls on them. The\\nsecond option is exercised via a feature currently called object tables (from\\n\"hash tables\", since they map a string, the instance handle, to an object --\\n\"object factories\" might be a less surprising term). After the object table\\ncreates an object, the server module then continues to manage the object\\'s existence -- in\\nthe same way(s) it manages other objects it creates. This means a server need not hold in\\nmemory all of its objects at once, which may be quite important. \\nA true kernel server may optionally include an object table, whose job is to map an\\ninstance handle (see section String Binding Handle) to\\nthe object it identifies. ILU\\'s runtime will consult the object table when a call is\\nreceived for an object not currently reified. The object table can either explicitly\\ncreate the named object, or refuse (thus declaring the instance handle invalid). \\nThis mapping operation is invoked with certain of the ILU runtime\\'s mutexes (see\\nsection Thread Synchronization) held, because it is an\\nextension of a delicate part of that runtime. The server\\'s mutex is held in all cases, and\\nthe global mutex \"gcmu\" is also held if the resulting object is expected to be\\nof a COLLECTIBLE type. The fact that these mutexes are held restricts what an\\napplication can do inside this mapping procedure. \\n  \\nServer Relocation\\nIt is sometimes useful to have a `dummy\\' true kernel server, that will redirect any\\nrequests to it to a real true kernel server somewhere else. This can be used for load\\nbalancing, automatic start-up of services, implementation of a redirecting name service,\\ncode migration, and other various purposes. ILU supports this via a mechanism called server\\nrelocation. This mechanism allows a function to be associated with a true kernel\\nserver, which is called when a request arrives at that kernel server over a connection\\nwhich uses a relocating protocol. A relocating protocol is a protocol that carries\\nrelocation requests, such as the CORBA IIOP, or the HTTP-NG w3ng\\nprotocol. The relocation function returns new contact info for the kernel server, which is\\nsent back to the caller. The caller then closes the existing connection and opens a new\\nconnection according to the specified contact info. \\n \\nGarbage Collection\\nA simple form of garbage collection is defined for ILU objects. If an object\\ntype is tagged as being collectible, a server that implements objects of that type expects\\nclients holding surrogate instances to register with it, passing an instance of a callback\\nobject. When a client finishes with the surrogate, the client unregisters itself. Thus the\\nserver may maintain a list of clients that hold surrogate instances. If no client is\\nregistered for an object, and the object has been dormant (had no methods called on it)\\nfor a period of time T1, the server may feel free to garbage collect the\\ninstance. T1 is determined by human concerns, not network performance: T1\\nis set long enough to allow useful debugging of a client. \\nTo deal with possible failure of a client process, we introduce another time-out\\nparameter. If an instance with registered clients has been dormant for a period of time T2,\\nthe server uses the callback instance associated with each client to see if the client\\nstill exists. If the client cannot be contacted for the callback, the server may remove it\\nfrom the list of registered clients for that instance. \\nIf a client calls a method on a surrogate instance of a true instance which has been\\ngarbage-collected (typically because of partitioning), it will receive the ilu.ProtocolError\\nexception, with detail code ilu.NoSuchInstanceAtServer. \\n \\nConnections\\nILU does not (directly) expose to the application programmer any notion of\\n\"connections\". That is, the called module has no pointer back to the caller, and\\nno notion of how to do anything with the caller aside from returning a result message.\\nCredentials passed in the request message can identify the caller, but not necessarily the\\nlocation the call is made from. Protocols that need such information should pass it\\nexplicitly as an argument (an instance of an object type with methods defined on it) to\\nthe method. \\n \\nPipelining\\nILU\\'s mechanisms avoid introducing blocking into a distributed program. This is because\\nILU does not try to track the identity of a thread of execution as it crosses program\\nboundaries. So if ILU were to make one call wait for the completion of another, this would\\nbe a potential cause of deadlock. \\nIt is possible for the programmer to explicitly inform ILU that one call\\'s execution is\\nnot necessary for the completion of another. This is done indirectly, via a concept called\\na pipeline. A client can create a pipeline (any number, actually), and associate\\nany collection of its calls with a pipeline (at most one pipeline per call). Making such\\nassociations asserts to ILU that none of the calls is needed for any other of them to\\ncomplete. This allows ILU to block some of them until others complete. \\nWhich will be blocked, and why would a client want to do this to itself? The answer has\\nto do with connections. You remember, those things the previous section says are not\\nexposed to applications. It\\'s true that they\\'re not directly exposed. But we\\'ll admit here\\nthat they exist, and consume resources. Sometimes it\\'s important to minimize those\\nresources. When using a non-concurrent RPC protocol, ILU avoids introducing blocking by\\nopening as many parallel connections as the client has concurrent calls to the same\\nserver. Some clients would prefer that their concurrent calls block instead of consume\\nmultiple connections. Such clients can use pipelines to enable this behavior. \\n    \\nCall Order Preservation\\nILU does not normally guarantee that the server application will receive calls in the\\nsame order that the client makes them (of course, ILU doesn\\'t promise to violate causality\\n-- it just doesn\\'t do any work to give you anything more). This is a particularly\\ninteresting issue when making a series of asynchronous calls (because there are no replies\\nto carry causality). You might think that when using a transport, such as TCP, that\\nguarantees ordering, call order preservation will follow as a consequence. But it\\'s not\\nthat simple (i.e., ILU may use multiple connections in parallel and series, and TCP\\nprovides no ordering guarantees between connections). \\nHowever, it\\'s possible for a client application to explicitly request a guarantee of\\ncall order preservation for a given collection of its calls. This is done indirectly\\nthrough an object called a serializer. A serializer represents an instance of the\\nserialization guarantee. This guarantee is with respect to a particular server and\\ncollection of calls. It guarantees that those calls will be received by the server\\napplication in the same order as they were made by the client application -- except that\\nclient calls that return after a barrier call may be received before client calls\\nthat return before that same barrier call. A barrier call is one that raises the BARRIER\\nexception, which is an ILU-specific system exception. Remember that ASYNCHRONOUS calls do\\nreturn, they just do so particularly quickly. \\nSpecial considerations apply when these calls are issued concurrently. Two calls are\\nconsidered to have been issued concurrently if each call is initiated before the other\\nreturns. In a multi-threaded runtime, they client may issue concurrent calls under the\\nsame instance of the serialization guarantee, and the ILU runtime will put them in some\\nserial order. Note that for two concurrently issued calls, either: (a) the one put first\\nis ASYNCHRONOUS, (b) they both are in the same pipeline, or (c) the one put second is\\ndelayed until the one put first returns. In a single-threaded runtime, the client may\\nissue two calls \"concurrently\" (taking advantage of a nested main loop), but\\nboth will execute successfully only if the client is lucky; otherwise, the second one will\\nraise the system exception BAD_PARAM with minor code ilu_bpm_serialConcurrent.\\nFurthermore, when single-threaded, issuing concurrent calls under the same instance of the\\nserialization guarantee but different pipelines will also cause some to raise\\nBAD_PARAM/serialConcurrent. \\nA client can create any number of serializers, and associate each one of its calls with\\nat most one serializer. This guarantee is only available for servers exported over\\nnon-concurrent RPC protocols and reliable transports. Due to current implementation\\nlimitations, the default port of the server must satisfy the protocol and transport\\nrestriction. If that port does not meet the protocol restriction, serialized calls will\\nfail with the system exception INV_OBJREF with a minor code of ilu_iom_conc_serial (where\\nno other error is noticed first) \\n     \\nSimple Binding\\nILU includes a simple binding/naming facility. It allows a module to publish an\\nobject, so that another module can import that object knowing only its object ID (as\\ndefined in section ILU Concepts). It is essentially just\\na way of binding a URN (the object\\'s ID) to a URL (the object\\'s string binding handle).\\nThe interface to this facility is deliberately quite simple; one reason is to allow\\nvarious implementations. \\nThe interface consists of three operations: Publish, Withdraw, and Lookup.\\nPublish takes one argument, an ILU object. Publish returns a\\nstring that is needed to successfully invoke Withdraw. Withdraw\\nundoes the effects of Publish, and takes two arguments: (1) the object in\\nquestion, and (2) the string returned from Publish. In some langauge\\nmappings, the string is not explicitly passed, but conveyed in the language mapping\\'s\\nrepresentation of ILU objects. Lookup takes two arguments: an object ID and a\\ntype the identified object should have. If the object with that ID is currently being\\npublished, and has the given type (among others), Lookup returns that object.\\n\\nThe implementation of simple binding shipped with ILU can use either an ILU service, or\\na shared filesystem directory, to store information on the currently published objects.\\nThis choice must be specified at system configuration time. If the shared filesystem\\napproach is used, this directory must be available by the same name, on all machines which\\nwish to interoperate. The way in which clients interact with binding is the same,\\nregardless of which approach is selected. See section Binding\\nNames in ILU for more information on these implementations. \\n  \\nError Signalling\\nILU uses the notion of an exception to signal errors between modules. An\\nexception is a way of passing control outside the normal flow of control. It is typically\\nused for handling of errors. The routine which detects the error signals an exception,\\nwhich is caught by some error-handling mechanism. The exception type supported in ILU is a\\ntermination-model exception, in which the calling stack is unrolled back to the frame\\nwhich defined the exception handler. Exceptions are signalled and caught using the native\\nexception mechanisms for the servers and clients. A raised exception may carry a single\\nparameter value, which is typed. \\n     \\nILU and OMG CORBA\\nThe type and exception model used by ILU is quite similar to that used by the Object\\nManagement Group\\'s Common Object Request Broker Architecture (CORBA). We have in fact\\nchanged ILU in some ways to more closely match CORBA. Our tools will optionally parse the\\nOMG\\'s Interface Definition Language (OMG IDL) as well as ILU\\'s ISL. \\nILU also attempts to address issues that are already upon us, but are not addressed in\\nCORBA 2.0, particularly a uniform way of indicating optional values, and distributed\\ngarbage collection. \\nILU provides two different interface definition languages, OMG IDL and ILU ISL to\\nenhance portability of ILU modules. The OMG IDL subset understood by ILU is a strict\\nsubset of OMG IDL; this means that any ILU modules developed using OMG IDL interfaces\\nshould be interoperable with any other CORBA system. Any non-CORBA extensions may only be\\nexpressed in ILU ISL, so that any modules which use these extensions must use ILU ISL to\\nexpress their interfaces, thereby underlining the fact that these modules are not\\nCORBA-compliant. We feel that this dual-interface-language approach will tend to enhance\\nboth portability and CORBA-compliance of ILU modules. \\nILU does not yet provide some of the features required by a full CORBA implementation.\\nNotably it does not provide a Dynamic Invocation Interface or Dynamic Server Interface, or\\nimplementations of either Interface Repository or Implementation Repository. It does not\\nprovide the Basic Object Adapter interface, either, but does provide an object adapter\\nwith most of the BOA\\'s capabilities, except for those connected with the Interface\\nRepository and/or Implementation Repository. \\nA number of concepts in CORBA that seem to require further thought are not yet directly\\nsupported in ILU: the use of #include (ILU uses a more limited notion of\\n\"import\"); the notion of using an IDL \"interface\" as both an object\\ntype and a name space (this seems to be a \"tramp idea\" from the language C++; in\\nILU the \"interface\" defines a name space, and the object type defines a type);\\nthe notion that all BOA objects are persistent (in ILU, the question of whether an object\\nis persistent is left up to that object\\'s implementation); the notion that type\\ndefinitions can exist outside the scope of any module or namespace (in ILU, all\\ndefinitions occur in some interface). Currently, there is no support in ILU for CORBA contexts.\\n\\n\\n[Previous]\\xa0\\xa0 [Next]\\n\\n\\n',\n",
       " \"\\n\\n\\n\\n\\n\\nStructure and Interpretation \\nof Computer Programs\\n\\n\\n\\n\\n\\n[Go to first, previous, next page; \\xa0\\xa0contents; \\xa0\\xa0index]\\n\\n\\n\\xa0\\nPreface to the Second Edition\\n\\n\\n\\n\\n\\n\\nIs it possible that software is not like anything else, that it\\nis meant to be discarded: that the whole point is to \\nalways see it as a soap bubble?\\nAlan J. Perlis\\n\\n\\n\\n\\n\\nThe material in this book has been the basis of MIT's entry-level\\ncomputer science subject since 1980.  We had been teaching this\\nmaterial for four years when the first edition was published, and\\ntwelve more years have elapsed until the appearance of this second\\nedition.  We are pleased that our work has been widely adopted and\\nincorporated into other texts.  We have seen our students take the\\nideas and programs in this book and build them in as the core of new\\ncomputer systems and languages.  In literal realization of an ancient\\nTalmudic pun, our students have become our builders.  We are lucky to\\nhave such capable students and such accomplished builders.\\n\\nIn preparing this edition, we have incorporated hundreds of \\nclarifications suggested by our own teaching experience and the\\ncomments of colleagues at MIT and elsewhere.  We have redesigned\\nmost of the major programming systems in the book, including\\nthe generic-arithmetic system, the interpreters, the register-machine\\nsimulator, and the compiler; and we have rewritten all the program\\nexamples to ensure that any Scheme implementation conforming to\\nthe IEEE Scheme standard (IEEE 1990) will be able to run the code.\\n\\nThis edition emphasizes several new themes.  The most important\\nof these is the central role played by different approaches to\\ndealing with time in computational models: objects with state,\\nconcurrent programming, functional programming, lazy evaluation,\\nand nondeterministic programming.  We have included new sections on \\nconcurrency and nondeterminism, and we have tried to integrate this\\ntheme throughout the book.\\n\\nThe first edition of the book closely followed the syllabus of our MIT\\none-semester subject.  With all the new material in the second\\nedition, it will not be possible to cover everything in a single\\nsemester, so the instructor will have to pick and choose.  In our own\\nteaching, we sometimes skip the section on logic programming\\n(section\\xa04.4), we have students use the\\nregister-machine simulator but we do not cover its implementation\\n(section\\xa05.2), and we give only a cursory overview of\\nthe compiler (section\\xa05.5).  Even so, this is still\\nan intense course.  Some instructors may wish to cover only the first\\nthree or four chapters, leaving the other material for subsequent\\ncourses.\\n\\nThe World-Wide-Web site www-mitpress.mit.edu/sicp\\nprovides support for users of this book.\\nThis includes programs from the book,\\nsample programming assignments, supplementary materials,\\nand downloadable implementations of the Scheme dialect of Lisp.\\n[Go to first, previous, next page; \\xa0\\xa0contents; \\xa0\\xa0index]\\n\\n\\n\",\n",
       " \"\\n\\n\\n\\nStatistical analysis\\n\\n\\n\\n\\n\\n\\n\\n\\nBASIC CONCEPTS OF STATISTICAL MODELS\\n\\nA probability experiment is one where the outcome is subject to chance.\\n\\n\\nConsider a probability experiment where one random observation, y[1], is \\ndrawn from a population whose mean is 10 and whose variance is 4.\\n\\n\\n\\nThe expected value of the single observation is its average if \\nwe were to repeat the experiment a great many times. If the experiment is \\nrepeated many times, we would find that the average of y[1] from all of \\nthese experiments would be 10, the mean of the population from which the \\nobservations is drawn. In symbols, E{y[1]} = 10. In words, the expected \\nvalue of the single observation is the mean of the population from which \\nit was drawn.\\n\\n\\nLikewise, if we were to repeat the experiment a great many times, we \\ncould calculate the variance of repeated observations of y[1] \\nand we would find that the variance is just equal to the variance of the \\npopulation from which y[1] is drawn. Var{y[1]} = 4. \\n\\n Thus, for a single observation, y[1], in a probability experiment, we\\nsay that its expected value is the mean of the population from which it\\ncomes and its variance is the variance of the population from which it\\ncomes.  When we give the expected value and variance of a single\\nobservation, we are actually referring to conceptual values that would\\noccur if the experiment were repeated many times. \\n Now, consider a probability experiment in which two values, y[1] and\\ny[2], are drawn at random, and independent of each other, from a\\npopulation with mean 10 and variance 4.\\n\\nExpected value of y[1] : E{y[1]} = 10, the mean of the population.\\nVariance of y[1] : Var{y[1]} = 4, the variance of the population. \\nExpected value of y[2] : E{y[2]} = 10, the mean of the population.\\nVariance of y[2] : Var{y[2]} = 4, the variance of the population. \\n\\nIf y[1] and y[2] are selected at random and independently of each other, \\nthere should be no covariance between them; Cov{y[1],y[2]} = 0. \\n\\n\\nSuppose, however, that we divide the original population into two groups \\n- putting all of the high values in group 1 and all of the low values in \\ngroup 2. Further, suppose that this grouping results in a variance among \\ngroup means of 3, and an average variance within each of the two groups \\nof 1; thus, Va = 3 and Vw = 1. \\n\\nNow, consider a probability experiment where sampling is restricted so\\nthat y[1] and y[2] must both come from the same group. We could choose\\ny[1] at random and then select y[2] at random from the same group from\\nwhich we selected y[1]. With this restriction, high values of y[1] will be\\nassociated with high values of y[2], and low values of y[1] will be\\nassociated with low values of y[2]. The two observations will have a\\npositive covariance! \\n\\n\\nIf m[1] is the mean of group 1 and m[2] is the mean of group 2, then, on \\naverage, whenever y[1] and y[2] are chosen from group 1, they will have \\nmeans of m[1]. Likewise, when they are both chosen from group 2, they \\nwill have means of m[2]. For this reason, their covariance will merely be \\nthe variance of groups means; Va = 3.\\n\\n\\nWith this particular type of probability experiment,\\nE{y[1]} = 10\\nVar{y[1]} = 4\\nE{y[2]} = 10\\nVar{y[2]} = 4\\nCov{y[1],y[2]} = 3\\n\\n\\nIn specifying models for statistical analysis, the data analyst must \\nspecify an expected value for every possible observation, a variance for \\nevery possible observation, and covariances between every pair of \\npossible observations.\\n\\n\\nEXAMPLES OF MODELS\\n\\nExample 1 : One-way analysis of variance (Completely Random Design)\\n\\nn[1] random observations, y[1j], from population 1 with mean 10 and \\nvariance 4.\\n n[2] random observations, y[2j], from population 2 with mean 15 and \\nvariance 6.\\nModel :\\nE{y[1j]} = 10\\nE{y[2j]} = 15\\nVar{y[1j]} = 4\\nVar{y[2j]} = 6\\nCov{y[1j],y[1j']} = 0; j not equal j'\\nCov{y[2j],y[2j']} = 0; j not equal j'\\nCov{y[1j],y[2j']} = 0; all j,j'\\n\\nNOTE : Usual models do not include heterogeneous errors among treatments.\\n\\n\\nExample 2 : Two-way analysis of variance (Randomized block design)\\n\\nr observations, y[1j], from population 1 with mean 10 and variance 4\\nr observations, y[2j], from population 2 with mean 15 and variance 4\\nobservations grouped by block; variance among blocks = 3\\nModel :\\nE{y[1j]} = 10\\nE{y[2j]} = 15\\nVar{y[ij]} = 4 for i = 1 or 2 and all j\\nCov{y[1j],y[2j']} = 3 for j = j'; 0 otherwise\\n\\nNOTE : Most textbooks consider block differences to be fixed so that \\nVar{y[ij]} is specified as variance within blocks (4 - 3 = 1); and all \\ncovariances are assumed to equal zero.\\n\\n\\nExample 3 : Simple linear regression\\none observation from population 1 with mean 2 + 2(0.7) and variance 5\\none observation from population 2 with mean 2 + 3(0.7) and variance 5\\none observation from population 3 with mean 2 + 5(0.7) and variance 5\\none observation from population 4 with mean 2 + 10(0.7) and variance 5\\nModel :\\nE{y[i]} = 2 + 0.7X where X = 2, 3, 5 or 10\\nVar{y[i]} = 5 for i = 1, 2, 3 or 4\\nCov{y[i],y[i']} = 0 for i not equal i'\\n\\n\\n\\nProceed to discussion of Mixed Model [Alt+M]\\nReturn to Table of contents [Alt+T]\\n\\n\\n\",\n",
       " ' telnet exited, press enter to close window.\\' read -r Waste_Var exit 0 This has a number of advantages, not the least of which being that I can pop up a \"telnet_window\" to anywhere. I don\\'t have to create links for each host (though I do create aliases for the most common hosts), and I can type \"telnet_window\" (or, e.g., \"tel_aix\") as a unix command. Also, if I lose the connection suddenly then the window stays around until I get a chance to see what happened. I use telnet instead of rsh because I generally connect to hosts which won\\'t accept rsh\\'s. (weiter mit 9.3 I can\\'t get my pictures in OmniWeb ) This document was converted from LaTeX using Karl Ewald \\'s latex2html . \\n\\t\\n\\n-->\\nThe NEXTSTEP/OpenStep FAQ -- 9.2 to 9.3\\n\\nThe NEXTSTEP/OpenStep FAQ\\n! to the table of contents\\n< to the previous section: \\n> to the next section: \\n9.2 Is there any way to change the text in the title bar of a terminal \\nwindow?\\n\\nThere is no way of changing the title bar of a Terminal.app \\nwindow in 2.x; in 3.x there is.  Check Preferences (Title Bar): \\nset CustomTitle, type in the title, and hit CR (or Set Window) \\nand voila!   \\n\\n[From: andre@ramsey.cs.laurentian.ca (Andre Roberge)]\\n\\nActually, there is a way to change the title bar of a Terminal \\nwindow in 2.x  (at least in 2.1 which is what I am using). It is \\nsomewhat limited but it might be useful to some.   \\n\\nThe trick is to make a symbolic link between  /bin/csh  (or whichever\\nshell one wishes to use) and a file in / named  \\n\"Whatever_you_want_to_appear_in_the_title_bar\".  Then select this new\\n\"shell\" in the terminal preference and, voila!, you\\'ll have\\nyour terminal window with  /Whatever_you..... in the title bar. \\n\\nYou can edit Stuart\\'s titlebar interactively from the\\n\"Window...\" Inspector (Command-3).  \\n\\nStuart provides emulation of certain Operating System Command (OSC)\\nsequences which can be used to modify the titlebar under subprocess\\ncontrol.\\n\\nStuart can change the title of the current window from the command \\nline. In Stuart is possible to get more descriptive titles by linking\\n/usr/ucb/rsh to /usr/hosts/. Then by adding /usr/hosts to\\nyour Stuart ShellPath you can then get the hostname into the title bar:      \\n\\n\\n\\n\\t$ dwrite StuartShellPaths :/usr/hosts\\n\\n\\n\\n\\t\\nYou should then type in the hostname as the shell to invoke (disable\\nthe \"Shell reads .login file\" for this. You can also add hosts to \\nyour .Stuartrc file:   \\n \\n\\n\\n\\n\\tShell=golem.ps.uci.edu\\n\\tSourceDotLogin=NO\\n\\tWinLocX=545\\n\\tWinLocY=563\\n\\tLines=24\\n\\t|\\n\\tWinLocX=76\\n\\tWinLocY=833\\n\\n\\n\\n\\nFor the localhost, link /bin/csh to /usr/hosts/, or even \\nbetter /usr/local/bin/tcsh instead of using rsh. \\n\\n[From: Garance A Drosehn ]\\n\\nFor what it\\'s worth, I do this with a script called\\n\"telnet_to\" and a (bash) function called\\n\"telnet_window\".  The function simply does a   \\n\\n\\n\\n\\tlocal soil_pars=\"-Lines 32 -Keypad YES -Reverse \\\\\\n\\tYES -Strict YES -TestExit YES\";\\n\\tsoil -Shell \"telnet_to $1\" $soil_pars\\n\\n\\n\\n\\nand the script is just:\\n\\n\\n\\n\\t#!bin/sh\\n\\t/usr/ucb/telnet $*\\n\\techo \\' \\'\\n\\techo \\'  -->   telnet exited, press enter to close window.\\'\\n\\tread -r Waste_Var\\n\\texit 0\\n\\n\\n\\n\\nThis has a number of advantages, not the least of which being that \\nI can pop up a \"telnet_window\" to anywhere.  I don\\'t have to create \\nlinks for each host (though I do create aliases for the most common \\nhosts), and I can type \"telnet_window\" (or, e.g., \"tel_aix\") as a unix \\ncommand. \\n\\nAlso, if I lose the connection suddenly then the window stays around\\nuntil I get a chance to see what happened.  I use telnet instead of \\nrsh because I generally connect to hosts which won\\'t accept rsh\\'s.    \\n\\n\\n(weiter mit 9.3 I can\\'t get my pictures in OmniWeb <2.0.)\\n\\nThis document was converted from LaTeX using Karl Ewald\\'s latex2html.\\n\\n',\n",
       " ' Examples Feature List Papers and Articles Model Editor The model editor is a graphical and textual interface, featuring intuitive drag-and-drop capabilities. The model is assembled using components from already built up component libraries. Read more... Screenshot of Model Editor Screenshot of Notebook textual interface --> AVI demo of user interface Simulation Center Simulations and results are presented in the simulation center. Here it is possible to trim the parameters and initial values of a model between simulations. Read more... Simulation Center Screenshot AVI of Visualizations The Bouncing Ball Example --> Documentation and Analysis Live documentation and mathematical analysis using Notebooks. Read more... Example Notebooks Documentation in Mathematica AVI of Documentation and Analysis --> Component Libraries Ready made or user defined component libraries. Read more... Component Libraries The Modelica Language Model Notebooks --> Communication Communicate and integrate with other software. --> Technologies Read here about the technologies used in MathModelica. Other links that might be of interest: MathModelica Examples · Feature List · Technical Documents · MathMore\\'s Products Questions or comments about the site? Please contact webmaster@mathcore.com . Thanks! \\n\\t\\n\\n-->\\n\\n\\n\\nWhat is MathModelica?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHome > Products > What is MathModelica?\\n\\nWhat is MathModelica?\\n\\n\\n\\n\\n\\n\\n\\n\\nMathModelica is a powerful engineering tool for virtual model building and \\nsimulation of all kinds of physical systems. MathModelica handles any kind of domain, \\nsuch as mechanics, electronics, hydraulics, control systems etc. This gives a \\nunique capability to model and simulate complex systems with high accuracy and \\nprecision, in an easy and intuitive computer environment. With its simplicity and \\nflexibility, it is a tool that supports updated and correct decision making in all stages \\nof a product\\'s life cycle.\\nGeneral Description\\nMathModelica is a modeling, analysis, and simulation environment for engineers \\nat companies that design complex products or systems of heterogeneous physical \\ncharacter. MathModelica is useful in a variety of fields such as the \\nautomotive and aircraft industries, robotics, and complex machinery. The tool \\nprovides a technology that makes it possible to \"lift\" modeling and simulation \\none or several levels closer to the real product than is possible with todays \\nsoftware tools. Virtual prototyping is a common term for parts of this \\nprocess but does sometimes only include the mechanical modeling part of the \\nwhole system. Full system simulation is a more accurate description of what \\nMathModelica provides.\\nIn MathModelica models are described using the Modelica language. The \\nModelica Association is the key to success in the \"lift\" since the focus of the \\nModelica effort has been on efficient multi-domain modeling and simulation from \\nthe very beginning in the language design. The term multi-domain (or \\nheterogeneous) means that subsystems or components from different physical \\napplication domains can be handled in the same model and by the same modeling \\ntool. A construction machine is one such example, containing mechanics, \\nhydraulics, electric servo motors, and one or several software-based regulators \\nor control systems. The development of products or systems of this character \\n(i.e., mixes of mechanics, hydraulics, electronics, and software) is  \\nsupported today by simulation tools with rather limited scope, which means that \\nmodels and simulations can usually only be carried out in subsystems within \\neach application area. The communication between designers and groups that \\ndevelop these different subsystems is usually done through specifications and \\ndiscussions during formal meetings. It is hard to verify that the given \\nspecifications correspond to a correct final product, and it is not unusual that \\nproblems are detected rather late in the development process when physical \\nprototypes are built and the different subsystems are integrated. Multi-domain \\nmodeling software overcomes these problems and makes it possible to integrate \\nthe work done by different design groups at a much earlier stage - \\ncollaborative engineering! The full system behavior can be tested and verified \\nvirtually by analyses and simulations much earlier and faster than before.\\nMathModelica is a suite of software including Mathematica, Visio, and the Dymola \\nkernel, which are very tightly integrated into a single engineering workbench \\nfor advanced modeling, simulation, analysis, and documentation. The user \\ninterface of MathModelica can be divided into The Model Editor and Notebooks. \\nThe Model Editor is an extension of Visio and is used for quick and easy \\ngraphical composition of models in a drag-n-drop fashion, using ready-made \\ncomponents from the Modelica Standard Library or other user defined component \\nlibraries. The Notebook interface provides excellent \"live\" documentation of \\nthe whole model and simulation process.Additionally it offers advanced scripting and pre- and \\npost processing capabilities. By integrating the powerful Modelica modeling and \\nsimulation technology with Mathematica, advanced modeling and simulation \\ncapabilities are merged seamlessly with a technical computing system complete with  \\nbuilt-in functions for pre- and post processing as well as advanced scripting \\ncapabilities.\\nThe process of translating the models described in Modelica into efficient, \\nexecutable simulation code (equation sorting, index reduction, symbolic \\nimplication, C-code generation, compilation, solver linking, etc) is handled by \\nthe Dymola kernel which is not directly visible to the user.\\nExamples and Documents\\nFind out more about MathModelica by examining examples and reading documents.\\n\\nExamples\\nFeature List\\nPapers and Articles\\nModel Editor\\nThe model editor is a graphical and textual interface, featuring intuitive drag-and-drop capabilities.\\nThe model is assembled using components from already built up component libraries. Read more...\\nScreenshot of Model Editor\\n\\nAVI demo of user interface\\nSimulation Center\\nSimulations and results are presented in the simulation center. Here it is \\npossible to trim the parameters and initial values of a model between simulations. \\nRead more... \\nSimulation Center Screenshot\\nAVI of Visualizations\\n\\nDocumentation and Analysis\\nLive documentation and mathematical analysis using Notebooks. Read more... \\nExample Notebooks\\n\\nComponent Libraries\\nReady made or user defined component libraries. Read more...\\nComponent Libraries\\nThe Modelica Language\\n\\n\\nTechnologies\\nRead here about the technologies used in MathModelica.\\n\\n\\nOther links that might be of interest:\\n\\nMathModelica Examples · \\nFeature List ·\\nTechnical Documents ·\\nMathMore\\'s Products\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nQuestions or comments about the site? Please contact webmaster@mathcore.com. Thanks!\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " \"\\n\\n\\n\\n\\n\\n1. Displays\\n\\n\\n\\n1. Displays\\n[\\xa0Home\\xa0] [\\xa0Up\\xa0]\\n\\xa0\\n\\n\\nGrouping\\n            Data and Frequency Tables\\n\\n\\nThe\\n                  number of classes depends on the number of numbers in your\\n                  data set\\nUnless you have a very large data set, it is\\n                  enough to define between 5 and 20 categories into which data are\\n                  to fall.\\xa0 In most cases use equally spaced categories\\n                  chosen so that each number in the data set will fall into one\\n                  and only one category.\\xa0 In deciding on the number of\\n                  categories and their boundaries, determine the number of\\n                  numbers in the dataset and the smallest and largest value in\\n                  the set of numbers.\\xa0 Once categories have been chosen\\n                  make a tally sheet by placing each number in its proper\\n                  category.A dataset consisting of\\n                  500 randomly selected Arizona State University Sophomores was\\n                  created by Weiss.\\xa0\\n                  This database contains several variables (which you can see by\\n                  opening the Webstat applet below).\\xa0 Among the scores are\\n                  SAT Math scores for the sampled students.\\xa0 You can see\\n                  them by opening the applet below. \\n\\n                  \\xa0 \\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\nFrequency\\n                  and Relative Frequency Tables\\nIn\\n                  constructing a frequency table for a single dataset you may\\n                  only want to keep track of the number of values in each group.\\xa0\\n                  In comparing information from two datasets, you will want to\\n                  make a relative frequency table where the relative frequency\\n                  for a group is equal to the frequency for the group divided by\\n                  the total frequency.\\xa0 The next table shows both the\\n                  frequency and relative frequency tables for the above SAT math\\n            scores.\\n            \\n\\xa0\\n\\n\\n\\nHistograms\\n            and Bar Graphs\\nFrequency tables and histograms are closely connected--histograms\\n  provide graphical representations of frequency tables.\\xa0 The next display\\n        shows the frequency and relative frequency histograms for the frequency\\n        and relative frequency tables just above.\\n\\n\\n\\nA histogram looks different as class widths are varied.\\xa0 When class\\n    widths are too small, the histogram will have too many bars, preventing you from\\n    recognizing patterns, while class widths that are too large will mask the general shape of\\n    the data set.\\n \\n\\nGo back to the Focus data shown above and make\\n            histograms for the SAT Math scores choosing several starting points and several interval\\n            widths.\\xa0 What statements can you make about the SAT Math scores\\n            based on the histograms that you have constructed.\\n\\n\\n\\n\\nStem and Leaf Plots\\xa0 \\nStem and Leaf Plots display information much like a histogram rotated\\n  through 90 degrees.\\xa0 In some cases individual data values that are lost\\n  when displaying the same information in a histogram are retained.\\xa0 Use\\n  the same Webstat applet shown above to make a stem and leaf plot of the SAT\\n  Math scores.\\xa0 What conclusions can you make from this?\\n  \\n\\n\\nBoxplots (Also called Box and Dot or Box and\\n              Whisker Plots)\\nYou will discuss quartiles and medians further in the next\\n    section but briefly the median is a number with the property that half the\\n    numbers are greater than or equal to it and half are less than or equal to\\n    it.\\xa0 The first quartile is a number with the property that 1/4 of the\\n    numbers are less than or equal to it and 3/4 are greater than or equal to\\n    it.\\xa0 The third quartile is the 'reverse' of the first\\n    quartile.\\xa0\\xa0\\nA boxplot provides a graphical display of the smallest number, largest number, median\\n    value, and 1st and 3rd quartiles for a set of numbers.\\xa0 You can see a boxplot\\n    of the SAT Math scores by using the Webstat applet above and selecting the boxplot choice\\n    under Graphics.\\xa0 The left vertical line is above the lowest SAT Mathscore\\n    in the dataset, the right vertical line is above the highest SAT Math score, the\\n    left edge of the box is above the first quartile, the right edge above the\\n    third quartile, and the white line inside the box is above the median SAT\\n    Math score.\\n\\n\\n\\xa0\\n\\n\",\n",
       " '\\n\\nStretching and Flexibility - Physiology of Stretching\\n\\nStretching and Flexibility - Physiology of Stretching\\n\\n\\nby Brad Appleton\\n<brad@bradapp.net>\\n\\n   http://www.bradapp.net/\\n\\n\\n\\n\\n\\nGo to the previous, next chapter.\\nPhysiology of Stretching\\n\\nFlexibility: (next chapter)\\nIntroduction: (previous chapter)\\n\\n\\nThe purpose of this chapter is to introduce you to some of the basic\\nphysiological concepts that come into play when a muscle is stretched.\\nConcepts will be introduced initially with a general overview and then\\n(for those who want to know the gory details) will be discussed in\\nfurther detail. If you aren\\'t all that interested in this aspect of\\nstretching, you can skip this chapter. Other sections will refer to\\nimportant concepts from this chapter and you can easily look them up on\\na \"need to know\" basis.\\n\\n\\nThe Musculoskeletal System\\nMuscle Composition\\nConnective Tissue\\nCooperating Muscle Groups\\nTypes of Muscle Contractions\\nWhat Happens When You Stretch\\n\\n\\nThe Musculoskeletal System\\n\\nMuscle Composition: (next section)\\nPhysiology of Stretching: (beginning of chapter)\\n\\n\\n\\n\\n\\n\\n\\nTogether, muscles and bones comprise what is called the\\nmusculoskeletal system of the body. The bones provide posture and\\nstructural support for the body and the muscles provide the body with\\nthe ability to move (by contracting, and thus generating tension). The\\nmusculoskeletal system also provides protection for the body\\'s internal\\norgans. In order to serve their function, bones must be joined together\\nby something. The point where bones connect to one another is called a\\njoint, and this connection is made mostly by ligaments\\n(along with the help of muscles). Muscles are attached to the bone by\\ntendons. Bones, tendons, and ligaments do not possess the ability\\n(as muscles do) to make your body move.  Muscles are very unique in this\\nrespect.\\n\\nMuscle Composition\\n\\nConnective Tissue: (next section)\\nThe Musculoskeletal System: (previous section)\\nPhysiology of Stretching: (beginning of chapter)\\n\\n\\nMuscles vary in shape and in size, and serve many different purposes.\\nMost large muscles, like the hamstrings and quadriceps, control motion.\\nOther muscles, like the heart, and the muscles of the inner ear, perform\\nother functions. At the microscopic level however, all muscles share the\\nsame basic structure.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAt the highest level, the (whole) muscle is composed of many strands of\\ntissue called fascicles. These are the strands of muscle that we\\nsee when we cut red meat or poultry. Each fascicle is composed of\\nfasciculi which are bundles of muscle fibers.  The muscle\\nfibers are in turn composed of tens of thousands of thread-like\\nmyofybrils, which can contract, relax, and elongate (lengthen).\\nThe myofybrils are (in turn) composed of up to millions of bands laid\\nend-to-end called sarcomeres. Each sarcomere is made of\\noverlapping thick and thin filaments called myofilaments.  The\\nthick and thin myofilaments are made up of contractile proteins,\\nprimarily actin and myosin.\\n\\n\\nHow Muscles Contract\\nFast and Slow Muscle Fibers\\n\\n\\nHow Muscles Contract\\n\\nFast and Slow Muscle Fibers: (next subsection)\\nMuscle Composition: (beginning of section)\\n\\n\\n\\nThe way in which all these various levels of the muscle operate is as\\nfollows: Nerves connect the spinal column to the muscle. The place where\\nthe nerve and muscle meet is called the neuromuscular junction.\\nWhen an electrical signal crosses the neuromuscular junction, it is\\ntransmitted deep inside the muscle fibers. Inside the muscle fibers, the\\nsignal stimulates the flow of calcium which causes the thick and\\nthin myofilaments to slide across one another. When this occurs, it\\ncauses the sarcomere to shorten, which generates force. When billions of\\nsarcomeres in the muscle shorten all at once it results in a contraction\\nof the entire muscle fiber.\\n\\nWhen a muscle fiber contracts, it contracts completely. There is no such\\nthing as a partially contracted muscle fiber. Muscle fibers are unable\\nto vary the intensity of their contraction relative to the load against\\nwhich they are acting. If this is so, then how does the force of a\\nmuscle contraction vary in strength from strong to weak?  What happens\\nis that more muscle fibers are recruited, as they are needed, to perform\\nthe job at hand. The more muscle fibers that are recruited by the\\ncentral nervous system, the stronger the force generated by the muscular\\ncontraction.\\n\\nFast and Slow Muscle Fibers\\n\\nHow Muscles Contract: (previous subsection)\\nMuscle Composition: (beginning of section)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe energy which produces the calcium flow in the muscle fibers comes from\\nmitochondria, the part of the muscle cell that converts glucose\\n(blood sugar) into energy. Different types of muscle fibers have\\ndifferent amounts of mitochondria. The more mitochondria in a muscle\\nfiber, the more energy it is able to produce. Muscle fibers are\\ncategorized into slow-twitch fibers and fast-twitch fibers.\\nSlow-twitch fibers (also called Type 1 muscle fibers) are slow to\\ncontract, but they are also very slow to fatigue.  Fast-twitch fibers\\nare very quick to contract and come in two varieties: Type 2A\\nmuscle fibers which fatigue at an intermediate rate, and Type 2B\\nmuscle fibers which fatigue very quickly.  The main reason the\\nslow-twitch fibers are slow to fatigue is that they contain more\\nmitochondria than fast-twitch fibers and hence are able to produce more\\nenergy. Slow-twitch fibers are also smaller in diameter than fast-twitch\\nfibers and have increased capillary blood flow around them. Because they\\nhave a smaller diameter and an increased blood flow, the slow-twitch\\nfibers are able to deliver more oxygen and remove more waste products\\nfrom the muscle fibers (which decreases their \"fatigability\").\\n\\nThese three muscle fiber types (Types 1, 2A, and 2B) are contained in\\nall muscles in varying amounts.  Muscles that need to be contracted much\\nof the time (like the heart) have a greater number of Type 1 (slow)\\nfibers. When a muscle first starts to contract, it is primarily Type 1\\nfibers that are initially activated, then Type 2A and Type 2B fibers\\nare activated (if needed) in that order. The fact that muscle fibers are\\nrecruited in this sequence is what provides the ability to execute\\nbrain commands with such fine-tuned tuned muscle responses. It also makes\\nthe Type 2B fibers difficult to train because they are not activated\\nuntil most of the Type 1 and Type 2A fibers have been recruited.\\n\\nHFLTA states that the the best way to remember the\\ndifference between muscles with predominantly slow-twitch fibers and\\nmuscles with predominantly fast-twitch fibers is to think of \"white\\nmeat\" and \"dark meat\". Dark meat is dark because it has a greater number\\nof slow-twitch muscle fibers and hence a greater number of mitochondria,\\nwhich are dark. White meat consists mostly of muscle fibers which are at\\nrest much of the time but are frequently called on to engage in brief\\nbouts of intense activity.  This muscle tissue can contract quickly but\\nis fast to fatigue and slow to recover.  White meat is lighter in color\\nthan dark meat because it contains fewer mitochondria.\\n\\nConnective Tissue\\n\\nCooperating Muscle Groups: (next section)\\nMuscle Composition: (previous section)\\nPhysiology of Stretching: (beginning of chapter)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLocated all around the muscle and its fibers are connective\\ntissues.  Connective tissue is composed of a base substance and two\\nkinds of protein based fiber. The two types of fiber are\\ncollagenous connective tissue and elastic connective tissue.\\nCollagenous connective tissue consists mostly of collagen (hence its\\nname) and provides tensile strength.  Elastic connective tissue consists\\nmostly of elastin and (as you might guess from its name) provides\\nelasticity. The base substance is called mucopolysaccharide and\\nacts as both a lubricant (allowing the fibers to easily slide over one\\nanother), and as a glue (holding the fibers of the tissue together into\\nbundles). The more elastic connective tissue there is around a joint,\\nthe greater the range of motion in that joint.  Connective tissues are\\nmade up of tendons, ligaments, and the fascial sheaths that envelop, or\\nbind down, muscles into separate groups.  These fascial sheaths, or\\nfascia, are named according to where they are located in the\\nmuscles:\\n\\n\\n\\n\\nendomysium\\nThe innermost fascial sheath that envelops individual muscle fibers.\\n\\n\\nperimysium\\nThe fascial sheath that binds groups of muscle fibers into individual\\nfasciculi (see section Muscle Composition).\\n\\n\\nepimysium\\nThe outermost fascial sheath that binds entire fascicles (see section Muscle Composition).\\n\\n\\n\\nThese connective tissues help provide suppleness and tone to the\\nmuscles.\\n\\nCooperating Muscle Groups\\n\\nTypes of Muscle Contractions: (next section)\\nConnective Tissue: (previous section)\\nPhysiology of Stretching: (beginning of chapter)\\n\\nWhen muscles cause a limb to move through the joint\\'s range of motion,\\nthey usually act in the following cooperating groups:\\n\\n\\n\\n\\n\\nagonists\\nThese muscles cause the movement to occur. They create the normal range\\nof movement in a joint by contracting.  Agonists are also referred to as\\nprime movers since they are the muscles that are primarily\\nresponsible for generating the movement.\\n\\n\\nantagonists\\nThese muscles act in opposition to the movement generated by the\\nagonists and are responsible for returning a limb to its initial\\nposition.\\n\\n\\n\\nsynergists\\nThese muscles perform, or assist in performing, the same set of joint\\nmotion as the agonists. Synergists are sometimes referred to as\\nneutralizers because they help cancel out, or neutralize, extra\\nmotion from the agonists to make sure that the force generated works\\nwithin the desired plane of motion.\\n\\n\\n\\nfixators\\nThese muscles provide the necessary support to assist in holding the\\nrest of the body in place while the movement occurs.  Fixators are also\\nsometimes called stabilizers.\\n\\n\\n\\nAs an example, when you flex your knee, your hamstring contracts, and,\\nto some extent, so does your gastrocnemius (calf) and lower buttocks.\\nMeanwhile, your quadriceps are inhibited (relaxed and lengthened\\nsomewhat) so as not to resist the flexion (see section Reciprocal Inhibition).  In this example, the hamstring serves as the agonist, or\\nprime mover; the quadricep serves as the antagonist; and the calf and\\nlower buttocks serve as the synergists.  Agonists and antagonists are\\nusually located on opposite sides of the affected joint (like your\\nhamstrings and quadriceps, or your triceps and biceps), while synergists\\nare usually located on the same side of the joint near the agonists.\\nLarger muscles often call upon their smaller neighbors to function as\\nsynergists.\\n\\nThe following is a list of commonly used agonist/antagonist muscle\\npairs:\\n\\n\\n\\npectorals/latissimus dorsi (pecs and lats)\\n\\nanterior deltoids/posterior deltoids (front and back shoulder)\\n\\ntrapezius/deltoids (traps and delts)\\n\\nabdominals/spinal erectors (abs and lower-back)\\n\\nleft and right external obliques (sides)\\n\\nquadriceps/hamstrings (quads and hams)\\n\\nshins/calves\\n\\nbiceps/triceps\\n\\nforearm flexors/extensors\\n\\n\\nTypes of Muscle Contractions\\n\\nWhat Happens When You Stretch: (next section)\\nCooperating Muscle Groups: (previous section)\\nPhysiology of Stretching: (beginning of chapter)\\n\\n\\nThe contraction of a muscle does not necessarily imply that the muscle\\nshortens; it only means that tension has been generated.  Muscles can\\ncontract in the following ways:\\n\\n\\n\\nisometric contraction\\n\\nThis is a contraction in which no movement takes place, because the load\\non the muscle exceeds the tension generated by the contracting muscle.\\nThis occurs when a muscle attempts to push or pull an immovable object.\\n\\nisotonic contraction\\n\\nThis is a contraction in which movement does take place, because\\nthe tension generated by the contracting muscle exceeds the load on the\\nmuscle. This occurs when you use your muscles to successfully push or\\npull an object.\\n\\nIsotonic contractions are further divided into two types:\\n\\n\\nconcentric contraction\\n\\nThis is a contraction in which the muscle decreases in length (shortens)\\nagainst an opposing load, such as lifting a weight up.\\n\\neccentric contraction\\n\\nThis is a contraction in which the muscle increases in length\\n(lengthens) as it resists a load, such as lowering a weight down\\nin a slow, controlled fashion.\\n\\n\\nDuring a concentric contraction, the muscles that are shortening\\nserve as the agonists and hence do all of the work.  During an\\neccentric contraction the muscles that are lengthening serve as\\nthe agonists (and do all of the work). See section Cooperating Muscle Groups.\\n\\n\\n\\nWhat Happens When You Stretch\\n\\nTypes of Muscle Contractions: (previous section)\\nPhysiology of Stretching: (beginning of chapter)\\n\\n\\nThe stretching of a muscle fiber begins with the sarcomere\\n(see section Muscle Composition), the basic unit of contraction in the\\nmuscle fiber.  As the sarcomere contracts, the area of overlap between\\nthe thick and thin myofilaments increases.  As it stretches, this area\\nof overlap decreases, allowing the muscle fiber to elongate.  Once the\\nmuscle fiber is at its maximum resting length (all the sarcomeres are\\nfully stretched), additional stretching places force on the surrounding\\nconnective tissue (see section Connective Tissue). As the tension increases,\\nthe collagen fibers in the connective tissue align themselves along the\\nsame line of force as the tension. Hence when you stretch, the muscle\\nfiber is pulled out to its full length sarcomere by sarcomere, and then\\nthe connective tissue takes up the remaining slack. When this occurs, it\\nhelps to realign any disorganized fibers in the direction of the\\ntension. This realignment is what helps to rehabilitate scarred tissue\\nback to health.\\n\\nWhen a muscle is stretched, some of its fibers lengthen, but other\\nfibers may remain at rest. The current length of the entire muscle\\ndepends upon the number of stretched fibers (similar to the way that\\nthe total strength of a contracting muscle depends on the number of\\nrecruited fibers contracting). According to SynerStretch you\\nshould think of \"little pockets of fibers distributed throughout the\\nmuscle body stretching, and other fibers simply going along for the\\nride\". The more fibers that are stretched, the greater the length\\ndeveloped by the stretched muscle.\\n\\n\\nProprioceptors\\nThe Stretch Reflex\\nThe Lengthening Reaction\\nReciprocal Inhibition\\n\\n\\nProprioceptors\\n\\nThe Stretch Reflex: (next subsection)\\nWhat Happens When You Stretch: (beginning of section)\\n\\n\\n\\n\\n\\n\\nThe nerve endings that relay all the information about the musculoskeletal\\nsystem to the central nervous system are called proprioceptors.\\nProprioceptors (also called mechanoreceptors) are the source of all\\nproprioception: the perception of one\\'s own body position and\\nmovement. The proprioceptors detect any changes in physical displacement\\n(movement or position) and any changes in tension, or force, within the\\nbody. They are found in all nerve endings of the joints, muscles, and\\ntendons. The proprioceptors related to stretching are located in the\\ntendons and in the muscle fibers.\\n\\n\\n\\n\\n\\n\\n\\n\\nThere are two kinds of muscle fibers: intrafusal muscle fibers and\\nextrafusal muscle fibers. Extrafusil fibers are the ones that\\ncontain myofibrils (see section Muscle Composition) and are what is usually\\nmeant when we talk about muscle fibers. Intrafusal fibers are also\\ncalled muscle spindles and lie parallel to the extrafusal fibers.\\nMuscle spindles, or stretch receptors, are the primary\\nproprioceptors in the muscle. Another proprioceptor that comes into play\\nduring stretching is located in the tendon near the end of the muscle\\nfiber and is called the golgi tendon organ. A third type of\\nproprioceptor, called a pacinian corpuscle, is located close to\\nthe golgi tendon organ and is responsible for detecting changes in\\nmovement and pressure within the body.\\n\\nWhen the extrafusal fibers of a muscle lengthen, so do the intrafusal\\nfibers (muscle spindles). The muscle spindle contains two different\\ntypes of fibers (or stretch receptors) which are sensitive to the change\\nin muscle length and the rate of change in muscle length.  When muscles\\ncontract it places tension on the tendons where the golgi tendon organ\\nis located. The golgi tendon organ is sensitive to the change in tension\\nand the rate of change of the tension.\\n\\nThe Stretch Reflex\\n\\nThe Lengthening Reaction: (next subsection)\\nProprioceptors: (previous subsection)\\nWhat Happens When You Stretch: (beginning of section)\\n\\n\\n\\n\\n\\n\\nWhen the muscle is stretched, so is the muscle spindle\\n(see section Proprioceptors). The muscle spindle records the change in\\nlength (and how fast) and sends signals to the spine which convey this\\ninformation.  This triggers the stretch reflex (also called the\\nmyotatic reflex) which attempts to resist the change in muscle\\nlength by causing the stretched muscle to contract.  The more sudden the\\nchange in muscle length, the stronger the muscle contractions will be\\n(plyometric, or \"jump\", training is based on this fact). This basic\\nfunction of the muscle spindle helps to maintain muscle tone and to\\nprotect the body from injury.\\n\\nOne of the reasons for holding a stretch for a prolonged period of time\\nis that as you hold the muscle in a stretched position, the muscle\\nspindle habituates (becomes accustomed to the new length) and reduces\\nits signaling.  Gradually, you can train your stretch receptors to allow\\ngreater lengthening of the muscles.\\n\\nSome sources suggest that with extensive training, the stretch\\nreflex of certain muscles can be controlled so that there is little\\nor no reflex contraction in response to a sudden stretch. While\\nthis type of control provides the opportunity for the greatest\\ngains in flexibility, it also provides the greatest risk of injury\\nif used improperly. Only consummate professional athletes and\\ndancers at the top of their sport (or art) are believed to actually\\npossess this level of muscular control.\\n\\n\\nComponents of the Stretch Reflex\\n\\n\\nComponents of the Stretch Reflex\\n\\nThe Stretch Reflex: (beginning of subsection)\\n\\n\\n\\n\\n\\n\\nThe stretch reflex has both a dynamic component and a static component.\\nThe static component of the stretch reflex persists as long as the\\nmuscle is being stretched.  The dynamic component of the stretch reflex\\n(which can be very powerful) lasts for only a moment and is in response\\nto the initial sudden increase in muscle length.  The reason that the\\nstretch reflex has two components is because there are actually two\\nkinds of intrafusal muscle fibers: nuclear chain fibers, which are\\nresponsible for the static component; and nuclear bag fibers,\\nwhich are responsible for the dynamic component.\\n\\nNuclear chain fibers are long and thin, and lengthen steadily when\\nstretched. When these fibers are stretched, the stretch reflex nerves\\nincrease their firing rates (signaling) as their length steadily\\nincreases. This is the static component of the stretch reflex.\\n\\nNuclear bag fibers bulge out at the middle, where they are the most\\nelastic.  The stretch-sensing nerve ending for these fibers is wrapped\\naround this middle area, which lengthens rapidly when the fiber is\\nstretched.  The outer-middle areas, in contrast, act like they are\\nfilled with viscous fluid; they resist fast stretching, then gradually\\nextend under prolonged tension.  So, when a fast stretch is demanded of\\nthese fibers, the middle takes most of the stretch at first; then, as\\nthe outer-middle parts extend, the middle can shorten somewhat.  So the\\nnerve that senses stretching in these fibers fires rapidly with the\\nonset of a fast stretch, then slows as the middle section of the fiber\\nis allowed to shorten again.  This is the dynamic component of the\\nstretch reflex: a strong signal to contract at the onset of a rapid\\nincrease in muscle length, followed by slightly \"higher than normal\"\\nsignaling which gradually decreases as the rate of change of the muscle\\nlength decreases.\\n\\nThe Lengthening Reaction\\n\\nReciprocal Inhibition: (next subsection)\\nThe Stretch Reflex: (previous subsection)\\nWhat Happens When You Stretch: (beginning of section)\\n\\n\\n\\n\\n\\n\\n\\nWhen muscles contract (possibly due to the stretch reflex), they produce\\ntension at the point where the muscle is connected to the tendon, where\\nthe golgi tendon organ is located. The golgi tendon organ records the\\nchange in tension, and the rate of change of the tension, and sends\\nsignals to the spine to convey this information (see section Proprioceptors).\\nWhen this tension exceeds a certain threshold, it triggers the\\nlengthening reaction which inhibits the muscles from contracting\\nand causes them to relax.  Other names for this reflex are the\\ninverse myotatic reflex, autogenic inhibition, and the\\nclasped-knife reflex.  This basic function of the golgi tendon\\norgan helps to protect the muscles, tendons, and ligaments from injury.\\nThe lengthening reaction is possible only because the signaling of golgi\\ntendon organ to the spinal cord is powerful enough to overcome the\\nsignaling of the muscle spindles telling the muscle to contract.\\n\\nAnother reason for holding a stretch for a prolonged period of time is\\nto allow this lengthening reaction to occur, thus helping the stretched\\nmuscles to relax. It is easier to stretch, or lengthen, a muscle when it\\nis not trying to contract.\\n\\nReciprocal Inhibition\\n\\nThe Lengthening Reaction: (previous subsection)\\nWhat Happens When You Stretch: (beginning of section)\\n\\n\\n\\n\\n\\nWhen an agonist contracts, in order to cause the desired motion, it\\nusually forces the antagonists to relax (see section Cooperating Muscle Groups). This phenomenon is called reciprocal inhibition because\\nthe antagonists are inhibited from contracting. This is sometimes called\\nreciprocal innervation but that term is really a misnomer since it\\nis the agonists which inhibit (relax) the antagonists. The antagonists\\ndo not actually innervate (cause the contraction of) the agonists.\\n\\nSuch inhibition of the antagonistic muscles is not necessarily required.\\nIn fact, co-contraction can occur. When you perform a sit-up, one would\\nnormally assume that the stomach muscles inhibit the contraction of the\\nmuscles in the lumbar, or lower, region of the back. In this particular\\ninstance however, the back muscles (spinal erectors) also contract. This\\nis one reason why sit-ups are good for strengthening the back as well as\\nthe stomach.\\n\\nWhen stretching, it is easier to stretch a muscle that is relaxed than\\nto stretch a muscle that is contracting.  By taking advantage of the\\nsituations when reciprocal inhibition does occur, you can get a\\nmore effective stretch by inducing the antagonists to relax during the\\nstretch due to the contraction of the agonists.  You also want to relax\\nany muscles used as synergists by the muscle you are trying to stretch.\\nFor example, when you stretch your calf, you want to contract the shin\\nmuscles (the antagonists of the calf) by flexing your foot. However, the\\nhamstrings use the calf as a synergist so you want to also relax the\\nhamstrings by contracting the quadricep (i.e., keeping your leg\\nstraight).\\n\\nGo to the previous, next chapter.\\n\\n\\n\\n\\n\\nBrad Appleton\\n<brad@bradapp.net>\\n\\n   http://www.bradapp.net/\\n\\n\\n',\n",
       " '\\n\\n\\n Components for Integrating Heuristic and Model-Based Diagnosis\\n\\n\\nComponents for Integrating Heuristic and Model-Based Diagnosis\\n\\nKent Andersson\\nComputing Science Department, Uppsala University,\\nBox 311, S-751 05 UPPSALA, SWEDEN.\\nE-mail: Kent.Andersson@csd.uu.se\\n\\n\\n PDF-version of the paper with better formatting. \\nABSTRACT\\nA set of components for the integration of heuristic and model-based diagnosis are identified by example. By introducing a metalogic representation structure with a metatheory and an object theory, an integration of the components is achieved that separates the model-based and the heuristic components while allowing them to be used together. The object theory of the system consists of the model-based components whereas the metatheory consists of (1) heuristic components, (2) components of knowledge compiled from principles of the domain, but not represented for reasons of complexity, and (3) a strategy component for computing diagnoses.\\n\\n1. INTRODUCTION\\nDiagnosis is a complex problem solving task with different kinds of knowledge that interact. In diagnosis we find several kinds of knowledge--principled domain knowledge, heuristic knowledge, control knowledge, diagnosis strategy knowledge and knowledge about the user\\x92s answers. The early diagnosis systems, such as MYCIN in (Davis, Buchanan and Shortliffe, 1977), used heuristic knowledge for making diagnoses. The heuristics mainly consisted of empirical associations from symptoms to causes, where the use of domain specific knowledge was the key to the success of these systems, but also one of their main weaknesses. The representation of the heuristics included both domain and control knowledge--a mix that became unmanageable when the knowledge base became large and necessary to revise (Carrico, Girard and Jones, 1989). Another problem with the heuristic approach is that it can be difficult to elicit the empirical knowledge that is needed for the rules (Gruber, 1989; Hayes-Roth, Waterman and Lenat, 1983; Hart, 1988). In response to the knowledge elicitation problem, a new type of diagnosis system evolved that did not include empirical knowledge about symptoms and diagnoses, but relied on principled knowledge of the domain. A diagnosis system of this type is called model-based, because it has a model of the diagnosis object, which describes the object (see, for example, de Kleer and B.C. Williams, 1987; Console, Dupré and Torasso, 1989). A model-based diagnosis system applies a general diagnosis algorithm to its model to find a diagnosis.\\nOriginally, a model-based system did not use heuristic knowledge about the domain to guide the diagnosis, because the knowledge in the model was principled and therefore not to be mixed with heuristics. As a consequence, the general diagnosis algorithm did not take advantage of the short-cuts that heuristic knowledge can provide. A diagnosis system that integrates model-based and heuristic diagnosis knowledge can take advantage of both kinds of knowledge. \\nIn this paper we present an approach to represent heuristic and compiled knowledge integrated with model-based knowledge. The purpose is to identify and exemplify components for representing problem solving knowledge such that its logical structure is preserved, it is represented in a modular fashion allowing parts to be replaced or modified without affecting other, the representation is transparent enough for the knowledge engineer to distinguish different kinds and the representation allows them to be used together. A metalogic representation structure is proposed in which we separate the different kinds of knowledge in order to preserve its logical structure and to make it possible to revise and edit. \\n\\n2. DIAGNOSIS IN A METALOGIC SYSTEM\\nA metatheory is a theory for the study of another theory. That other theory is called the object theory because it is our object of study. The study is usually carried out in an informal manner. However, there is nothing that stops us from formalizing it as a formal metatheory. With a formal metatheory we can represent knowledge in the metatheory and relate this knowledge to the object theory. Since the world of a metatheory is made up of formulas of an object theory it is possible to relate one formula to another and to define when one follows from the other (or from a set of formulas.) We can also handle different object theories and modifications of object theories as terms in the metatheory. This capability gives us a possibility to represent various kinds of reasoning as declarative relationships in the metatheory. \\n\\nThe model-based approach to diagnosis of a device is based on a comparison of an (incorrect) device and a representation of an ideal (correct) device. This approach requires a separation of the knowledge about each device. \\nFig. 1 \\n  illustrates how knowledge about the device that we would like to diagnose, and about an ideal device can be separated and related to each other as two logic theories. The arrows in \\nFig. 1 \\nindicate where the knowledge for a theory comes from. In the metatheory hypotheses and conclusions about the system are defined in terms of the theorems of OT and knowledge about the device. The metatheory also formalizes a diagnostic strategy which studies the hypotheses and conclusions of MT in the diagnosis of the device. Knowledge about the real world, e.g. measurements, is represented in MT.\\n\\n\\n\\n\\n\\n\\n\\nFig 1\\n\\n\\n: \\n\\n\\nThe structure of the diagnosis system.\\n\\n\\n\\n\\n\\n\\n3. MODEL-BASED COMPONENTS AS AN OBJECT THEORY\\n\\n\\n\\nIn this section we investigate the definition of an object theory for a diagnosis domain. The kind of domain the theory is designed to capture is a system of a set of devices connected with connectors, through which some kind of directed flow is transported. The devices display various observable behaviours and measurable flows. The system is controlled with various settings of the devices. As an example we use a stereo system with a set of devices connected with cables. A domain with a similar structure could be a monitoring and control system for a machine on an assembly line in industry. We do not intend to capture other types of domains with this theory.\\n\\n\\n\\n\\xa0\\n\\n\\n\\nAn ordinary stereo system consists of an amplifier, some source devices, two loudspeakers, cables for music signals and for power supply. A person can use the stereo for reproducing music from different source devices, so the system can be used in different \\nmodes\\n, e.g. the turntable mode. The user controls the system by adjusting controls on the stereo devices, so depending on the user\\x92s intention on what mode to use the system in, the user makes different \\nsettings\\n of the devices. A central part of a stereo system is the amplifier to which all other devices are connected. \\n\\n\\n\\n\\n\\n3.1 Overview of the Object Theory\\n\\n\\n\\nThe theory consists of two parts: (1) definitions of the structure of a system and (2) definitions of the function of a system (in diagnosis terminology: the model of correct behaviour). The structure definitions fall into two categories: (a) the objects of the system (the set of components) and (b) how the objects are connected to each other (the structure model). Examples of the first category are stereo devices and signal cables. An example of the second category is a connection between an amplifier and a cable for a loudspeaker, which consists of four simple connections--two for the loudspeaker cable and two for the amplifier. The main definition of function, \\noutput\\n, builds on two definitions: \\ninput\\n and \\ntransformation\\n. \\n\\n\\n\\n\\n\\n3.2 Structure\\n\\n\\n\\nThe structure part of the theory falls into two parts--the objects and the configuration of the objects. We consider three types of objects: devices, connectors and external objects. Devices are the objects of a system that perform some function: e.g. a CD-player. Connectors connect the objects of a system and transport some flow: e.g. a music cable connected to the CD-player transports the source signals. External objects are not really part of the system but provide some essential service: e.g. a mains power outlet provides electricity. The objects can be represented as lists. For example, the devices \\nX\\n is a list where each element is a pair \\n[Type, Identity]\\n. This representation will help us to formulate compact heuristics in the metatheory. Since \\nType\\n is a part of the object, a heuristic can be formulated in terms of the type of object, for example a loudspeaker type. For our stereo system example, we represent the objects of the system as shown below.\\n\\n\\n\\n\\xa0\\n \\ndevices(X) <-\\n\\n \\n\\tX = [ [amplifier,am],[tuner,tu1],[cassette_deck,ca1],\\n\\n \\n\\t\\t \\t[turntable,tt1],[cd_player,cd1], \\n\\n \\n\\t\\t\\t[loudspeaker,sp1],[loudspeaker,sp2]]\\n\\n \\n\\xa0\\n\\n\\n\\nWe will now define the subparts for each type of object, to know what the objects consist of. The knowledge of how the objects are constructed is separate from the definitions of the objects. We can easily add objects to the theory by extending a list of objects. For example, we can add a second CD-player \\ncd2\\n to the theory by adding it to the list of \\ndevices\\n, without again specifying the subparts of a CD-player. To add a new type of object, however, we need to extend the definitions of function to cover the new type. Below we find the definition of subparts for the amplifier. In this definition \\nS\\n are the subparts of a stereo object \\nO\\n. Each subpart in this definition has the same structure--a pair\\n [Type, Identity]\\n. For example, \\n[mu_port, in(cd1/r,N)]\\n is a music port with the identity \\nin(cd1/r,N)\\n, which indicates the right channel CD1 input port on an object identified by \\nN\\n. Using the identity, \\nN\\n, of the object as part of the identity of the subparts gives us some generality in the representation, since the same definition holds for all objects of a particular type, e.g., for all CD-players.\\n\\n\\n\\n\\xa0\\n \\nsubparts(O,S) <-\\n \\nO = [amplifier,N] &\\n \\nS = [[mu_port,out(rec/r,N)],[mu_port,out(rec/l,N)],[mu_port,in(pb/r,N)],\\n \\n\\t[mu_port,in(pb/l,N)],[mu_port,in(tu/r,N)],[mu_port,in(tu/l,N)],\\n \\n\\t[mu_port,in(tt/r,N)],[mu_port,in(tt/l,N)],[mu_port,in(cd1/r,N)],\\n \\n\\t[mu_port,in(cd1/l,N)],[mu_port,in(cd2/r,N)],[mu_port,in(cd2/l,N)],\\n \\n\\t[mu_port,in(aux/r,N)],[mu_port,in(aux/l,N)], [sp_port,out(a/l/pl,N)],\\n \\n\\t[sp_port,out(a/l/mi,N)],[sp_port,out(a/r/pl,N)],[sp_port,out(a/r/mi,N)],\\n \\n\\t[sp_port,out(b/l/pl,N)],[sp_port,out(b/l/mi,N)],[sp_port,out(b/r/pl,N)],\\n \\n\\t[sp_port,out(b/r/mi,N)],[power,inv(220,N)],[light,on(N)],\\n \\n\\t[button,volume(N)], [button,speaker(N)],[button,on(N)],[button,tuner(N)],\\n \\n\\t[button,cassette_deck(N)],[button,turntable(N)],\\n \\n\\t[button,cd_player(cd1,N)],[button,cd_player(cd2,N)]]\\n\\n\\n\\n\\xa0\\n\\n\\n\\nThe objects can be connected in different ways--different system configurations. For example, the loudspeakers in a stereo system may be connected in different ways to the amplifier. We represent this in \\nconfiguration\\n as a relation between a system \\nSy\\n and a configuration \\nC\\n of devices, connectors and external objects. A configuration is a list where each element is a structure:\\n\\n\\n\\n\\xa0\\n \\n\\tObject1 -- Connections1 -- Connector -- Connections2 -- Object2\\n\\n\\n\\n\\xa0\\n\\n\\n\\nsuch that \\nObject1\\n and \\nObject2 \\nare connected via \\nConnector\\n in the system.\\n\\n\\n\\n\\xa0\\n \\nconfiguration(SY,C) <-\\n\\t SY = s1 & \\n \\nC = [ [cd_player,cd1] --\\n \\n\\t\\t\\t[ [mu_port,out(cd/rl,cd1)] -- [pin,out(plmi,w1)] ] --\\n \\n\\t[music_cable(cd_player),w1] --\\n \\n\\t\\t\\t[ [pin,in(pl,w1)] -- [mu_port,in(cd/r,am)],\\n \\n\\t\\t\\t [pin,in(mi,w1)] -- [mu_port,in(cd/l,am)] ] --\\n \\n\\t[amplifier,am], \\t... ] \\n \\n\\xa0\\n\\n\\n\\nThe ellipsis (...) indicates that only a part of the configuration relation is given. \\n\\n\\n\\n\\n\\n3.3 \\n\\nFunction\\n\\n\\n\\nThe second part of the object theory is a formalization of the function of a system. \\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\nOutput\\n To represent the function of a system we define a relation \\noutput\\n between an object \\nX\\n, a system use \\nU\\n and a list \\nOut\\n of output signals and behaviours. \\nU\\n is a list \\n[Sy,M,S]\\n, where \\nSy\\n is the system configuration, \\nS \\nis a list of settings for the system and \\nM\\n is a mode of use. The relation is \\n\\n\\n\\ntrue when there is a list of inputs \\nI\\n (defined in the theory) to \\nX\\n that can be transformed to a list \\nOut\\n of outputs.\\n\\n\\n\\n\\xa0\\n \\noutput(X,U,Out) <-\\n \\n\\tU = [Sy,M,S] & object(X) &\\n \\n\\tinput(X,U,I) & transformation(I,X,U,Out) \\n \\n\\xa0\\n\\n\\n\\n\\n\\n\\n\\n\\nInput\\n The input signals to an object depend on the configuration of the system and the settings of the objects. For example, the cable connections of a stereo system determine what inputs the stereo devices receive; the settings of an amplifier determine what inputs a loudspeaker receives. We can represent this as a relation \\ninput\\n between an object \\nX\\n, a system use \\nU\\n and a list \\nI\\n of inputs. We state a clause for each type of object in the definition. \\n \\n\\xa0\\n \\ninput(X,U,I) <-\\n \\n\\tU = [Sy,M,S] &\\n \\n\\tX = [loudspeaker,N] &\\n \\n\\tconfiguration_unit(St, X2--Conns1--Y--Conns2--X) &\\n \\n\\toutput(Y,U,Out) & \\n \\n\\tpropagate(Out,Conns2,I)\\n \\n\\xa0\\n\\n\\n\\nThe settings of a device affect its function. We capture this in the definition of \\ninput\\n where the system use \\nU\\n involves a list \\nS\\n of settings for the system. However, we can represent a set of default settings, to be used if settings needed to determine the function of a device are not given in \\nS\\n. These are stated in the relation \\nstandard_settings\\n. So we state that \\nS1\\n is a setting, either if it is in a list \\nS\\n of settings, or if it is a standard setting for the object \\nX\\n in mode \\nM\\n.\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\nTransformation\\n As defined above, we represent an output as an input that can be \\ntransformed\\n. Transformation of input signals to output signals and behaviours can be represented as a relation \\ntransformation\\n between a list \\nI\\n of inputs, an object \\nX\\n, a system use \\nU\\n, and a list \\nOut\\n of output signals and behaviours. We will state a clause for each type of object that has its own kind of transformation rules for its input signals. The \\nout_signals\\n relation will state transformation rules for subparts of objects. For example, the transformation rules for an amplifier are stated over its output speaker ports. The setting of the speaker button determines which set of speaker ports (A or B) will have output signals. The predicate \\nout_signals\\n in turn specifies the transformation rules for these ports. \\n \\n\\xa0\\n \\ntransformation(I,X,U,Out) <-\\n \\n\\tU=[Sy,M,S] &\\n \\n\\tX = [amplifier,N] & \\n \\n\\tsetting(setting(Sy,X,M,[button,speaker(N)],Sp), X, M, S) &\\n \\n\\tspeaker_ports(P,X,Sp) & \\n \\n\\tout_signals(P,X,I,U,Out1) & \\n \\n\\tOut = [behaviour(Sy,X,M,[light,on(N)],on) | Out1] \\n \\n\\xa0\\n\\n\\n\\nNote that transformation only specifies a rule, \\nout_signal\\n, for the transformation of the input signals to an object--it does not specify the internal function of the object in detail. \\n\\n\\n\\n\\n\\n4. COMPONENTS OF A METATHEORY\\n\\n\\n\\nIn this section we investigate the prospects of giving a representation of a diagnosis strategy that is separated from the knowledge of a particular domain. We define a theory for the diagnosis strategy and for heuristic and compiled knowledge. We will refer to this theory as the \\nmetatheory\\n, and the theory for the diagnosis domain as the \\nobject theory\\n. The object theory contains domain knowledge (the structure and function of the diagnosis domain), whereas the metatheory contains knowledge about the diagnosis strategy. By using two separate theories we aim to achieve our goals for the representation of diagnosis knowledge: (1) logical structure: the two-level structure of the knowledge can be retained when domain and diagnosis knowledge are separated into two theories that have a two-level relationship (object -\\xa0meta), (2) modularity: we can modify the object theory if the diagnosis domain changes without modifying the metatheory, or we can alter the metatheory for a new diagnosis strategy without changing the object theory, and (3) transparency: with two theories we do not have to mix knowledge of different kinds, instead we can represent the different kinds clearly apart, promoting transparency of knowledge. In the next sections we will discuss the components of a metatheory for integrating model-based diagnosis, starting with observations.\\n\\n\\n\\n\\n\\n4.1 Observations\\n\\n\\n\\nSome observations made by the user apply only to a specific mode of use of a system, whereas other observations apply to all modes. For example, in a stereo system let us assume that a CD-player is faulty and causes the loudspeaker\\x92s sound to deteriorate. An observation that a loudspeaker sounds bad would then apply to the CD-player mode but not to the turntable mode, whereas an observation that two devices are connected would apply equally well to both modes. We thus need to indicate, for some of the observations, in which mode the observation is made by the user. A system is controlled by setting controls of the devices, therefore settings are interesting observations, for example in a stereo system.\\n\\n\\n\\n\\xa0\\n\\n\\n\\nSome observations require some kind of instrument to measure a value, such as the voltage of a device (a signal), whereas some observations can be made directly by the senses, for example that a light is on or off (a behaviour). This difference between kinds of observations can be important when the diagnosis system decides which questions to ask the user. In consequence, we should specialize observations with respect to the mode of use for the diagnosis domain, and also specialize observations with respect to how the parameter can be observed. Therefore we distinguish four kinds of observations--observations of connections, settings, signals and behaviour. \\n\\n\\n\\n\\xa0\\n\\n\\n\\nBelow we define four kinds of observation terms to represent the four kinds of observations that we want to distinguish. These observation terms should be seen as a basic set of terms for the representation of user reports about a diagnosis domain, satisfactory for our example. We choose to handle the observations in the metatheory because they represent what is known about the diagnosis domain in the real world--we do not include them in the object theory which represents the ideal world.\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\nDefinition 1\\n\\n\\n An \\nobservation \\nis a term that has one of the following forms:\\n \\n\\tsignal(System, Object, Mode, Type, Value)\\n\\n \\n\\tbehaviour(System, Object, Mode, Type, Value)\\n\\n \\n\\tsetting(System, Object, Mode, Type, Value)\\n\\n \\n\\tconn(System, Obj1, Obj2)\\n\\n\\n\\n\\nwhere \\nSystem\\n represents a system configuration, \\nObject \\nan object in the diagnosis domain, \\nMode \\nthe mode of use for the diagnosis domain, \\nType \\na type specification of \\nValue\\n, which is a simple value. \\nObj1\\n and \\nObj2\\n represent connected objects in the domain. \\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\nExample\\n: The user might observe, first, that the loudspeaker \\nsp1\\n sounds bad when the stereo system is used in CD-player mode and, second, that the plus input port on loudspeaker \\nsp1\\n has a value of 10 watts. These two facts are individually represented as two metatheory terms\\n\\n\\n\\n\\nbehaviour(s1, [loudspeaker, sp1], cd_player, sound, bad)\\n\\n\\n\\n\\n\\nsignal(s1, [loudspeaker, sp1], cd_player, [sp_port, in(pl,sp1)], 10 W).\\n\\n\\n\\n\\n\\n\\n4.2 Hypotheses\\n\\n\\n\\nWe can view a hypothesis as pointing out a problem area of the diagnosis domain. A problem area identifies an incorrect property, or class of properties, of an object, or class of objects, in the diagnosis domain. The settings of a device and the internal function of the loudspeakers are examples of hypotheses pointing out problem areas. \\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\nAbnormal Observations\\n The starting point for a diagnosis is a report, from the user, of a fault in the diagnosis domain. Since we have a theory of the diagnosis domain in the form of an object theory, it is convenient to define faults relative the object theory. Everything that can be deduced from an object theory representing the ideal world for a particular diagnosis domain, can be considered as correct (for that theory). Consonant with this, statements that contradict what can be deduced from the object theory, can be seen as faults (for the domain that the theory formalizes). We take this view as a starting point to classify the observation terms of Definition 2 in the metatheory. \\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\nDefinition 2\\n A \\nnormal observation\\n\\nO\\n for an object theory \\nT\\n, is an observation term that corresponds to a theorem of the predicates \\noutput\\n or \\nconfiguration\\n of \\nT\\n. \\n\\n\\n\\nAn \\nabnormal observation \\n\\nO\\n for an object theory\\n T\\n, is an observation term and does not correspond to a theorem of the predicates \\noutput\\n or \\nconfiguration\\n of \\nT\\n.\\n\\n\\n\\n\\xa0\\n\\n\\n\\nFollowing Definition 1 and Definition 2 we can give logic programs for \\nnormal\\n and \\nabnormal\\n as relations between a theory and an observation term. Below we give the first clause for each program, which cover the observation term \\nsignal\\n. Each clause covers one kind of the observation terms \\nsignal\\n, \\nbehaviour\\n, \\nsetting\\n and \\nconn\\n.\\n \\n\\xa0\\n \\nnormal(O, T) <-\\n\\n \\n\\tO = signal(System, Object, Mode, Type, Value1) & \\n\\n \\n\\tdemo(T, output(Object, [System,Mode,Settings], Out) ) &\\n\\n \\n\\tmember( signal(System, Object, Mode, Type, Value2), Out) &\\n\\n \\n\\tdemo(T, equal(Value1, Value2, Type) )\\n\\n \\n\\n\\xa0\\n \\nabnormal(O, T) <-\\n\\n \\n\\tO = signal(System, Object, Mode, Type, Value1) &\\n\\n \\n\\tdemo(T, output(Object, [System,Mode,Settings], Out) ) &\\n\\n \\n\\tnot ( member( signal(System, Object, Mode, Type, Value2), Out) &\\n\\n \\n\\t      demo(T, equal(Value1, Value2, Type) )    )\\n\\n \\n\\xa0\\n\\n\\n\\nIn the second conjunct, \\ndemo\\n, of these programs, \\noutput(Object, [System, Mode, Settings], Out)\\n, is a partial name (containing metavariables) for the object theory formula\\n\\n\\n\\n\\nexists Object\\n\\n\\n\\nexists System\\n\\n\\n\\nexists Mode\\n\\n\\n\\nexists Settings\\n\\n\\n\\nexists Out(output(Object,[System,Mode,Settings], Out) )\\n\\n.\\n\\n\\n\\n\\n\\xa0\\n\\n\\n\\nThe definitions of \\nnormal\\n and \\nabnormal\\n derive their observation terms from an object theory. We need a demo predicate to relate a theory and a partial name for an object level formula (a non-ground metalevel term). The predicate is true when the formula named by the partial name can be derived from the theory. The demo predicate was originally proposed by Bowen and Kowalski (1982) as a representation of the derivability relation in an object language, part of an amalgamation of the object language and the metalanguage. \\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\nThe Signal Graph\\n We can take advantage of the object theory when we generate hypotheses for a fault (an abnormal observation) reported by the user. The formalization tells us which objects influence the output observed by the user, so by studying the object theory, hypotheses for the fault can be generated. We therefore analyze the object theory to see what could have caused the fault. To aid the analysis we use the concept of signal path. This can be understood as a sequence of objects that a signal traverses in order to construct an output from an object in the diagnosis domain, such as a sound from a loudspeaker. It is possible to view these paths as a directed acyclic graph since the signals are going in a certain direction, and the signals do not form any cycles. Each combination of object and mode in the diagnosis domain thus defines a particular \\nsignal graph\\n. For example, the output of the loudspeaker port on the amplifier in cd-player mode has a graph that is different from the output of the on-light on the cd-player, which both are different from the graph for the output of the loudspeaker port on the amplifier when it is used in the turntable mode. Therefore, there can be many possible signal graphs for one diagnosis domain. Since the signal graph tells us what objects are involved for each output, it is a convenient tool in the diagnosis of faulty outputs. Note that the signal graph is not represented explicitly in the object theory--it is constructed in the metatheory by analyzing sentences of the object theory. This construction is made during the diagnosis. It would be possible to \\x91pre-compile\\x92 the signal graphs of a given object theory before the diagnosis, but any modification of the object theory during the diagnosis affecting the signal paths would then necessitate the system to reconstruct the signal graphs during diagnosis.\\n\\n\\n\\n\\xa0\\n\\n\\n\\nThe signal graph is a directed acyclic graph (DAG) where the nodes correspond to objects and the arcs correspond to signal flows of the diagnosis domain. In this DAG the root is the device that displays the fault reported by the user, and the leaves are the devices that do not have any input signals. As an example, consider the graph for \\nloudspeaker1\\n when a stereo system is used in cd-player mode, in Fig. 2. \\n\\n\\n\\n\\n\\n\\n\\nFig 2\\n: \\nA signal graph\\n\\n\\n\\n\\n\\xa0\\n\\n\\n\\nThe signal graph for an output is constructed by a metalevel analysis of the object theory. We have a meta-interpreter \\nsolve_construct\\n for the analysis of an object theory. A standard meta-interpreter can be specialized to construct the signal graph, by using knowledge about the structure of the object theory. The knowledge needed is that the signal paths in the object theory are defined in terms of the \\ninput\\n and the \\noutput\\n predicates, so the meta-interpreter has special cases for the meta-interpretation of these predicates. \\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\nThe Generation of Hypotheses\\n\\n\\n The set of hypotheses for each type of fault is defined in the metatheory. The hypotheses are generated in relation to an object theory, so if the object theory were to be modified in restricted aspects, for example, in what objects there are, or what their connections are, there would be no need to alter the program for the generation of hypotheses. However, the program would have to be altered if the object theory is replaced with an object theory for another type of diagnosis domain. This is because the domains may differ in what the suitable hypotheses are for the objects in the domain. For example, diagnosis domains that consist of objects connected with some type of directed flow between them are quite different from domains that have a continuous process, such as a blast furnace in which iron is made. Consequently, the set of suitable hypotheses are not the same for different types of domains.\\n\\n\\n\\n\\xa0\\n\\n\\n\\nThe next step of the diagnosis process after the construction of the signal graph, is to generate the hypotheses for the fault reported by the user. Since the graph tells us which objects take part in the construction of a particular output, only the nodes of the DAG can be responsible for the abnormal observation. Therefore, the only hypotheses that are necessary to construct are those that can be based on the signal graph.The predicate \\nhypotheses\\n below is a relation between an object theory, an abnormal observation and a list of hypotheses. \\n \\n\\xa0\\n \\nhypotheses(T, O, G, H) <-\\n\\n \\n\\tabnormal(O, T) &\\n\\n \\n\\tsignal_graph(T, O, G) &\\n\\n \\n\\tobjects(G, Xs) &\\n\\n \\n\\thave_settings(T, Xs, SXs) &\\n\\n \\n\\tconnections_in_graph(G, Cs) &\\n\\n \\n\\tsystem(O, Sy) &\\n\\n \\n\\tmode(O, M) & \\n\\n \\n\\tH1= [ intern(Xs, Sy, M), settings(SXs, Sy, M), conns(Sy, Cs) ] &\\n\\n \\n\\tmake_individual(H1, H)\\n\\n \\n\\xa0\\n\\n\\n\\nThis program states that the hypotheses for an observation \\nO \\nof an object theory \\nT\\n are \\nH\\n, determined by a graph \\nG\\n, if the following holds: \\nO \\nis an abnormal observation for \\nT\\n, \\nG\\n is the signal graph for \\nO \\nin \\nT\\n, \\nXs\\n are names for the objects in the signal graph, \\nSXs\\n are names for the objects having settings (as determined by \\nT)\\n, \\nCs\\n are names for the connections between the objects in the graph, \\nSy\\n is the name for the configuration of the diagnosis domain involved in the observation \\nO\\n, and \\nM\\n is the name for the mode of use of the diagnosis domain. The hypotheses will then consist of the following three groups of hypotheses: (1) internal error in an object in mode \\nM\\n, (2) a setting of an object that is incorrect in mode \\nM\\n, and (3) an incorrect connection between objects in the configuration \\nSy\\n.\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\n\\nObject Level Refutation\\n Since the object theory is a model of the diagnosis domain the theory is a prime source of knowledge that can be used in the investigation of the hypotheses to locate the fault more precisely. Let us therefore distinguish two methods for investigating hypotheses, based on whether or not they rely on the model of the diagnosis domain. The first uses the model of the diagnosis domain to investigate the hypotheses. This method can be called \\nobject level refutation\\n. It compares statements of the object theory regarding structure and behaviour of the diagnosis domain with the user\\x92s observations of the real world. If the observations confirm the statements, then the hypothesis is refuted--the diagnosis domain is correct with respect to the area identified by the hypothesis. The statements of the object theory must of course be relevant for the hypothesis. For example, the hypothesis that a device has an internal fault could be refuted if all statements about outputs from it are confirmed by user observations of the actual device. The method of object level refutation will give correct diagnoses provided the object theory is an accurate representation of a non-faulty domain. If the real world observations confirm the theory statements, then the hypothesis cannot identify the cause of the malfunction. \\n\\n\\n\\n\\xa0\\n\\n\\n\\nThe second method can be called \\nmetalevel refutation\\n because hypotheses are eliminated on the grounds of knowledge formalized in the metatheory. In its basic form it does not use the model of the diagnosis domain to investigate the hypotheses. The metatheory is based on diagnosis experience of the diagnosis domain, knowledge that is not formalized in the object theory either because it is heuristic or because it would require a more detailed formalization than is expressed in the object theory. The first case illustrates that this method might not always be correct. Therefore, it would be desirable to distinguish between hypotheses that have been eliminated by object level refutation from those that have been eliminated by metalevel refutation. \\n\\n\\n\\n\\xa0\\n\\n\\n\\nObject level refutation tests a hypothesis by deriving statements in the object theory and asking the user if the statements can be observed in the malfunctioning diagnosis domain. The relation between a hypothesis and the relevant statements can be formalized as a metatheory program, \\nrefutation\\n, between an object theory \\nT\\n, a list \\nL\\n of names of statements of \\nT\\n and a hypothesis, such that the statements derivable from \\nT\\n is a refutation of the hypothesis.\\n \\n\\xa0\\n \\nrefutation(T, L, intern(X, Sy, M)) <-\\n\\n \\n\\tdemo(T, output(X, [Sy, M, S], Out) ) & \\n\\n \\n\\tL = Out\\n\\n \\nrefutation(T, L, settings(X, Sy, M)) <-\\n\\n \\n\\tdemo(T, standard_settings(X, M, S) ) &\\n\\n \\n\\tL = S\\n\\n \\nrefutation(T, L, conns(Sy, X, Y)) <-\\n\\n \\n\\tdemo(T, configuration_unit(Sy, X1--UV1--Y--UV2--X2) ) & \\n\\n \\n\\t( X = X1 & L = UV1\\n\\n \\n\\tor X = X2 & L = UV2 )\\n\\n \\n\\xa0\\n\\n\\n\\nLet us now investigate the use of \\nrefutation\\n for a particular hypothesis. Assume that we try to refute a hypothesis about an internal fault in a loudspeaker, so that we have the query \\nrefutation(stereo, L, intern(loudspeaker1, s1, cd_player)\\n ) which would result in the following call to \\ndemo: demo(stereo, output(loudspeaker1, [s1,cd_player,S], Out) ) \\nwhere \\noutput(loudspeaker1, [s1,cd_player,S], Out) \\nis a partial name of the object theory formula: \\n\\n$S $Out(output(loudspeaker1,[s1,cd_player,S], Out) ).\\n\\n\\n\\n\\n\\xa0\\n\\n\\n\\nThe result of the \\nrefutation\\n query is that \\nL\\n will be a list of names of statements derived from the object theory predicate \\noutput\\n.\\n\\n\\n\\n\\nL = [ \\tsignal(s1, loudspeaker1, cd_player, volume, 79 dB), behaviour(s1, loudspeaker1, cd_player, sound, good)\\t\\t\\t\\t\\t  ]\\n\\n \\n\\xa0\\n\\n\\n\\nWe assume in the object theory certain standard settings if none is specified in the query to the theory, for instance, the volume control has a standard setting that allows us to derive the value of 79 dB above. This example illustrates how we relate a hypothesis to object theory statements. The hypothesis \\nintern(X,Sy,M)\\n points out the problem area \"internal function of an object \\nX\\n of a system \\nS\\ny in mode \\nM\\n  \" and the object theory definition of \\noutput\\n formalizes the output signals and observable behaviours of the object \\nX\\n, which depend on the internal function of the object. Therefore, it can be used for deriving statements from the object theory to test the hypothesis. \\n\\n\\n\\n\\xa0\\n\\n\\n\\nThe second part of the refutation method is to verify if the statements of the object theory can be observed by the user. If they are then the hypothesis is refutable, i.e. it does not identify the fault and can thus be removed from consideration. We must of course decide how many statements to require for a refutation, the degree of verification by the user and what interval of values can be accepted for an observation. These decisions are mainly domain and application dependent. The method of object level refutation can be represented in the metatheory as the following program.\\n \\n\\xa0\\n \\nobject_refute(T, Obs, Hyp) <-\\n\\n \\n\\trefutation(T, L, Hyp) & \\n\\n \\n\\tobservable(L, Obs)\\n\\n \\n\\xa0\\n\\n\\n\\nThe program \\nobject_refute\\n is a relation between a theory \\nT\\n, a list of user observations \\nObs\\n, and a hypothesis \\nHyp\\n. The hypothesis is refutable when: (1) there is a list \\nL\\n of names of statements derivable in the object theory \\nT\\n, and (2) the user answers \\nObs\\n determine that the statements of \\nL\\n can be observed by the user. \\n\\n\\n\\n\\n\\n4.3 \\n\\nMetalevel Refutation\\n\\n\\n\\nIn this section we will discuss the second method for the refutation of hypotheses. This method uses primarily knowledge that is not present in the object theory which represents the diagnosis domain, but rather knowledge based on experience. Knowledge acquired from problem solving in a particular domain made the first generation expert systems powerful problem solvers in some domains, where it was possible to collect enough quality knowledge to achieve expert performance. Knowledge was usually represented in a form compiled from knowledge of various kinds, for example, structural, causal, functional, associational and experiential knowledge, into a compact form for a particular domain. This knowledge expressed associations between symptoms and conclusions that could be very effective for problem solving. As discussed in the introduction to this paper the use of compiled knowledge is not without problems. One of them is to know what knowledge to modify if the structure or function of the domain changes, since the knowledge is based on the configuration of a particular domain and it is not always easy to trace such knowledge back to the structure of the domain. Both the acquisition and modification of knowledge thus requires experts in the domain, which makes the development and maintenance of these systems expensive. Therefore, model-based systems with an explicit representation of a model for a domain (e.g. a structural and functional model) have been developed as an alternative to the first generation expert systems. However, since compiled knowledge can be useful in diagnosis systems because of its compactness and effectiveness, it is valuable to integrate the use of such knowledge with the use of model-based knowledge. In (Chandrasekaran, 1991) it is stated that \"almost all nontrivial human diagnostic reasoning uses a combination of methods\". Chandrasekaran refers to the combination of compiled knowledge with various types of model based knowledge, and gives three examples of combination. The first is accessing a model selectively when compiled knowledge for evaluating a hypothesis is missing; the second is falling back on a model when some of the data cannot be satisfactorily explained or when a better explanation can be obtained with the model; the third is to use the model to find an explanation to support the diagnosis arrived by compiled knowledge. In the following we will discuss our approach for integration, where we will combine compiled knowledge with model-based knowledge in two ways; first by falling back on our model when the hypotheses cannot be ruled out with compiled knowledge; second to focus the reasoning on the most likely part of the model. We integrate compiled knowledge with the model by defining compiled knowledge\\n in terms\\n of the model and by using both types in the diagnosis method.\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\nTwo Kinds of Compiled Knowledge \\nWe will distinguish two kinds of compiled knowledge that we would like to integrate with model-based knowledge. The first kind is compiled knowledge of a heuristic character, i.e., problem solving knowledge that is often true but not always. An example of heuristic knowledge is the following rule: \"If a loudspeaker of the stereo system does not work and the settings of the amplifier have been checked, then the connections between the devices are faulty.\" This rule is clearly not always true, but it is a useful heuristic for finding a diagnosis. It compiles knowledge of structural, functional and experiential character in a compact form. Knowledge about the connections of the stereo system (structural knowledge), about the functionality of the loudspeakers, and experiential knowledge from diagnosing stereo systems is compiled into a single rule. We include heuristics in the metatheory as a guidance for the use of the model-based knowledge in the object theory.\\n\\n\\n\\n\\xa0\\n\\n\\n\\nThe second kind of compiled knowledge has a complexity that surpasses the level of abstraction of the domain theory. Such knowledge is inconvenient to represent explicitly in the object theory. The reason is that an explicit representation would force the level of abstraction lower than that chosen by the designer. In the design of a domain theory you must choose a level of detail that is not too difficult and expensive to represent and has an acceptable computational complexity. This issue is an instance of the general problem of granularity level of representation as discussed by Genesereth and Nilsson (1987). Also Console and Torasso (1988) discuss the problem of constructing a complete model. They argue that any model will contain abstractions, therefore it is not complete in the sense that it does not formalize the domain completely. An example of the choice of granularity level is the following complex compiled knowledge in the stereo system: \"If a loudspeaker of the stereo system produces some sound, then the power-supply cannot be faulty.\" The knowledge is always correct because we know that a loudspeaker requires that at least some device has electricity for a loudspeaker to function (we disregard the possibility of a reduced mains power in our example). The mechanism of how devices depend on the power supply in the stereo system, which is behind this knowledge, is not represented in our object theory. It would be possible to represent it as model-based knowledge in the theory for the diagnosis domain, but that would increase the complexity of the theory further than that chosen in our example. If we represented the power-supply knowledge in the object theory, we would not be able to deduce enough new interesting things for the diagnosis to outweigh the increased complexity of the representation. Both kinds of compiled knowledge--heuristic and complex--can be represented in the metatheory of the diagnosis system. \\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\n\\nA Representation of Complex Compiled Knowledge \\nWe can integrate complex compiled knowledge with model-based knowledge through the methods of refutation. Model-based knowledge is used in object level refutation to refute hypotheses, whereas in metalevel refutation compiled knowledge can be used to refute hypotheses. Both methods would discuss the same kind of objects (hypotheses), but base their reasoning on different kinds of knowledge, which gives us an integration of compiled and model-based knowledge in the refutation process. Object level refutation bases the refutation on names for object theory formulas which guarantee that the conclusions are correct because the formulas are expressed in terms of the structure and function of the domain and derived in the object theory. Metalevel refutation bases the refutation on compiled knowledge so we can go directly from observations to conclusions without reference to the object theory. \\n\\n\\n\\nLet us discuss three examples of a possible representation of compiled knowledge suitable for metalevel refutation. In the first example we will see how we can represent compiled knowledge as a relation between the observations \\nObs\\n of the user and a hypothesis. Consider the following example: The rule \"If a loudspeaker of the stereo system produces some sound, then the power-supply cannot be faulty\", would be represented as the following clause.\\n \\n\\xa0\\n \\nmeta_refute(Obs, intern([power, X], Sy, M) ) <-\\n\\n \\n\\tobservable(O, Obs) & \\n\\n \\n\\tO = behaviour(Sy, [loudspeaker, X1], M2, volume, Z dB) &\\n\\n \\n\\tZ > 0\\n\\n \\n\\xa0\\n\\n\\n\\nThis rule means that all hypotheses of the form \\nintern([power, X], M)\\n are refutable when (1) there is an observation \\nbehaviour(Sy, [loudspeaker, X1], M2, volume, Z dB) \\nwhich says the user has observed that there is a volume of \\nZ\\n dB from a loudspeaker, and (2)\\n Z\\n is higher than \\n0\\n. When the above clause is true all hypotheses concerning internal faults in the power supply are refutable, so if the system has two hypotheses about internal faults in the power supply, this rule would refute both hypotheses. \\n\\n\\n\\n\\xa0\\n\\n\\n\\n In our approach we can generalize compiled knowledge by stating it relative an object theory so that the knowledge is general over some class of object theories for different systems. By generalizing compiled knowledge, our metatheory can be kept more compact since one general rule can replace more numerous specific rules. As the second example let us therefore extend the relation \\nmeta_refute\\n to include an object theory, so that we can represent a general rule regarding hypotheses about the power supply. A rule such as \"If an object of the stereo system produces some behaviour or signal that is \\nnormal\\n for the stereo system, then the power-supply, the cable between that device and the power supply, as well as the connections between the device and the cable, are OK This can be represented as below, where the refutable hypotheses are represented as a list.\\n \\n\\xa0\\n \\nmeta_refute(T, Obs, [ intern([power, X], Sy, M), \\n\\n \\n\\t \\xa0\\xa0(conns(Sy, Y1,[power,X]), conns(Sy,X2,Y1), intern(Y1,Sy, M2) ) ] ) <-\\n\\n \\n\\tobservable(O, Obs) & \\n\\n \\n\\tO = Otype(Sy, X2, M2, Ty, V) &\\n\\n \\n\\tmember(Otype, [signal, behaviour]) & \\n\\n \\n\\tnormal(O, T) \\n\\n \\n\\xa0\\n\\n\\n\\nWhen this clause is true all hypotheses \\nintern([power, X], Sy, M)\\n and all combinations of the hypotheses \\nconns(Sy,Y1,[power,X])\\n, \\nconns(Sy,X2,Y1) \\nand \\nintern(Y1,Sy, M2)\\n are refutable. The idea behind this compiled knowledge is that any signal or behaviour in the stereo system domain requires electricity, therefore it is possible to rule out the power supply, the cable and the connections as the problem. We have generalized the first clause by making the second clause relative an object theory, so if the object theory is modified (e.g., new devices) the second clause would not have to be modified. Thus the second clause is less sensitive than the first to changes in the domain, but it is still a powerful compilation of knowledge. Although the clause is general it still contains compiled knowledge since there is no explicit representation of how the power supply affects the function of devices--this is still implicit, compiled knowledge. Furthermore, this general rule replaces specific rules for each kind of normal output of the stereo system. Thus the metalogic approach helps us to keep our representation compact.\\n\\n\\n\\n\\xa0\\n\\n\\n\\nA third, more advanced, example of how complex compiled knowledge can be formalized as metalevel reasoning will discuss different object theories. This would make it possible to write programs that reason hypothetically over distinct object theories. If we have a relation \\nmodify_theory\\n between a theory \\nT1\\n, a set \\nObs\\n of observations and a theory \\nT2\\n where \\nT2\\n is the result of modifying \\nT1\\n such that the observations in \\nObs\\n are statements of \\nT2\\n, we can reason with different object theories. An example would be causal reasoning about which things affect what in a domain. For instance, if two observations \\nO1\\n and \\nO2\\n are abnormal for a theory \\nT1\\n, but \\nO2\\n is normal for \\nT1\\n modified for \\nO1\\n, then we could draw the conclusion that \\nO2\\n is a result of \\nO1\\n (i.e.,\\n O1\\n explains \\nO2\\n), and therefore exclude the possibility of an internal fault in the device that \\nO2\\n mentions. That is, \\nO2\\n is a consequence of \\nO1\\n and not a proper fault. We could represent this reasoning as the following two programs, starting with \\nexplains\\n:\\n \\n\\xa0\\n \\nexplains(O1, O2, T) <-\\n\\n \\n\\tabnormal(O1, T) & \\n\\n \\n\\tO1 = signal(Sy,X,M,Ty,V) &\\n\\n \\n\\tabnormal(O2, T) &\\n\\n \\n\\tmodify_theory(T, [out_signal(O1,I)], T2) &\\n\\n \\n\\tnormal(O2, T2)\\n\\n \\n\\xa0\\n\\n\\n\\nThe second program for this example is \\nmeta_refute\\n: Recall that the signal graph describes the signal paths for the initial abnormal observation reported by the user and the direction of the signals in the stereo system. The root of the signal graph is thus the object that displays the abnormality observed by user. We can take advantage of this to determine which hypotheses can be ruled out as consequences of the real fault. This program states that the case where there are two observations \\nO1\\n and \\nO2\\n, \\nO1\\n is a signal of an object \\nX1\\n and \\nO1\\n explains \\nO2\\n (\\nO2\\n would be normal for a theory modified for \\nO1\\n), implies that \\nO1\\n causes \\nO2\\n, thus all objects following \\nX1\\n in the signal graph are actually OK and in consequence the \\nintern\\n hypotheses about these objects are refutable.\\n \\n\\xa0\\n \\nmeta_refute(T, Obs, G,  H) <-\\n\\n \\n\\tobservable(O1, Obs) &  \\n\\n \\n\\tO1 = signal(Sy, X1, M, Ty, V) &\\n\\n \\n\\troot(X2, G) &\\n\\n \\n\\tobject(X2, O2) &\\n\\n \\n\\tobservable(O2, Obs) &   O1  O2 & \\n\\n \\n\\texplains(O1, O2, T) &\\n\\n \\n\\tobjects_after(X, X1, G) & \\n\\n \\n\\tmake_individual( [intern(X, Sy, M) ], H) \\n\\n \\n\\n\\xa0\\n\\n\\n\\nIn the above program we see that metalevel reasoning where an object theory is regarded as a term in the language gives us an approach for writing declarative and powerful reasoning programs. It is modular since the program is general over different theories, so it works even if we replace the theory \\nT\\n for another theory \\nT\\x92 \\nwith a different set of stereo devices or some similar technical system. So, despite the fact that we represent complicated compiled knowledge we have the means to preserve a certain generality of the knowledge. \\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\n\\nA Representation of Heuristic Compiled Knowledge \\nAs previously discussed, heuristic compiled knowledge should not be allowed to give us conclusions--it should only guide the diagnosis process. For meta refutation, complex compiled knowledge is represented as a set of clauses in the relation \\nmeta_refute\\n. We would like to represent heuristic compiled knowledge in a similar fashion because the kind of knowledge to be expressed is rather similar to complex compiled knowledge, the difference being that heuristics can only be taken as recommendations. We can represent heuristic compiled knowledge as a separate relation, \\nprobably_refutable\\n.\\n\\n\\n\\n\\xa0\\n\\n\\n\\nThe relation \\nprobably_refutable\\n relates a hypothesis \\nH\\n, a theory \\nT\\n, the user observations \\nObs\\n and the hypotheses \\nTHs\\n that have been tested in the diagnosis. The relation is true when the hypothesis \\nH\\n can be recommended for refutation. Recall our example of heuristic knowledge: \"If a loudspeaker of the stereo system does not work and the settings of the amplifier have been checked, then the connections between the devices are faulty.\" Here, the knowledge clearly refers to the progress of the diagnosis process. By including the tested and refutable hypotheses \\nTHs\\n in the relation \\nprobably_refutable\\n we can refer to the progress of the diagnosis. This example can be represented as a clause of the relation \\nprobably_refutable\\n as illustrated below.\\n \\n\\xa0\\n \\nprobably_refutable(T, Obs, THs, [ (conns(Sy, X2, Y1), conns(Sy,Y1,X1) ) ] ) <-\\n\\n \\n\\tobservable(O, Obs) &\\n\\n \\n\\tobject(X2, O)  &\\n\\n \\n\\ttype(X2, loudspeaker) & \\n\\n \\n\\tabnormal(O, T) &\\n\\n \\n\\ttype(X1, amplifier) &\\n\\n \\n\\tmember( settings(X1,Sy,M), THs)\\n\\n \\n\\xa0\\n\\n\\n\\nThis clause states that the hypotheses \\nconns(Sy, X2, Y1)\\n and \\nconns(Sy,Y1,X1)\\n can be recommended for (object) refutation when there is an observation \\nO\\n in the user observations \\nObs\\n, and the object of that observation is a loudspeaker, and the observation \\nO\\n is abnormal for the theory \\nT\\n, and the hypothesis regarding the settings of the amplifier is tested (refuted). \\n\\n\\n\\n\\xa0\\n\\n\\n\\nWith a set of clauses such as this one we can get a recommendation of which hypotheses to test. Each hypothesis in the list of hypotheses generated by the program \\nhypotheses\\n could be tested for the property of \\nprobably_refutable\\n as an indication of its potential for refutation.\\n\\n\\n\\n\\n\\n4.4 A Representation of a Diagnosis Strategy \\n\\n\\n\\nWe shall discuss a definition of diagnosis and look at an example diagnosis strategy.\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\nA Definition of Diagnosis \\nWe define diagnosis as a relation in the metatheory between an object theory \\nT\\n, a set of user observations \\nObs\\n and a hypothesis \\nH\\n. The relationship between these objects is that \\nObs\\n contains an observation, abnormal for \\nT\\n and related to an area of the domain identified by \\nH\\n. Informally, \\nH\\n is a diagnosis for the abnormal observations in \\nObs\\n, in the domain formalized in \\nT\\n. Below we find the definition of \\ndiagnosis\\n.\\n\\n\\n\\n\\nThe relation \\ndiagnosis\\n is true when we have an object theory \\nT\\n, a set of user observations \\nObs\\n, and a hypothesis \\nH\\n, for which the following holds: There is an observation \\nO\\n in \\nObs\\n that is abnormal with respect to \\nT\\n, the result of generating hypotheses from \\nT\\n, \\nO\\n and the signal graph \\nG\\n are \\nHs\\n, and \\nH\\n is a hypothesis in \\nHs\\n which is neither refutable in the metatheory, nor with the object theory \\nT\\n. We now take the definition of \\ndiagnosis\\n above as a starting point for the design of a strategy. There is not only one unique strategy that can find a diagnosis as defined in \\ndiagnosis\\n. However, we look at one example strategy that meets this definition.\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\nA Diagnosis Strategy\\n The basic idea for this strategy is to ask the user for an object theory and an observation of some faulty behaviour (or signal), and generate the hypotheses using the model. The strategy then tries to refute (eliminate) each hypothesis until one is found that cannot be. This hypothesis will then be presented as the answer of the diagnosis. In more detail; after generating the hypotheses the strategy proceeds as follows. The relation \\nprobably_refutable\\n of heuristics is used to sort the hypotheses in the order of likelihood. A simple selection criterion is then used to pick hypotheses for testing--e.g. the first hypothesis. The refutation process then first tries meta refutation, because it might use some non-heuristic compiled knowledge that gives us a short-cut in the diagnosis. If meta refutation does not eliminate the hypothesis, object refutation will be tried. If \\nobject_refute\\n is true for the hypothesis it is moved from the list of active to the list of tested hypotheses \\nTHs\\n and the rest are tried. On the other hand, if \\nobject_refute\\n is not true, the hypothesis and the user\\x92s observations are presented as a diagnosis for the object theory, because the hypothesis identifies an area of the domain that does not show the same behaviour as asserted by the object theory. \\n\\n\\n\\n\\xa0\\n\\n\\n\\nIn this strategy we have no specification of how and when questions of observations should be put to the user. A simple solution would be to ask all that are prompted by \\nmeta_refut\\ne and \\nobject_refute\\n, as well as by \\nprobably_refutable\\n, i.e. when an observation is required in e.g. \\nmeta_refute\\n a question would always be asked about the observation, as long as it had not been asked before. To avoid a potentially unfocused questioning of the user if we have many \\nmeta_refute\\n clauses, an alternative solution would be to ask only those prompted by\\n object_refute\\n and let the answers be the only way to get information from the user. A third solution would be to ask questions prompted by \\nobject_refute\\n, but also investigate the potential of the clauses of \\nmeta_refute \\nto asses the usefulness of questions put to the user. Such investigation could estimate the number of hypotheses that could be ruled out by a positive answer to a particular question, by comparing the list of hypotheses with the hypotheses in each clause. The utility of questions for \\nprobably_refutable\\n could be handled similarly. A simple approach to this solution is to separately define a question to the user, \\npotential_question\\n, for each clause of \\nmeta_refute\\n. It will prompt the user for an observation that could make the clause true. The only formal connection between the \\nmeta_refute\\n clause and the question would be that the same hypotheses are given in the clause as in the definition of the question. However, informally the system designer would know that the question is connected to the clause. Since all clauses of \\nmeta_refute\\n can be tested each time a hypothesis is attempted for meta refutation there would be no need for any further formal connection. The system can use \\npotential_question\\n to evaluate how many hypotheses might be refuted if a positive answer is given by the user to the question.\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\nDiagnosis of Multiple Faults\\n In the previous discussion we have assumed that a single fault is responsible for the first observed abnormality. One reason for this assumption has been to keep the presentation simple. There is also the reason of computational complexity, since the assumption of multiple faults can lead to an exponential number of hypotheses with regard to the size of the diagnosis object. This is due to the number of combinations of hypotheses. In GDE (de Kleer and Williams, 1987) methods to limit this complexity are introduced. These methods could be incorporated into our diagnosis system. Multiple faults could be handled analogously to GDE in our system. Our metalevel analysis of the object theory to construct the signal graph is analogous to GDE\\x92s propagation of values through the device models to predict the output values of the device and in this way identify conflicts. In GDE minimal candidates are generated from a conflict by taking one component from the conflict to construct each minimal candidate. When a new conflict that is not explained by any candidate, new minimal candidates are generated by examining the immediate supersets of the previous candidates to see if they can explain all conflicts. This method could be applied in our diagnosis strategy by extending the present hypotheses, from a candidate space, to explain multiple faults when new observations are reported by the user that are abnormal and not explained by the present hypotheses. \\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\nGeneralization of the diagnosis system \\nThe long term goal of this work is to construct systems for developing diagnosis systems in different classes of domains. The stereo domain in this paper and the domain of a monitoring and control system for a machine on an assembly line in industry might be two members of a class. A development system could reuse components based on an ontology for a class of domains represented as a meta-metatheory (of the metatheory). The theory would contain schemata for the problem solving methods and domain knowledge, and it would define a method for instantiating the schemata to generate new diagnosis systems.\\n\\n\\n\\n\\n\\n5. RELATED WORK\\n\\n\\n\\nIn the Second Generation Expert Systems manifesto (Steels, 1985) discussed the idea of exploiting more than one type of knowledge, for example by integrating heuristic knowledge with some form of deep knowledge. The focus of his approach is on the integration and cooperation at the conceptual level of different types of knowledge. Our approach emphasizes integration and cooperation between the heuristic and the model-based level, but also uses the representation to formulate heuristic knowledge in terms of the model-based knowledge. The meta-object architecture allows us to represent semi-general heuristics at the metalevel in terms of model-based knowledge at the object-level.\\n\\n\\n\\n\\xa0\\n\\n\\n\\nThe CHECK system (Console et al., 1992) consists of a heuristic level cooperating with a causal level. The heuristic level is invoked first and generates a set of hypotheses to be discriminated (or confirmed) by the causal level. The causal level tries to find a part of the causal network that covers the observations. The authors discuss the problem of inconsistency between the two types of knowledge. In particular, it is difficult to verify that they are consistent if they are acquired in different knowledge acquisition processes. In the AID system (Console et al., 1992) compiled knowledge is used as a focusing component for the causal reasoner. So called \"rule-out\" conditions are compiled from the causal model that allows the system to safely prune the search space for the abductive solver. By considering necessary conditions associated with states (having a cause) in the causal model, it is possible to immediately exclude a state when its necessary condition is inconsistent with the data. To compare, we generate hypotheses in the metatheory. The generator defines hypotheses in terms of model-based knowledge in the object theory, but could also use heuristic knowledge. The diagnosis strategy first tries to eliminate hypotheses based on compiled knowledge before resorting to model-based knowledge in the object theory. The metatheory also contains heuristic knowledge used to rank the hypotheses for discrimination.\\n\\n\\n\\n\\xa0\\n\\n\\n\\nSimmons and Davis (Simmons and Davis, 1987) combine causal and associational reasoning in the GORDIUS system in a generate-test-debug paradigm. The system uses associational knowledge to generate an initial hypothesis which is tested and, if incorrect, debugged into a solution using a causal explanation for the failure. Several iterations can be made until a satisfactory solution is found, when the hypothesis passes the test. \\n\\n\\n\\n\\xa0\\n\\n\\n\\nKoton\\x92s system CASEY (Koton, 1988) integrates a model-based component (causal model) with case-based reasoning in a system that can make model-based modifications to previous solutions (cases). The case-based component uses the causal model to determine what characteristics are important in an old case when it evaluates how well the case matches new current case--essentially those that played a role in the causal explanation of old case. If the case-based component cannot find an old case sufficiently similar to the current so that it can be adapted using the causal model, it falls back on the causal model to solve the case from scratch. The use of model-based modification of old cases have similarities to our use of semi-general heuristics defined in terms of model-based knowledge. They both involve modification of associational knowledge with the help of model-based knowledge integrating different types of knowledge. They both also attempt to reduce the number of inferences needed to find a solution. In Koton\\x92s case by modifying an old case supported by model-based knowledge, and in our case by eliminating hypotheses or focusing on some hypotheses with associational knowledge supported by model-based knowledge.\\n\\n\\n\\n\\xa0\\n\\n\\n\\nIn (ten Teije, van Harmelen, Schreiber and Wielinga, 1996; ten Teije 1997) parametric design is used for automated configuration of problem solving methods using a meta architecture, to build a system with flexible reasoning. The architecture consists of three logic theories: an object theory APPL containing the domain knowledge and problem data for an application such as a causal model, a metatheory METH containing problem solving methods together with definitions of method components, and a meta-metatheory FLEX containing strategy knowledge and a general schema for the problem solving methods in METH as well as relations between components and methods. ten Teije\\x92s architecture has similarities to our architecture, in that the model-based knowledge is represented in an object theory and the problem solving methods in a metatheory, and they communicate via a reflection mechanism using naming of terms. One difference is that our metatheory contains compiled and heuristic knowledge integrated with the model-based knowledge in the object theory to support diagnosis. Another difference is that we represent the diagnosis strategy in the metatheory and intend a meta-metatheory to only control the instantiating of schemata for building a diagnosis system. A major difference between the architectures is the scope of the systems where ten Teije\\x92s system can capture a number of different notions of diagnosis and furthermore configure problem solving methods automatically for different problem. In contrast our architecture does not currently encompass different notions of diagnosis, although it could be extended in that direction.\\n\\n\\n\\n\\xa0\\n\\n\\n\\nNejdl and colleagues (1995) discuss explicit representation of diagnosis strategies for model-based diagnosis systems. They introduce a metalanguage to express strategic knowledge in an explicit way. In contrast to the approach discussed in this paper, which uses standard first-order predicate logic, they employ modal logic. In a related paper (Damásio, Nejdl, Pereira and Schroeder, 1995) a logic meta-programming approach is used to represent a model-based diagnosis process, expressing preferences and strategies, based on extended logic programs. \\n\\n\\n\\n\\xa0\\n\\n\\n\\nLibraries of problem-solving methods and ontologies have been discussed by several authors, for example (Benjamins, 1993). The object- and metatheory discussed in this paper could be reused by generalizing the components of the theories to a schematic representation, in the form of a library. A library could be designed as a meta-metatheory of schemata, or templates, for the formulas of the object- and metatheory. The process of instantiating these schemata would be controlled by the meta-metatheory, formalizing the process. \\n\\n\\n\\n\\n\\n6. CONCLUSIONS\\n\\n\\n\\nThis paper studies the problem of integrating model-based and heuristic diagnosis in a single system. The logical structure of knowledge is preserved by the division of the metatheory and object theory where strategic and compiled knowledge is represented in a metatheory and model-based domain knowledge is represented in an object theory. The integration of model-based and compiled knowledge via the refutation methods in the metatheory allows them to be used together. Powerful inferences can be represented in the metatheory as semi-general metarules stated in terms of an object theory. The system has been designed to capture a class of domains characterizable in terms of connected devices with a directed flow between them. We have identified by examples a set of components for integrating heuristic and model-based diagnosis as a set of predicates of an object theory and a metatheory. They could be generalized as schemata in a meta-metatheory which would act as a theory for building diagnosis systems from reusable components. \\n\\n\\n\\n\\n\\nACKNOWLEDGEMENTS\\n\\n\\n\\nThe author wishes to thank the referees for useful comments and suggestions.\\n\\n\\n\\n\\n\\nREFERENCES\\n\\n\\n\\n\\n\\nAndersson, K. (1997). Integrating Heuristic and Model-Based Diagnosis, Uppsala Theses in Computing Science 27/97, Uppsala: Uppsala University.\\n\\n\\n\\n\\xa0\\n\\n\\n\\nBenjamins, R. (1993). Problem Solving Methods for Diagnosis, Ph.D. Thesis, Amsterdam: University of Amsterdam. \\n\\n\\n\\n\\xa0\\n\\n\\n\\nBowen, K.A. and Kowalski, R.A. (1982). Amalgamating Language and Metalanguage in Logic Programming, in: Clark, K.L. and Tärnlund, S.-Å. (Eds.), Logic Programming, London: Academic Press.\\n\\n\\n\\n\\xa0\\n\\n\\n\\nCarrico, \\n\\n\\n\\nM.A., Girard, J.E. and Jones, J.P. (1989). Building Knowledge Systems, New York: McGraw-Hill.\\n\\n\\n\\n\\xa0\\n\\n\\n\\nChandrasekaran, B. (1991). Models versus Rules, Deep versus Compiled, Content versus Form--some Distinctions in Knowledge Systems Research. IEEE Expert, 6(2), April 1991, pages 75-79.\\n\\n\\n\\n\\xa0\\n\\n\\n\\nConsole, L, Portinale, L, Dupré, D.T. and Torasso, P. (1993). Combining Heuristic Reasoning with Causal Reasoning in Diagnostic Problem Solving. Second Generation Expert Systems, (Berlin: Springer-Verlag) pages 46-68.\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\nConsole, L., Dupré, D. and Torasso, P. (1989). A Theory of Diagnosis for Incomplete Causal Models, Proc. Eleventh International Joint Conference on Artificial Intelligence, Detroit, pages 1311-1317.\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\nConsole, L. and Torasso, P. (1988). A Logical Approach to Deal with Incomplete Causal Models in Diagnostic Problem Solving, Lecture Notes in Computer Science \\n313\\n, Berlin: Springer Verlag, pages 255-264.\\n\\n\\n\\n\\xa0\\n\\n\\n\\nDague, P. (1995). Qualitative Reasoning: A Survey of Techniques and Applications, AI Communications \\n8\\n(3/4), pages 119-192.\\n\\n\\n\\n\\xa0\\n\\n\\n\\nDamásio, C.V, Nejdl, W., Pereira, L.M. and Schroeder, M. (1995). Model-Based Diagnosis Preferences and Strategies Representation with Logic Meta-programming, in K.R. Apt and F. Turini (Eds.), Meta-logics and Logic Programming, MIT Press, pages 269-311.\\n\\n\\n\\n\\xa0\\n\\n\\n\\nDavis, R., Buchanan, B. and Shortliffe, E. (1977). Production Rules as a Representation for a Knowledge-Based Consultation Program, Artificial Intelligence \\n8(1)\\n, 15-45. \\n\\n\\n\\n\\xa0\\n\\n\\n\\nde Kleer, J. and Williams, B.C. (1987). Diagnosing Multiple Faults, Artificial Intelligence \\n32(1)\\n, pages 97-130.\\n\\n\\n\\n\\xa0\\n\\n\\n\\nGenesereth, M.R. and Nilsson, N.J. (1987). Logical Foundations of Artificial Intelligence, Palo Alto: Morgan Kaufmann Publishers.\\n\\n\\n\\n\\xa0\\n\\n\\n\\nGruber, T.R (1989). The Acquisition of Strategic Knowledge, San Diego: Academic Press.\\n\\n\\n\\n\\xa0\\n\\n\\n\\nHart, A. (1988). Expert systems: an introduction for managers, London: Kogan-Page.\\n\\n\\n\\n\\xa0\\n\\n\\n\\nHayes-Roth, F., Waterman, D.A. and Lenat, D.B. (Eds.), Building Expert Systems, Reading, Mass.: Addison-Wesley.\\n\\n\\n\\n\\xa0\\n\\n\\n\\nKoton, P. (1993). Combining Causal Models and Case-Based Reasoning. Second Generation Expert Systems, (Berlin: Springer-Verlag) 69-78.\\n\\n\\n\\n\\xa0\\n\\n\\n\\nNejdl, W., Frölich, P. and Schroeder, M. (1995). A Formal Framework for Representing Diagnosis Strategies in Model-Based Diagnosis Systems. Proc. 14th International Joint Conference on Artificial Intelligence, Montréal, pages 1721-1727.\\n\\n\\n\\n\\xa0\\n\\n\\n\\nSimmons, R. and Davis, R. (1987). Generate, Test, and Debug: Combining Associational Rules and Causal Models. Proc. Tenth International Joint Conference on Artificial Intelligence, pages 1071-1078.\\n\\n\\n\\n\\xa0\\n\\n\\n\\nSteels, L. (1985). Second Generation Expert Systems. Future Generation Computer Systems, 1(4) pages 213-221.\\n\\n\\n\\n\\xa0\\n\\n\\n\\nten Teije, A. (1997). Automated Configuration of Problem Solving Methods in Diagnosis. Ph.D. Thesis, (Amsterdam: University of Amstedam).\\n\\n\\n\\n\\xa0\\n\\n\\n\\nten Teije, A., van Harmelen, F., Schreiber, G. and Wielinga, B. (1996). Construction of Problem-Solving Methods as Parametric Design. In Proceedings of the Tenth Workshop on Knowledge Acquisition for Knowledge-Based Systems (KAW\\x9296), Banff, Alberta.\\n\\n\\n\\n',\n",
       " ' Technology? You\\'re here because you\\'ve probably heard of the word \"Java\" in some form and want to know more. Let\\'s start with the big picture: The Java TM platform is based on the power of networks and the idea that the same software should run on many different kinds of computers, consumer gadgets, and other devices. Since its initial commercial release in 1995, Java technology has grown in popularity and usage because of its true portability. The Java platform allows you to run the same Java application on lots of different kinds of computers. Any Java application can easily be delivered over the Internet, or any network, without operating system or hardware platform compatibility issues. For example, you could run a Java technology based application on a PC, a Machintosh computer, a network computer, or even new technologies like Internet screen phones. Furthermore, the Java platform was designed to run programs securely on networks, which means that it integrates safely with the existing systems on your network. The idea is simple: Java technology-based software can work just about everywhere. Java technology components don\\'t care what kind of computer, phone, TV, or operating system they run on. They just work, on any kind of compatible device that supports the Java platform. Java technology allows programmers and users to do new things with Web pages that were not possible before. With Java technology, the Internet and private networks become your computing environment. For example, users can securely access their personal information and applications when they\\'re far away from the office by using any computer that\\'s connected to the Internet; soon they\\'ll be able to access tailored applications from a mobile phone based on the Java platform, or even use smart cards as a pass key to everything from the cash machine to ski lifts. Smart Card or Smart Java Card TM ? A smart card is a credit card-sized plastic card with an integrated circuit (IC) inside. The IC contains a microprocessor and memory, which gives smart cards the ability to process, as well as store, more information than was previously possible. In the case of the Java Card platform, applications in the form of byte-code are loaded into the memory zone of the smart card\\'s microprocessor, where they are run by the Virtual Machine. The executable code is platform independent so that any card incorporating a Java Card technology-based interpreter can run the same application. Multiple Java Card technology-based applications can reside on a single card, each allocated to their own secure memory areas to ensure their integrity and eliminate program tampering, either by individuals or through program interference. Read the feature story Java Card Technology Turns Five The Java platform is being built into next-generation telephones, TV set-top boxes, smart cards that fit in your wallet, and many other consumer and business devices. Java technology-based software includes: programs written in the Java programming language can run directly on your computer (without requiring a browser), or on servers, on large mainframe computers, or other devices. For example, Java technology-based software running on servers in large companies monitors transactions and ties together data from existing computer systems. Other companies are using Java technology-based software on their internal Web sites to streamline communication and the flow of information between departments, suppliers and customers. Programs written in the Java programming language run on so many different kinds of systems thanks to a component of the platform called the Java virtual machine or \"JVM TM \"* -- a kind of translator that turns general Java platform instructions into tailored commands that make the devices do their work. Getting the Java Platform If you\\'re reading this in a Web browser on a personal computer or workstation, you\\'ve probably already got the Java platform. It\\'s incorporated into all major Web browsers. In the event that a JVM is not incorporated, Sun provides a fully supported, high-performance virtual machine as part of the Java 2 Runtime Environment, Standard Edition (JRE), available for download from the J2SE TM platform homepage (see below for more information). Ease of Use You don\\'t need to be a mechanic to drive a car. Why should you have to be a \"system administrator\" to use a computer? With Java software, you don\\'t have to be one. Java technology eliminates many of the problems associated with installing and running applications. That\\'s because generally the Java user does not have to configure, load, or install anything. Instead, computing devices tap into the network and funnel its power to the user. Upgrades are automatic, making installation and configuration obsolete. Ease of Development: Two Real-Life Examples Developing on the Java platform means that projects are completed faster and with less debugging. These two real-life examples serve as testimonials to this claim. Life Time Fitness - Having completed sizable development projects over the last three years, using different technologies, Life Time Fitness has come to endorse only the Java 2 Platform, Enterprise Edition for enterprise Web application development. Read the full Feature Story \"Leveraging the J2EE TM [platform\\'s] suite of technologies enabled us to focus more of our technical resources on creating solutions to business problems, rather than laboring to maintain proprietary software from Microsoft or Allaire. In addition, Java technology\\'s underlying object oriented architecture allowed us to design our systems for maximum code reuse,\" says Wesley Bertch, director of software systems, Life Time Fitness. \"We have also taken advantage of Java technology\\'s portability by running our production application on both Windows NT and Solaris TM 8.0 Operating Environment with no code changes.\" Friendly Giants and BLAM! - Mark Ripley and Jay Minn are two gaming industry seniors, now leaders of small, independent game-development companies, that have created hit titles on their own for over five years using Java technology. Read the full Feature Story \"We realized that we could use Java technology to write games once and deploy them across many platforms. The whole idea was to get away from the established gaming community,\" said Ripley, chief executive of Friendly Giants. \"We at BLAM! have always been true believers in Java technology as a viable gaming platform,\" said Jay Minn, president, BLAM! \"By applying our years of development experience, we have been able to produce high quality and immediately accessible, fun games like BUMP! and Golf, based on Java technology, for our audiences.\" Friendly Giants and BLAM! have taken advantage of the Java platform\\'s portability by leveraging the technology to rapidly construct and later tweak a game or engine environment. As a result, they have saved themselves months of development time, trimming budgets and easing timelines. The Platforms Recognizing that \"one size doesn\\'t fit all,\" Sun has grouped its innovative Java technologies into three editions: Java 2 Platform, Micro Edition (J2ME TM technology), Java 2 Platform, Standard Edition (J2SE TM technology), and the Java 2 Platform, Enterprise Edition (J2EE TM technology). Each edition is a developer treasure chest of tools and supplies that can be used with a particular product. --> Java 2 Platform, Standard Edition (J2SE) The J2SE platform is a fast and secure foundation for building and deploying client-side enterprise applications. In today\\'s .com world of nanosecond response times and information gratification, J2SE technology provides the speedy performance and high functionality that is demanded by Web users. For end users, J2SE technology enables faster and easier use of functionally rich Web applications, such as corporate intranets and interactive shopping aids for eCommerce. For enterprise developers, the improved J2SE technology serves as the base tool for creating sophisticated, valuable applications that can be brought to market quickly. Java 2 Platform, Enterprise Edition (J2EE) J2EE technology simplifies enterprise applications by basing them on standardized, modular and re-usable components Enterprise JavaBeans TM (EJB TM ), providing a complete set of services to those components, and handling many details of application behavior automatically. By automating many of the time-consuming and difficult tasks of application development, J2EE technology allows enterprise developers to focus on adding value, that is, enhancing business logic, rather than building infrastructure. Java 2 Platform, Micro Edition (J2ME) J2ME technology specifically addresses the vast consumer space, which covers the range of extremely tiny commodities such as smart cards or a pager all the way up to the set-top box, an appliance almost as powerful as a computer. J2ME technology enables device manufacturers, service providers, and content creators to gain a competitive advantage and capitalize on new revenue streams by rapidly and cost-effectively developing and deploying compelling new applications and services to their customers worldwide. Interested in learning more about Java technology? Read on! Feature stories about Java technology http://java.sun.com/features/ New to Java Programming Center http://developer.java.sun.com/developer/onlineTraining/new2java The Java Tutorial http://java.sun.com/docs/books/tutorial/ Java technology history What is Java technology - 1996 http://java.sun.com/java2/whatis/1996/ Java technology: An Early History - 1998 http://java.sun.com/features/1998/05/birthday.html Java technology timeline - 2000 http://java.sun.com/features/2000/06/time-line.html Credits: This document was written with contributions from Martin Hardee, Mary Smaragdis, Frank Rimalovski, Casey Cameron, Dave Macias, Bruce Goldberg, Tim Beyers, John Byous, Jennifer Umstattd, Liz Doughty, Tony Welch, Heidi Dailey, Kammie Kayl, Dana Nourie, Adam Wisnewski and Gladina Guinto . *As used on this web site, the terms \"Java virtual machine\" or \"JVM\" mean a virtual machine for the Java platform. Company Info | About SDN | Press | Contact Us | Employment How to Buy | Licensing | Terms of Use | Privacy | Trademarks Copyright 1994-2004 Sun Microsystems, Inc. A Sun Developer Network Site Unless otherwise licensed, code in all technical manuals herein (including articles, FAQs, samples) is provided under this License . Content Feeds \\n\\t\\n\\n-->\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJava[tm] 2 Platform\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ndevelopers.sun.com\\n\\xa0\\n»\\xa0search tips\\xa0\\xa0|\\xa0\\xa0Search:\\xa0\\n\\n\\xa0\\n\\nin Developers\\' Site\\nin Sun.com\\n\\n\\xa0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\n\\n\\nDevelopers Home > Products & Technologies > Java Technology > Reference > Documentation >\\n\\n\\n\\n\\nProfile and Registration | \\nWhy Register?\\n\\n\\n\\n\\n\\n\\xa0\\n\\n\\n\\nArticle\\nJava[tm] 2 Platform\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\nPrintable Page\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Technology?\\n\\n\\n\\n\\n\\n\\n\\n\\nYou\\'re here because you\\'ve probably heard of\\nthe word \"Java\" in some form and want to know more.\\n\\n\\nLet\\'s start with the big picture:\\nThe JavaTM platform is based on the power of networks and the idea that\\nthe same software should run on many\\ndifferent kinds of computers, consumer gadgets, and other devices. Since\\nits initial commercial release in 1995, Java technology has grown in popularity \\nand\\nusage because of its true portability. The Java platform allows you to run the\\nsame Java application on lots of different kinds of computers.\\n\\n\\nAny Java application can easily be delivered over the Internet, or any\\nnetwork, without operating system or hardware platform compatibility\\nissues. For example, you could run a Java technology based application on a PC, \\na Machintosh\\ncomputer, a network computer, or even new technologies like Internet screen \\nphones. Furthermore, the\\nJava platform was designed to run programs securely on networks, which means \\nthat it\\nintegrates safely with the existing systems on your network.\\n\\n\\nThe idea is simple: Java technology-based software can work just about\\neverywhere. Java technology components don\\'t care what kind of computer, phone, \\nTV, or operating\\nsystem they run on.\\nThey just work, on any kind of compatible device that supports the Java\\nplatform.\\n\\n\\nJava technology allows programmers and users to\\ndo new things with Web pages that were not possible before. With Java\\ntechnology, the Internet and private networks become your computing environment. \\nFor\\nexample, users can securely access their personal information and applications \\nwhen\\nthey\\'re far away from the office by using any computer that\\'s connected to the\\nInternet; soon they\\'ll be able to access tailored applications from a\\nmobile phone based on the Java platform, or even use smart cards as a pass key \\nto\\neverything from the cash machine to ski lifts.\\n\\n\\n \\n\\n Smart Card orSmart Java CardTM?\\nA smart card is a credit card-sized plastic card with an integrated circuit\\n(IC) inside. The IC contains a microprocessor and memory, which gives smart\\ncards the ability to process, as well as store, more information than was\\npreviously possible.\\nIn the case of the Java Card platform, applications in the form of byte-code\\nare loaded into the memory zone of the smart card\\'s microprocessor, where they\\nare run by the Virtual Machine. The executable code is platform independent so\\nthat any card incorporating a Java Card technology-based interpreter can run\\nthe same application.\\nMultiple Java Card technology-based applications can reside on a single card,\\neach allocated to their own secure memory areas to ensure their integrity and\\neliminate program tampering, either by individuals or through program\\ninterference.\\nRead the feature story Java Card Technology Turns Five\\n\\n\\n\\n\\n\\n\\n\\nThe Java platform is being built into next-generation\\ntelephones, TV set-top boxes, smart cards that fit in your wallet, and\\nmany other consumer and business devices. Java technology-based software\\nincludes: programs written in the Java programming language can run directly on\\nyour computer (without requiring a browser), or on servers, on large mainframe\\ncomputers, or other devices.\\n\\n\\nFor example, Java technology-based software running on servers in large\\ncompanies monitors transactions and ties together data from existing\\ncomputer systems. Other companies are using Java technology-based\\nsoftware on their internal Web sites to streamline communication and the flow of\\ninformation between departments, suppliers and customers.\\n\\n\\nPrograms written in the Java programming language run on so many\\ndifferent kinds of systems thanks to a component of the platform called\\nthe Java virtual machine or \"JVMTM\"* -- a kind of translator that \\nturns general\\nJava platform instructions into tailored commands that make the devices do\\ntheir work.\\n\\n\\nGetting the Java Platform\\nIf you\\'re reading this in a Web browser on a personal computer or\\nworkstation, you\\'ve probably already got the Java platform. It\\'s\\nincorporated into all major Web browsers. In the event that a JVM is not \\nincorporated, Sun\\nprovides a fully supported, high-performance virtual machine as part of the\\nJava 2 Runtime Environment, Standard Edition (JRE), available for\\ndownload from the J2SETM platform homepage (see below for more information).\\n\\nEase of Use\\nYou don\\'t need to be a mechanic to drive a car. Why should you have to\\nbe a \"system administrator\" to use a computer?  With Java software, you\\ndon\\'t have to be one. Java technology eliminates many of the\\nproblems associated with installing and running applications. That\\'s\\nbecause generally the Java user does not have to configure, load, or\\ninstall anything. Instead, computing devices tap into the network and funnel its\\npower to the user. Upgrades are automatic, making installation and configuration\\nobsolete.\\n\\nEase of Development: Two Real-Life Examples\\nDeveloping on the Java platform means that projects are completed faster\\nand with less debugging. These two real-life examples serve as\\ntestimonials to this claim.\\n\\nLife Time Fitness - \\nHaving completed sizable development projects over the last three years,\\nusing different technologies, Life Time Fitness has come to endorse only\\nthe Java 2 Platform, Enterprise Edition for enterprise Web application\\ndevelopment.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRead the full Feature Story\\n\\n\\n\\n\\n\\n\\n\\n\"Leveraging the J2EETM [platform\\'s] suite of technologies enabled us to\\nfocus more of our technical resources on creating solutions to business\\nproblems, rather than laboring to maintain proprietary software from\\nMicrosoft or Allaire. In addition, Java technology\\'s underlying object\\noriented architecture allowed us to design our systems for maximum code\\nreuse,\" says Wesley Bertch, director of software systems, Life Time\\nFitness. \"We have also taken advantage of Java technology\\'s portability\\nby running our production application on both Windows NT and SolarisTM 8.0\\nOperating Environment\\nwith no code changes.\"\\n\\nFriendly Giants and BLAM! - \\nMark Ripley and Jay Minn are two gaming industry seniors, now leaders of\\nsmall, independent game-development companies, that have created hit\\ntitles on their own for over five years using Java technology.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRead the full Feature Story\\n\\n\\n\\n\\n\\n\\n\\n\"We realized that we could use Java technology to write games once and\\ndeploy them across many platforms. The whole idea was to get away from\\nthe established gaming community,\" said Ripley, chief executive of Friendly \\nGiants.\\n\"We at BLAM! have always been true believers in Java technology as a\\nviable gaming platform,\" said Jay Minn, president, BLAM! \"By applying our\\nyears of development experience, we have been able to produce high quality and \\nimmediately\\naccessible, fun games like BUMP! and Golf, based on Java technology, for our \\naudiences.\"\\nFriendly Giants and BLAM! have taken advantage of the Java platform\\'s\\nportability by leveraging the technology to rapidly construct and later\\ntweak a game or engine environment. As a result, they have saved themselves \\nmonths of\\ndevelopment time, trimming budgets and easing timelines.\\n\\nThe Platforms\\nRecognizing that \"one size doesn\\'t fit all,\" Sun has grouped its\\ninnovative Java technologies into three editions: Java 2 Platform, Micro \\nEdition (J2METM\\ntechnology), Java 2 Platform, Standard Edition (J2SETM technology), and the \\nJava 2 Platform, Enterprise Edition \\n(J2EETM technology).\\nEach edition is a developer treasure chest of tools and supplies that can be \\nused with a\\nparticular product.\\n\\n\\nJava 2 Platform, Standard Edition (J2SE)\\nThe J2SE platform is a fast and secure foundation for building and\\ndeploying client-side enterprise applications. In today\\'s .com world of\\nnanosecond response times and information gratification, J2SE technology\\nprovides the speedy performance and high functionality that is demanded\\nby Web users.\\n\\n\\nFor end users, J2SE technology enables faster and easier use of\\nfunctionally rich Web applications, such as corporate intranets and interactive \\nshopping aids for eCommerce.\\nFor enterprise developers, the\\nimproved J2SE technology serves as the base tool for creating\\nsophisticated, valuable applications that can be brought to market quickly.\\n\\nJava 2 Platform, Enterprise Edition (J2EE)\\nJ2EE technology simplifies enterprise applications by basing them on\\nstandardized, modular and re-usable components Enterprise JavaBeansTM\\n(EJBTM), providing a complete set of services to those components, and\\nhandling many details of application behavior automatically. By\\nautomating many of the time-consuming and difficult tasks of application \\ndevelopment,\\nJ2EE technology allows enterprise developers to focus on adding value, that\\nis, enhancing business logic, rather than building infrastructure.\\n\\nJava 2 Platform, Micro Edition (J2ME)\\nJ2ME technology specifically addresses the vast consumer space, which\\ncovers the range of extremely tiny commodities such as smart cards or a\\npager all the way up to the set-top box, an appliance almost as powerful as a\\ncomputer. J2ME technology enables device manufacturers, service providers, and \\ncontent\\ncreators to gain a competitive advantage and capitalize on new revenue\\nstreams by rapidly and cost-effectively developing and deploying\\ncompelling new applications and services to their customers worldwide.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nInterested in learning more about Java technology? Read on!\\n\\nFeature stories about Java technology\\nhttp://java.sun.com/features/\\n\\n\\nNew to Java Programming Center\\nhttp://developer.java.sun.com/developer/onlineTraining/new2java\\n\\n\\nThe Java Tutorial\\nhttp://java.sun.com/docs/books/tutorial/\\n\\nJava technology history\\n\\nWhat is Java technology - 1996\\nhttp://java.sun.com/java2/whatis/1996/\\n\\n\\nJava technology: An Early History - 1998\\nhttp://java.sun.com/features/1998/05/birthday.html\\n\\n\\nJava technology timeline - 2000\\nhttp://java.sun.com/features/2000/06/time-line.html\\n\\nCredits: This document was written with contributions\\nfrom Martin Hardee, Mary Smaragdis, Frank Rimalovski,\\nCasey Cameron, Dave Macias, Bruce Goldberg, Tim Beyers,\\nJohn Byous, Jennifer Umstattd, Liz Doughty, Tony Welch,\\nHeidi Dailey, Kammie Kayl, Dana Nourie, Adam Wisnewski and Gladina Guinto .\\n\\n\\n\\n\\n\\n*As used on this web site, the terms \"Java virtual machine\" or \"JVM\" mean a virtual machine for the Java platform.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCompany Info \\xa0|\\xa0\\nAbout SDN \\xa0|\\xa0\\nPress \\xa0|\\xa0\\nContact Us \\xa0|\\xa0\\nEmployment\\nHow to Buy \\xa0|\\xa0\\nLicensing \\xa0|\\xa0\\nTerms of Use \\xa0|\\xa0\\nPrivacy \\xa0|\\xa0\\nTrademarks\\n\\xa0\\n\\xa0\\nCopyright 1994-2004 Sun Microsystems, Inc.\\n\\n\\nA Sun Developer Network Site\\n\\n\\nUnless otherwise licensed, code in all technical manuals herein (including articles, FAQs, samples) is provided under this License.\\n\\xa0\\n\\xa0Content Feeds\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " \"\\n\\n\\n\\n\\n\\nUse of Algebraic Geometry\\n\\n\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n\\n\\n\\n\\nWhat can algebraic geometry be used for?\\nMany problems can be translated into solving polynomial equations, \\nand algebraic\\ngeometry is precisely the study of the solutions of such equations. \\nThe wavelet transformation uses a central theorem in algebraic geometry\\ncalled Bezout's Theorem. Netscape's browser has a plug-in for image compression\\nbased on wavelets. \\nAnother example is Groebner bases (or standard bases) which is a \\nmethod for finding certain bases for ideals. Groebner bases has many applications.\\nFor example, the problem of adjusting the joints of a robot arm to bring\\nthe grabber from point A to point B can be solved using Groebner bases.\\nSee J. Baillieul et al (1990). Groebner bases are also of interest in connection\\nwith artificial intelligence where the method can be used for automated\\ntheorem proving in geometry. See W.-T. Wu (1983) which is a reprint of\\nthe original ideas for this. Groebner bases also has uses in image processing.\\nIn order to draw three dimensional objects one has to work with projections\\nand this can be done in, what is called, a projective space. See Foley\\net al (1990). \\nIt is possible to define addition of points on an elliptic curve. This\\narithmetic has been used successfully in coding theory and cryptography.\\nIt is assumed, but not proved, that cryptosystems based on such arithmetic\\nare more secure then cryptosystems based on conventional arithmetic. \\n\\nReferences\\n\\n\\nJ. Baillieul et al. (1990), Robotics, Proceedings of Symposia in\\nApplied Mathematics 41, American Mathematical Society, Providence,\\nRhode Island.\\n\\nJ. Foley, A. van Damm, S. Feiner, and J. Hughes (1990), Computer Graphics:\\nPrinciples and Practice, Second Edition, Addison-Wesley, Reading, Massachusetts.\\n\\nW.-T. Wu (1983), On the decision problem and the mechanization of theorem-proving\\nin elementary geometry, in Automated Theorem Proving: After\\n25 Years, edited by W. Blesoe and D. Loveland, Contemporary Mathematics\\n29, American Mathematical Society, Provedence, Rhode Island, 213-234.\\n\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n\\n\\n\\n\\n\",\n",
       " \"\\n\\nNIST: Spectrum of Platinum - Introduction\\n\\n\\n\\n\\n\\n\\n1. Introduction\\n\\n\\n\\nTable of Contents\\n1. Introduction\\n2. Photographic Observations\\n3. Photoelectric Observations\\n4. Description of the Atlas\\n5. Accuracy of Wavelengths\\n6. References and Acknowledgments\\xa0\\n\\n\\n\\n\\nThe deployment of the Hubble Space Telescope (HST) on April\\xa024, 1990,\\nlaunched a new era in astronomy.  With the HST, stars and other astronomical\\nobjects are being observed with unprecedented clarity.  The improvement over\\nground-based telescopes is most significant in the ultraviolet region of the\\nspectrum, where the earth's atmosphere absorbs most of the radiation. \\nAlthough the much-publicized spherical aberration in the HST's primary mirror \\n[1] greatly reduces the quality of star images, \\nmany experiments of a spectroscopic nature are not severely affected because \\nthey do not require high spatial resolution.  For example, for the Goddard High \\nResolution Spectrograph (GHRS), the highest resolution spectrograph on HST, the \\nspherical aberration in the primary mirror does not degrade the spectral \\nresolution noticeably when the small science aperture is used \\n[2]. However, because of enlargement of the point \\nspread function, the exposure time must be increased by a factor of \\nabout\\xa05 to produce the signal-to-noise ratio of prelaunch expectations [2].\\nNevertheless, spectra of very high quality have been obtained [2].\\n\\n\\nThe region of observation of GHRS is 1100\\xa0Å to 3200\\xa0Å. In \\nits echelle mode it has a resolving power of 90,000 and a wavelength accuracy \\nof a few parts in 106. Line-of-sight velocities of stellar objects \\ncan thus be determined to an accuracy of about 1\\xa0km/s. In order to \\nachieve this accuracy, of course, an accurate wavelength scale must be \\nestablished. This is accomplished by illuminating the spectrograph with an \\nonboard platinum/neon hollow-cathode lamp during periods in which stellar \\nobservations are not being made [3]. The use of \\na Pt/Ne lamp for this purpose and its space-qualified design are due to Mount, \\nYamasaki, Fowler, and Fastie [4], who originally \\nsuggested it for wavelength calibration of the International Ultraviolet \\nExplorer (IUE) satellite.\\n\\n\\nTo achieve the accuracy for which GHRS was designed, the calibration\\nwavelengths must be accurate to about 0.002\\xa0Å.  However, tests \\ncarried out in our laboratory in 1983 indicated that the best available \\nwavelengths for Pt [5] had errors ranging to \\nabout 0.015\\xa0Å.  We thus began a program to measure the spectra \\nemitted by a Pt/Ne hollow-cathode lamp similar to the one to be used with GHRS. \\nThis work was carried out with our high resolution 10.7\\xa0m \\nnormal-incidence vacuum spectrograph at NIST. At about the same time Engleman\\n[6] recorded the spectrum of a Pt hollow-cathode \\nlamp with a Fourier-transform spectrometer.  He obtained accurate wavelengths \\nfor 320\\xa0lines of Pt\\xa0I in the region \\n2200\\xa0Å to 7220\\xa0Å, optimized the energy level values, and \\ncalculated accurate Ritz-type wavelengths for 81\\xa0lines in the region \\n1724\\xa0Å to 2250\\xa0Å. Many of these lines were used in \\ncalibrating our grating measurements.\\n\\n\\nSome of the results of our work have appeared in two previous papers. In the \\nfirst [7] we determined accurate values for \\n100\\xa0energy levels of Pt\\xa0II by combining our \\nnew grating measurements for over 500\\xa0Pt\\xa0II \\nlines in the ultraviolet with measurements of lines at longer wavelengths made \\nby Engleman by Fourier transform spectroscopy. In the second \\n[8] we reported wavelengths with accuracies of \\n0.002\\xa0Å or better for some 3000\\xa0lines emitted by a Pt/Ne lamp in the \\nregion 1032\\xa0Å to 4100\\xa0Å. In this second report we also provided \\nrelative intensities of the spectral lines of the Pt/Ne lamp that were \\ndetermined by recording the spectra photoelectrically with the same \\nspectrograph used for the wavelength measurements. \\n\\n\\nOur wavelengths for the Pt/Ne lamp are currently being used for calibration of \\nGHRS as well as for wavelength calibration of the Faint Object Spectrograph on \\nHST, which uses a Pt-Cr/Ne hollow-cathode lamp for both wavelength and \\nradiometric calibration [9]. Our data are also \\nbeing used for revised calibrations of spectra from the IUE satellite \\n[10], and for calibration of spectra obtained \\nwith sounding rockets, which also use onboard Pt/Ne hollow cathode lamps \\n[11]. In a different type of application, the \\ndata are being used to interpret the spectra of stars that contain Pt in\\nanomalously high abundances [12].\\n\\n\\nIn the present paper we present a comprehensive report of our observations of \\nthe Pt/Ne hollow-cathode lamp.  For completeness we give a full account of the \\nexperimental work and data analysis.  Some of this information has been given \\nin our previous papers.\\n\\n\\nOur results are presented in the form of an atlas of the spectrum emitted by a \\nPt/Ne hollow-cathode lamp in the region 1130\\xa0Å to 4330\\xa0Å. \\nThe atlas consists of plots of the spectrum accompanied by tables that include \\nthe wavelengths, wavenumbers, intensities, and identifications or \\nclassifications where known for more than 5600\\xa0lines. We have attempted to \\nprovide the best available wavelength data, substituting values from the \\nliterature or calculated Ritz-type wavelengths where these are more accurate \\nthan our measurements.\\n\\n\\nThe line list developed in this work was communicated to J.\\xa0Blaise and\\nJ.-F.\\xa0Wyart of the Laboratoire Aimé Cotton, Orsay, France, who have used \\nit to substantially extend the energy level analysis of \\nPt\\xa0II.  Based on our measurements they have located \\nnearly 150 new Pt\\xa0II levels.  Their report on the \\nanalysis appears as a companion paper in the same issue of this journal \\n[13]. Blaise and Wyart have also located about \\n100\\xa0new levels of Pt\\xa0I.  The new line \\nidentifications for Pt\\xa0I and \\nPt\\xa0II have been provided to us and are incorporated \\nin the atlas.\\n\\n\\nThe data included in this atlas should be of use not only for astronomical \\nspectroscopy but also for the calibration of general laboratory spectra \\nobtained with medium to high resolution diffraction grating spectrographs. No \\nother source provides such a dense and complete coverage of this spectral \\nregion with lines suitable for use as reference wavelengths.  The Pt/Ne hollow \\ncathode is easy to operate and is commercially available at moderate cost.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\n",
       " '\\n\\n\\n\\n\\n\\n\\nsaxon-spanglish\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nweb hosting\\ndomain names\\nemail addresses\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nmembers.fortunecity/rapidrytr/saxon-spanglish.htm\\xa0\\xa0\\nwith two Graphics\\xa0\\xa0 phonemes\\nsource\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nlern\\ntu wrait alfabeticly in\\nSAXON -\\nSPANGLIC\\nlern\\ntu wrait in a mor concis\\xa0 & consistent orthografy\\n\\n\\n\\n\\n\\n\\nFor\\na\\xa0 version with grafics - click here> graphics\\n\\nInstead\\nof retaining the old Saxon alphabet, the English speaking world has retained\\na mix of archaic M.E. spellings. By doing so, the relationship between\\nspelling and pronunciation has been all but lost. English spelling went\\nfrom being over 90% phonemic in the 10th Century to being less than 40%\\nphonemic today.\\n\\n\\n\\n\\ninternational\\nspelling\\nbad\\narguments against reform\\n\\xa0advantages\\nof\\nsimplification\\nfeedback\\n\\xa0rationale\\n\\xa0next\\npage >>\\n\\n\\nAn\\nalphabet represents to the eye the sounds of a language by means of written\\nsymbols.\\xa0\\xa0 It follows that in the most rational alphabet -\\xa0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 (1)\\nEvery simple sound will be associated with a single distinct symbol, and\\xa0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n(2) There will be a consistent relationship between each sound and its\\nsymbol.\\nThe Roman alphabet provides a very legible\\nand distinct set of characters.\\xa0\\xa0 It just doesn\\'t provide enough\\nof them:\\xa0 5 letters for 12 vowels.\\xa0 While there is a shortage\\nof vowel letters, the difficulty of our present English spelling lies not\\nso much in any of the inherent\\xa0 defects of the Roman alphabet as in\\nour irrational use of it. -Sweet\\nparaphrased\\n\\xa0 The\\nimmediate practical questions of Spelling or Orthographic Reform are -\\xa0\\n\\nWhat are the\\nsimple vowel sounds in English speech? and\\xa0\\n\\nBy what arrangement\\nof the existing alphabet can these\\n\\xa0 sounds be best represented?\\nIf we exclude new letters as impractical,\\nwe are obliged to either fall back on digraphs or two-letter combinations,\\nsuch as th and ch, or to merge phonemes.\\xa0 The obvious\\nobjection to digraphs is that they can violate the principle of denoting\\nevery simple sound by a simple sign.\\xa0\\n[ITA\\navoided this by making digraphs such a ch into ligatures and thus\\na single symbol]\\nDigraphs representing diphthongs are another\\nmatter.\\xa0 There is nothing wrong with using two letters to represent\\na blend of two simple sounds.\\xa0 What we want to avoid is unpronounced\\nor silent letters.\\xa0 There is a\\xa0 problem when one of the letters\\nis a silent or used as a marker.\\xa0 In NS [nu speling]\\xa0\\npoet is pronounced /poat/.\\xa0 In SP, poet would\\nbe pronounced /paw-eht/.\\xa0 /pou-et/ would be spelled poeet in\\nNS.\\xa0 The letter string,\\npoeet, would have two different pronunciations\\nin NS depending on the syllable division:\\xa0 poe-et or po-eet.\\nAs with most northern European languages, English\\nhas 12 simple [uncombined] vowel sounds.\\xa0 Latin had 10 simple vowels\\n[and 5 vowel letters] so those who adopted the Latin alphabet often had\\nto augment it.\\xa0 The Saxon augmented Latin\\nalphabet is shown below:\\nThe representation of the obscure mid lax vowel\\nor schwa in Saxon was almost as ambiguous as it is today.\\xa0 e\\ncould be interpreted as either [eh] or as an unstressed\\xa0 è\\n[uh].\\xa0 The unstressed à [uh] had the same schwa pronunciation.\\xa0\\xa0\\nThe ash [æ] is the most obvious addition\\nto the Latin alphabet. The extended e [listen to sweete]\\nand extended æ\\xa0 are no longer used\\nin English.\\xa0 In the chart, the [ee] position is used for the vowel\\nin *her.\\xa0\\xa0\\xa0 Maroon\\ntype indicates that the passage is written in Saxon-SPanglic.\\n\\xa0\\n\\n\\n\\n\\n\\n12\\nSaxon Vowels\\nOld English 700-1060\\nMidl English\\n1250-1400\\n\\n\\nLetter\\n\\n\\nChecked\\n\\n\\nFree\\n\\n\\n\\n\\nA\\n\\n\\nuh, ago\\n\\n\\nah\\xa0 wand\\n\\n\\n\\n\\nÆ\\n\\n\\nash, ax\\n\\n\\nhæt\\n= hat\\n\\n\\n\\n\\nE\\n\\n\\nelbow\\neh\\n\\n\\n\\xa0her\\n*bird\\n\\n\\n\\n\\nI\\n\\n\\nih, ich\\n\\n\\nski ring, *eel\\n\\n\\n\\n\\nO\\n\\n\\naw cost\\nholy\\n=holly\\n\\n\\noh [ow]\\ngood=o:\\n\\n\\n\\n\\nU\\n\\n\\nhook\\nhwk\\n\\n\\nhoop\\nhoure\\n\\n\\n\\n\\na/e\\xa0 eR\\n\\n\\nuh\\n\\n\\nuh-r\\n\\n\\n\\nLern yur ah-bi-ciz\\nMiddl English yuzd the seim\\nvaul tabl\\nbut the speling was chanjd\\n[u->ou]\\n\\n\\n\\n\\nSaxon-Spanglic\\n(SP) is won of several world english (winglish) proposals tu restor alfabetic\\nspelling.\\xa0 SS restors the original Saxon ogmented Latin alfabet tu\\nunshift the vowels found in many English word pronunciations.\\xa0 This\\nset of grafim-fonim (letter-sound) corespondences is yusd tu pronounce\\xa0\\nor sound out each letter in a werd.\\xa0 The result is a new dialect of\\nEnglish that is neither GA [general american]\\nnor RP [british].\\xa0 Saxon\\nis easy to read since so few words are respelled, the phonemic\\nversion requires an adjustment.\\nOnly thos\\nwords that cannot be understood when sounded out ar respeld.\\xa0 The\\neffect of this simpl reform is tu mak English wonce again clos tu 90% fonemic\\nand consistent.\\xa0 SP allaus twu sounds per spelling so ther is yusually\\nsom ambiguity.\\xa0 Thus f=v / f,\\xa0 v=v/\\'a,\\xa0 s=z / s, -ce=ts/se,\\xa0\\nsi=si / shi, w=w/\\'u, o=aw/ow,\\xa0 ow=/ou/\\n*owld bowt, a= ah / uh\\n/ ae.\\xa0 The 1755 spelling conventnions that SP drops include the silent\\ne, the magic e, ph for f, g=j, a-=awe.\\xa0 Dobl consonants ar retained\\nat sillabl boundaries hwer the root vowel is short.\\nSpelling\\nreformers\\ntypically want to spell according to the pronounciation guide in the dictionary.\\xa0\\nThis phonemic approach respells over 60% of the words as shown below.\\xa0\\nIt can be done but why bother except in the pronunciation guide?\\xa0\\nAmbigious Spanglish [2 sounds /letter] is easier to read than accent clarified\\nSpanglish.\\n\\nSaxon-SPanglic\\n(SP)\\xa0 ìz\\n.wàn\\n.ov .sevèr\\'al\\n.w\\'rld\\nenglish (wenglish) proposàls.\\ntu .ristor\\nalfàbetic\\nspeling.\\xa0 SP restorz the orìginàl\\nSaxon ogmented Latin alfabet tu ùnshift\\nthe vaulz faund\\xa0 ìn\\nmeny\\nEnglish prànànsieishàns.\\nDhìs\\nset ov grafim-fonim (letr-saund) corespondensès\\xa0\\nìz yuzd tu\\npronauns or saund aut iich letr ìn\\na wèrd.\\nDhè\\nresult ìz\\'a\\nnu daiàlectov\\nEnglish dhat\\xa0 ìznidhèr\\nGA nor RP. O\\'nly dhowz wèrdz.dhæt.canot.bi.ànderst\\'udh\\nhwen saunded aut ar respeld.\\xa0 Dhè.efect.ov.dh\\nìz simpl riform\\xa0\\nìztumeik English wàns.àgen.clo\\'s.tu\\n90% fonemic ænd\\nconsistent.\\xa0\\xa0 [à\\xa0\\nè\\xa0 ì\\xa0 ò\\xa0 ù æ]\\nThe addition of markers and\\ndiacritics may remove ambiguous pronunciation but it makes the text more\\ndifficult to read and less like traditional spelling.\\xa0 Compare the\\nabove to \"...iz won ov sevral world english proposals\\ntu restor alfabetic spelling.\"\\xa0 A little\\nambiguity [up to 2 related sounds per letter] is OK.\\n1.\\nToday\\'s English orthography [TO] is only 40% alphabetic.\\n2.\\nThe Saxon alphabet can be restored - making English over 80% alphabetic\\n3.\\nSPanglic [Saxon] is a spelling pronunciation reform not a phonemic reform\\n4.\\nSPanglic restores the Saxon alphabet and uses it to sound out the letters\\nin words.\\n5.\\nSPanglic does not sound like any particular English dialect but can be\\nunderstood.\\n6.\\nSPanglic corrects for Vowel shifts - or pronunciation distortions\\n7.\\nSPanglic is based on International pronunciation and international spelling\\nconventions\\nIn the\\nearly 1800\\'s, Noah\\nWebster remarked, \"Letters, the\\nmost useful invention that ever blessed mankind, lose a part of their value\\nby no longer being representatives of the sounds orignally annexed to them.\"\\xa0\\nThe effect is, \"to destroy the benefits of the alphabet.\"\\nWebster was aware that there\\nwas a time in English history when the language had a functional alphabet.\\nTenth century clerics devised a Latin based alphabet for English that made\\nit possible to spell words as they were pronounced and pronounce words\\nas they were spelled. This and ease of learning are the principle benefits\\nof alphabetical writing systems.\\nCould the restoration of\\nthe benefits of the alphabet be as simple as restoring the Saxon alphabet?\\xa0\\nCould the usefulness of the alphabet be restored by restoring the sounds\\noriginally annexed to the letters?\\xa0 That is the guarded conclusion\\nof at least two spelling-pronunciation reform proposals:\\xa0 See Englisc.\\xa0\\nA spelling pronunciation\\n[SP] reform differs from a phonemic\\nreform.\\xa0 Instead of referencing a particular dialect, the reference\\nis to traditional spelling.\\xa0 This kind of reform minimizes the number\\nof words needing to be respelled by creating an artificial dialect that\\ncan be understood by all English speakers.\\xa0 Some have argued that\\nyou cant restore a sound based alphabet because English has too many dialects.\\nThe SP proposal gets around this objection.\\xa0 Only words that cannot\\nbe understood when pronounced according to the Saxon alphabet are respelled.\\xa0\\nThe proposed reform brings pronunciation more in line with international\\npronunciaton and spelling more in line with international spelling.\\xa0\\nOn the negative side, pronouncing\\nwords as they are spelled results in a strage dialect of English.\\xa0\\nTo some it sounds something like a Middle English dialect.\\xa0 To others,\\nit sounds like a foreigner trying to pronounce English words according\\nto international conventions.\\xa0 For example: IDEA = ee-dey-ah rather\\nthan ai-dih-uh.\\nThe proposed reform results\\nin at least three scripts or notations: a slightly modified traditional\\nspelling (26 letters), a broad phonemic spelling (merged phonemes- 26 letters,\\n26 symbols) and a narrow phonemic spelling (26-33 letters, 34 symbols).\\xa0\\nThe eight new symbols that can be used to extend the character set and\\nreduce ambigity are all ASCII\\nbased [ae, \\'a\\xa0 \\'e\\xa0 \\'i\\xa0 \\'o\\xa0 \\'u\\xa0 \\'r\\xa0 o\\' \\'v\\n] and available in Latin 1 [æ\\nà\\xa0 è\\xa0 ì\\xa0 ò\\xa0 ù\\xa0\\n\\'r\\xa0 ó\\xa0 û ].\\xa0\\nSPanglic looks something\\nlike English written in a Spanish orthography.\\xa0 One could justifiably\\ncall it restored English alphabetic spelling or\\nNew Saxon because it is nearly identical to the system used\\nwhen English speakers first adopted the Roman alphabet.\\nThe great vowel distortion\\naka\\nThe Great Vowel Shift\\nAround 1400 (the end of the\\nMiddle English period), many words started to be pronounced in untraditional\\nand\\xa0 unalphabetic ways.\\xa0 Linguists often call the change systematic\\nand natural but not all words were affected.\\xa0 Some words retained\\ntheir traditional pronounciation while others changed.\\xa0 AS hus\\nhad already been respelled hous by Norman French scribes. Around\\n1400 the pronunciation changed from /hu:s/\\xa0 to /haus/\\n/hæ+ùs/\\xa0\\xa0 In SPanglic, house /haw-uus/ does\\nnot have to be respelled since its Saxon pronunciation is close enough.\\xa0\\nPhonemic Saxon would spell the word the same as IPA, haus.\\nBy the mid 9th century, England\\nhad a near perfect sound based spelling system known as West\\nSaxon Standard. Old English (850-1060 AD)\\nwas written in a church Latin inspired alphabet in a way consistent with\\nhow it was pronounced in the 10th century. Anglo\\nSaxon\\nused grapheme-phoneme correspondences almost identical to those shown in\\nthe Spanglish vowel table. In the augmented Latin alphabet, each Roman\\nletter was associated with a specific sound. The six vowel letters were\\nassociated with two sounds - the long and short version of the vowel. The\\nletter [a] referenced the Roman /a/ sound [ah].\\xa0 A new letter had\\nto be added to reference the Saxon ash.\\xa0 The ash [æ]*\\nprovided a way to reference the sound that differed from the Italian A.\\xa0\\nWhen England adopted the\\nRoman alphabet (8th century),\\nthey also adopted the sounds associated with the letters.\\xa0 To make\\na 5 vowel alphabet work with a language that had 12 vowels, the Latin alphabet\\nwas augmented.\\xa0 The West Saxon standard (ca. 900\\nAD) added several runic letters for the missing sounds.\\xa0\\xa0\\nThe West Saxon alphabet had 6 vowel letters, each letter had a long and\\nshort pronunciation as shown below:\\n\\xa0\\n\\n\\n\\n\\nThe\\nWest Saxon Standard - Englisc\\n\\n\\xa0\\n\\na\\n\\n\\næ\\n\\n\\ne\\n\\n\\ni\\n\\n\\no\\n\\n\\nu\\n\\n\\n\\'r\\nèr\\n\\n\\n\\n\\nlong\\n\\n\\nah\\n\\n\\n*\\n\\n\\neh-ey\\n\\n\\neel\\n\\n\\nawe-oh\\n\\n\\nhoop\\n\\n\\nher\\n\\n\\n\\n\\nshort\\n\\n\\nago\\n\\n\\næsh\\n\\n\\nej-edge\\n\\n\\nill\\n\\n\\nawe*\\n\\n\\nhook\\n\\n\\nothèr\\n\\n\\n\\n\\naccents\\n\\n\\nà\\n\\n\\n*\\n\\n\\nè\\n\\n\\n\\xa0ì\\n\\n\\nò\\nó\\n\\n\\nù\\n\\n\\nà\\nè\\n\\n\\n\\n\\nWhy make such big changes as A=ah,\\nI=eel, O=awe, U=ooze?\\xa0 The main reason is that this set of correspondences\\nallows learners to use spelling pronunciation.\\xa0 Pronouncing\\nall A\\'s as ah produces understandable results.\\xa0 The alt. of\\npronouncing\\nHa as Hay doesn\\'t quite work.\\xa0 Pronouncing\\nall O\\'s as\\nawe unless in the terminal position also works better\\nthan other alternatives.\\xa0\\n\\n\\n.\\nBy the 10th century, English\\nhad a highly consistent spelling system known as the West Saxon standard.\\xa0\\nThe sounds that corresponded to the letters A\\nE I O U were ah, eh,\\nee, awe, oo.\\xa0\\nIf these letter sound correspondences were fully restored, English could\\nonce again have a functioning alphabet.\\xa0\\n\\nSpanglish does not look\\nlike\\nOld English\\nbecause English words are not pronounced the same as they were before the\\nNorman conquest (1066 AD).\\xa0\\nSpanglish, however, is built from the same basic set of grapheme-phoneme\\ncorrespondences as Old English.\\xa0 This is what is being restored.\\xa0\\n\\xa0\\n\\n\\n\\n\\n\\nSome\\nOld English\\nSpellings and Pronunciations\\n\\n\\nOld\\nSaxon\\n\\n\\nOE\\n& ME\\nPron.\\n\\n\\nME\\n/ Std\\nSpelling\\n\\n\\n\\n\\ndai\\n\\n\\ndie/dai/\\n\\n\\nday /dei/\\n\\n\\n\\n\\ndæg\\n\\n\\nda?\\n\\n\\nday /dei/\\n\\n\\n\\n\\nwæd\\n\\n\\nwæd?\\n\\n\\nwater /wotr/\\n\\n\\n\\n\\nwæpn\\n\\n\\nwæpn\\n\\n\\nweapon\\n\\n\\n\\n\\nheeth\\n\\n\\nheth-\\'\\n\\n\\nheeth/hi:th/\\n\\n\\n\\n\\nsweete\\n\\n\\nsway-t\\'\\n\\n\\nsweet/swi:t/\\n\\n\\n\\n\\nsee\\n\\n\\nsay/zay\\n\\n\\nsea\\n/si:/\\n\\n\\n\\n\\nol\\n\\n\\nol\\n\\n\\nall\\n/ol/\\n\\n\\n\\n\\ngood\\n\\n\\ngode\\n\\n\\ngood /gud/\\n\\n\\n\\n\\nlawe\\n\\n\\nlau-w\\'\\n\\n\\nlaw /lo:/\\n\\n\\n\\n\\nAugust\\n\\n\\nau=ah\\n\\n\\naug /o:g/\\n\\n\\n\\n\\ntime\\n\\n\\nteam\\nti:m\\n\\n\\ntime /taim/\\n\\n\\n\\n\\ndo\\n\\n\\ndoe\\ndou\\n\\n\\ndo /du:/\\n\\n\\n\\n\\nto\\n\\n\\nto\\xa0\\n/taw?/\\n\\n\\nto /tu:/\\n\\n\\n\\n\\nis\\n\\n\\ni:s\\n\\n\\nice /ais:/\\n\\n\\n\\n\\npund\\n\\n\\npu:nd\\n\\n\\npound/pau/\\n\\n\\n\\n\\nhus\\n\\n\\nhu:s\\n\\n\\nhouse\\n/au/\\n\\n\\n\\n\\nhlud\\n\\n\\nlu:d\\n\\n\\nloud\\n/laud/\\n\\n\\n\\nThomas Cable, A History\\nof the English Language, R. Burchfield, The English Language, Oxford, 1985,\\nL.S. Smith\\nZachrisson, R.E. Pronunciation\\nof English Vowels, 1913; Ellis, A. 1880\\naiff Soundfiles heeth\\n\\n\\n\\n\\nInstead of retaining the\\nold correspondence table, the English speaking world has retained many\\narchaic spellings. By doing so, the relationship between spelling and pronunciation\\nhas been all but lost. English went from being over 90% phonemic to being\\nless than 40% phonemic. [proof].\\xa0\\nThe ability to spell a word is directly related to its phonemic accuracy\\n- especially for children during the first 4 years of schooling. [K.\\nSpencer]\\nRestoring the alphabet makes\\nspelling-pronunciation\\npossible.\\xa0 Instead of 20 sounds per vowel letter there would be only\\ntwo.\\xa0 Spelling pronunciation does not exactly duplicate any particular\\ndialect. In some cases the dialect resembles Middle English because this\\nis the way some words are spelled. It slightly distorts the pronunciation\\nof some words, [for example -- ox becomes aux] but the distortion or mispronunciation\\nis not so great as to prevent understanding. Many historical spellings\\nare from Middle English.\\xa0 When these words are not respelled,\\xa0\\ntheir spelling pronunciation approximates Middle English. [time\\n/ti:m\\' /\\nis pronounced team]\\nThere are some historical\\nspellings that do not make sense or that would make more sense if misleading\\nletter[s] were removed. For example, [gh] is no longer pronounced in any\\ncontemporary dialect of English.\\xa0 Thus, the spelling pronunciation\\nof <through> would be nearly unintelligible.\\xa0 The spelling pronunciation\\nof [thru], however, exactly duplicates current pronunciation.\\xa0\\nThere are 12 pure vowel phonemes\\nin present day English. A complete alphabet would need 12 vowel letters.\\nIn addition to the 12 uncombined sounds, there are at least 12 vowel combinations.\\nUsing the Saxon, the combination of sounds is represented by the combination\\nof letters.\\xa0 ai = ah + ee [the vowel in\\neye]\\nThe Spanglish proposal is\\nto limit the sounds associated with vowel letters to two and to substitute\\nnew letters in traditional spellings [TS] only when the spelling pronunciation\\nof TS cannot be understood. This approach has some fuzzy edges or boundaries\\nsince the degree to which a spelling pronunciation of a word can be understood\\nin context varies.\\xa0\\nAn alphabet is a consistent\\nset of relations between the way a word is pronounced and the way it is\\nspelled. In other words, in an alphabetic system, words that rhyme are\\nspelled the same. Spanglish does a much better job of attaining this alphabetic\\nideal than TO.\\xa0\\nAcross all English dialects,\\nthere are two ways to vocalize the word DAY.\\xa0 The vocalic sound in\\nDAY can therefore be spelled two ways ay /ai/ or ey /ei/\\xa0\\n[ah-ee or eh-ee].\\xa0 Both Spanish and English orthographies prefer a\\ny\\nin the terminal position to mark a syllable boundary . Since the pronunciation\\nof the traditional spelling, DAY /dah-ee/,\\nis understandable, Spanglish does not require respelling.\\xa0 The only\\ntime the respelling of [day] would be required in a broad romic notation\\nwould be in a dictionary pronunciation guide where the objective was to\\nrepresent a particular dialect such as General American [GA].\\nday\\xa0\\n/dey/ [GA]\\nNotice that in Spanglish\\neach letter is pronounced and the same notation is used for both quasi\\ntraditional spelling and for phonemic spelling.\\xa0 Except for H, there\\nare no silent letters in Spanish or Spanglish.\\n\\n\\nThe development of number\\nwords illustrates how pronunciation changes over time.\\xa0 According\\nto the linguists, all of these pronunciations developed from a common pronunciation.\\xa0\\nAround 500 A.D., Anglo Saxon, Dutch, Danish, and German number pronunciations\\nwere almost identical.\\xa0 The word for [one] was ahn or ehn\\n(an, en).\\xa0 If the word [one] is pronounced as spelled, aw-nuh,\\nit is still close to the Anglo Saxon word [an]. The French influenced silent\\ne was added around 1200 when the Spelling was typically [onne] so the pronunciation\\nprobably remained about the same: /ahn/.\\xa0\\nIndo\\nEuropean Number Words - At one time they were all pronounced\\nthe same...\\n\\n\\n\\xa0\\n\\n1\\n\\n\\n2\\n\\n\\n3\\n\\n\\n4\\n\\n\\n5\\n\\n\\n6\\n\\n\\n7\\n\\n\\n8\\n\\n\\n\\nEnglish\\nonne-one\\ntwo\\nthree\\nfour\\nfive\\nsix\\nseven\\neight\\n\\n\\nSPanglic\\nwan\\ntwu\\nthri\\nfor-fowr\\nfaiv\\nsix\\nseven\\neit\\n\\n\\nAnglo Sax\\nan [ahn]\\ntweene\\ntwegan\\nthri\\ntrie\\nfoewer\\nfeower\\nfif\\nsehs\\nsiex\\nsibun\\nsiofan\\nocht\\neahta\\n\\n\\nGothic\\n\\xa0\\ntwa\\n\\xa0\\nfidwer\\nfimf\\nsehs\\nsibun ?\\n\\xa0\\n\\n\\nGerman\\nen-ein\\nzwei\\ndrei\\nvier\\nfunf\\nsechs\\nsieben\\nacht\\n\\n\\nLatin\\nuno\\nduo\\n\\xa0\\nquatro\\nquinque\\nsex\\nseptem\\nocto\\n\\n\\nGreek\\nhen\\n\\xa0\\ntri?\\ntettares?\\npente\\nhex\\nhepta\\nokto\\n\\n\\n*the W does not have\\nthe \\' oo-WAW\\n\\' sound except at the beginning of a word\\nor syllable.\\xa0 In all other positions it has the short u /u/\\nsound.\\xa0 Old English TWEENE/TWIN\\nwas\\nprobably pronounced /tui:n/\\xa0 on-line source\\nToday over 600 million people\\nin the world speak a Latin based language. More than double that number\\nuse a Latin based alphabet with Latin letter sound values.\\xa0\\nIn Latin, a vowel letter has one sound which is also the letter\\'s name.\\xa0\\nExcept for the vowels the letter names are not all that much different\\nfrom English.\\xa0 English speakers have disroted or shifted the original\\nLatin vowel letter soundsThe vowel sounds.\\xa0\\n\\n\\nSeveral graphic images are imported\\nbelow:\\xa0 These may not show up.\\nTo\\nmaintain the alphabet, when the pronunciation of words change, the spelling\\nhas to change.\\xa0\\nIn\\nEnglish, most of the pronunciation shifts affected the vowels.\\xa0 Words\\nsuch as fif /feefv/ began to be pronounced /faiv/.\\xa0 One of\\nthe middle English spellings for /feefv/ or /fi:v/\\nwas FIVE.\\xa0\\nAfter the 14th century vowel shift, this old spelling became associated\\nwith the new pronunciation /faiv/.\\xa0\\nIn a related example, ice\\nused to be spelled is and pronounced /i:s/\\xa0 [ees].\\xa0 After\\nthe Battle of Hastings (1066) scribes spent most of their time writing\\nNorman French which unlike Saxon, had a highly illogical spelling system.\\xa0\\nThese scribes tended to write English in a French way [Scragg, 1974]. is\\nstarted to be spelled ice around 1200 A.D.\\xa0\\nLater, during the great vowel shift\\nthe pronunciation of the word changed to /ais/.\\xa0 Instead of respelling\\nthe word again to reflect this change in pronunciation, the spelling remained\\nthe same. English became populated with words that were spelled one way\\nan pronounced another. England standardized their word spelling around\\n1755 with the publication of the first popular dictionary.\\xa0 No attempt\\nwas made to standardize below the word level so FIVE\\nbecame one of many ways to spell the vowel in /faiv/.\\xa0 Before this\\ntime some people probably spelled it FYV.\\xa0\\nThis vowel spelling stuck with sky, fly, and my but\\nnot with five and ice.\\nThe great vowel shift took\\nplace around 1400 when the six long vowels began to change their values\\nin a systematic way.\\xa0 Chaucer would have pronounced the middle vowel\\nin time like that in modern team.\\xa0 see would\\nhave sounded like say, fame like farm without the\\nR, so like saw, and do like doe, and now\\nlike naw-oo.\\xa0 The great vowel shift resulted in a major barrier\\nto intelligibility between middle and modern English.\\nAs illustrated in the chart\\nof number words, changes in pronunciation over a 200 year period are not\\nunusual.\\xa0 Other languages have coped with it and retained their alphabet\\nby making corresponding changes in their spelling.\\xa0 This might have\\nbeen relatively easy in the early 1800s when Benjamin Franklin and Noah\\nWebster recommended that we follow the reforms that were taking place in\\nthe orthographies of other European languages.\\xa0 Franklin and Webster\\nthought this was the only chance we had of preserving what was left of\\nour alphabet.\\nSPanglic or Spelling Pronunciation Anglic uses\\nthe Saxon alphabet and only respells traditionally spelled words when they\\ncannot be pronounced.\\xa0 The spelling pronunciation does not have to\\nbe perfect, just close enough to be understood.\\xa0\\nUnfamiliar words would be spelled the way they\\nwere pronounced rather than historically.\\xa0 As a result, students would\\nstill mispell some words.\\xa0 The difference is that most students would\\nspell unfamiliar words the same way.\\xa0 When students use invented spellings,\\nthere can be 14 or more different spellings.\\nSPanglic is systematic but not very phonemic since\\nthere are usually two sounds associated with a letter.\\xa0 To eliminate\\nthis ambiguity, diacritics can be added.\\xa0 This makes it possible to\\nuse basically the same notation for a pronunciation guide.\\nSince 80% of the words are spelled historically\\nand every word can be pronounced, SPanglic is very easy to read aloud.\\xa0\\nThe dialect may be a little odd, but completely understandable.\\xa0 Known\\nwords can be and probably will be converted to their regional pronunciation\\nas with TES.\\nPhonemic Saxon, a related notation, is just another\\nanalog or isomorph of IPA [the International Phonetic Alphabet]\\xa0 -\\nthere is a one to one correspondence between the two notations.\\xa0 As\\na result Phonemic Saxon respells 60% of the words in the dictionary.\\xa0\\nPhonemic Saxon would be used as a pronunciation guide and all pronunciation\\nguides respell at least 60% of the words.\\nMost people using this parallel notation would\\nprobably read in SPanglic and write in Saxon.\\xa0 It is relatively easy\\nto use a phonemic notation to spell a word as you pronounce it.\\nThere are three more pages\\non\\xa0click\\non the button to go to the illustrated Spanglish page\\nPlease send questions to\\nSteve\\nBett\\n\\n\\nhttp://victorian.fortunecity.com/vangogh/555/Spell/sitemap-l.html\\nwordlist\\nOld\\nEnglish Dictionary\\xa0 http://www.mun.ca/Ansaxdat/vocab/wordlist.html\\nhat=hot, hus=house, ham=home,\\xa0\\n\\xa0\\n\\xa0\\n\\n\\nVisit these related pages on applied linguistics and rationalized spelling\\n\\nnU @lfabets for EGliS\\nx simplifYd speliG sOsYeti\\n\\namerican litRasi kWnsL\\xa0\\n\\xa0simplifYd speliG E-group\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSubscribe to saundspel\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPowered by www.egroups.com\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nsitemap-L\\xa0\\n\\nspelling\\nring\\xa0\\xa0 this\\nsite\\xa0\\xa0 phonetic\\nalphabet-IPA\\nhttp://www.egroups.com/files/saundspel/saxon-spanglish.html\\n\\n\\n\\n\\n\\n\\nweb hosting • domain names • web designonline games • online dating  • long distancedigital cameras  • advertising online\\n\\n\\n\\n\\n\\n',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPRESS RELEASE\\n\\n\\n\\n\\n\\n\\nPRESS RELEASE\\n\\xa0\\nFor Immediate Release\\n\\xa0\\n\\xa0\\nElsevier Science widens the scope of Scirus\\n\\xa0\\nAmsterdam, September 21, 2001 - Elsevier Science, the\\nleading international publisher of scientific information, announced it has\\nwidened the scope of its Scirus search engine to include searches for patents\\nfrom the databases of the United States Patent Trademark Office (USPTO). This\\nextra functionality is part of a new release of the powerful science specific\\nsearch tool. The new version contains significant enhancements aimed at\\nproviding Scirus users with even greater search options. These include a sort\\nby date function, extra searchable subject areas and a tool for finding\\nscientific conferences and abstracts. \\n\\xa0\\nThanks to a link up with the USPTO, Scirus will now cover more\\nthan 650,000 patents.\\nThe new release of the search engine contains a tool for\\nsearching the USPTO databases of granted patents and published patent\\napplications. Information can be gathered on US patents issued since 1976,\\nincluding bibliographic data, abstracts, and invention and claims details. \\n\\xa0\\nScirus users can now receive results sorted by date and have\\nfour more subject areas to search for information on \\x96 psychology, language and\\nlinguistics, law and sociology. They can also use Scirus to search for\\nscience-related conferences and abstracts. In addition, the \\x93more like this\\x94\\ntool has now been set up to provide narrower and more focused results.\\nImprovements to the email results tool and the FAQ\\x92s section are further useful\\nenhancements to the search engine.\\n\\xa0\\nTo coincide with the new version of Scirus, Elsevier has\\nlaunched an affiliate programme, offering web site owners the chance to add the\\nScirus search tool to their home pages for free. In exchange for adding Scirus\\nto their site, web site owners get a tailor-made tool for meeting the research\\nneeds of their visitors, enhancing user experience and encouraging repeat\\nvisits.\\n\\xa0\\nAbout Scirus\\n\\xa0\\nScirus is the most comprehensive science specific search\\nengine available on the Internet. Driven by the latest in search engine\\ntechnology, it covers more than 60 million science-related pages and can\\npinpoint precise scientific information that other search engines can not\\nreach, including pdf files and peer reviewed articles. It was launched in April\\n2001 by Elsevier Science (www.elsevier.com),\\nthe world\\x92s largest scientific, technical and medical information provider, and\\nis part of the ScienceDirect® family of products. \\n\\xa0\\n# # #\\n\\xa0\\nContact:\\xa0 \\n\\xa0\\nSandra\\nde Gelder\\nElsevier\\nScience\\n+31\\n(0)20 485 3851\\ns.gelder@elsevier.nl\\n\\n\\n\\n',\n",
       " '\\n\\n\\nMAS: Modula-2 Algebra System\\n\\n\\nMAS: Modula-2 Algebra System\\n\\n                   Computer Algebra Group \\n                University of Passau, Germany \\n\\n                   Version 1.01, March 1998 \\ncag.fmi.uni-passau.de\\n\\n\\n\\nAbstract\\n\\nMAS (Modula-2 Algebra System) is an experimental computer algebra\\nsystem, developed at the University of Passau. MAS combines imperative\\nprogramming facilities with algebraic specification capabilities for\\ndesign and study of algebraic algorithms. It contains a large library\\nof implemented Groebner basis algorithms for nearly all algebraic\\nstructures where such methods exist. MAS further includes algorithms\\nfor real quantifier elimination, parametric real root counting, and\\nfor computing in (noncommutative) polynomial rings.\\n\\n\\nDistribution files\\n\\nThe distribution is by means of gnuzip\\'ed tar files. Form the\\ndistribution files you need at least an executable, the tutorial and\\nthe examples.\\nThe files can be found at \\n\\nalice.fmi.uni-passau.de. \\n\\n\\n\\nIf you pick the source code we recommend to pick also the definition\\nmodule and indexes document along with it. To compile the source code\\nyou will also need the Modula-2 to C translator \"mtc\", the \"reuse\"\\nlibrary, gnumake, and a C-compiler (preferably gcc).  Further we\\nrecommend the GNU readline library and the kpathsea library.\\n\\n\\nThe following files are available\\n\\n\\n- executables\\n\\n  mas-hppa1.1-hp-hpux9.03-1.00.tar.gz     HP 9000, running HP-UX\\n  mas-i386-unknown-os2-1.00.tar.gz        Intel PC, running OS/2\\n  mas-i486-unknown-linux-1.00.tar.gz      Intel PC, running LINUX\\n  mas-mab-next-nextstep3-1.00.tar.gz      NextStep\\n  mas-rs6000-ibm-aix3.2.5-1.00.tar.gz     RS/6000, running AIX 3 \\n  mas-sparc-sun-sunos4.1.3C-1.00.tar.gz   Sun Sparc, running SunOS\\n\\n\\n- documentation:\\n\\n  mastut.tar.gz   MAS tutorial and interactive users guide in LaTeX.\\n  mastut.ps.gz    as PostScript\\n\\n  masdef.tar.gz   MAS Modula-2 definition modules, 3 indexes \\n                  and specifications in LaTeX.\\n  masdef.ps.gz    as PostScript\\n\\n\\n- examples and test files:\\n\\n  masexam.tar.gz  examples, help files, test files. \\n\\n\\n- Modula-2 source code:\\n\\n  massrc.tar.gz\\n\\nInstallation\\nUnpack\\n\\nTo install MAS unpack the respective files. E.g. with \\n\\n   gnutar xfvz masexam.tar.gz \\n\\nor \\n\\n   gzip -d -c masexam.tar.gz | tar -cvf -\\n\\n\\nin some directory. The files will unpack into the following \\nsubdirectories:\\n\\nmas/doc \\n       /mastut          Tutorial and Users Guide.\\n       /masdef          Definitions and indexes document.\\n       /massys          System document.\\n   /<machine>           Executables in respective machine directory \\n       mas              Unix executable. \\n       mas.exe          OS2 executable, requires /dll.\\n       emx.exe mas.out  DOS extender and executable. \\n   /exam                Examples *.in\\n                        Copying information   \\n        .masrc          initialization file.\\n        mas.ini         initialization file.\\n        helpup.mas      Help initialization file.\\n        testall.mas     Driver file for /test directory.\\n        /help           Comments and module information.\\n        /spec           Example specifications.\\n        /test           Test files.\\n   /dll                 emx dyn. link libs for OS2.\\n   /src                 Modula-2 source code \\n       /maskern         System dependent files, memory, IO, ... \\n       /maslisp         LISP interpreter, parsers, ...\\n       /masmain         Main program, interfaces, ...\\n       /masarith        Basic Arithmetic, integer, rational, ...\\n       /maspoly         Polynomial systems, recursive, distributive, ...\\n       /masring         Ideals, Groebner bases, algeb. geometry, ...\\n       /masmodul        Linear algebra, diophantine equations, syzygies, ...\\n       /masnc           Non-commutative solvable polynomial rings, ...\\n       /sacring         Polynomial factorization, real roots, gcd, res, ...\\n       /masroot         Real root counting, ...\\n       /maslog          Logic formuals, Real Quantifier Elimination, ...  \\n       /masdom          Domain coefficients, comprehensive G bases, ...\\n       /masib           Involutive bases, ...\\n\\n\\n\\nFile naming conventions: \\n\\n        *.md            Modula-2 definition modules.\\n        *.mi            Modula-2 implementation modules.\\n\\t*.mip\\t\\tModula-2 implementation modules with cpp-statements.\\n        *.h             C header files.\\n        *.c             C code files.\\n        *.o             object code files.\\n        *.a             library archives.\\n\\n        *.ini           MAS initialization files (obsolete).\\n        *.hlp           MAS help information.\\n        *.in            MAS input files (obsolete).\\n        *.mas           MAS input files.\\n        *.out           MAS output files.\\n\\n\\nTest\\n\\nTest the installation with the following command \\n\\n             [path]mas -m 4000 -E -e -o test-all\\n\\n\\nfrom the /exam directory. This produces a file \\'test-all.out\\' which\\nyou can compare to the supplied \\'test-all.orig\\' file. The warnings are\\nintentional and only lines with timings should \"diff\"er.\\n\\n\\nStart - Stop \\n\\nAdd the mas/bin directory to the PATH or use the complete pathname\\nin the following examples. \\n\\n\\n   - start         \\'mas\\' or \\'mas.exe\\' or \\'emx mas.out\\'  \\n\\n   - banner        \\'This is MAS the Modula-2 Algebra System, Version 1.xxx.\\'\\n\\n   - system prompt \\'MAS: \\' \\n\\n   - system answer \\'ANS: \\'\\n\\n   - input (e.g.)  \\'a:=2*3.\\' A statement is terminated by period \\'.\\' \\n \\n   - help with     \\'help.\\' or \\'help(name).\\' or \\'help(name,Loaded).\\'\\n  \\n   - interupt      ^C     CNTRL-C \\n \\n   - leave with    \\'EXIT.\\' or \\'exit.\\' or \\'quit.\\'\\n\\n\\nPath and Compile\\n\\n\\nOn all systems add the mas/<machine> directory to the PATH or use the\\ncomplete pathname to call the MAS executables.\\n\\nOn OS2 systems also add the mas/dll directory to the LIBPATH and\\nreboot or use your existing emx dlls.\\n\\nTo compile MAS unpack the source code and create a directory\\n\"<machine>\" for your machine type. From the directory mas/<machine>\\nexecute ../configure to generate the Makefile, then execute gnumake to\\ncompile a mas executable.\\n\\nFurther details can be found in the readme file accompanying the\\nsource code.\\n\\n\\nNotes\\n\\n    Some help facilities need an \\'awk\\' program. \\n\\n    The Makefiles for the source code may need an \\n        \\'awk\\' or \\'sed\\' program or other unix utilities.  \\n\\nRelease and Change Notes\\n\\nMajor mathematical library changes of the \\nversion 1.0 (Oktober 1996) are: \\n\\n arbitrary domain polynomial system extended to a generic\\n       Gröbner base package.\\n\\n   added an optimized Gröbner base package (including the\\n       ``sugar\\'\\'-method) by C. Rose,\\n\\n   added a package to compute factorized Gröbner bases by\\n       J. Pfeil,\\n\\n   Improved comprehensive Gröbner Basis algorithms using\\n       factorization, condition evaluation and case elimination by\\n       M. Pesch,\\n\\n   a package for involutive bases by R. Grosse-Gehling,\\n\\n   added a package for invariant polynomials by M. Goebel,\\n\\n   added a package for counting real roots based on Hermites\\n       method by F. Lippold,\\n\\n   added a logic formula representation with simplification\\n       package and real quantifier elimination package by A. Dolzmann,\\n\\n   the linear algebra package of MAS has been improved.\\n\\n\\n\\n\\nMajor system changes of the current version 1.0 are:\\n\\n\\n MAS language accepts small letter key words and braces {} to\\n       denote list expressions.\\n\\n   Distribution now uses GNU autoconf for easy compilation.\\n       \\n   GNU readline for easier command line editing.\\n\\n   Using Kpathsearch Library from K. Berry.  \\n\\n   Improved batch processing capabilities. \\n\\n   Improved error handling and user signal processing to examine\\n       long running computations.\\n\\n   Generic garbage collection support for most architectures.\\n\\n\\n\\nThe major system changes between release 0.6 and 0.7 (April 1993) are:\\n\\n Distribution based on Modula-2 to C translator and \\n       a C distribution which will work on \\'most\\' workstations. \\n\\n   New support for PC 386 and higher (OS2 2.0 and higher) with \\n       emx dll runtime libraries. \\n      \\n   New support for PC 386 and higher (DOS 5.0) with \\n       emx DOS extender (10 times faster).\\n\\n   Dropped support for the Atari, Amiga and PC XT up to 286 versions.\\n       That means, that we do no more distribute executables for these \\n       systems, but if you have the maskern(el) you can get the \\n       new source code (except maskern) and compile it on your system. \\n\\n   The HELP and help command has been changed to provide \\n       name ranges and more information from the procedure comments. \\n\\n\\nMajor mathematical library changes between release 0.6 and 0.7 are:\\n\\n added comprehensive Groebner base package by E. Schoenfeld,\\n\\n   arbitrary domain polynomial system implemented, \\n\\n   added several new basic arithmetic packages: \\n       complex numbers, quaternion numbers, octonion numbers, finite fields,\\n\\n   added package for computation in non-commutative polynomial \\n       rings of solvable type: *-product, left Groebner base, \\n       two-sided Groebner base, elements in the center,\\n\\n   added a package for the computation of generators for the module \\n       of syzygies of systems of homogeneous polynomial equations and \\n       Groebner bases for modules over polynomial rings \\n       (also available for solvable polynomial rings) by J. Phillip,\\n\\n   added universal Groebner base package by T. Belkahia,\\n\\n   added d-Groebner base and e-Groebner base packages  \\n       for Groebner bases over the integers and univariate rational \\n       polynomial rings by W. Mark. \\n\\n\\nThe major changes between release 0.3 and 0.6 (March 1991) are:\\n\\n added language extensions for specification capabilities,\\n\\n   added a parser for the ALDES language and \\n       possibility for interpretation of ALDES programs,\\n\\n   added a linear algebra package,\\n\\n   added an interface between the MAS language \\n       and the distributive polynomial system,\\n\\n   improved symbol handling by hash tables combined with balanced trees,\\n\\n   EMS support for IBM PC implementations.\\n\\n\\nThe minor changes between release 0.3 and 0.6 are:\\n\\n PRAGMA construct for the state definition of the MAS executable.\\n\\n   Overloading of MAS arithmetical operators by generic function names.\\n\\n   Typed string constants in MAS expressions.\\n\\n   VAR parameters in MAS procedure declarations in ALDES style.\\n\\n   Static scope analysis by the parser.\\n\\n   Explicit stack overflow check since not all compilers \\n       handled stack overflow correctly.\\n\\n\\n\\nRelease notes for Version 0.3 (November 1989): \\n\\n The MAS parser has been changed for better Modula-2 \\n       compatibility. \\n\\n   MAS LISP has been made more robust against incorrect user input.\\n\\n   The MAS main program has been enhanced to recognize \\n       the following command line parameters: \\n\\n                -m number-of-KB     \\n                -f data-set-name\\n\\n\\n the memory option \\'-m\\' gives the number of Kilo-Byte\\n          storage, requested from the operating system. \\n\\n      the file name option \\'-f\\' can be used to overwrite the\\n          default file name \\'MAS.INI\\' during startup. With this\\n          option MAS can be run in batch mode if the EXIT \\n          statement is contained in the data set.\\n     \\n\\n\\n\\nIntroduction to MAS\\n\\nStarting point for the development of MAS was the requirement for a\\ncomputer algebra system with an up to date language and design which\\nmakes the existing ALDES / SAC-2 algorithm libraries available.  At this\\ntime there have been about 650 algorithms in ALDES / SAC-2 and in\\naddition I had 450 algorithms developed on top of ALDES / SAC-2.  The\\ntension of reusing existing software in an interactive environment with\\nspecification capabilities contributes most to the evolution of MAS. \\n\\n\\nThe resulting view of the software has many similarities with the model\\ntheoretic view of algebra.  The abstract specification capabilities are\\nrealized in a way that an interpretation in an example structure (a\\nmodel) can be denoted.  This means that is is not only possible to\\ncompute in term models modulo some congruence relation, but it is\\nmoreover possible to exploit an fast interpretation in some optimized\\n(or just existing) piece of software. \\n\\n\\nOverview\\n\\nMAS is an experimental computer algebra system combining imperative\\nprogramming facilities with algebraic specification capabilities for\\ndesign and study of algebraic algorithms.  The goal of the MAS system is\\nto provide:\\n\\n\\n an interactive computer algebra system\\n comprehensive algorithm libraries, including\\n     the ALDES/SAC-2 system [Collins 82],\\n a familiar program development system\\n     with an efficient compiler,\\n an algebraic specification component for  \\n     data structure and algorithm design    \\n algorithm documentation open to the users.\\n\\n\\nKey attributes of the MAS system are:\\n\\n\\n portability (it is portable to a\\n     computer during a student exercise `Praktikum\\'),\\n     machine dependencies isolated in a small kernel,\\n extendability (it is possible to add\\n     and interface to external algorithm libraries),\\n     open system architecture,\\n     transparent low level facilities: \\n storage management (garbage collection\\n        is provided without user cooperation),\\n      stable error handling (no system break down on\\n        misspelled expressions and runtime exceptions),\\n      input / output with streams (no changes are \\n        required to existing libraries to redirect I/O).\\n effectivity (critical parts can be compiled and \\n      still be accessed interactively) \\n expressiveness (possiblitity to specify \\n      abstract algebraic concepts like rings or fields)\\n\\n\\nThe goals and attributes have been achieved by the following main design\\nconcepts:\\n\\nMAS replaces the ALDES language [Loos 76] and the FORTRAN implementation\\nsystem of SAC-2 by the Modula-2 language [Wirth 85].  Modula-2 is well\\nsuited for the development of large program libraries; the language is\\npowerful enough to implement all parts of a computer algebra system and\\nthe Modula-2 compilers have easy to use program development\\nenvironments. \\n\\nTo provide an interactive calculation system a LISP interpreter is\\nimplemented in Modula-2 with full access to the library modules.  For\\nbetter usability a Modula-2 like imperative (interaction) language was\\ndefined, including a type system and function overloading capabilities. \\nTo increase expressiveness high-level specification language constructs\\nhave been included together with conditional term rewriting\\ncapabilities.  They resemble facitilies known from algebraic\\nspecification languages like ASL [Wirsing 86]. \\n\\nFurther design issues are:\\n\\nMAS views mathematics in the sense of universal algebra and model theory\\nand is in some parts influenced by category theory.  In contrast to\\nother computer algebra systems (like Scratchpad II [Jenks 85]), the MAS\\nconcept provides a clean seperation of computer science and mathematical\\nconcepts.  The MAS language and its interpreter has no knowledge of\\nmathematics and mathematical objects; however it is capable to describe\\n(specify) and implement mathematical objects and to use libraries of\\nimplemented mathematical methods.  Further the imperative programming,\\nthe conditional rewriting and function overloading concepts are\\nseperated in a clean way. \\n\\nMAS includes the capability to join specifications and to rename sorts\\nand operations during import of specifications.  This allows both the\\nspecification of abstract objects (rings, fields), concrete objects\\n(integers, rational numbers) and concrete objects in terms of abstract\\nobjects (integers as a model of rings).  Specifications can be\\nparameterized in the sense of lambda-abstraction. \\n\\nThe semantics of a specification can be described either by\\nimplementations, axioms or models.  The implementation part describes\\n(imperative) procedures and data representations. \\n\\nThe axioms part describes conditional rewrite rules which define a\\nreduction relation on the term algebra generated by the sorts and\\noperations of the specification.  The semantics is therefor the class of\\nmodels of the term algebra modulo the (congruence) relation.  Currently\\nthere are no facilities to solve conditional equations. \\n\\nThe model part describes the association between abstract specifications\\n(like rings) and concrete specifications (like integers).  The semantics\\nis the interpretation of the (abstract) function in the model. \\nOperations in models can be compiled functions, user defined imperative\\nfunctions or term rewrite rules.  The function overloading capabilities\\nare realized by this concept.  Dynamic abstract objects like finite\\nfields can be handled by a descriptor concept. \\n\\nEvaluation of functional terms is as follows: If there is a model in\\nwhich the function has an interpretation and a condition on the\\nparameters is fulfilled, then the interpretation of the function in this\\nmodel is applied to the interpretation (values) of the arguments.  If\\nthere is an imperative procedure, then the procedure body is evaluated\\nin the procedure context.  If the unification with the left hand side of\\na rewrite rule is possible and the associated condition evaluates to\\ntrue, then the right hand side of the rewrite rule is evaluated. \\nOtherwise the functional term is left unchanged. \\n\\nIn contrast to functional programming languages (like SML [Appel 88])\\nwhich implement typed lambda calculus the types of operations are not\\ndeduced from the program text but must be explicitly defined in the\\nspecification of an operation, in a variable declaration or in a typed\\nstring expression. \\n\\nA weak point in the current MAS design is that the language is only\\ninterpreted.  This is actualy not a handicap in execution speed since\\ncompiled libraries can be used, but in a too weak semantic analysis of\\nthe specifications.  This means that certain errors in the\\nspecifications are only detected during actual evaluation of an\\nexpression. \\n\\nAchievements and Current State\\n\\nThe steps towards the MAS system have been:\\n\\n\\n definition of a syntax transformation scheme between\\n     ALDES and Modula-2;\\n     development of a translator and translation of most of \\n     the ALDES / SAC-2 libraries to Modula-2;\\n development of a storage management system in Modula-2 with \\n     automatic garbage collection in an uncooperative environment;\\n     implementation of a input / output system in Modula-2\\n     with streams;\\n implementation of a LISP interpreter in Modula-2 with\\n     access to the compiled procedures (using Modula-2 procedure\\n     types and variables);\\n definition of an imperative Modula-2 like interaction language; \\n implementation of a parser for the interaction language and\\n     corresponding modifications to the LISP interpreter;\\n design of high-level language constructs for algebraic \\n     specification and a type system with function overloading \\n     capabilities;\\n     modification of the language parser and the interpreter;\\n design of specifications for relevant algebraic structures.\\n\\n\\nSteps 1 and 2 were subject to the restriction that the interface had to\\nbe compatible with the existing ALDES / SAC-2 libraries.  Steps 1 and 2\\nhave been reported in [Kredel 88].  Reports on steps 3, 4, 5 and\\nprogress reports on step 6 have been given in [Kredel 90].  The state \\nof step 6 has been reported in [Kredel 91].  \\n\\nVersions of the MAS system are running on Atari ST (TDI and SPC Modula-2\\ncompilers), IBM PC/AT (M2SDS and Topspeed Modula-2 compilers) and\\nCommodore Amiga (M2AMIGA compiler).  The actual implementations \\nrun on UN*X workstations (e.g. IBM RS6000/AIX, HP 9000/HP-UX, NextStep, \\nSun Sparc with a Modula-2 to C translator) \\nand PCs 386, 486, 586 (DOS, OS2 and Linux). \\n\\nThe ALDES/SAC-2 libraries have been implemented including the Polynomial\\nFactorization System and the Real Root Isolation System.  From the DIP\\nsystem the Buchberger Algorithm System and the Ideal Decomposition and\\nIdeal Real Root System have been implemented.  Groebner Bases are also\\navailable for non-commutative polynomial rings of solvable type.  The\\ncombination of the MAS programs with numerical Modula-2 libraries has\\nbeen tested.  The mathematical libraries have been enlarged by a package\\nfor linear algebra. Further new developments are syzygies,  \\nmodule Groebner bases, comprehensive Groebner bases, \\n(parametric) real root counting, real quantifier elimination, \\ninvolutive bases and invariant polynomials. \\n\\nSome logic programming facilities have been incorporated by means of the\\nconditional rewriting capabilities of the algebraic specification\\ncomponent.  Further there is a parser for the ALDES language and the MAS\\ninterpreter is now able to evaluate ALDES statements (although with low\\nperformance).  In the symbol table system the unbalanced symbol tree has\\nbeen repaced by a hash table with balanced symbol tree entries. \\n\\nThe current development concentrates on filling some gaps in the ALDES /\\nSAC-2 and DIP libraries, the design of suitable specifications for\\nrelevant algebraic structures and completing the system documentation. \\n\\n\\nBibliography\\n\\n[Appel 88] \\nA. W. Appel, R. Milner, R. W. Harper, D. B. MacQueen,\\n        \"Standard ML Reference Manual (preliminary draft)\",\\n        University of Edinburgh, LFCS Report, 1988.\\n\\n[Collins 82] \\nG.E. Collins, R. Loos,\\n        \"ALDES/SAC-2 now available\",\\n        SIGSAM Bulletin 1982, and several reports distributed\\n        with the ALDES/SAC-2 system.\\n\\n[Jenks 85] \\nR. D. Jenks et al.,\\n        \"Scratchpad II Programming Language Manual\",\\n        Computer Algebra Group, IBM, Yorktown Heights, NY, 1985.\\n\\n[Kredel 88] \\nH. Kredel,\\n        \"From SAC-2 to Modula-2\",\\n        Proc. ISSAC\\'88 Rome, LNCS 358, pp 447-455, Springer, 1989.\\n\\n[Kredel 90] \\nH. Kredel,\\n        \"MAS Modula-2 Algebra System\",\\n        Proc. DISCO 90 Capri, LNCS 429, pp 270-271, Springer, 1990.\\n\\n[Kredel 91] \\nH. Kredel,\\n        \"The MAS Specification Component\",\\n        Proc. PLILP 91 Passau, LNCS 528, pp 39-50, Springer, 1991.\\n\\n[Loos 76] \\nR. G. K. Loos.\\n        \"The Algorithm Description Language ALDES (Report)\",\\n        SIGSAM Bulletin 14/1, pp 15-39, 1976.\\n\\n[Wirsing 86] \\nM. Wirsing,\\n        \"Structured Algebraic Specifications: A Kernel Language\",\\n        Theoretical Computer Science 42, pp 123-249,\\n        Elsevier Science Publishers B.V. (North-Holland) (1986).\\n\\n[Wirth 85] \\nN. Wirth,\\n        \"Programming in Modula-2\",\\n        Springer, Berlin, Heidelberg, New York, 1985.\\n\\n\\nAvailability\\n\\nMAS (0.3x, 0.6x, 0.7x and 1.0x) is available from: \\n\\nftp://alice.fmi.uni-passau.de/pub/ComputerAlgebraSystems/mas\\n\\n\\nYou can get more information about MAS from:\\n\\nhttp://alice.fmi.uni-passau.de/mas.html\\n\\nSend bug-reports, questions, remarks to:\\n\\nmailto:mas@alice.fmi.uni-passau.de\\nAcknowledgements\\n\\nDue to limited space we apologize for only global thanks to all who\\nmade contributions and influenced the project.\\n\\nCopyrights:\\n\\n MAS:         (c) 1989-1998, by H. Kredel, University Mannheim, \\n                             M. Pesch, University Passau.\\n ALDES/SAC-2: (c) 1982, by G.E.Collins, R.Loos.  \\n\\n\\nAll Rights Reserved. Permission is granted for unrestricted\\nnoncommercial use and noncommercial redistribution if the copyright\\nnotice is retained when a copy is made. There are no known bugs,\\nhowever we disclaim any usefulness and make no warranty on the\\ncorrectness of the Modula-2 Algebra System. For certain machines\\nand/or operating systems further copying restrictions apply, e.g. see\\nthe files\\n\\n    copying.mas, copying.reuse, copying.mtc, \\n    copying.emx, copying, copying.lib and copying.bsd\\n\\nin the exam directory. \\nThe C code has been generated from the Modula-2 sources of MAS\\nwith the \\'mtc\\' \\'Modula-2 to C\\' translator by GMD Karlsruhe. \\nAlthough it is not required, you should get a copy of it from some \\nftp site to have the sources of the used libraries. \\nThe executables for PC have been compiled using the GNU gcc compiler \\nwith the emx runtime system by Ernst Mattes. The latest versions and \\ndocumentation of emx can also be found on ftp servers. \\n\\nHeinz Kredel,\\nUniversity of Mannheim, \\nL 15, 16, D-68131 Mannheim, Germany.\\nTel: +49/621/181-3171, \\nE-mail: kredel@rz.uni-mannheim.de. \\n\\n\\nMichael Pesch, University of Passau, \\nInnstrasse 33, D-94030 Passau, Germany.\\nTel: +49/851/509-3123 or -3121, \\nE-mail: pesch@alice.fmi.uni-passau.de. \\n\\n\\nLast modification\\nat 26. Oktober 2000. \\n\\n\\n',\n",
       " '\\n\\nNIST: Photoionization of CO2 (ARPES) - 2. Experimental Procedure\\n\\n\\n\\nCO2 Vibrational Branching Ratios and Asymmetry \\n           Parameters\\n2. Experimental Procedure\\n\\nThis experiment was carried out on the 5\\xa0m normal incidence monochromator \\nfitted to a beamline at the Daresbury SRS, providing a photon flux of \\n\\xa01010\\nphotons/s within a bandpass of 0.1\\xa0Å in the spectral region \\nfrom 650\\xa0Å to 840\\xa0Å [24]. The \\nlight was brought into the experimental chamber by a 2\\xa0mm internal \\ndiameter glass capillary light guide whose capillary aperture was placed close \\nto the exit slit of the monochromator. In addition to offering a low-loss \\ntransport for the vacuum ultraviolet radiation, the capillary also served to \\nmaintain a pressure differential between the experimental chamber and the ultra \\nhigh vacuum of the optical monochromator. A schematic diagram of the \\nexperimental apparatus is shown in Fig.\\xa01. The\\nlight guide extends from the exit slit, EX in Fig.\\xa01, to the interaction \\nregion above the gas entrance tube, GS in Fig.\\xa01, and can be as long as \\n30\\xa0cm dependent upon exact placement of the experimental chamber with \\nrespect to the monochromator.\\n\\n\\nThe electron spectrometer system comprises two 100\\xa0mm mean radius \\nhemispherical analysers, one rotatable about the incoming light beam as an axis \\nand the other fixed, contained in a chamber shielded from magnetic fields by \\nthree layers of µ-metal. The radiation from the monochromator is \\npolarized, its polarization depending upon the optics of the monochromator and \\nthe subtended angles of acceptance of the synchrotron radiation. In our \\nconfiguration, the light is polarized with between 75\\xa0% and 80\\xa0% of \\nthe light intensity having its electric field vector  (E-vector) perpendicular to the plane of the paper in \\nFig.\\xa01. In this configuration, the fixed analyser accepts electrons \\nejected parallel to the  of the incident \\nradiation. The fixed analyser is ES-2 in Fig.\\xa01. The other electron \\nspectrometer is rotatable about an axis defined by the direction of the light \\nand hence collects electron in the plane of the . This movement allows the angular distribution of the \\nphotoelectrons to be explored completely and is sufficient to determine the \\nangular asymmetry parameter for the scattering process. The entrance lenses for \\nthe spectrometers are three element zoom lenses based upon the design of \\nHarting and Read [25]. The entrance cone to the lens \\nsystem has a small aperture, usually about 1\\xa0mm in our experiments, which \\nacts as the limiting aperture for determining both the energy and angular \\nresolution of the system. The zoom lens focuses the electrons from a small \\ninteraction volume determined by the size of the light beam exiting the \\ncapillary and the size of the gas jet exiting the gas entrance tube onto the \\nentrance plane of the hemispheres. The pass energy of the electron analyzer and \\nfocus voltages are set by external controls. The pass energy remains fixed for \\na particular set of experiments, and the other voltages are appropriately \\nvaried to scan the electron energy spectrum as required by using an automated \\ndata control system. The electrons are dispersed upon passing through the \\nanalyzer hemispheres and focussed on the hemisphere exit plane. Since the \\napparatus was first described in the literature, the electron spectrometers \\nhave been fitted with position sensitive detectors which are placed near the \\nexit focal plane of the hemispheres [26]. This \\nallows for the simultaneous detection of a range of energies in the \\nphotoelectron spectra and thereby improves the data quality for a given period \\nof data accumulation, compared to using a conventional electron multiplier \\nbehind an exit slit.\\n\\n\\nThe polarization of the incoming light was measured using a three mirror \\npolarizer with tungsten mesh and plate photodiodes which could be rotated with \\nthe rotatable analyzer through 90° in order to determine the light \\npolarization. The polarization detection device was constructed based upon \\nconsiderations given by Horton et\\xa0al. [27]. The \\npolarization was checked frequently since small movements in the storage ring \\nbeam position, and the mirrors focussing the light onto the entrance slit of \\nthe monochromator, can have a marked effect on the polarization. It was found \\nthat, provided the pre-mirror adjustments were kept optimized for maximum \\nphoton flux at the exit slit of the monochromator, the polarization would \\nremain stable during the accumulation of a particular data set. The tungsten \\nwire mesh at the entrance of the polarizer served as a photocathode for \\nmonitoring the intensity of the incoming light beam. A tungsten plate serving \\nas a second photocathode collected the beam remaining after three reflections \\nof the beam. The ratios of these photocurrents as a function of the angular \\nposition of the movable electron spectrometer provided the data necessary to \\ndetermine the polarization of the light.\\n\\n\\nCalibration of the energy response of the analyzers was performed using the \\nknown values of the cross section and asymmetry parameters for argon or helium \\ngas and following standard procedures outlined in the literature \\n[28-30]. For all the spectra reported here the \\nelectron spectrometer resolution was determined from the rare gas calibration \\nto be 41\\xa0meV for the fixed analyser and 46\\xa0meV for the rotatable \\nanalyzer. The 5\\xa0m monochromator resolution was 0.1\\xa0Å (\\xa02\\xa0meV) for the \\nmeasurements taken at wavelengths shorter than 750\\xa0Å. At wavelengths \\nlonger than this, where the structure in the absorption spectrum is less dense \\nthe resolution requirements could be relaxed and a wavelength resolution of \\n0.2\\xa0Å was used. The data were accumulated by simultaneously taking \\nphotoelectron spectra at two angles with respect to the polarization direction \\nby utilizing both electron spectrometer systems. Since the two analyzers could \\nbe positioned at different angles, a particular data point did not require \\nrotation of the movable electron spectrometer system. The wavelength on the \\nmonochromator was then incremented by 0.1\\xa0Å and another set of \\nphotoelectron spectra taken. The light polarization was checked periodically by \\na 90° rotation of the movable electron spectrometer. During data \\naccumulation, the time spent at a particular electron kinetic energy was \\ndetermined by integration of the light flux signal to some predetermined amount\\nso that all the points in a particular data set would be correctly normalized \\nto the same total light flux.\\n\\n\\nThe differential cross section for photoabsorption in the dipole approximation \\nfor a randomly oriented gas may be expressed as\\n\\n \\n\\n\\n(eq. 1)\\n\\n\\nwhere  is the angle between the major \\npolarization axis and the ejected electron, P is the degree of \\npolarization of the incoming light, \\nis the solid angle of collection of the photoelectrons, and \\nv is the partial \\ncross section for the vibrational-electronic channel corresponding to the \\nphotoelectron being detected. The total cross section for a particular \\nelectronic channel is the sum of the partial cross sections of the individually \\nresolved vibrational \\nchannels. The vibrational branching ratio is defined as the partial cross \\nsection for that channel divided by the total cross section for the electronic \\nstate. The number of photoelectrons per steradian per unit light flux, \\ndN/d is proportional to the \\ndifferential cross section and hence we can recast the above equation into one \\nthat refers to the measurable parameters of the experiment:\\n\\n \\n\\n\\n(eq. 2)\\n\\n\\nNv is the total number of electrons detected in a \\nparticular vibrational channel summed over the all solid angles. Measurements \\nwere made simultaneously at =0° \\nand =90° and hence \\nv \\nand Nv could be directly deduced from the two spectra \\n[31]. The branching ratio for a particular \\ntransition is the  ratio of Nv with respect to the sum \\nof all the Nv for a particular electronic excitation. The \\ndata reported here were taken only for transitions that left the \\nCO2+ molecule in the X\\xa02g ground electronic state. \\nAltogether about 1500 data sets were taken in the wavelength region \\n650\\xa0Å to 890\\xa0Å.\\n\\n\\nFigure 2 shows the photoionization efficiency \\n(relative photoionization cross section) for CO2+ in the \\nwavelength region of interest in the present study. The data were taken using a \\nrotationally cooled sample of CO2 and a laboratory light source \\ncoupled to a quadrupole mass filter [16, \\n32]. A one meter near-normal incidence monochromator \\nprovided dispersed radiation with a wavelength resolution of 0.12\\xa0Å. \\nThis rotationally-cooled spectrum shows more detailed and sharper structure, \\nparticularly near ionization onset, than does the spectrum obtained by \\nBerkowitz [33] with a slightly better wavelength \\nresolution of 0.07\\xa0Å. The spectrum shows members of the TO series \\nthat have as limits the A state of CO2+ vibrational \\nlevels. The notation A\\xa0nv(TO) \\nn\\xa0=\\xa04,5,\\xa0...\\xa0etc., means the level is a member of \\nthe TO series having the nv vibrational level of the A state \\nof CO2+ as its limit. The symbol \\nB\\xa0n0(s,d) n\\xa0=\\xa03,4,...etc. is a Rydberg \\nlevel of quantum number n having as its series limit the B state of \\nCO2+ in the vibrational ground state.  The s or d refers \\nto the Hennings sharp or diffuse series. The notation \\nA\\xa03v(L) v\\xa0=\\xa00,... refers to a level of \\nthe Lindholm series with principal quantum number 3 that has the A state as its \\nlimit and has a vibrational excitation of v in the symmetric stretch \\nmode.\\n\\n\\n\\nIntroduction \\xa0 \\n| \\xa0 Experimental Procedure \\xa0 \\n| \\xa0 Analysis of the Data \\xa0 \\n| \\xa0 Discussion \\xa0 \\n| \\xa0 References\\n\\n\\n\\n\\n\\n\\n',\n",
       " '\\n\\n\\nModel Intro\\n\\n\\nModel-Based Reasoning\\n\\nA common theme in any qualitiative reasoning system is to explain how physical\\nsystems work.  Each attempts to formalize this \"common sense knowledge\" about\\nthe physical world.  \\n\\nNaive Physics\\nQualitative Reasoning\\nCommon Sense Reasoning\\n\\nCausal Reasoning\\nDeep Reasoning  (vs Compiled)\\n\\nWith different researchers, each views the task somewhat differently.  One\\nuniversal criterion for these systems was that the description of the system\\nmust be derivable from the structure of the system - the components, their\\nbehavior, and their connections.  (No-function-in-structure Principle of\\ndeKleer)  In representing systems in this way, we also obtain explanation\\nthrough \"local\" propogation of effects (through \"causal chains\").\\nOne could also call this reasoning: What-would-happen-if\\n\\ndeKleer & Brown  - physical components\\nKuipers - constraints\\nForbus - processes\\nKeuneke - radical, revolutionary :-) functional components\\n\\nThe methods of deKleer/Brown and Forbus illustrate two of the choices possible\\nfor primitive elements of the structural description.  The device-centered\\nparadigm of deKleer and Brown chooses as primitives the component devices\\nthemselves.  Network laws provide constraints (on variables of component\\ndevices) at connection points.  The system then can specify components behavior\\nindependently from a particular situation - thus the device model need not\\nembed unstated assumptions about the context in which the device exists.\\n\\nForbus, in contrast, uses a processed based schema. Here a single process can\\neffect a number of substances.  A process is the basic source of change in\\nphysical systems.  Using this technique Forbus\\' system can express combinations\\nof effects more easily (candle example). However, then it is harder to\\ndetermine all ways (completeness and compactness) that a system can effect a\\nspecific substance (as compared to deKleer and Brown where connections are\\nexplicit and determine possible effects.)\\n\\nKuiper\\'s work is based on simulations and constraints using the QSIM algorithm.\\nHere structural descriptions would be simply constraints (arithmetic,\\nderivative, inequality, functional, and conditional).  Qualitative simulation\\nis an inference process which is applied to contraint equations to predict\\nbehaviors of a system.\\n\\nEnvisioning - \\twhat will happen if\\n\\t\\thow things work\\n\\nKeuneke....  One universal criterion for these systems was that the description\\nof the system must be derivable from the structure of the system ... problems\\nin paper acceptance, etc...\\nPaper in the Journal for Experimental and Theoretical AI\\n\\nThe following looks at a the work of deKleer & Brown, Forbus and Kuipers.\\nI will not go into full detail of any method, but you should look at them and\\nbe able to discuss high-level perspectives\\n\\nQualitative Reasoning About Physical Processes\\nKenneth Forbus\\n\\nForbus uses the notion of physical processes as the basis of his\\nqualitative physics theory. He proposes a Qualitative Process Theory  (QP\\ntheory).\\n\\nReasoning involves capturing changes in the physical world.   ?? \\tThey all\\ndo...\\n\\ndeKleer deals with causal reasoning, particularly causal explanations of\\ndevices (in the form of logic proofs).\\n\\nKuiper\\'s uses various types of constraints on objects and a causal structural\\ndescription to arrive at various causal explanations of devices.\\n\\nForbus qualitiative dynamics theory is more general in that process is aimed at\\na range of phenomena, those which correspond to basic physical laws to the\\nworking of (simple or complicated) mechanical devices which require more than\\none such basic principle.\\n\\nHe would say that causal reasoning (aka the other two) are just special cases\\nof the qualitative reasoning on the process.\\n\\nQP theory is involved with determining \"primitives\" of process.\\n\\nClaim:  while deKleer\\'s work is very useful for causal reasoning \"it cannot be\\nused to deduce the limits of physical processes.  This is because it does not\\nrepresent quantities, only changes in them.\"\\n\\nExample (Forbus paper)\\n\\ndeKleer could deduce that the temperature of the water is rising, but not that\\nit might boil.\\n\\nA parameter of a physical system is represented by a quantity.  For the\\npurposes of QP analysis, a quantity had three components:\\n\\tan amount\\n\\ta rate\\n\\tan IQ value (IQ is for Incremental Qualitative Physics...vaules can be (U, D,\\nC, or ?) for increasing, decreasing, constant, or indeterminate)\\n\\nA process is specified by :   (see paper, pg. 327)\\n\\nquantity conditions\\n\\t\\tpreconditions\\n\\t\\tthe relations among variables\\n\\t\\tthe influences imposed on variables\\n\\n\\n\"Discontinuous changes in processes occur at limit points, which serve\\nas boundary conditions.  The points are chosen according to the quantity\\nconditions of the processes that can affect that parameter.  For example, the\\ntemperature  quantity space for a fluid could be: T(ice) --> T(boiling)\\nwhere temperatures at T(ice) and below correspond to the solid state,\\ntemperatures at T(boiling) and above correspond to the gaseous state, and any\\ntemperatures in between to being a liquid.\"\\n\\nForbus work is very good in that it is general.  \\nIt describes (for processes) rules for detecting and determining changes.  All\\nrules that are satisfied are activated and run in parallel.  The consequences\\nindicate changes. \\nMore specifically, his work addresses how to locate interesting points in time\\n- places where interesting things happen (of course, this is a matter of\\nperspective)\\n\\n\\nFoundations of Envisioning\\nJohan deKleer and John Seely Brown\\n\\n\"The kinds of mental models of a mechanistic system that we are interested in\\nare generated, metaphorically speaking, by running a qualitative simulation in\\nthe mind\\'s eye.  We call the result of such simulation \\'envisionment\\'.\"\\nIn \"Mental Models of Physical Mechanisms and Their Acquisition\"\\n\\nEnvisioning is a form of reasoning that produces a causal explanation of a\\nphysical device by explaining how disturbances from equilibrium in the device\\npropogate.\\n\\ndeKleer and Brown have implemented envisioning in a program called ENVISION\\nwhich can analyze a wide variety of devices.  They use a Pressure Regulator as\\nan example giving output of ENVISION in the form of causal proofs.\\n\\n\\nStructure and Function (Behavior)\\n\\nobjective: causal  reasoning that correctly predicts ensuing behavior in that\\ndevice\\n\\n\"Is causal reasoning doing something interesting, or is most of the work it\\nappears to be doing actually pre-encoded in the evidence provided to it?\"  They\\nclaim the worth of a qualitative physics system is measured by the complexity\\nof the relationship between input and output...if these relationships are\\nprecompiled, then this is not interesting... (thus no-function-in-structure)\\n\\tQuestion might be:  At what level of causation?  \\n\\npage 435  - discuss behavior/function meanings \\n\\t\\t\\t\\t(how chose states?)\\n\\nDevice Topology\\nComponent Models - devices\\n\\ncomponents and connections\\n\\t\\tconfluences of components (qualitative equations\\n\\nExtra Knowledge\\n\\ndevice state (i.e., valve open, closed, etc.)\\n\\t\\tprocess confluences\\n\\t\\tassumptions (if necessary)\\n\\n\\nProcess\\n\\nOutput of the system tries to be a compelling causal explanation of the device\\nbehavior (in the form of a logical proof)\\n\\nUndesirable characteristics\\n\\nintroduction of premises is unmotivated and arbitrary\\n\\t\\texplanation-proofs are non-unique\\t\\n\\t\\texplanation proofs can be causally inverted (no cause-effect order)\\n\\n\\n(VonFraussen - on logical proofs as explanation)\\n\\nNote: Architecture of problem-solving...\\n\\nbased on logic proofs, carries with it any problems of logic\\n\\n\\nProblems with proof as explanation\\n\\nprocess of reasoning vs reconstructed explanation\\n\\n\\nHow many explanations can be generated?  (15 for 2 components)  (page 105)\\n\\ncausality?  (page 106)\\n\\nHow does one know when through?\\n\\nHow does one know when a statement of interest has been reached?  no focus ...\\nexcept through limiting states (or confluences) that can generate only\\nbehaviors of interest\\n(no-function-in-structre?)\\n\\n\\n\\nQualitative Simulation\\nBenjamin Kuipers\\n\\nPurpose: to provide an efficient and robost means to providing knowledge of\\n\"deep models\" in which an underlying mechanism, whose state variables may\\nnot be directly observable, accounts for the observable facts\\n\\nApproach: using qualitative causal models\\n\\nDomain of Origin: Medical physiology\\n\\t\\t\\t\\t\\tdeKleer(?) calls \"textbook\" definition\\n\\nQualitative Simulation: the derivative of a description of the behavior of a\\nmechanism from the qualitative constraint equations. (Kuipers claims that all\\nof the above authors are using a form of QS)\\n\\nConstraints:\\n\\nArithmetic   \\t( X = Y + Z)\\n\\t\\tFunctional\\t( Y = M+(X))   monotonic increasing \\n\\t\\tDerivative\\t( Y = dX/dt)     changes in X over time\\n\\t\\tInequality\\n\\t\\tConditional\\n\\n\\nQualitative Simulation of a system starts with a description of the known\\nstructure of a system, and an initial state\\nand produces a directed graph consisting of the possible future states of the\\nsystem and the \"immediate successor\" relation between states.  (page 293)\\n\\nThe structure of the system is described as\\n\\nPhysical parameters: A set of symbols describing the structure of a system\\n\\t\\tConstraint equations: describe how those parameters may be related to each other.\\n\\n\\nEach physical parameter is a continuously differentiable real-valued function\\nof time.  Its value at any given point in time is specified\\nquantitatively in terms of its relationship with a totally ordered set\\nof landmark values.\\n\\nThe landmark values may be either numerical or symbolic.\\n\\nTwo qualitative states in the same operating region are identical if all\\nparameters are equal to the landmark values, and all the directions of change\\nare the same.  This allows the ability to detect cycles!\\n\\nExample: the water tank U-tube\\n\\nConstraints:\\n\\nEach tank has a pressure which depends on its amount of water\\n\\n\\tThe rate of flow through the tube depends on the difference between the\\npressures\\n\\n\\tA flow through the tube increases the amount in one tank and decreases the\\nother\\n\\nBehavior:\\n\\tAfter the increment of water, the amount and pressure are increased in tank\\nA, leading to a flow from A to B\\n\\n\\tWater flows from A to B.  The level in A falls, the level in B rises, and the\\npressure difference and rate of flow approach zero\\n\\n\\tThe pressure difference and rate of flow become zero as the U-tube reaches\\nequilibrium with the level in both tanks higher than before the increment\\n(page 295)\\n\\nDon\\'t forget here - on Kuiper\\'s systems, we are not sure how the \"thing\"\\nworks... we are working with constraints\\n\\n',\n",
       " '\\n\\n\\n\\nILAN Proxy Server\\n\\n\\n ILAN Proxy Server \\n\\n\\n\\nWhat is proxy?\\nWhy should I use proxy?\\nHow do I use proxy?\\n \\n   \\n What is proxy? \\n\\nA proxy server is a server that serves as a gateway between a local \\nnetwork (ILAN\\'s in this case) and the outside world. The proxy server \\nallows various services, mainly caching of the requests and a more secure \\nmeans of obtaining documents. \\n\\nA proxy cache is a way for a client to get remote pages via the server \\nrather than direct from the original site. When the server gets a \\nrequest, it either provides it from the cache, or gets it through the \\noriginal site (while caching it here). Any future requests are answered \\nfrom the local copy of this document, thus saving multiple page transfers \\nfrom abroad. Each page is dowloaded only once, thus saving time and \\nnetwork bandwidth for any future accesses to the cached documents. The \\ncache will expire old entries, and will avoid caching results that are \\nactually from programs (which may change each time called). \\n  \\n Why should I use a proxy? \\n\\nThe proxy cache will help you work faster and save bandwidth. If a \\ncertain document is in the cache, you wouldn\\'t have to wait for it to be \\ntransfered from abroad (which saves you time, and helps make the \\nconnection faster for other users). \\n\\nIf the document you asked for is not in the cache, it would take exactly \\nthe same amount of time for you (it will be downloaded from abroad) - but \\nit would save a lot of time for the next user (who might very well be \\nyou) that would download this document. You have an interest in him \\ngetting the document faster - the international line would be less \\ncrowded, and you would get your transfers faster.\\n  \\n How can I use proxy? \\n\\nConfigure your client to have wwwproxy.ac.il as it\\'s proxy \\nserver. On the Unix servers at the TAU Computation Center, it is done \\nautomatically. The configuration depends on the client:\\n\\n\\n On Mosaic for UNIX and lynx, set the following environment variables before calling Mosaic or lynx Bourne shell:\\n \\nhttp_proxy=\"http://wwwproxy.ac.il:8080/\" \\nftp_proxy=\"$http_proxy\" \\nwais_proxy=\"$http_proxy\" \\ngopher_proxy=\"$http_proxy\" \\nexport http_proxy ftp_proxy wais_proxy gopher_proxy \\nno_proxy=\"ac.il\" \\nexport no_proxy \\n \\nNote: not all versions of Mosaic and lynx obey no_proxy.  \\n On Macintosh using Mac Mosaic, select Preferences -> Gates:\\n\\n Set WAIS Gateway:   http://wwwproxy.ac.il:8080/ \\n Turn on \\'Use CERN Proxy Service\\' and set\\n \\nHTTP Gate:      wwwproxy.ac.il : 8080 \\nGopher Gate:    wwwproxy.ac.il : 8080 \\nFTP Gate:       wwwproxy.ac.il : 8080 \\n   \\n On a PC using Mosaic for Windows, add the following to your MOSAIC.INI (probably found in C:\\\\WINDOWS):\\n \\n[Proxy Information] \\nhttp_proxy=http://wwwproxy.ac.il:8080/ \\nftp_proxy=http://wwwproxy.ac.il:8080/ \\ngopher_proxy=http://wwwproxy.ac.il:8080/ \\nwais_proxy=http://wwwproxy.ac.il:8080/ \\n  \\n For Netscape 2.0 and up (Unix, Mac and Windows), select the menu \\nitem Options -> Network Preferences. Go to the subsection \\'Proxies\\'  and \\nselect Manual Proxy Configuration. View the configuration. It should be: \\n\\nFTP Proxy:wwwproxy.ac.ilPort:8080\\nGopher Proxy:wwwproxy.ac.ilPort:8080\\nHTTP Proxy:wwwproxy.ac.ilPort:8080\\nSecurity Proxy:wwwproxy.ac.ilPort:8080\\nWAIS Proxy:wwwproxy.ac.ilPort:8080\\nSOCKS Host:Port:1080\\nNo Proxy for:ac.il\\n\\nSelect \\'OK\\'. To avoid using the cache when not necessary users should \\nexclude local pages. Set the \"No proxy for\" box to be \"ac.il\". \\n\\n\\n\\n\\nFurther information about proxies is available from the World Wide Web Consortium.\\n\\n\\n\\nAdapted from the proxy page at \\nHebrew University Computer Science written by Ariel Nowersztern.\\nLast updated 31/3/96.\\n\\n\\n\\n',\n",
       " ' 20%. (I also got 1848 hits for _gage_.) For _misspell/misspelling_, some use the hyphenated forms _mis-spell_ etc. instead. Further, _misspelled_ is common in the U.S., while _misspelt/mis-spelt_ is the usual form in most other English-speaking countries. I didn\\'t count the hyphenated forms of _mis-spell_ one way or the other. I found 486 instances of _misspell_, 52 for _mis-spell_, and 87 for _mispell_. Cases of _misspelled_ total 2254; _mis-spelled_ 209; _mispelled_ 452. For _misspelt_, 148; _mis-spelt_ 70; _mispelt_ 32. Note about _supersede_... There is Asymetrix\\'s \"SuperCede for Java.\" Some of the posts I found in searching for _supercede_ were indeed about Asymetrix\\'s SuperCede. (It\\'s not case sensitive, so _supercede, Supercede, SuperCede_, etc. all turn up). I searched for occurrences of _supersede, supercede, superceed_, and _superseed_. The form _supersede_ made up about 45% of the total. There were just a few hits for _superceed_ or _superseed_, while almost all the rest -- almost 55% -- were _supercede_. From reading thru many of the posts, I determined that at least two-thirds of the time _supercede_ was being used for the word _supersede_, while one-third or less it was for Asymetrix\\'s SuperCede. Notes about _mischievous_ and _height_... With each of these words, some of the non-standard spellings I found also reflect widespread non-standard pronunciations each of these words have. _Mischievous_ is often (erroneously) pronounced as if there were an \"i\" (or \"e\") between the \"v\" and the \"ous.\" The spellings and number of occurrences found in this survey: mischievous 1203 mischevious 255 mischeivous 51 mischievious 182 mischevous 15 mischeivious 30 mischivous 8 mischieveous 7 mischiveous 7 mischeiveous 2 _Height_ is sometimes pronounced as if the final \"t\" were \"th\" as in _think_. However, only some of the misspellings of _height_ were those ending in \"-th\"; and the most common non-standard spellings of _height_ were _hight_ and _hieght_. _Hight_, incidentally, is given as a valid variant of _height_ in some unabridged dictionaries. Here are the numbers of individual spellings found for some of the other words in this study: occurrence 5508 occurring 6747 occurance 2109 occuring 3823 occurence 1983 ocurring 111 occurrance 221 ocuring 4 ocurrence 43 ocurrance 8 ocurence 3 accommodate 9220 accommodation 4135 accomodate 5862 accomodation 2395 accomadate 233 accomadation 104 accommadate 32 acommodation 57 acommodate 23 accommadation 42 acomodate 21 acomodation 6 acommadate 1 acommadation 1 noticeable 7888 harass 3037 noticable 3819 harrass 1262 noticible 322 harras 305 noticeble 36 haress 15 noticiable 18 herass 13 noteceable 5 harress 1 notacible 1 embarrass 2425 embarrassment 6585 embarass 877 embarassment 7121 embarras 135 embarrasment 578 embarress 33 embarasment 75 embaress 19 embarressment 63 embaras 1 embaressment 48 (and a handful each of: emberassment, emberrasment, embaresment, emberasment) privilege 7035 priviledge 1050 privalege 38 privelege 521 privelidge 36 privledge 331 privlage 21 priveledge 238 privaledge 18 privilage 212 privalage 15 privelage 134 privellege 12 privelige 61 privlidge 12 privlege 40 privlige 11 (and a few each of: privillege, privalige, privalidge, privillige, privillage, privelledge) definitely 90565 rhythm 8341 definately 17904 rythm 452 definitly 2434 rythym 307 definatly 1609 rythem 140 definitley 322 rhythym 121 definatley 247 rhythem 78 defenitely 158 rythum 46 defenitly 48 rhythum 19 defenatly 44 rithm 11 (a few each of: defenately, (a few each of: rythim, rhythim, defanatly, defanately) rithum, rithem, rithim) ================================================= Before doing the study of Internet newsgroups, I surveyed published lists of frequently misspelled words. The scope of the lists was American English (I live in the U.S. and used what material I could find), and the study covered nine lists which I found in books. I have recently added into that study two lists from British sources, a total now of eleven lists. The words found most often on those lists: On ten of the eleven lists of commonly misspelled words: accommodate embarrass grammar forty separate On nine of the lists: business harass necessary parallel privilege And on eight of the eleven: all right existence occurrence calendar government perseverance commitment height rhythm committee immediately seize conscientious indispensable transferred description maintenance (For the pool of words I used in the study of Usenet postings, I used those listed here which appeared in the majority of the published lists of commonly misspelled words, and added to that other words that were high-ranking on those lists or which had been mentioned as \"candidates\" by others.) Below I have a sort of \"combined results,\" and of course there are a number of ways that one could show combined results for this. I have taken the words which appeared on nine or more of the eleven published lists; I have taken the words which were misspelled 20% or more of the time in my study of posts to Usenet discussion groups; and I have combined them, into the following chart. This then has those words that appeared on at least nine of the published lists *and* were misspelled 20%-plus of the time on the Usenet discussion groups; it also has those which appeared on at least nine of the lists, but which were misspelled *less* than 20% of the time on Usenet; and it has those which were misspelled 20%-plus of the time on Usenet, but which appeared on *fewer* than nine of the published lists: Misspelled On N of Misspelled On N of Approx N% the 11 Approx N% the 11 of Time Published of Time Published on Usenet Lists on Usenet Lists accommodate 33%+ 10 embarrass 20-33% 10 harass 33+ 9 separate 20-33 10 occurrence 33+ 8 privilege 20-33 9 perseverance 33+ 8 indispensable 20-33 8 embarrassment 33+ fewer than 8 definitely 20-33 fewer than 8 inoculate 33+ (A final note: Many search engines, such as Deja News, now allow one to search items from just a particular language, so this could be conducted in other languages for comparison.) ================================================= Here are the sources for the eleven published lists of commonly misspelled words: Cassell Encyclopaedia Dictionary (1990), (205 Misspellings). This list is headed \"Words Commonly Mis-Spelt\" Davidson, Wilma, _Business Writing What Works, What Won\\'t_ (1994), St. Martin\\'s Press (New York), pages 196-201 (397 Such Words). Aimed, as the title says, at those writing business letters, etc. The heading for this list is \"Easily Misspelled Words.\" Furness, Edna L., _Guide to Better English Spelling_ (1990), National Textbook Company (Lincolnwood, Illinois), pages 233-236 (500 Words). This book actually has several lists of such words, based on different levels. This particular list is headed, \"THE REMINGTON RAND LIST OF WORDS MOST FREQUENTLY MISSPELLED BY ADULTS.\" Furness, Edna L., _Guide to Better English Spelling_ (1990), National Textbook Company (Lincolnwood, Illinois), pages 175-195 (605 Demons). As stated above, this book contains a number of lists for different levels, and those other lists (like the Remington Rand list noted above) are from other sources as well. This particular list is the author\\'s own, the introduction of which begins \"The 605 spelling demons. . . . . These 605 are among the most frequently misspelled words in the English language.\" Lederer, Richard, _Adventures of a Verbivore_ (1994), Simon & Schuster (New York), pages 242-243 (100 Words). This book, aimed at the general reader, contains many \"tidbits\" and anecdotes about the English language. Part of the preface to this particular list reads, \"During my thirty years as a high school English teacher, I have compiled a list of the hundred words that my students have most consistently misspelled.\" Mersand, Joseph and Griffith, Francis, _Spelling Your Way to Success_ (1974), Barron\\'s Educational Series, pages 161-165 (500 Common Misspellings). The introduction to this list includes, \"The 500 words which follow are those which are most commonly misspelled in business correspondence. . . . . The list was compiled by the National Office Management Association after a comprehensive study of 10,652 letters collected from business concerns and government agencies. . . . . \" The New Webster\\'s Desk Reference (formerly _The New Lexicon Library of Knowledge_) (1991), Lexicon Publications (New York), pages 64-71 (A List of 440). Aimed at the general reader seeking information. The preface to this list reads \"440 WORDS FREQUENTLY MISSPELLED In business letters and reports and in reports or papers prepared by students in high school and college, there are certain words which are misspelled more often than others. Following is a list of words which are frequently misspelled. . . . . \" The New York Public Library Writer\\'s Guide to Style and Usage (1994), HarperCollins (New York), page 390 (50 words). This book is aimed at journalists and other professional writers. The heading for this list reads: \"THE TOP 50 MISSPELLED WORDS Here we include our candidates for the 50 most commonly misspelled words. Not included are words that are troublesome pairs with similar spellings but different meanings principle/principal, affect/effect, complement/ compliment, stationery/stationary, and capital/capitol. . . . . \" Pitman Secretarial School, (120 Words). A list of 120 \"spelling demons.\" Shaw, Harry, _Spell It Right!_ (Fourth Edition) (1993), HarperCollins (New York), pages 158-170 (860 Trip-Ups). The heading for this is \"List of 860 Words Often Misspelled.\" The footnote to that heading reads: \"The list given has been checked against some of the major studies of frequency word use and frequency misspellings . . . . \" and cites some of these references. The World Almanac and Book of Facts 1995 (1995), Funk & Wagnalls (Mahwah, New Jersey), page 597 (52 such words). This book is aimed at the general reader seeking information. The heading for this list of 52 words simply reads, \"Commonly Misspelled English Words.\" By Cornell Kimball \\n\\t\\n\\n-->\\n A Study of Some of the Most Commonly Misspelled Words\\n\\n\\n\\n  A Study of Some of the Most Commonly Misspelled Words\\n\\n\\n    I made a search of posts to Internet (Usenet) newsgroups using\\nDeja News < http://www.dejanews.com/ > coming up with a count of how \\nmany times a word was spelled with the conventional spelling and \\ncomparing this with how often it was spelled with an alternative form, \\nor \"misspelled.\"  (This was inspired by a discussion in the newsgroup \\n\"alt.usage.english\" in which a similar, but smaller, study of a few \\nmisspelled forms was done using a search engine.)\\n  \\n    Spellings other than the standard form were used more than 33% of \\nthe time for these words in posts to Usenet discussion groups:\\n\\n minuscule\\t68% of the time spelled otherwise\\n millennium57\\n embarrassment55    (embarrassing  35%)\\n occurrence44    (occurring  37)\\n accommodate40    (accommodation  39)\\n perseverance36\\n supersede     35-50   (superseded  44)\\n noticeable35\\n harass34\\n inoculate34\\n\\n\\n    Next are words that were misspelled between about 20% and 33% of \\nthe time in posts to the newsgroups:\\n\\n     mischievous  32% \\n\\t      pastime\\t 24%\\n     occurred       31\\t\\t\\n       separate   23  (inseparable 21%)\\n     embarrass   30 (embarrassed 29)  \\npreceding  22  (preceded  21)\\n     indispensable  29\\n    definitely 20\\n     privilege     28\\t\\t\\n gauge 20  (gauges  25)\\n     questionnaire  28\\n\\n\\n\\nAnd these were misspelled between 10% and 19% of the time:\\n  existence 18%    miniature     14%\\n  publicly18    precede\\t    13  (precedes  11%)\\n  weird\\t 17    rhythm\\t    12\\n  separately  17 (separated 16%)  conscientious 11  (conscious 10)\\n  misspell 16    hierarchy  11\\n  grammar 15    calendar\\t    10\\n  withhold 15\\n\\n   Anecdotal evidence and personal observation indicate that a few \\nother cases where a \"non-standard\" spelling is frequently used on the \\nInternet are _alright_ for _all right_, _alot_ for _a lot_, and _it\\'s_ \\nfor the possessive _its_.  However, search engines don\\'t look for \\nextremely common words (asking one to search for posts with the word \\n_the_ would turn up just about every post ever made) and looking for \\ntwo-word phrases (_all right, a lot_) is trickier than for a single \\nword.  Too, to determine the rate for the possessive _its,_ one must \\nlook thru each individual post to determine whether _it\\'s_ was used \\nincorrectly for the possessive _its_ or whether _it\\'s_ was used \\ncorrectly for the contractions _it is/it has_.  So, I didn\\'t search \\nfor these three cases, but it may well be that _all right, a lot_, and \\nthe possessive _its_ are among the most frequently misspelled words in \\nInternet postings.\\n\\n    A few other words to note, with misspelling rates below 10% in \\nitems posted to Usenet discussion groups: commitment, conceding, \\noccasionally, seize 9%;  conceded, paralleled, sovereign 8%; \\nrepetition 7%;  commission, concede, counterfeit, forfeit, \\nmaintenance 6%;  concedes, height, receive, threshold 5%;  committee, \\ndeceive, forty, immediately, proceed 4%;  conscience, foreign, \\nparallel, proceeds, sincerely 3%;  government 2%;  business, \\nnecessary 1%.\\n\\n\\n    Notes about _minuscule_, _gauge,_ and _misspell_....   For these\\nwords there are also dictionary-given alternative spellings. \\n_Minuscule_ has been \"misspelled\" as _miniscule_ so many times in so \\nmany places that _miniscule_ is now a valid variant in some \\ndictionaries, >\\n as you can read in this linkas you can read in this link.  Now since _miniscule_\\nis given as a variant spelling in dictionaries, it could be said that\\ntechnically one isn\\'t \"misspelling\" _minuscule_ by writing _miniscule_.\\nHowever, the purpose of the study here is to see how many people spell\\na word in a way other than the (one) accepted standard -- thus the \\ndistinction is between \"standard\" spellings on one hand versus \\nanything else.\\n\\n    With _gauge_, the dictionary-accepted variant is _gage_, and the\\nsituation is different from that of _miniscule_.  In certain fields \\n(science, engineering), _gage_ is the more commonly used, that is \\n\"standard,\" spelling.  In the figures above, I have simply not \\ncounted _gage_ one way or the other (by using the criteria I mentioned \\nin the previous paragraph, _gage_ would be in with the \"other than \\nstandard forms.  Thus, by Deja News, I had 8879 hits for _gauge_ and \\n2211 for _guage_.  I computed the position above from those\\ntwo: 2211 / (8879+2211) --> 20%.  (I also got 1848 hits for _gage_.)\\n\\n    For _misspell/misspelling_, some use the hyphenated forms \\n_mis-spell_ etc. instead.  Further, _misspelled_ is common in the U.S., \\nwhile _misspelt/mis-spelt_ is the usual form in most other \\nEnglish-speaking countries.  I didn\\'t count the hyphenated forms of \\n_mis-spell_ one way or the other.  I found 486 instances of _misspell_, \\n52 for _mis-spell_, and 87 for _mispell_.  Cases of _misspelled_ \\ntotal 2254;  _mis-spelled_ 209;  _mispelled_ 452.  For _misspelt_, 148;\\n_mis-spelt_ 70;  _mispelt_ 32.\\n\\n\\n    Note about _supersede_...   There is Asymetrix\\'s \"SuperCede for\\nJava.\"  Some of the posts I found in searching for _supercede_ were \\nindeed about Asymetrix\\'s SuperCede.  (It\\'s not case sensitive, so\\n_supercede, Supercede, SuperCede_, etc. all turn up).  I searched for \\noccurrences of _supersede, supercede, superceed_, and _superseed_. \\nThe form _supersede_ made up about 45% of the total.  There were just \\na few hits for _superceed_ or _superseed_, while almost all the rest \\n-- almost 55% -- were _supercede_.  From reading thru many of the \\nposts, I determined that at least two-thirds of the time _supercede_ \\nwas being used for the word _supersede_, while one-third or less it\\nwas for Asymetrix\\'s SuperCede.\\n\\n\\n    Notes about _mischievous_ and _height_...   With each of these\\nwords, some of the non-standard spellings I found also reflect\\nwidespread non-standard pronunciations each of these words have.\\n_Mischievous_ is often (erroneously) pronounced as if there were an\\n\"i\" (or \"e\") between the \"v\" and the \"ous.\"  The spellings and number\\nof occurrences found in this survey:\\n      mischievous  1203    mischevious   255    mischeivous  51\\n\\t\\t\\t   mischievious  182\\tmischevous   15\\n\\t\\t\\t   mischeivious   30    mischivous    8\\n\\t\\t\\t   mischieveous    7\\n\\t\\t\\t   mischiveous     7\\n\\t\\t\\t   mischeiveous    2\\n\\n    _Height_ is sometimes pronounced as if the final \"t\" were \"th\" as\\nin _think_.  However, only some of the misspellings of _height_ were\\nthose ending in \"-th\"; and the most common non-standard spellings of \\n_height_ were _hight_ and _hieght_.  _Hight_, incidentally, is given\\nas a valid variant of _height_ in some unabridged dictionaries.\\n\\n\\n    Here are the numbers of individual spellings found for some of \\nthe other words in this study:\\n\\n\\toccurrence  5508        occurring  6747\\n\\t   occurance   2109\\t   occuring   3823\\n\\t   occurence   1983\\t   ocurring    111\\n\\t   occurrance\\t221\\t   ocuring\\t 4\\n\\t   ocurrence\\t 43\\n\\t   ocurrance\\t  8\\n\\t   ocurence\\t  3\\n\\n\\taccommodate  9220\\taccommodation  4135\\n\\t   accomodate\\t5862\\t   accomodation   2395\\n\\t   accomadate\\t 233\\t   accomadation    104\\n\\t   accommadate\\t  32\\t   acommodation     57\\n\\t   acommodate\\t  23\\t   accommadation    42\\n\\t   acomodate\\t  21\\t   acomodation\\t     6\\n\\t   acommadate\\t   1\\t   acommadation      1\\n\\n\\tnoticeable  7888\\tharass\\t  3037\\n\\t   noticable   3819\\t   harrass   1262\\t   \\n\\t   noticible\\t322\\t   harras     305\\t   \\n\\t   noticeble\\t 36\\t   haress      15\\t   \\n\\t   noticiable\\t 18\\t   herass      13\\t   \\n\\t   noteceable\\t  5\\t   harress\\t1\\t   \\n\\t   notacible\\t  1\\n\\n\\tembarrass  2425 \\tembarrassment  6585\\n\\t   embarass    877\\t   embarassment   7121\\n\\t   embarras    135\\t   embarrasment    578\\n\\t   embarress\\t33\\t   embarasment      75\\n\\t   embaress\\t19\\t   embarressment    63\\n\\t   embaras\\t 1\\t   embaressment\\t    48\\n\\t\\t\\t    (and a handful each of: emberassment,\\n\\t\\t\\t   emberrasment, embaresment, emberasment)\\n\\n\\tprivilege   7035\\n\\t   priviledge  1050\\t   privalege\\t38\\n\\t   privelege\\t521\\t   privelidge\\t36\\n\\t   privledge\\t331\\t   privlage\\t21\\n\\t   priveledge\\t238\\t   privaledge\\t18\\n\\t   privilage\\t212\\t   privalage\\t15\\n\\t   privelage\\t134\\t   privellege\\t12\\n\\t   privelige\\t 61\\t   privlidge\\t12\\n\\t   privlege\\t 40\\t   privlige\\t11\\n   (and a few each of: privillege, privalige, privalidge,\\n             privillige, privillage, privelledge)\\n\\n\\tdefinitely  90565\\t rhythm   8341\\n\\t   definately  17904\\t    rythm    452\\n\\t   definitly\\t2434\\t    rythym   307\\n\\t   definatly\\t1609\\t    rythem   140\\n\\t   definitley\\t 322\\t    rhythym  121\\n\\t   definatley\\t 247\\t    rhythem   78\\n\\t   defenitely\\t 158\\t    rythum    46\\n\\t   defenitly\\t  48\\t    rhythum   19\\n\\t   defenatly\\t  44\\t    rithm     11\\n (a few each of: defenately,    (a few each of: rythim, rhythim,\\n   defanatly, defanately)\\t     rithum, rithem, rithim)\\n\\n\\n\\t   =================================================\\n\\n\\n    Before doing the study of Internet newsgroups, I surveyed published\\nlists of frequently misspelled words.   The scope of the lists was\\nAmerican English (I live in the U.S. and used what material I could\\nfind), and the study covered nine lists which I found in books.  I have \\nrecently added into that study two lists from British sources, a total\\nnow of eleven lists.  The words found most often on those lists:\\n\\nOn ten of the eleven lists of commonly misspelled words:\\n\\t\\t    accommodate\\n\\t\\t    embarrass\\n\\t\\t    grammar\\n\\t\\t    forty\\n\\t\\t    separate\\n\\nOn nine of the lists:\\n\\t\\t    business\\n\\t\\t    harass\\n\\t\\t    necessary\\n\\t\\t    parallel\\n\\t\\t    privilege\\n\\nAnd on eight of the eleven:\\n     all right\\t\\texistence\\t    occurrence\\n     calendar\\t\\tgovernment\\t    perseverance\\n     commitment \\theight\\t\\t    rhythm\\n     committee\\t\\timmediately\\t    seize\\n     conscientious\\tindispensable\\t    transferred\\n     description\\tmaintenance\\n\\n(For the pool of words I used in the study of Usenet postings, I used \\nthose listed here which appeared in the majority of the published \\nlists of commonly misspelled words, and added to that other words that \\nwere high-ranking on those lists or which had been mentioned as \\n\"candidates\" by others.)\\n\\n\\n    Below I have a sort of \"combined results,\" and of course there are\\na number of ways that one could show combined results for this.  I \\nhave taken the words which appeared on nine or more of the eleven \\npublished lists;  I have taken the words which were misspelled 20% or \\nmore of the time in my study of posts to Usenet discussion groups; \\nand I have combined them, into the following chart.  This then has \\nthose words that appeared on at least nine of the published lists \\n*and* were misspelled 20%-plus of the time on the Usenet discussion \\ngroups;  it also has those which appeared on at least nine of the \\nlists, but which were misspelled *less* than 20% of the time on \\nUsenet;  and it has those which were misspelled 20%-plus of the time \\non Usenet, but which appeared on *fewer* than nine of the published \\nlists:\\n\\n\\t   Misspelled   On N of\\t\\t       Misspelled   On N of\\n\\t    Approx N%\\tthe 11\\t\\t\\tApprox N%    the 11\\n\\t     of Time   Published \\t\\t of Time   Published\\n\\t    on Usenet    Lists \\t \\t\\ton Usenet    Lists\\n\\n  accommodate\\t 33%+\\t 10\\t     embarrass\\t   20-33%    10\\n  harass \\t 33+\\t  9\\t     separate\\t   20-33     10\\n  occurrence\\t 33+\\t  8\\t     privilege\\t   20-33      9\\n  perseverance\\t 33+\\t  8\\t     indispensable 20-33      8\\n  embarrassment\\t 33+  fewer than 8   definitely    20-33  fewer than 8\\n  inoculate\\t 33+\\t <8\\t     gauge\\t   20-33     <8\\n  millennium\\t 33+\\t <8\\t     mischievous   20-33     <8\\n  minuscule\\t 33+\\t <8\\t     occurred      20-33     <8\\n  noticeable\\t 33+\\t <8\\t     pastime\\t   20-33     <8\\n  supersede\\t 33+\\t <8\\t     preceding\\t   20-33     <8\\n\\t\\t\\t\\t     questionnaire 20-33     <8\\n\\t\\t\\t\\t     grammar\\t   10-20     10\\n\\t\\t\\t\\t     forty\\t    <10      10\\n\\t\\t\\t\\t     business\\t    <10       9\\n\\t\\t\\t\\t     necessary\\t    <10       9\\n\\t\\t\\t\\t     parallel\\t    <10       9\\n\\n\\n\\n    (A final note: Many search engines, such as Deja News, now allow \\none to search items from just a particular language, so this could be \\nconducted in other languages for comparison.)\\n\\n\\n\\t   =================================================\\n\\n\\n    Here are the sources for the eleven published lists of commonly \\nmisspelled words:\\n\\nCassell Encyclopaedia Dictionary (1990), (205 Misspellings).  This\\n  list is headed \"Words Commonly Mis-Spelt\"\\n\\nDavidson, Wilma, _Business Writing   What Works, What Won\\'t_ (1994), \\n  St. Martin\\'s Press (New York), pages 196-201 (397 Such Words).\\n  Aimed, as the title says, at those writing business letters, etc.\\n  The heading for this list is \"Easily Misspelled Words.\" \\n\\nFurness, Edna L., _Guide to Better English Spelling_ (1990), National\\n  Textbook Company (Lincolnwood, Illinois), pages 233-236 (500 Words).\\n  This book actually has several lists of such words, based on\\n  different levels.  This particular list is headed, \"THE REMINGTON\\n  RAND LIST OF WORDS MOST FREQUENTLY MISSPELLED BY ADULTS.\"\\n\\nFurness, Edna L., _Guide to Better English Spelling_ (1990), National\\n  Textbook Company (Lincolnwood, Illinois), pages 175-195 (605 Demons).\\n  As stated above, this book contains a number of lists for different\\n  levels, and those other lists (like the Remington Rand list noted\\n  above) are from other sources as well.  This particular list is the\\n  author\\'s own, the introduction of which begins \"The 605 spelling\\n  demons.  . . . .  These 605 are among the most frequently misspelled\\n  words in the English language.\"\\n\\nLederer, Richard, _Adventures of a Verbivore_ (1994), Simon & Schuster\\n  (New York), pages 242-243 (100 Words).  This book, aimed at the\\n  general reader, contains many \"tidbits\" and anecdotes about the\\n  English language.  Part of the preface to this particular list reads,\\n  \"During my thirty years as a high school English teacher, I have\\n  compiled a list of the hundred words that my students have most\\n  consistently misspelled.\"\\n\\nMersand, Joseph and Griffith, Francis, _Spelling Your Way to Success_\\n  (1974), Barron\\'s Educational Series, pages 161-165 (500 Common\\n  Misspellings).  The introduction to this list includes, \"The 500\\n  words which follow are those which are most commonly misspelled in\\n  business correspondence.  . . . .   The list was compiled by the\\n  National Office Management Association after a comprehensive study\\n  of 10,652 letters collected from business concerns and government\\n  agencies.   . . . . \"\\n\\nThe New Webster\\'s Desk Reference (formerly _The New Lexicon Library of\\n  Knowledge_) (1991), Lexicon Publications (New York), pages 64-71 (A\\n  List of 440).  Aimed at the general reader seeking information.  The\\n  preface to this list reads \"440 WORDS FREQUENTLY MISSPELLED    In\\n  business letters and reports and in reports or papers prepared by\\n  students in high school and college, there are certain words which\\n  are misspelled more often than others.  Following is a list of words\\n  which are frequently misspelled.   . . . . \"\\n\\nThe New York Public Library Writer\\'s Guide to Style and Usage (1994),\\n  HarperCollins (New York), page 390 (50 words).  This book is aimed\\n  at journalists and other professional writers.  The heading for this\\n  list reads:  \"THE TOP 50 MISSPELLED WORDS    Here we include our\\n  candidates for the 50 most commonly misspelled words.  Not included\\n  are words that are troublesome pairs with similar spellings but\\n  different meanings principle/principal, affect/effect, complement/\\n  compliment, stationery/stationary, and capital/capitol.    . . . . \"\\n\\nPitman Secretarial School, (120 Words).  A list of 120 \"spelling\\n  demons.\"\\n\\nShaw, Harry, _Spell It Right!_ (Fourth Edition) (1993), HarperCollins\\n  (New York), pages 158-170 (860 Trip-Ups).  The heading for this is\\n  \"List of 860 Words Often Misspelled.\"  The footnote to that heading\\n  reads: \"The list given has been checked against some of the major\\n  studies of frequency word use and frequency misspellings . . . . \"\\n  and cites some of these references.\\n\\nThe World Almanac and Book of Facts 1995 (1995), Funk & Wagnalls\\n  (Mahwah, New Jersey), page 597 (52 such words).  This book is aimed\\n  at the general reader seeking information.  The heading for this\\n  list of 52 words simply reads, \"Commonly Misspelled English Words.\"\\n\\n\\n\\nBy Cornell Kimball\\n\\n',\n",
       " \"\\n\\n\\n\\n5.5 Evaluating One Policy While Following Another\\n\\n\\n\\n\\n\\n\\n\\n    \\n Next: 5.6 Off-Policy Monte Carlo \\nUp: 5 Monte Carlo Methods\\n Previous: 5.4 On-Policy Monte Carlo \\n  \\n5.5 Evaluating One Policy While Following Another\\n\\nSo far we have considered methods for estimating the value functions for a\\npolicy given an infinite supply of episodes generated using that policy. \\nSuppose now that all we have are episodes generated from a different policy. \\nThat is, suppose we wish to estimate   or , but all we have\\nare episodes following , where .  Can we do it?\\n\\nHappily, in many cases we can.  Of course, in order to use episodes from \\nto estimate values for , we require that every action taken under \\nis also taken, at least occasionally, under .  That is, we require\\nthat  imply\\n.  In the episodes generated using , consider the\\nith first visit to state s and the complete sequence of states and actions\\nfollowing that visit.  Let  and\\n denote the probabilities of that complete sequence happening given\\npolicies  and  and starting from s.  Let \\ndenote the corresponding observed return from state s.  To average\\nthese to obtain an unbiased estimate of , we need only weight each\\nreturn by its relative probability of occurring under  and , that\\nis, by\\n.  The desired Monte Carlo estimate after observing \\nreturns from state s is then\\n\\xa0\\n\\nThis equation involves the probabilities  and ,\\nwhich are normally considered unknown in applications of Monte Carlo methods. \\nFortunately, here we need only their ratio, , which \\ncan be determined with no knowledge of the environment's dynamics.  Let\\n be the time of termination of the i th episode involving state\\ns.  Then\\n\\nand\\n\\nThus the weight needed in (5.3), , depends\\nonly on the two policies and not at all on the environment's dynamics.\\n\\n\\n\\n Exercise .\\n\\nWhat is the Monte Carlo estimate analogous to (5.3) for\\n action values, given returns generated using ?\\n\\n    \\n Next: 5.6 Off-Policy Monte Carlo \\nUp: 5 Monte Carlo Methods\\n Previous: 5.4 On-Policy Monte Carlo \\n  \\n \\n\\nRichard Sutton \\nFri May 30 13:20:35 EDT 1997\\n\\n\\n\",\n",
       " \"\\n\\n\\n\\nDocument has Moved\\n\\n\\n\\nYou have moved into a dark place.\\nIt is pitch black. You are likely to be eaten by a grue.\\n\\nYou're probably looking for: http://hr.uoregon.edu/davidrl/samba.html\\n\\n\\n\\n\",\n",
       " '\\n\\n\\nBrian W. Kernighan: Programming in C: A Tutorial\\n\\n\\n\\n\\n\\nProgramming in C:\\xa0 A Tutorial\\n\\n\\nBrian W. Kernighan\\n\\t\\tBell Laboratories, Murray Hill, N. J.\\n\\n\\n\\nDisclaimer:  This ``tutorial\\'\\' is presented as a historical document,\\nnot as a tutorial.\\xa0 Although it has lost little of its didactic\\nvalue, it describes a language that C compilers today do no longer understand:\\nthe C of 1974, four years before Kernighan\\nand Ritchie published the first edition\\nof ``The C Programming Language\\'\\'.\\n\\n\\nTable of Contents:\\n\\n Introduction\\n A Simple C Program\\n A Working C Program;\\n\\tVariables; Types and Type Declarations\\n Constants\\n Simple I/O -- getchar, putchar, printf\\n If;\\n\\trelational operators;\\n\\tcompound statements\\n While Statement;\\n\\tAssignment within an Expression;\\n\\tNull Statement\\n Arithmetic\\n Else Clause;\\n\\tConditional Expressions\\n Increment and Decrement Operators\\n Arrays\\n Character Arrays; Strings\\n For Statement\\n Functions; Comments\\n Local and External Variables\\n Pointers\\n Function Arguments\\n Multiple Levels of Pointers;\\n\\t\\tProgram Arguments\\n The Switch Statement;\\n\\t\\tBreak;\\n\\t\\tContinue\\n Structures\\n Initialization of Variables\\n Scope Rules: Who Knows About What\\n #define, #include\\n Bit Operators\\n Assignment Operators\\n Floating Point\\n Horrors! goto\\'s and labels\\n Acknowledgements\\n\\n\\n\\n\\n  1.\\xa0 Introduction.\\n\\n       C is a computer language available on the GCOS and UNIX operating\\n  systems at Murray Hill and (in preliminary form) on OS/360 at\\n  Holmdel.\\xa0 C  lets  you  write  your  programs  clearly\\n  and simply it has decent\\n  control flow facilities so your code can be  read  straight  down  the\\n  page, without labels or GOTO\\'s; it lets you write code that is compact\\n  without being too cryptic;  it encourages modularity and good  program\\n  organization; and it provides good data-structuring facilities.\\n\\n       This  memorandum  is a tutorial to make learning C as painless as\\n  possible.\\xa0 The first part concentrates on the central features \\n  of  C; the second part discusses those parts of the language which are\\n  useful (usually for getting more efficient and smaller code) but which\\n  are not  necessary  for  the  new user.\\xa0 This is not\\n  a reference manual.\\xa0 \\n  Details and special cases will be skipped ruthlessly,  and no  attempt\\n  will be made to cover every language feature.\\xa0 \\n  The  order  of presentation is hopefully pedagogical instead of\\n  logical.\\xa0 Users  who\\n  would  like  the full story should consult the \"C\\n  Reference Manual\" by D. M.  Ritchie [1],\\n  which should be read\\n  for details anyway.\\xa0 \\n  Runtime support is described in [2] and\\n  [3];  you will have to read one of these to\\n  learn how to compile and run a C program.\\n\\n       We will assume that  you  are  familiar  with  the  mysteries  of\\n  creating files, text editing, and the like in the operating system you\\n  run on, and that you have programmed in  some language before.\\n\\n\\n\\n                          2.\\xa0 A Simple C Program\\n\\n\\n       main( ) {\\n               printf(\"hello, world\");\\n       }\\n\\n       A C program consists of one  or   more   functions,   which  are\\n  similar  to  the functions and subroutines of a Fortran program or the\\n  procedures of PL/I, and perhaps some external data  definitions.\\xa0 \\nmain\\n  is such a function,  and in fact all C programs must have  a\\n  main.\\xa0 \\n  Execution  of  the  program begins  at  the  first statement of \\n  main.\\xa0 \\nmain  will  usually  invoke  other functions to perform its job,\\n  some coming  from the same program, and others from libraries.\\n\\n       One method of  communicating  data  between   functions   is  by\\n  arguments.\\xa0 \\n  The parentheses following the function name surround the argument\\n  list; here main is a  function  of  no arguments, indicated\\n  by  ( ).\\xa0 The {} enclose the statements of the\\n  function.\\xa0 Individual statements end with a semicolon\\n  but are otherwise free-format.\\n\\nprintf is a library function which will format and  print\\n  output on the terminal (unless some other destination is specified).\\xa0 \\n  In this  case it prints\\n\\n       hello, world\\n\\n  A function is invoked by naming it, followed by  a  list  of arguments\\n  in parentheses.\\xa0 There is no CALL statement as in Fortran or PL/I.\\n\\n\\n\\n      3.\\xa0 A Working C Program; Variables; Types and Type Declarations\\n\\nHere\\'s  a  bigger  program\\n       that adds  three  integers  and prints their sum.\\n\\n       main( ) {\\n               int a, b, c, sum;\\n               a = 1;  b = 2;  c = 3;\\n               sum = a + b + c;\\n               printf(\"sum is %d\", sum);\\n       }\\n\\n       Arithmetic and the assignment statements are  much  the  same  as\\n  in  Fortran  (except  for  the  semicolons) or PL/I.\\xa0 The format of C\\n  programs is quite  free.\\xa0 We  can  put  several statements on a line\\n  if  we  want,  or  we  can split a statement among several lines if it\\n  seems desirable.\\xa0 The split may  be between  any  of  the operators or\\n  variables,  but not in the middle of a name or operator.\\xa0 As\\n  a matter of style, spaces, tabs, and newlines should  be  used  freely  to\\n  enhance readability.\\n\\n       C has four fundamental types of variables:\\n\\n\\n int\\xa0 integer (PDP-11: 16 bits; H6070: 36 bits; IBM360: 32 bits)\\n char\\xa0 one byte character (PDP-11, IBM360: 8 bits; H6070: 9 bits)\\n float\\xa0 single-precision floating point\\n double\\xa0 double-precision floating point\\n\\n\\n  There are also arrays and structures of these  basic  types,  pointers\\n  to  them  and  functions  that return them,  all of which we will meet\\n  shortly.\\n\\n       All variables in a C program must be declared, although this  can\\n  sometimes  be  done implicitly by context.\\xa0 Declarations must\\n  precede executable statements.\\xa0 The declaration\\n\\n       int a, b, c, sum;\\n\\n  declares a, b, c, and sum\\n  to be integers.\\n\\n       Variable names have one to eight characters, chosen from  A-Z,\\n  a-z,  0-9, and _,  and start with a non-digit.\\xa0 \\n  Stylistically, it\\'s much better to use only  a  single  case  and  \\n  give   functions and   external  variables  names  that  are  unique \\n  in  the first six characters.\\xa0 (Function and external\\n  variable  names  are  used  by various assemblers, some of which are\\n  limited in the size and case  of identifiers  they  can handle.)\\n  Furthermore, keywords and library functions may only be recognized\\n  in one case.\\n\\n\\n              4.\\xa0 Constants\\n\\n       We  have already seen decimal integer constants  in  the\\n       previous\\n  example-- 1, 2, and 3.\\xa0 \\n  Since C is often used for system programming\\n  and bit-manipulation,  octal  numbers  are an  important  part  of the\\n  language.\\xa0 In C, any number that begins with 0 (zero!) is an  octal\\n  integer (and  hence can\\'t have any 8\\'s or 9\\'s in it).\\xa0 \\n  Thus 0777 is an octal constant, with decimal value 511.\\n\\n       A ``character\\'\\' is one  byte  (an  inherently   machine-dependent\\n  concept).\\xa0 Most often this is expressed as a character constant,\\n  which is one character  enclosed  in  single quotes.\\xa0 However,\\n  it  may  be any quantity that fits in a byte, as in flags below:\\n\\n       char quest, newline, flags;\\n       quest = \\'?\\';\\n       newline = \\'\\\\n\\';\\n       flags = 077;\\n\\n       The sequence `\\\\n\\'  is  C  notation  for   ``newline  character\\'\\',\\n  which,  when printed,  skips the terminal to the beginning of the next\\n  line.\\xa0 Notice that `\\\\n\\' represents only a single character.\\xa0 \\nThere are  several other ``escapes\\'\\' like `\\\\n\\'  for representing hard-to-get\\n  or invisible characters, such  as  `\\\\t\\'  for tab,  `\\\\b\\' for backspace,\\n  `\\\\0\\' for end of file, and `\\\\\\\\\\' for the backslash itself.\\n\\nfloat and double constants are discussed\\n       in section 26.\\n\\n\\n   5.\\xa0  Simple I/O -- getchar, putchar, printf\\n\\n\\n       main( ) {\\n               char c;\\n               c = getchar( );\\n               putchar(c);\\n       }\\n\\n\\ngetchar  and  putchar  are the basic\\n  I/O library functions in C.  getchar fetches one character\\n  from the  standard  input  (usually  the\\n  terminal)  each time it is called,   and  returns that  character  as\\n  the  value  of  the function.\\xa0 When it reaches the end  of  whatever\\n  file  it  is   reading,   thereafter  it   returns   the   character\\n  represented by `\\\\0\\' (ascii NUL,  which has value zero).\\xa0 We will see\\n  how  to  use  this  very shortly.\\n\\nputchar puts one character out on the standard output\\n  (usually\\n  the terminal) each time it is called.\\xa0 So the program above reads one\\n  character  and  writes  it  back  out.\\xa0 By itself,   this isn\\'t very\\n  interesting, but observe that if we put a loop around this,  and add a\\n  test for end of  file, we have a complete program for copying one file\\n  to another.\\n\\nprintf is a more complicated  function for producing formatted\\n  output.\\xa0 We will talk about only the simplest use of it.\\xa0 Basically,\\n  printf uses its first argument as formatting information, and\\n  any successive arguments as variables to be output.\\xa0 Thus\\n\\n       printf (\"hello, world\\\\n\");\\n\\n  is  the  simplest use.\\xa0 The  string  ``hello,   world\\\\n\\'\\'  is printed\\n  out.\\xa0 No formatting information, no variables, so the string is dumped\\n  out  verbatim.\\xa0 The  newline is necessary to put this out on a\\n  line by itself.\\xa0 (The construction\\n\\n       \"hello, world\\\\n\"\\n\\n  is really an array of chars.\\xa0 More\\n  about this shortly.)\\n\\n       More complicated, if sum is 6,\\n\\n       printf (\"sum is %d\\\\n\", sum);\\n\\n  prints\\n\\n       sum is 6\\n\\n  Within the first argument of printf, the  characters\\n  ``%d\\'\\'  signify that the next argument in the argument list is to be\\n  printed as a base 10 number.\\n\\n       Other useful formatting commands are  ``%c\\'\\'  to  print  out   a\\n  single character,  ``%s\\'\\'  to  print  out an entire string, and ``%o\\'\\'\\n to print a number  as  octal  instead  of decimal (no leading  zero).\\xa0 \\n  For example,\\n\\n       n = 511;\\n       printf (\"What is the value of %d in octal?\", n);\\n       printf (\"%s! %d decimal is %o octal\\\\n\", \"Right\", n, n);\\n\\n  prints\\n\\n       What is the value of 511 in octal?  Right! 511  decimal\\n       is 777 octal\\n\\n  Notice that there is no newline at the end of the first output   line.\\xa0 \\n  Successive  calls  to printf (and/or putchar,\\n  for that matter) simply\\n  put out characters.\\xa0 \\n  No  newlines  are printed unless  you  ask  for\\n  them.\\xa0 Similarly,  on input, characters are read one at a time as you\\n  ask  for  them.\\xa0 Each line is generally terminated\\n  by a newline  (\\\\n), but there is otherwise no concept of record.\\n\\n\\n             6.\\xa0 If;\\n\\trelational operators; compound statements\\n\\n       The basic conditional-testing statement in C is the  if\\n  statement:\\n\\n       c = getchar( );\\n       if( c == \\'?\\' )\\n               printf(\"why did you type a question mark?\\\\n\");\\n\\n  The simplest form of if is\\n\\n       if (expression) statement\\n\\n\\n       The  condition  to  be  tested  is  any   expression  enclosed in\\n  parentheses.\\xa0 It is followed  by  a  statement.\\xa0 \\n  The  expression  is evaluated, and if its value is non-zero, the\\n  statement  is  executed.\\xa0 \\n  There\\'s an optional else clause, to be described soon.\\n\\n       The character sequence `==\\'  is one of \\nthe  relational operators in C; here is\\nthe complete set:\\n\\n       ==      equal to (.EQ. to Fortraners)\\n       !=      not equal to\\n       >       greater than\\n       <       less than\\n       >=      greater than or equal to\\n       <=      less than or equal to\\n\\n\\n       The  value  of  ``expression  relation  expression\\'\\'  is 1 if the\\n  relation is true, and 0 if false.\\xa0 Don\\'t forget that the equality test\\n  is `==\\';  a single `=\\'  causes an assignment, not a test, and\\n  invariably leads to disaster.\\n\\n       Tests can be combined with the operators `&&\\' (AND),\\n  `||\\' (OR), and `!\\' (NOT).\\xa0 For example, we can\\n  test whether a character is blank or tab or newline with\\n\\n       if( c==\\' \\' || c==\\'\\\\t\\' || c==\\'\\\\n\\' ) ...\\n\\n  C  guarantees that `&&\\' and `||\\' are evaluated left to right --\\n  we shall soon see cases where this matters.\\n\\nOne of the nice things about C is  that\\n  the  statement part of an if can  be  made  arbitrarily complicated\\n  by enclosing a set of statements in {}.\\xa0 As a simple example,\\n  suppose we want to ensure that a is bigger than b,\\n  as part of a sort routine.\\xa0 The interchange of a \\n  and b takes three statements in C, grouped together by {}:\\n\\n       if (a < b) {\\n               t = a;\\n               a = b;\\n               b = t;\\n       }\\n\\n\\n       As  a  general  rule  in  C,  anywhere  you  can  use   a  simple\\n  statement, you can use any compound statement,  which is just a number\\n  of simple or compound ones enclosed in  {}.\\xa0 There is  no  semicolon\\n  after  the } of a compound statement, but there is a semicolon after\\n  the last  non-compound statement inside the {}.\\n\\n       The  ability to replace  single  statements  by  complex ones  at\\n  will is one feature that makes  C  much  more  pleasant  to  use  than\\n  Fortran.\\xa0 Logic  (like  the exchange in the previous  example) which\\n  would require several GOTO\\'s and labels in Fortran can and  should  be\\n  done in C  without any, using compound statements.\\n\\n\\n   7.\\xa0 While Statement; Assignment within an Expression; Null Statement\\n\\n       The  basic  looping  mechanism in C is the while  statement.\\xa0 \\n  Here\\'s a program that copies its input to its output a character at  a\\n  time.\\xa0 Remember that `\\\\0\\' marks the end  of file.\\n\\n       main( ) {\\n               char c;\\n               while( (c=getchar( )) != \\'\\\\0\\' )\\n                       putchar(c);\\n       }\\n\\n  The while statement is a loop, whose general form is\\n\\n       while (expression) statement\\n\\n  Its meaning is\\n(a)\\xa0 evaluate the expression\\n    (b)\\xa0 if its value is true (i.e., not zero)\\n                       do the statement, and go back to (a)\\n\\n  Because the expression is tested  before  the  statement  is executed,\\n  the statement  part  can  be  executed  zero  times,  which  is  often\\n  desirable.\\xa0 As  in   the   if   statement,   the  expression\\n  and the statement can both be arbitrarily complicated,  although we\\n  haven\\'t seen that yet.\\xa0 Our example gets the  character,\\n  assigns  it  to c, and then tests if it\\'s a `\\\\0\\'\\'.\\xa0 If it is\\n  not a  `\\\\0\\',  the  statement part of the  while is executed,\\n  printing the  character.\\xa0 The while  then\\n  repeats.\\xa0 When the input character is finally a `\\\\0\\',\\n  the while terminates, and so does main.\\n\\nNotice that we used an assignment\\n       statement\\n\\n       c = getchar( )\\n\\n  within an expression.\\xa0 This is a handy notational shortcut  which\\n  often  produces  clearer  code.\\xa0 (In fact it is often the only way to\\n  write the code cleanly.\\xa0 As  an  exercise,  rewrite  the  file-copy\\n  without using an assignment inside an expression.)\\xa0 It works\\n  because an assignment statement has  a value,  just as any other\\n  expression does.  Its value is the value of the right hand side.\\xa0 \\n  This also implies that we can use multiple assignments like\\n\\n       x = y = z = 0;\\n\\n  Evaluation goes from right to left.\\n\\n       By the way, the extra  parentheses  in  the  assignment statement\\n  within  the conditional were really necessary: if we had said\\n\\n       c = getchar( ) != \\'\\\\0\\'\\n\\nc  would be set to 0 or 1 depending on whether the \\n  character fetched\\n  was an end of file or not.\\xa0 This  is  because  in  the  absence   of\\n  parentheses  the  assignment  operator  `=\\'  is evaluated  after the\\n  relational  operator  `!=\\'.\\xa0 When  in  doubt, or even if not,\\n  parenthesize.\\n\\nSince putchar(c)\\n  returns  c\\n  as its  function  value,   we could also copy the input to the output\\n  by nesting the calls to getchar  and putchar:\\n\\n   main( ) {\\n           while( putchar(getchar( )) != \\'\\\\0\\' ) ;\\n   }\\n\\n  What  statement  is being repeated?   None,  or technically,  the null\\n  statement,   because all the work is really done within the test  part\\n  of  the  while.\\xa0 This  version  is   slightly   different  from  the\\n  previous  one,  because the final `\\\\0\\' is copied to the output  before\\n  we decide to stop.\\n\\n\\n                              8.\\xa0 Arithmetic\\n\\n       The arithmetic operators are the usual `+\\',   `-\\',  `*\\', and  `/\\'\\n  (truncating  integer  division if the operands are both int),  and the\\n  remainder or mod operator `%\\':\\n\\n       x = a%b;\\n\\n  sets x to the remainder after a is divided by b\\n  (i.e., a mod b).\\xa0 The results  are machine dependent unless\\n  a and b are both positive.\\n\\n       In arithmetic, char variables can  usually  be \\n  treated like  int\\n  variables.\\xa0 Arithmetic   on   characters  is quite legal,\\n  and often makes sense:\\n\\n       c = c + \\'A\\' - \\'a\\';\\n\\n  converts a single lower case ascii character stored in  c\\n  to  upper case,  making  use  of the fact that corresponding ascii\\n  letters are a fixed distance apart.\\xa0 The rule governing\\n  this arithmetic is that all chars  are  converted to int\\n  before the arithmetic is done.\\xa0 Beware\\n  that  conversion may  involve sign-extension if  the leftmost bit of a\\n  character  is  1,  the  resulting  integer  might be negative.\\xa0 \\n  (This doesn\\'t happen with genuine characters on any current machine.)\\n\\n       So to convert a file into lower case:\\n\\n       main( ) {\\n               char c;\\n               while( (c=getchar( )) != \\'\\\\0\\' )\\n                       if( \\'A\\'<=c && c<=\\'Z\\' )\\n                               putchar(c+\\'a\\'-\\'A\\');\\n                       else\\n                               putchar(c);\\n       }\\n\\n  Characters have different  sizes  on  different  machines.\\xa0 Further,\\n  this code won\\'t work on an IBM machine,  because the  letters  in  the\\n  ebcdic alphabet are not contiguous.\\n\\n\\n\\n                 9.\\xa0 Else Clause;\\n\\t\\t Conditional Expressions\\n\\n      We  just used an else after an  if.\\xa0 \\n      The  most  general form of if is\\n\\n       if (expression) statement1 else statement2\\n\\n  the else part is optional, but often useful.\\xa0 \\n  The  canonical example sets x to the minimum of a\\n  and b:\\n\\n       if (a < b)\\n               x = a;\\n       else\\n               x = b;\\n\\n  Observe that there\\'s a semicolon after x=a.\\n\\nC provides an alternate form of conditional\\n  which   is  often more concise.\\xa0 \\n  It is called the ``conditional expression\\'\\' because it\\n  is a conditional  which  actually   has   a  value  and  can  be  used\\n  anywhere an expression can.\\xa0 The value of\\n\\n       a<b ? a : b;\\n\\n  is a if a is less than b; it\\n  is b  otherwise.\\xa0 In  general, the form\\n\\n       expr1 ? expr2 : expr3\\n\\n  means  ``evaluate  expr1.\\xa0 If it is not zero,\\n  the value of the whole thing is expr2; otherwise the value\\n  is expr3.\\'\\'\\n\\n       To set x to the minimum of a and\\n       b, then:\\n\\n       x = (a<b ? a : b);\\n\\n  The parentheses aren\\'t necessary because `?:\\'   is   evaluated  before\\n  `=\\', but safety first.\\n\\n       Going  a  step further,  we could write the  loop  in  the\\n       lower-case program as\\n\\n       while( (c=getchar( )) != \\'\\\\0\\' )\\n               putchar( (\\'A\\'<=c && c<=\\'Z\\') ? c-\\'A\\'+\\'a\\' : c );\\n\\n\\n       If\\'s and else\\'s can be used  to  construct  logic  that  branches\\n  one of several ways and then rejoins, a common programming structure,\\n  in this way:\\n\\n       if(...)\\n               {...}\\n       else if(...)\\n               {...}\\n       else if(...)\\n               {...}\\n       else\\n               {...}\\n\\n  The conditions are tested in order, and exactly one block is executed;\\n  either  the first one whose if is satisfied,  or the one for the  last\\n  else.\\xa0 When this block is finished, the next\\n  statement executed is the one after the last else.\\xa0 \\n  If  no  action  is  to  be  taken  for  the ``default\\'\\' case, omit\\n  the last else.\\n\\n       For example,  to count letters, digits and others  in  a file, we\\n  could write\\n\\n  main( ) {\\n          int let, dig, other, c;\\n          let = dig = other = 0;\\n          while( (c=getchar( )) != \\'\\\\0\\' )\\n                  if( (\\'A\\'<=c && c<=\\'Z\\') || (\\'a\\'<=c &&  c<=\\'z\\') )\\n                        ++let;\\n                  else if( \\'0\\'<=c && c<=\\'9\\' ) ++dig;\\n                  else  ++other;\\n          printf(\"%d letters, %d digits, %d others\\\\n\", let, dig, other);\\n  }\\n\\n  The `++\\' operator means ``increment by 1\\'\\'; we will  get  to it in the\\n  next section.\\n\\n\\n\\n                  10.\\xa0 Increment and Decrement Operators\\n\\n       In  addition  to  the  usual  `-\\',   C   also   has   two   other\\n  interesting  unary    operators,    `++\\'    (increment)    and    `--\\'\\n  (decrement).\\xa0 Suppose we want to count the lines in a file.\\n\\n       main( ) {\\n               int c,n;\\n               n = 0;\\n               while( (c=getchar( )) != \\'\\\\0\\' )\\n                       if( c == \\'\\\\n\\' )\\n                               ++n;\\n               printf(\"%d lines\\\\n\", n);\\n       }\\n\\n++n is equivalent to n=n+1 but clearer,\\n  particularly when   n  is   a\\n  complicated  expression.\\xa0 `++\\'  and `--\\' can be applied only to\\n  int\\'s and char\\'s (and pointers which we haven\\'t\\n  got to yet).\\n\\n       The unusual feature of `++\\' and `--\\' is that  they  can  be  used\\n  either before or after a variable.\\xa0 The value of ++k\\n  is the value of k after it has been\\n  incremented.\\xa0 The  value of k++ is k\\nbefore  it  is incremented.\\xa0 Suppose k\\n  is 5.\\xa0 Then\\n\\n       x = ++k;\\n\\n  increments k to 6 and then sets x to \\n  the  resulting  value, i.e., to 6.\\xa0 But\\n\\n       x = k++;\\n\\n  first sets x  to  to  5,  and   then \\n  increments   k   to   6.\\xa0 The\\n  incrementing effect  of  ++k and k++ is\\n  the same, but their values are respectively 5 and 6.\\xa0 \\n  We shall soon see examples where both of these uses are important.\\n\\n\\n\\n                                11.\\xa0 Arrays\\n\\n       In C,  as in Fortran or PL/I,  it  is  possible  to  make arrays\\n  whose elements are basic types.\\xa0 Thus we can  make  an  array \\n  of  10 integers with the declaration\\n\\n       int x[10];\\n\\n  The square brackets mean subscripting; parentheses are  used only  for\\n  function references.\\xa0 Array indexes begin at zero, so the\\n  elements of x are\\n\\n       x[0], x[1], x[2], ..., x[9]\\n\\n  If an array has n elements, the largest subscript is\\n  n-1.\\n\\n       Multiple-dimension arrays are provided,  though  not  much  used\\n  above two  dimensions.\\xa0 The declaration and use look like\\n\\n       int name[10] [20];\\n       n = name[i+j] [1] + name[k] [2];\\n\\n  Subscripts can be  arbitrary  integer  expressions.\\xa0 \\n  Multi-dimension\\n  arrays  are  stored  by  row  (opposite to Fortran),  so the rightmost\\n  subscript varies fastest; name has 10 rows and 20 columns.\\n\\n       Here is a program which reads a line,  stores  it  in  a buffer,\\n  and prints its length (excluding the newline at the end).\\n\\n       main( ) {\\n               int n, c;\\n               char line[100];\\n               n = 0;\\n               while( (c=getchar( )) != \\'\\\\n\\' ) {\\n                       if( n < 100 )\\n                               line[n] = c;\\n                       n++;\\n               }\\n               printf(\"length = %d\\\\n\", n);\\n       }\\n\\n\\n       As a more complicated problem,  suppose we  want  to  print  the\\n  count  for  each   line   in  the  input,  still storing the first 100\\n  characters of each line.\\xa0 Try it  as  an  exercise before looking  at\\n  the solution:\\n\\n       main( ) {\\n               int n, c; char line[100];\\n               n = 0;\\n               while( (c=getchar( )) != \\'\\\\0\\' )\\n                       if( c == \\'\\\\n\\' ) {\\n                               printf(\"%d0, n);\\n                               n = 0;\\n                       }\\n                       else {\\n                               if( n < 100 ) line[n] = c;\\n                               n++;\\n                       }\\n       }\\n\\n\\n\\n       12.\\xa0 Character Arrays; Strings\\n\\n       Text is usually kept as an array of characters,  as  we did  with\\n  line[ ] in the example above.\\xa0 By convention in C,\\n  the last character\\n  in  a  character  array should be a `\\\\0\\' because  most  programs that\\n  manipulate  character  arrays expect  it.\\xa0 For  example,\\n  printf uses the `\\\\0\\' to detect  the end of a character\\n  array when printing it out with a `%s\\'.\\n\\nWe can copy a character array s\\n       into another t like this:\\n\\n               i = 0;\\n               while( (t[i]=s[i]) != \\'\\\\0\\' )\\n                       i++;\\n\\n\\n       Most  of the time we have to put in our own `\\\\0\\' at  the end  of\\n  a string;  if we want to print the line with printf,  it\\'s\\n  necessary.\\xa0 This code prints the character count before the line:\\n\\n       main( ) {\\n               int n;\\n               char line[100];\\n               n = 0;\\n               while( (line[n++]=getchar( )) != \\'\\\\n\\' );\\n               line[n] = \\'\\\\0\\';\\n               printf(\"%d:\\\\t%s\", n, line);\\n       }\\n\\n  Here we increment n in the subscript  itself,\\n  but  only   after  the previous  value   has  been  used.\\xa0 \\n  The character is read,  placed in\\n  line[n], and only then n is incremented.\\n\\n       There is one place and one place only where C  puts  in the  `\\\\0\\'\\n  at  the  end  of  a  character  array  for  you,  and  that  is in the\\n  construction\\n\\n       \"stuff between double quotes\"\\n\\n  The compiler puts a `\\\\0\\' at  the  end  automatically.\\xa0 Text enclosed\\n  in  double  quotes  is  called  a  string;  its  properties  are\\n  precisely those of an (initialized) array of characters.\\n\\n\\n                            13.\\xa0 For Statement\\n\\n       The for statement is a somewhat generalized\\n   while  that  lets  us\\n  put  the  initialization  and  increment parts of a loop into a single\\n  statement along with the  test.\\xa0 The  general form of the\\n  for is\\n\\n       for( initialization; expression; increment )\\n               statement\\n\\n  The meaning is exactly\\n\\n        initialization;\\n        while( expression ) {\\n               statement\\n               increment;\\n        }\\n\\n  Thus,  the  following code does the same  array  copy  as\\n  the example in the previous section:\\n\\n        for( i=0; (t[i]=s[i]) != \\'\\\\0\\'; i++ );\\n\\n  This slightly more ornate example adds up the elements of an array:\\n\\n        sum = 0;\\n        for( i=0; i<n; i++)\\n               sum = sum + array[i];\\n\\n\\n       In the for statement,  the initialization  can  be  left out  if\\n  you want,  but the semicolon has to be there.\\xa0 The increment is also\\n  optional.\\xa0 It is not followed by  a  semicolon.\\xa0 \\n  The  second clause,\\n  the test, works the same way as in the while:  if \\n  the  expression  is true   (not   zero)  do another  loop,  otherwise get\\n  on with the next statement.\\xa0 As with the while,\\n  the for loop may be done zero times.\\xa0 If the expression is left out,\\n  it is taken to be always true, so\\n\\n        for( ; ; ) ...\\n\\n  and\\n\\n        while( 1 ) ...\\n\\n  are both infinite loops.\\n\\n       You  might ask why we use a for since it\\'s so much\\n       like a while.\\xa0 \\n  (You might also ask why we use a while because...)\\xa0 \\n\\xa0 The for is  usually\\n  preferable  because   it   keeps   the   code  where   it\\'s   used and\\n  sometimes eliminates the need for compound  statements,   as  in  this\\n  code  that  zeros  a two-dimensional array:\\n\\n        for( i=0; i<n; i++ )\\n               for( j=0; j<m; j++ )\\n                       array[i][j] = 0;\\n\\n\\n\\n                         14.\\xa0 Functions; Comments\\n\\n       Suppose we want,  as part of a  larger  program,  to   count  the\\n  occurrences of the ascii characters in some input text.\\xa0 Let us also\\n  map illegal characters (those with value>127  or <0)  into  one  pile.\\xa0 \\n  Since  this  is  presumably  an  isolated  part of the program,  good\\n  practice dictates making  it  a separate function.\\xa0 Here is one way:\\n\\n       main( ) {\\n               int hist[129];      /*  128 legal chars + 1 illegal group*/\\n               ...\\n               count(hist, 128);   /* count the letters into hist */\\n               printf( ... );      /* comments look like this; use them */\\n               ...        /* anywhere blanks, tabs or newlines could appear */\\n       }\\n\\n       count(buf, size)\\n          int size, buf[ ]; {\\n               int i, c;\\n               for( i=0; i<=size; i++ )\\n                       buf[i]  =  0;                     /*  set buf to zero */\\n               while(  (c=getchar(  )) != \\'\\\\0\\' ) {       /* read til eof */\\n                       if( c > size || c < 0 )\\n                               c = size;                 /* fix illegal input */\\n                       buf[c]++;\\n               }\\n               return;\\n       }\\n\\n  We  have already seen many examples of calling a function,  so let  us\\n  concentrate  on how to define one.\\xa0 Since count\\n  has two arguments, we need to declare  them, as shown, giving their types,\\n  and in the\\n  case of buf, the fact that it is an array.\\xa0 The\\n  declarations of\\n  arguments go between the argument  list and the opening `{\\'.\\xa0 There\\n  is no need to specify the size of the array buf,\\n  for it is defined outside of count.\\n\\n       The  return  statement  simply  says  to  go back to the calling\\n  routine.\\xa0 In fact, we could have omitted it, since a return is implied\\n  at the end of a function.\\n\\n       What  if we wanted count to  return  a  value,\\n       say  the number of characters read?\\xa0 The return statement\\n       allows for this too:\\n\\n               int i, c, nchar;\\n               nchar = 0;\\n               ...\\n               while( (c=getchar( )) != \\'\\\\0\\' ) {\\n                       if( c > size || c < 0 )\\n                               c = size;\\n                       buf[c]++;\\n                       nchar++;\\n               }\\n               return(nchar);\\n\\n  Any expression can appear within the parentheses.\\xa0 Here is a  function\\n  to compute the minimum of two integers:\\n\\n       min(a, b)\\n          int a, b; {\\n               return( a < b ? a : b );\\n       }\\n\\n\\n\\n       To copy a character array, we could write the function\\n\\n       strcopy(s1, s2)         /* copies s1 to s2 */\\n          char s1[ ], s2[ ]; {\\n               int i;\\n               for( i = 0; (s2[i] = s1[i]) != \\'\\\\0\\'; i++ );\\n       }\\n\\n  As is often the case, all the work is done by the assignment statement\\n  embedded in the test part of the for.\\xa0 Again, the declarations of the\\n  arguments s1  and  s2  omit  the  sizes,  because  they  don\\'t  matter\\n  to strcopy.\\xa0 (In the section on\\n  pointers, we will see a more efficient way to do a string copy.)\\n\\n       There is a subtlety in  function  usage   which   can   trap  the\\n  unsuspecting Fortran programmer.\\xa0 Simple variables (not arrays) are\\n  passed in C by ``call by  value\\'\\',   which  means  that   the   called\\n  function  is  given  a  copy of its arguments,  and doesn\\'t know their\\n  addresses.\\xa0 This makes it  impossible to change the value of  one \\n  of the actual input arguments.\\n\\n       There  are two ways out of this dilemma.\\xa0 One is to make special\\n  arrangements to pass to the function the address of a variable instead\\n  of its value.\\xa0 The  other  is to  make  the variable  a  global  or\\n  external variable, which is known to each function by its name.\\xa0 We\\n  will discuss both  possibilities in the next few sections.\\n\\n\\n                  15.\\xa0 Local and External Variables\\n\\n       If we say\\n\\n       f( ) {\\n               int x;\\n               ...\\n       }\\n       g( ) {\\n               int x;\\n               ...\\n       }\\n\\n  each x is local to its own routine -- the x\\n  in f is unrelated to  the x in\\n  g.\\xa0 (Local variables are also called\\n  ``automatic\\'\\'.)\\xa0 \\n  Furthermore each local variable in a routine appears  only  when   the\\n  function  is  called,  and  disappears  when the function is\\n  exited.\\xa0 Local variables have no memory from one call to the next and\\n  must be explicitly initialized upon each entry.\\xa0 (There is a\\n  static storage class for making local variables  with  memory;\\n  we  won\\'t discuss it.)\\n\\n       As  opposed to local variables,  external  variables  are defined\\n  external  to  all  functions,  and are (potentially) available to  all\\n  functions.\\xa0 External  storage always remains in \\n  existence.\\xa0 To make\\n  variables external we have to define them external to  all  functions,\\n  and, wherever we want to use them, make a declaration.\\n\\n       main( ) {\\n               extern int nchar, hist[ ];\\n               ...\\n               count( );\\n               ...\\n       }\\n\\n       count( ) {\\n               extern int nchar, hist[ ];\\n               int i, c;\\n               ...\\n       }\\n\\n       int     hist[129];      /* space for histogram */\\n       int     nchar;          /* character count */\\n\\n  Roughly speaking,  any function  that  wishes  to  access  an external\\n  variable must contain an extern declaration for it.\\xa0 The declaration\\n  is the same as others, except for the added keyword\\n  extern.\\xa0 \\n  Furthermore,  there must  somewhere be a definition  of  the  external\\n  variables external to  all functions.\\n\\n       External variables can be initialized;  they are set  to zero  if\\n  not explicitly initialized.\\xa0 In its simplest form, initialization is\\n  done  by  putting  the  value  (which  must  be  a constant) after the\\n  definition:\\n\\n       int     nchar   0;\\n       char    flag    \\'f\\';\\n         etc.\\n\\n  This is discussed further in a later section.\\n\\n\\n       This  ends our discussion of what might  be  called  the central\\n  core of C.\\xa0 You now have enough to write quite substantial C programs,\\n  and  it would probably be a good idea if you  paused long enough to do\\n  so.\\xa0 The rest  of  this  tutorial will describe some more ornate\\n  constructions, useful but not essential.\\n\\n\\n                               16.\\xa0 Pointers\\n\\n       A  pointer  in  C is the address of something.\\xa0 It is a rare\\n  case indeed  when  we  care what the specific address itself  is,  but\\n  pointers are a quite common way to get at the contents  of  something.\\xa0 \\n  The unary operator `&\\' is used to produce the address of an object, if\\n  it has one.\\xa0 Thus\\n\\n          int a, b;\\n          b = &a;\\n\\n  puts the address of a into b.\\xa0 \\n  We  can\\'t  do  much  with   it  except\\n  print it or pass it to some other routine,  because we haven\\'t given\\n  b the right kind of declaration.\\xa0 \\n  But  if   we  declare   that   b  is indeed a pointer\\n  to an integer, we\\'re in good shape:\\n\\n          int a, *b, c;\\n          b = &a;\\n          c = *b;\\n\\nb  contains the address of a and `c = *b\\' means\\n  to  use  the value in\\n  b as an address,  i.e.,  as a pointer.\\xa0 \\n  The effect is that  we   get\\n  back the contents   of   a, albeit rather indirectly.\\xa0 \\n  (It\\'s always the case that `*&x\\' is the same as x if\\n  x has an address.)\\n\\n       The  most  frequent  use  of  pointers  in  C  is   for   walking\\n  efficiently along arrays.\\xa0 In fact, in the implementation of an array,\\n  the array  name represents  the  address  of  the  zeroth  element  of\\n  the array, so you can\\'t use it on the left side of an expression.\\xa0 \\n  (You can\\'t  change  the  address of something by assigning to it.)\\xa0 \\n  If we say\\n\\n       char *y;\\n       char x[100];\\n\\ny is of type pointer to character (although it doesn\\'t yet\\n  point anywhere).\\xa0 We can make y point to an element\\n  of x by either of\\n\\n       y = &x[0];\\n       y = x;\\n\\n  Since x is the address of x[0] this is\\n  legal and consistent.\\n\\n       Now `*y\\' gives x[0].\\xa0 More importantly,\\n\\n       *(y+1)  gives x[1]\\n       *(y+i)  gives x[i]\\n\\n  and the sequence\\n\\n               y = &x[0];\\n               y++;\\n\\n  leaves y pointing at x[1].\\n\\n       Let\\'s use pointers in a function length  that  computes how  long\\n  a  character  array  is.\\xa0 Remember  that  by convention all\\n  character\\n  arrays are terminated with  a   `\\\\0\\'.\\xa0 (And   if  they   aren\\'t,\\n  this program will blow up inevitably.)\\xa0 The old way:\\n\\n       length(s)\\n          char s[ ]; {\\n               int n;\\n               for( n=0; s[n] != \\'\\\\0\\'; )\\n                       n++;\\n               return(n);\\n       }\\n\\n  Rewriting with pointers gives\\n\\n       length(s)\\n          char *s; {\\n               int n;\\n               for( n=0; *s != \\'\\\\0\\'; s++ )\\n                       n++;\\n               return(n);\\n       }\\n\\n  You  can  now see why we have to say  what  kind  of  thing  s\\n  points to -- if we\\'re to increment it with s++ we have to\\n  increment  it  by  the right amount.\\n\\n       The  pointer  version  is more efficient (this  is  almost always\\n  true) but even more compact is\\n\\n               for( n=0; *s++ != \\'\\\\0\\'; n++ );\\n\\n  The `*s\\'  returns  a  character;   the  `++\\'  increments  the pointer\\n  so we\\'ll  get the next character next time around.\\xa0 As you can see,\\n  as we make things more  efficient, we  also make them less clear.\\xa0 \\n  But `*s++\\' is an idiom so common that you have to know it.\\n\\n\\nGoing a step further, here\\'s our function\\n    strcopy  that copies a character array s to another t.\\n\\n       strcopy(s,t)\\n          char *s, *t; {\\n               while(*t++ = *s++);\\n       }\\n\\n  We  have  omitted  the   test   against   `\\\\0\\',    because   `\\\\0\\'   is\\n  identically zero;  you  will  often  see the code this way.\\xa0 (You\\n  must\\n  have a space after the `=\\': see section 25.)\\n\\n       For  arguments   to   a   function,    and   there   only,    the\\n  declarations\\n\\n       char s[ ];\\n       char *s;\\n\\n  are  equivalent  --  a   pointer   to   a   type,   or   an   array  of\\n  unspecified size of that type, are the same thing.\\n\\n       If this all seems mysterious,  copy  these   forms   until  they\\n  become   second  nature.\\xa0 You   don\\'t  often  need  anything  more\\n  complicated.\\n\\n\\n\\n                          17.\\xa0 Function Arguments\\n\\n       Look back at the function strcopy in the previous\\n  section.\\xa0 We\\n  passed  it  two  string names as arguments,  then proceeded to clobber\\n  both of them by  incrementation.\\xa0 So   how  come  we  don\\'t\\n  lose  the original strings in the function that called strcopy?\\n\\n       As we said before, C is a ``call by  value\\'\\'  language: when  you\\n  make a function call like f(x),  the value of x\\n  is  passed,  not  its address.\\xa0 So  there\\'s  no way to alter\\nx  from inside  f.\\xa0 If x\\n  is an array (char x[10]) this isn\\'t a  problem,  because  x\\nis  an  address anyway,  and you\\'re not trying to  change  it,\\n  just what it addresses.\\xa0 \\n  This is why strcopy works as it does.\\xa0 \\n  And it\\'s convenient not to have to  worry about making temporary\\n  copies of the input arguments.\\n\\nBut what if x is a scalar and you do want to change it?\\xa0 \\n   In  that\\n  case,  you have to pass the address of x to\\n  f,  and then use it  as  a pointer.\\xa0 Thus\\n  for  example,  to  interchange two integers,  we must\\n  write\\n\\n   flip(x, y)\\n      int *x, *y; {\\n           int temp;\\n           temp = *x;\\n           *x = *y;\\n           *y = temp;\\n   }\\n\\n  and to call flip, we have to pass the addresses of the variables:\\n\\n       flip (&a, &b);\\n\\n\\n\\n\\n          18.\\xa0 Multiple Levels of Pointers;\\n\\t  Program Arguments\\n\\n       When  a C program is called,  the arguments on  the  command line\\n  are made available to the main program as an argument count argc\\n  and an  array  of  character  strings  argv containing\\n  the arguments.\\xa0 \\n  Manipulating these arguments is one of  the  most  common   uses   of\\n  multiple  levels  of pointers (``pointer  to  pointer to ...\\'\\').\\xa0 By\\n  convention,  argc is  greater  than  zero;  the  first s\\n  argument  (in argv[0]) is  the command name itself.\\n\\n       Here is a program that simply echoes its arguments.\\n\\n       main(argc, argv)\\n          int argc;\\n          char **argv; {\\n               int i;\\n               for( i=1; i < argc; i++ )\\n       }\\n\\n  Step by step:  main is called with two arguments,\\n  the  argument count\\n  and  the  array  of arguments.\\xa0 argv is a\\n  pointer to an array, whose\\n  individual elements are pointers to arrays  of   characters.\\xa0 \\n  The zeroth  argument  is  the  name of the command itself,  so we start to\\n  print with the  first argument, until  we\\'ve  printed them all.\\xa0 Each\\n  argv[i] is a character array, so we use a `%s\\' in the printf.\\n\\n       You will sometimes see the declaration of argv  written as\\n\\n       char *argv[ ];\\n\\n  which is equivalent.\\xa0 But we can\\'t  use\\n  char argv[  ][  ], because\\n  both  dimensions  are variable and there would be no way to figure out\\n  how big the array is.\\n\\n       Here\\'s  a  bigger  example  using  argc  and \\n       argv.\\xa0 A common\\n  convention   in  C  programs is that if the first argument is `-\\', it\\n  indicates a flag of some sort.\\xa0 For example, suppose we want a program\\n  to be callable as\\n\\n       prog -abc arg1 arg2 ...\\n\\n  where  the  `-\\'  argument  is  optional;  if it is present,  it may be\\n  followed by any combination of a, b, and c.\\n\\n       main(argc, argv)\\n          int argc;\\n          char **argv; {\\n               ...\\n               aflag = bflag = cflag  = 0;\\n               if( argc > 1 && argv[1][0] == \\'-\\' ) {\\n                       for( i=1; (c=argv[1][i]) != \\'\\\\0\\'; i++ )\\n                               if( c==\\'a\\' )\\n                                       aflag++;\\n                               else if( c==\\'b\\' )\\n                                       bflag++;\\n                               else if( c==\\'c\\' )\\n                                       cflag++;\\n                               else\\n                                       printf(\"%c?\\\\n\", c);\\n                       --argc;\\n                       ++argv;\\n               }\\n               ...\\n\\n\\n       There  are several  things  worth  noticing  about  this code.\\xa0 \\n  First,   there  is  a real need for the left-to-right evaluation  that\\n  &&   provides;   we   don\\'t  want  to  look  at argv[1]\\n  unless we know it\\'s there.\\xa0 Second, the statements\\n\\n               --argc;\\n               ++argv;\\n\\n  let us march along the argument list by one position,  so  we can skip\\n  over the flag argument as if it had never existed;  the  rest  of  the\\n  program  is  independent   of   whether   or   not  there   was a flag\\n  argument.\\xa0 This only works because argv is a \\n  pointer  which  can  be incremented.\\n\\n\\n\\n                19.\\xa0 The Switch Statement;\\n\\t\\t\\tBreak; Continue\\n\\n       The  switch statement can be used to replace the  multi-way  test\\n  we used in the last example.\\xa0 When the tests are\\n  like this:\\n\\n       if( c == \\'a\\' ) ...\\n       else if( c == \\'b\\' ) ...\\n       else if( c == \\'c\\' ) ...\\n       else ...\\n\\n  testing a value against a series of  constants,  the  switch statement\\n  is often  clearer and usually gives better code.\\xa0 Use it like this:\\n\\n       switch( c ) {\\n\\n       case \\'a\\':\\n               aflag++;\\n               break;\\n       case \\'b\\':\\n               bflag++;\\n               break;\\n       case \\'c\\':\\n               cflag++;\\n               break;\\n       default:\\n               printf(\"%c?\\\\n\", c);\\n               break;\\n       }\\n\\n\\n  The case statements  label  the  various  actions  we  want; \\n  default\\n  gets done if none of the other cases  are  satisfied.\\xa0 (A\\n  default is\\n  optional;  if it isn\\'t there,  and none  of  the cases match, you just\\n  fall out the bottom.)\\n\\n       The break statement in this  example  is  new.\\xa0 It   is  there\\n  because the  cases are just labels,  and after you do one of them, you\\n  fall through to the next  unless  you  take some  explicit  action  to\\n  escape.\\xa0 This is a mixed blessing.\\xa0 On the positive side,\\n  you can have multiple cases on a single  statement;\\n  we might want to  allow  both upper and lower\\n\\n       case \\'a\\':  case \\'A\\':    ...\\n\\n       case \\'b\\':  case \\'B\\':    ...\\n        etc.\\n\\n  But what if we just want to get out after doing case  `a\\' ?  We  could\\n  get  out of a case of the switch with a label\\n  and a goto,  but this is really ugly.\\xa0 The break\\n  statement lets  us exit without either goto or label.\\n\\n       switch( c ) {\\n\\n       case \\'a\\':\\n               aflag++;\\n               break;\\n       case \\'b\\':\\n               bflag++;\\n               break;\\n        ...\\n       }\\n       /* the break statements get us here directly */\\n\\n  The break statement also works in for and while statements;  it causes\\n  an immediate exit from the loop.\\n\\n       The continue  statement   works   only  inside\\n       for\\'s   and while\\'s;\\n       it  causes  the  next  iteration of the loop to be started.\\xa0 \\n  This means it goes to the increment  part  of  the for  and\\n  the  test part of the while.\\xa0 We could have used\\n  a continue in our example to get on with the next iteration of the\\n  for, but it seems  clearer  to  use\\n  break instead.\\n\\n\\n\\n                              20.\\xa0 Structures\\n\\n       The  main  use of structures is to lump together  collections  of\\n  disparate variable types,  so they can conveniently be  treated  as  a\\n  unit.\\xa0 For example, if  we  were  writing  a compiler  or  assembler,\\n  we  might need for  each  identifier  information  like  its  name  (a\\n  character  array),  its  source line  number  (an integer),  some type\\n  information  (a  character,  perhaps),  and  probably  a  usage  count\\n  (another integer).\\n\\n               char    id[10];\\n               int     line;\\n               char    type;\\n               int     usage;\\n\\n\\n       We  can  make a structure out of this quite  easily.\\xa0 We first\\n  tell  C what  the structure will look like,  that is,  what  kinds  of\\n  things it contains;  after that we can actually reserve  storage  for\\n  it,  either  in the same statement or separately.\\xa0 The simplest thing\\n  is to define it and allocate storage all at once:\\n\\n       struct {\\n               char    id[10];\\n               int     line;\\n               char    type;\\n               int     usage;\\n       } sym;\\n\\n\\n       This  defines  sym to be a structure with  the  specified shape;\\n  id,  line,  type and usage\\n  are members of the structure.\\xa0 The way we\\n  refer to any  particular  member  of  the structure is\\n\\n       structure-name . member\\n\\n  as in\\n\\n               sym.type = 077;\\n               if( sym.usage == 0 ) ...\\n               while( sym.id[j++] ) ...\\n                  etc.\\n\\n  Although the names of structure members  never   stand   alone,  they\\n  still  have  to be unique;  there can\\'t be another id or usage in some\\n  other structure.\\n\\n      So  far  we   haven\\'t   gained   much.\\xa0 The   advantages of\\n  structures start to come when we have arrays of structures, or when we\\n  want to pass complicated  data  layouts  between functions.\\xa0 \\n  Suppose we wanted to make a symbol table for up to 100\\n  identifiers.\\xa0 We could\\n  extend our definitions like\\n\\n               char    id[100][10];\\n               int     line[100];\\n               char    type[100];\\n               int     usage[100];\\n\\n  but  a  structure lets us rearrange this  spread-out  information  so\\n  all the data about a single identifer is collected into one lump:\\n\\n       struct {\\n               char    id[10];\\n               int     line;\\n               char    type;\\n               int     usage;\\n       } sym[100];\\n\\n  This  makes sym an array of structures;   each  array  element has the\\n  specified shape.\\xa0 Now we can refer to members as\\n\\n               sym[i].usage++; /* increment usage of i-th identifier */\\n               for( j=0; sym[i].id[j++] != \\'\\\\0\\'; ) ...\\n                  etc.\\n\\n  Thus to print a list of all identifiers   that   haven\\'t   been  used,\\n  together with their line number,\\n\\n            for( i=0; i<nsym; i++ )\\n                    if( sym[i].usage == 0 )\\n                            printf(\"%d\\\\t%s\\\\n\", sym[i].line, sym[i].id);\\n\\n\\n       Suppose we now want to write  a   function   lookup(name)  which\\n  will  tell us if name already exists in sym,\\n  by giving its index,  or\\n  that it doesn\\'t, by returning a -1.\\xa0 We  can\\'t pass \\n  a structure to a\\n  function directly;  we have to either define it externally,  or pass a\\n  pointer to it.\\xa0 Let\\'s  try the first way first.\\n\\n       int     nsym    0;      /* current length of symbol table */\\n\\n       struct {\\n               char    id[10];\\n               int     line;\\n               char    type;\\n               int     usage;\\n       } sym[100];             /* symbol table */\\n\\n       main( ) {\\n               ...\\n               if( (index = lookup(newname)) >= 0 )\\n                       sym[index].usage++;              /* already there ... */\\n               else\\n                       install(newname, newline, newtype);\\n               ...\\n       }\\n\\n       lookup(s)\\n          char *s; {\\n               int i;\\n               extern struct {\\n                       char    id[10];\\n                       int     line;\\n                       char    type;\\n                       int     usage;\\n               } sym[ ];\\n\\n               for( i=0; i<nsym; i++ )\\n                       if( compar(s, sym[i].id) > 0 )\\n                               return(i);\\n               return(-1);\\n       }\\n\\n       compar(s1,s2)           /*  return 1 if s1==s2, 0 otherwise */\\n          char *s1, *s2; {\\n               while( *s1++ == *s2 )\\n                       if( *s2++ == \\'\\\\0\\' )\\n                               return(1);\\n               return(0);\\n       }\\n\\n\\n  The  declaration  of  the  structure  in lookup isn\\'t  needed  if the\\n  external definition precedes its use in the same source  file,  as  we\\n  shall see in a moment.\\n\\n       Now what if we want to use pointers?\\n\\n       struct  symtag {\\n               char    id[10];\\n               int     line;\\n               char    type;\\n               int     usage;\\n       } sym[100], *psym;\\n\\n               psym = &sym[0]; /* or p = sym; */\\n\\n  This makes psym a pointer  to  our  kind  of  structure \\n  (the  symbol table), then initializes it to point to the first element\\n  of sym.\\n\\n       Notice  that we added something after the word struct:  a ``tag\\'\\'\\n  called symtag.\\xa0 This puts a name on our structure definition\\n  so we can\\n  refer  to  it  later  without  repeating the definition.\\xa0 \\n  It\\'s  not necessary but useful.\\xa0 In fact we could have said\\n\\n       struct  symtag {\\n               ... structure definition\\n       };\\n\\n  which wouldn\\'t have assigned any storage at all, and then said\\n\\n       struct  symtag  sym[100];\\n       struct  symtag  *psym;\\n\\n  which would define the array and the pointer.\\xa0 This could be\\n  condensed further, to\\n\\n       struct  symtag  sym[100], *psym;\\n\\n\\n       The  way  we  actually  refer  to  an  member of a structure by a\\n  pointer is like this:\\n\\n               ptr -> structure-member\\n\\n  The  symbol  `->\\'  means   we\\'re   pointing   at   a   member   of   a\\n  structure; `->\\'  is  only  used  in that context.\\xa0 ptr\\n  is a pointer to\\n  the (base  of) a  structure  that  contains  the  structure  member.\\xa0 \\n  The   expression  ptr->structure-member refers to the indicated member\\n  of the pointed-to structure.\\xa0 Thus we have constructions like:\\n\\n       psym->type = 1;\\n       psym->id[0] = \\'a\\';\\n\\n  and so on.\\n\\n       For  more  complicated  pointer  expressions,  it\\'s  wise  to use\\n  parentheses  to  make it clear who goes with what.\\xa0 For example,\\n\\n       struct { int x, *y; } *p;\\n       p->x++  increments x\\n       ++p->x  so does this!\\n       (++p)->x        increments p before getting x\\n       *p->y++ uses y as a pointer, then increments it\\n       *(p->y)++       so does this\\n       *(p++)->y       uses y as a pointer, then increments p\\n\\n  The way to remember these is that ->,  .  (dot),  ( ) and  [  ]  bind\\n  very  tightly.\\xa0 An  expression involving one of these is treated as a\\n  unit.\\xa0 p->x, a[i],  y.x\\n  and  f(b)  are  names exactly as abc is.\\n\\n       If p is a pointer to a structure,  any arithmetic  on\\n       p  takes\\n  into  account   the actual size of the structure.\\xa0 For instance,\\n  p++ increments p by the correct amount to get\\n  the next  element   of  the\\n  array of structures.\\xa0 But don\\'t assume that the size of a structure is\\n  the sum of the sizes of  its  members   --  because  of  alignments  of\\n  different sized objects, there may be ``holes\\'\\' in a structure.\\n\\n      Enough  theory.\\xa0 Here  is  the  lookup   example,  this time with\\n  pointers.\\n\\n       struct symtag {\\n               char    id[10];\\n               int     line;\\n               char    type;\\n               int     usage;\\n       } sym[100];\\n\\n       main( ) {\\n               struct symtag *lookup( );\\n               struct symtag *psym;\\n               ...\\n               if( (psym = lookup(newname)) )  /* non-zero pointer */\\n                       psym -> usage++;                 /* means already\\n  there */\\n               else\\n                       install(newname, newline, newtype);\\n               ...\\n       }\\n\\n       struct symtag *lookup(s)\\n          char *s; {\\n               struct symtag *p;\\n               for( p=sym; p < &sym[nsym]; p++ )\\n                       if( compar(s, p->id) > 0)\\n                               return(p);\\n               return(0);\\n       }\\n\\n  The function compar doesn\\'t  change:  `p->id\\'  refers  to  a string.\\n\\n       In main we test the pointer returned by  lookup\\n       against  zero,\\n  relying  on   the fact that a pointer is by definition never zero when\\n  it really points at  something.\\xa0 The  other pointer manipulations are\\n  trivial.\\n\\n       The only complexity is the set of lines like\\n\\n       struct symtag *lookup( );\\n\\n  This  brings  us  to an area that we will treat only  hurriedly;   the\\n  question of function  types.\\xa0 So  far,  all  of  our  functions  have\\n  returned integers (or characters, which are  much the  same).\\xa0 What\\n  do we do when the function returns something else, like a pointer to a\\n  structure?\\xa0 The  rule  is that any  function  that  doesn\\'t return an\\n  int has to say explicitly what it does return.\\xa0 The\\n  type  information\\n  goes  before  the   function   name   (which can make the name hard to\\n  see).\\n\\n  Examples:\\n\\n       char f(a)\\n          int a; {\\n               ...\\n       }\\n\\n       int *g( ) { ... }\\n\\n       struct symtag *lookup(s) char *s; { ... }\\n\\n  The  function  f  returns  a  character,  g\\n  returns a  pointer  to an\\n  integer,  and lookup returns a pointer to a structure\\n  that looks  like\\n  symtag.\\xa0 And if we\\'re going to use one\\n  of  these functions,  we have\\n  to make a declaration where we use it, as we did in main\\n  above.\\n\\n       Notice the parallelism between the declarations\\n\\n               struct symtag *lookup( );\\n               struct symtag *psym;\\n\\n  In effect,  this says that lookup( ) and psym\\n  are  both  used the same\\n  way - as a pointer to a structure -- even though one is a  variable  and\\n  the other is a function.\\n\\n\\n      21.\\xa0 Initialization of Variables\\n\\n       An  external  variable  may  be  initialized  at compile time by\\n  following its name with an initializing value when it is defined.\\xa0 The\\n  initializing  value  has  to  be   something   whose value is known at\\n  compile time, like a constant.\\n\\n       int     x       0;      /* \"0\" could be any constant */\\n       int     a       \\'a\\';\\n       char    flag    0177;\\n       int     *p      &y[1];  /* p now points to y[1] */\\n\\n  An external array can be initialized by following  its   name  with  a\\n  list of initializations enclosed in braces:\\n\\n       int     x[4]    {0,1,2,3};  /* makes x[i] = i */\\n       int     y[ ]    {0,1,2,3};  /* makes y big enough for 4 values */\\n       char    *msg    \"syntax error\\\\n\";   /* braces unnecessary here */\\n       char *keyword[ ]{\\n               \"if\",\\n               \"else\",\\n               \"for\",\\n               \"while\",\\n               \"break\",\\n               \"continue\",\\n               0\\n       };\\n\\n  This last one is very useful -- it makes keyword an array  of  pointers\\n  to  character  strings,  with a zero at the end so we can identify the\\n  last element easily.\\xa0 A simple lookup  routine   could   scan   this\\n  until  it either  finds a match or encounters a zero keyword pointer:\\n\\n   lookup(str)             /* search for str in keyword[ ] */\\n      char *str; {\\n           int i,j,r;\\n           for( i=0; keyword[i] != 0; i++) {\\n                   for( j=0; (r=keyword[i][j]) == str[j] && r != \\'\\\\0\\'; j++ );\\n                   if( r == str[j] )\\n                           return(i);\\n           }\\n           return(-1);\\n   }\\n\\n\\n       Sorry  --  neither  local  variables  nor  structures can    be\\n  initialized.\\n\\n\\n\\n            22.\\xa0 Scope Rules: Who Knows About What\\n\\n       A  complete  C  program  need  not be compiled all at  once;  the\\n  source text of  the  program  may  be  kept  in  several  files,  and\\n  previously  compiled routines  may  be  loaded   from libraries.\\xa0 How\\n  do we arrange that data gets passed from one routine to  another?\\xa0 We\\n  have  already  seen how to  use function  arguments  and  values,   so\\n  let us talk about external data.\\xa0 Warning: the words  declaration  and\\n  definition   are  used precisely in this section; don\\'t treat them as\\n  the same thing.\\n\\n       A major shortcut exists for making extern declarations.\\xa0 If the\\n  definition  of a variable appears before its use in some function,\\n  no extern declaration is needed within  the function.\\xa0 Thus, if a file\\n  contains\\n\\n       f1( ) { ... }\\n\\n       int foo;\\n\\n       f2( ) { ... foo = 1; ... }\\n\\n       f3( ) { ... if ( foo ) ... }\\n\\n  no declaration of foo is needed  in  either  f2 \\n  or  or  f3,  because\\n  the external definition of foo appears before them.\\xa0 \\n  But if f1 wants to\\n  use foo, it has to contain  the  declaration\\n\\n       f1( ) {\\n               extern int foo;\\n               ...\\n       }\\n\\n       This is true  also  of  any   function  that  exists  on  another\\n  file;  if  it  wants  foo it has to use an extern declaration\\n  for  it.\\xa0 \\n  (If somewhere there  is  an  extern declaration  for something,  there\\n  must also eventually be an external definition of it, or you\\'ll get an\\n  ``undefined symbol\\'\\' message.)\\n\\n       There are some  hidden  pitfalls  in  external  declarations  and\\n  definitions if you use multiple source files.\\xa0 To avoid them, first,\\n  define and initialize  each   external   variable  only  once  in  the\\n  entire set of files:\\n\\n       int     foo     0;\\n\\n  You can get away with multiple external definitions on UNIX,  but  not\\n  on GCOS, so don\\'t ask for trouble.\\xa0 Multiple  initializations  are\\n  illegal  everywhere.\\xa0 Second,  at  the   beginning   of any file that\\n  contains functions needing a variable  whose  definition  is  in  some\\n  other file,  put  in  an extern declaration, outside of any function:\\n\\n       extern  int     foo;\\n\\n       f1( ) { ... }\\n          etc.\\n\\n\\n       The #include compiler control line,\\n       to  be  discussed shortly,\\n  lets you  make  a  single  copy  of  the external declarations  for  a\\n  program  and  then stick them into each  of the source files making up\\n  the program.\\n\\n\\n\\n                          23.\\xa0 #define, #include\\n\\n       C provides a very limited macro facility.\\xa0 You can say\\n\\n       #define name            something\\n\\n  and thereafter anywhere ``name\\'\\' appears  as  a  token,  ``something\\'\\'\\n  will be substituted.\\xa0 This is particularly useful in parametering\\n  the sizes of arrays:\\n\\n       #define ARRAYSIZE       100\\n               int     arr[ARRAYSIZE];\\n                ...\\n               while( i++ < ARRAYSIZE )...\\n\\n  (now  we  can alter the entire program by  changing  only  the define)\\n  or in setting up mysterious constants:\\n\\n       #define SET             01\\n       #define INTERRUPT       02      /* interrupt bit */\\n       #define ENABLED 04\\n        ...\\n       if( x & (SET | INTERRUPT | ENABLED) ) ...\\n\\n  Now we have meaningful  words  instead  of  mysterious constants.\\xa0 \\n  (The  mysterious   operators `&\\' (AND) and `|\\' (OR) will be covered in\\n  the  next section.)\\xa0 \\n  It\\'s  an  excellent practice  to  write  programs without any\\n  literal constants except in #define statements.\\n\\n       There  are  several  warnings  about  #define.\\xa0 First, there\\'s\\n  no semicolon at the end of a #define;  all the text from the  name  to\\n  the  end  of  the  line  (except  for  comments) is  taken  to  be the\\n  ``something\\'\\'.\\xa0 When it\\'s put into the text, blanks are placed around\\n  it.\\xa0 Good  style typically makes the name in the #define upper case;\\n  this makes parameters more visible.\\xa0 Definitions affect  things  only\\n  after they occur, and only within the file in which they occur.\\xa0 \\n  Defines can\\'t be nested.\\xa0 Last, if there is a #define in a file,\\n  then the  first  character  of the file must be a `#\\',  to signal the\\n  preprocessor that definitions exist.\\n\\nThe other control word known  to  C\\n       is  #include.\\xa0 To include one\\n  file in your source at compilation time, say\\n\\n       #include \"filename\"\\n\\n  This is useful for putting a lot of heavily used data definitions  and\\n  #define statements at the beginning of a file to be compiled.\\xa0 \\n  As with\\n  #define,  the  first  line  of  a file containing  a  #include  has to\\n  begin with a `#\\'.\\xa0 And #include can\\'t be nested -- an included\\n  file can\\'t contain  another #include.\\n\\n\\n                            24.\\xa0 Bit Operators\\n\\n       C has several  operators  for  logical  bit-operations.\\n  For example,\\n\\n       x = x & 0177;\\n\\n  forms the bit-wise AND of x and 0177, effectively\\n  retaining only the\\n  last seven bits of x.\\xa0 Other operators are\\n\\n       |       inclusive OR\\n       ^       (circumflex) exclusive OR\\n       ~       (tilde) 1\\'s complement\\n       !       logical NOT\\n       <<      left shift (as in x<<2)\\n       >>      right shift     (arithmetic on PDP-11;  logical on H6070,\\n  IBM360)\\n\\n\\n25.\\xa0 Assignment Operators\\n       An unusual feature of C is that the normal binary operators\\n like `+\\',  `-\\',   etc.\\xa0 can be combined with the assignment operator\\n  `=\\' to form new  assignment  operators.\\xa0 For example,\\n\\n       x =- 10;\\n\\n  uses the assignment operator `=-\\' to decrement x by 10, and\\n\\n       x =& 0177\\n\\n  forms the AND of x and  0177.\\xa0 \\n  This convention is a useful\\n  notational  shortcut, particularly if x is a complicated\\n  expression.\\xa0 The classic example is summing an array:\\n\\n       for( sum=i=0; i<n; i++ )\\n               sum =+ array[i];\\n\\n  But the  spaces  around  the  operator  are  critical!\\xa0 For\\n\\n       x = -10;\\n\\n  sets x to -10, while\\n\\n       x =- 10;\\n\\n  subtracts 10 from x.\\xa0 When no space is present,\\n\\n       x=-10;\\n\\n  also  decreases  x  by  10.\\xa0 This   is   quite  contrary  to   the\\n  experience of most programmers.\\xa0 In particular, watch out for things\\n  like\\n\\n       c=*s++;\\n       y=&x[0];\\n\\n  both of which are almost certainly not what you wanted.\\xa0 Newer\\n  versions  of  various compilers are courteous enough to warn you about\\n  the ambiguity.\\n\\n       Because  all  other  operators  in  an  expression  are evaluated\\n  before  the  assignment  operator,  the  order of evaluation should be\\n  watched carefully:\\n\\n       x = x<<y | z;\\n\\n  means ``shift x left y places, then OR with z, and store  in x.\\'\\' But\\n\\n       x =<< y | z;\\n\\n  means ``shift x left by y|z places\\'\\', which is  rather  different.\\n\\n\\n                            26.\\xa0 Floating Point\\n\\n       We\\'ve skipped over  floating  point  so  far,  and  the treatment\\n  here will be hasty.\\xa0 C has single and double precision numbers (where\\n  the precision depends on  the  machine at hand).\\xa0 For example,\\n\\n               double sum;\\n               float avg, y[10];\\n               sum = 0.0;\\n               for( i=0; i<n; i++ )\\n                       sum =+ y[i];\\n               avg = sum/n;\\n\\n  forms the sum and average of the array y.\\n\\n       All floating arithmetic is done  in  double   precision.\\xa0 Mixed\\n  mode  arithmetic is legal;  if an arithmetic operator in an expression\\n  has both operands int or char, the  arithmetic\\n  done  is  integer,  but\\n  if one operand is int or char and the\\n  other is float or double,  both\\n  operands  are converted  to double.\\xa0 Thus if i and j are int and x is\\n  float,\\n\\n       (x+i)/j         converts i and j to float\\n       x + i/j         does i/j integer, then converts\\n\\n  Type conversion may be made by assignment; for instance,\\n\\n               int m, n;\\n               float x, y;\\n               m = x;\\n               y = n;\\n\\n  converts x to integer (truncating toward  zero),   and \\n  n  to floating point.\\n\\n       Floating constants are just like those   in   Fortran   or  PL/I,\\n  except that the exponent letter is `e\\' instead of `E\\'.\\xa0 Thus:\\n\\n               pi = 3.14159;\\n               large = 1.23456789e10;\\n\\nprintf will format floating point numbers: ``%w.df\\'\\' in the\\n  format  string  will  print  the  corresponding  variable in a field w\\n  digits wide, with d decimal places.\\xa0 \\n  An e instead of an f will produce\\n  exponential notation.\\n\\n\\n\\n          27.\\xa0 Horrors! goto\\'s and labels\\n\\n       C  has  a goto statement and labels,  so  you  can  branch about\\n  the way you  used  to.\\xa0 But most of the time goto\\'s  aren\\'t\\n  needed.\\xa0 (How  many  have we used up  to  this  point?)\\xa0 \\n  The  code  can  almost\\n  always  be more clearly expressed by for/while, if/else,  and compound\\n  statements.\\n\\n       One  use  of  goto\\'s  with some legitimacy is in a  program which\\n  contains a long loop, where a while(1) would\\n  be too extended.\\xa0 \\n  Then you might write\\n\\n          mainloop:\\n               ...\\n               goto mainloop;\\n\\n  Another use is to implement a break out of more than one level\\n  of for  or  while.\\xa0 goto\\'s can only\\n  branch to  labels  within  the  same function.\\n\\n\\n\\n                28.\\xa0 Acknowledgements\\n\\n       I  am indebted to a veritable host of readers  who  made valuable\\n  criticisms  on  several  drafts  of  this  tutorial.\\xa0 They ranged  in\\n  experience  from   complete  beginners  through several  implementors\\n  of  C compilers  to  the  C language designer  himself.\\xa0 \\n  Needless  to say,  this  is  a   wide   enough  spectrum  of opinion that\\n  no one is satisfied (including me); comments and suggestions are still \\n  welcome, so  that  some future version might be improved.\\n\\n\\n\\nReferences\\n\\n       C is an extension of B, which was  designed  by  D.  M. Ritchie\\n  and   K.   L.   Thompson  [4].\\n  The  C  language  design  and   UNIX implementation are the work of\\n  D. M. Ritchie.\\xa0 \\n  The GCOS version  was begun  by A. Snyder and B. A. Barres, and completed\\n  by S.  C.  Johnson and M. E. Lesk.\\xa0 \\n  The IBM version is primarily  due  to T. G. Peterson, with the assistance\\n  of M. E. Lesk.\\n\\n[1]D. M. Ritchie, C Reference Manual.\\xa0 Bell  Labs,  Jan. 1974.\\n\\n[2]  M.  E.  Lesk & B.  A.  Barres, The  GCOS  C  Library. Bell Labs,\\n  Jan. 1974.\\n\\n[3]  D.  M.   Ritchie   &   K.   Thompson,  UNIX Programmer\\'s Manual.\\n  5th Edition, Bell Labs, 1974.\\n\\n[4]  S.  C. Johnson & B.  W.  Kernighan,  The Programming Language B.\\n  Computer Science  Technical  Report  8, Bell  Labs, 1972.\\n\\n\\n\\n',\n",
       " \"\\n\\nUCI Machine Learning Group\\n\\n\\n\\nMachine learning investigates the mechanisms by which knowledge is acquired through experience. Research at UCI spans the spectrum of models for learning, including those based on statistics, logic, mathematics, neural structures, information theory, and heuristic search algorithms. \\n\\nOur research involves the development and analysis of algorithms that identify patterns in observed data in order to make predictions about unseen data. New learning algorithms often result from research into the effect of problem properties on the accuracy and run-time of existing algorithms. \\n\\nWe investigate learning from structured databases (for applications such as screening loan applicants), image data (for applications such as character recognition), and text collections (for applications such as locating relevant sites on the World Wide Web). UCI also maintains the international machine learning database repository, an archive of over 100 databases used specifically for evaluating machine learning algorithms. \\nKnowledge Discovery and Data Mining \\nDatabases with millions of records and thousands of fields are now common in business, medicine, engineering, and the sciences. The problem of extracting useful information from such data sets is an important practical problem. Research on this topic focuses on key questions such as how can one build useful descriptive models that are both accurate and understandable? Probabilistic and statistical techniques in particular, play a key role in both analyzing the inference process from a theoretical viewpoint and providing a principled basis for algorithm development. Ongoing projects include the integration of image and text health-care data for finding diagnostic rules, automated analysis of time-series engineering data from the Space Shuttle, and discovery of recurrent spatial patterns in historical pressure records of the Earth's upper-atmosphere. \\n\\n\\nUCI Machine Learning Information\\n\\nThe Machine Learning Database Repository\\nSome machine learning programs\\nMachine learning papers from UCI\\nArchive issues of the Machine Learning List\\nOther information related to machine learning\\n\\n\\n\\n\\n[AI Home |\\nICS Home |\\n UCI Home]\\n\\n\\n\\n\",\n",
       " '\\n\\n\\n\\n\\n\\nSDL\\n\\n\\n\\n\\n \\n \\n\\n SDL Forum Society\\n\\n\\xa0 \\n\\n\\n\\nWhat Is SDL?\\nA full tutorial about SDL88 can be found here and a tutorial about\\nSDL-92 can be found online here (and\\nin PDF\\nformat here). Some introductory slides on SDL-2000 can be\\nfound here, and a\\nset of powerpoint slides presented at the 2nd SAM work shop can\\nbe found here\\n(and as a zipped\\npdf file here).\\nSDL is a Specification and Description Language standardized\\nas ITU (International\\nTelecommunication Union) Recommendation Z.100. The key features\\nof the language are: \\n\\nthe ability to be used as a wide spectrum language from\\n        requirements to implementation;\\nsuitability for real-time, stimulus-response systems;\\npresentation in a graphical form;\\na model based on communicating processes (extended finite\\n        state machines)\\nobject oriented description of SDL components.\\n\\nAlthough SDL is widely used in the telecommunications field,\\nit is also now being applied to a diverse number of other areas\\nranging over aircraft, train control, medical and packaging\\nsystems. SDL is a general purpose description language for\\ncommunicating systems. The basis for description of behaviour is\\ncommunicating Extended State Machines that are represented by\\nprocesses. Communication is represented by signals and can take\\nplace between processes or between processes and the environment\\nof the system model. Some aspects of communication between\\nprocesses are closely related to the description of system\\nstructure. An Extended State Machine consists of a number of\\nstates and a number of transitions connecting the states. The\\nmachine start in a transition leading to an initial state.\\nThe language has been evolving since the first Z.100\\nRecommendation in 1980 with updates in 1984, 1988, 1992, 1996 and\\n1999. Object Oriented features were included in the language in\\n1992. This was extended in the latest version (SDL-2000) to give\\nbetter support for object modelling and for code generation.\\n\\nNOTE: SDL-88, SDL-92 and SDL-2000 refer\\n    to versions of the standard; SDL \\'99 and SDL\\'01 refer to the\\n    1999 SDL Forum and 2001 SDL Forum.\\n\\nStability of the SDL language is an important attribute to\\nusers, and SDL-92 was effectively a superset of SDL-88.\\nTherefore, any SDL that conforms to SDL-88 was also (with a few\\nexceptions) valid SDL-92. However, SDL-92 has many advantages in\\nthe way that systems can be structured using object features of\\nthe language, and the most popular tools now support SDL-92\\nfeatures. For SDL-2000, a different approach was taken.\\nIn 1996 a few updates were made to the language in an addendum\\nto the SDL defined by the 1992 Z.100 standard. The addendum\\nrelaxed a number of rules for the language to make it easier to\\nuse in a even more flexible way.\\nFor SDL-2000, the opportunity was taken to remove some\\nfeatures that were not strongly supported by tools. Object\\nmodelling in SDL was strengthened and better support given for\\nprogramming directly in SDL. In particular the data model was\\nrevised to give such features as global data and referenced data\\nobjects. The structuring features (blocks and processes) were\\nharmonized into an agent concept. Support for ASN.1 was\\nstrengthened so that the use of ASN.1 modules with SDL no longer\\nrequires any change main body of the language. To enable these\\nchanges, a few elements of old SDL models may need a few changes\\nfor SDL-2000 tools, but the updates have been designed and\\ndocumented for automatic tool conversion.\\nFor systems engineering SDL is usually used in combination\\nwith other languages: MSC, ASN.1 and TTCN. The use of the object\\nmodel notation of SDL-2000 in combination with MSC, traditional\\nSDL state models and ASN.1is a powerful combination that covers\\nmost aspects of system engineering. This set of notations meets\\ncriteria for UML. There has been work relating the SDL and TTCN\\nsemantic models, and the TTCN is used for testing validating\\nstandards and systems written using SDL. \\nThese languages have been studied in the same group within the\\nITU, and therefore also come within the scope of the SDL Forum\\nSociety. ITU Z.105(11/99) and Z.107(11/99) standards define the\\nuse of SDL with ASN.1 The Z.109(11/99) standard defines a UML\\nprofile for SDL, and the Z.120(11/99) standard defines Message\\nSequence Charts.\\nResults from the standards work\\nA version of the November 1999 Recommendations are available\\nthrough the Society to members. The definitive versions are\\npublished by ITU-T.\\nThe Society has been granted permission from\\nITU-T to distribute a version of the SDL-2000 and MSC-2000\\nRecommendations via its site.\\nThe SDL-2000 version can be found as a Zip file containing four\\nMS Word 97 documents at:\\n<ftp://membername:password@ftp.sdl-forum.org/SDL2000.zip>. \\nThe MSC-2000 version can but\\nfound as a PDF document at\\n<ftp://membername:password@ftp.sdl-forum.org/MSC2000.pdf>\\nin pdf format, and \\n<ftp://membername:password:@ftp.sdl-forum.org/msc2000htm.zip>\\nas a set of browser files.\\nIf you are a member but need the name/password\\ncombination mailto:admin@sdl-forum.org. \\n\\nSociety members can also participate in ongoing SDL and\\n        MSC standards work and therefore have access to all the\\n        latest documents, changes and corrections to SDL and MSC.\\n\\xa0\\nMaster\\n        list of corrections to Z.100 (from Addendum 1). \\nThis is a PKzipped Word 6 file of the list of corrections\\n        to Z.100(10/93) bringing it up to the level of Addendum 1\\n        to SDL-92 (\"SDL-96\").\\n        The corrections are in section order.\\n        Word 6 source: 133k; PKzipped file 49k.\\n        The document is free to all. It does not cover changes\\n        for SDL-2000.\\n\\n\\xa0\\n\\nSend mail to webmaster\\nwith questions or comments about this web site.\\nCopyright © 1997 - 2003 SDL Forum Society\\nLast update: 10/18/00\\n      \\n\\n\\n',\n",
       " '\\n\\n\\nNumerical Methods Background Page\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Background\\nAlgorithms and Computers\\nApplications\\nChallenges\\n\\n\\n\\n Numerical Methods are intimately linked to Mathematical Models of the most diverse kind of Phenomena. Traditionally, most phenomena or processes of interest have been of physical character. Fluid dynamics, a branch of Continuum Mechanics, has for centuries been a rich source of mathematical models, which take the form of Ordinary Differential Equations, Partial Differential Equations and Integral Equations.\\nFor compressible fluids, such as a gas or a liquid at very high pressures, such mathematical models result from the formal statement of three basic physical principles, namely the principles of conservation of mass, momentum and energy. Chemical, biological, and more recently, financial phenomena, also lead to mathematical models that are expressed in terms of differential equations. Mathematical models of simple phenomena may consist of algebraic equations; these may also arise as a result of finding numerical approximations to differential equations. Differential, integral and algebraic equations are mathematical relationships satisfied by  solutions that, hopefully, model the behaviour of the processes of interest. \\n\\nAssuming that the fundamental properties of the phenomena of interest are actually embodied in these mathematical models, then one has the possibility of understanding such phenomena by studying the equations and their solutions. The processes of interest can thereby be simulated, with pencil and paper or with the aid of a computer.\\nA basic problem is that of finding solutions to these mathematical objects. Indeed an even more basic problem is that of determining whether the equations  do actually have a solution at all. Closely related questions of interest are:  under what conditions do solutions exist, are there multiple solutions and if  so which solutions are meaningful to the problem being solved and which are  simply spurious mathematical solutions. \\nMost of these issues are the concern of professional mathematicians. Engineers or scientists would be interested in actually finding solutions to the equations. There are at present two ways of finding solutions, namely analytical methods and numerical methods. The former produces, when possible, exact analytical solutions in the form of general mathematical expressions. Solutions of differential equations will give expressions for functions, which are distinct from discrete  numerical values. Numerical methods on the other hand produce approximate solutions in the form of discrete values or numbers. \\nThe simplest of the equations mentioned above is a linear algebraic equation; the exact solution of this is immediate and consists of a single value or point. Algebraic quadratic equations can also be solved exactly, if solutions exist, leading in general to two solutions. Finding exact solutions to higher-order algebraic equations will not, in general, be a feasible task and numerical methods must be employed to find approximate solutions instead. For the case of two or more coupled non-linear algebraic equations, numerical methods are routinely used today to find approximate solutions. \\nOrdinary and partial differential equations are in general much more difficult to solve exactly. Exact solutions are possible only in very special circumstances. Realistic mathematical models will consists of non-linear differential equations or sets of coupled equations, possibly to be satisfied on general domains and  subject to complicated initial and boundary conditions. These equations may depend on time and on one, two or three space dimensions. In such circumstances, explicit exact solutions will remain the dream of the mathematician for the foreseeable future. \\nThe differential equations governing the behaviour of an inviscid gas, the Euler equations, have been known to scientists for centuries, but the exact solutions of these equations available today are only valid for very simple physical situations. Of course, whenever available, exact solutions are most valuable in: identifying the parameters of the problem, providing an understanding of the qualitative behaviour of the phenomena of interest and displaying the way in which such phenomena will depend on the identified parameters.\\n Exact solutions, even for unrealistic cases, are also helpful in assessing numerical methods intended for use in more general situations. Moreover, there are numerical methods that rely on exact solutions to the equations under simplified initial conditions assumed to hold locally. \\nNumerical methods are techniques for finding approximate solutions to mathematical problems. Such techniques have been known for a long time; Euler is credited with having introduced, in 1768, finite differences to approximate derivatives in ordinary differential equations. The finite difference method, one of many methods available, is today capable of producing numerical solutions to both ordinary and partial differential equations. \\nReal progress on the development and application of numerical methods is the product of the 20th century, the beginning of which, witnessed increased efforts by scientists to invent, analyse and apply numerical methods to solve differential equations that represented mathematical models of physical phenomena. \\nEarly contributions of historical importance were made by scientists such as \\n  Runge, Richardson, Liebmann, Courant, Friedrichs, von Neumann, amongst many \\n  others.\\nBack to top\\n\\n\\n\\nProgress on numerical methods was made only when two basic ingredients were \\n  available, namely, numerical algorithms, along with a better understanding of \\n  their mathematical properties such as stability, and computing machines. \\nThe first ingredient has given rise to a new branch of Mathematics, namely \\n  Numerical Analysis. Advances in this area have been slow and yet exceedingly \\n  useful in providing some basic guidelines to computational practice. Proving \\n  general theorems has been as difficult as finding exact solutions to the governing \\n  equations in the first place. Numerical methods are procedures that depend on \\n  many repetitive calculations, possibly millions; for sufficiently simple problems \\n  these can be carried out by hand, a procedure actually utilised in the early \\n  stages. \\nFor situations of practical interest, performing the calculations by hand and \\n  storing the required information is an impossible task; it is here where the \\n  staggering progress made by computer manufactures in the last five decades has \\n  finally provided numerical methods with the necessary calculating power to make \\n  them useful approximate solution techniques. \\nThe cost of computing resources has also decreased dramatically. It is estimated \\n  that for the last 40 years the cost of computation has dropped by one order \\n  of magnitude every eight years. However, impressive as the role of computers \\n  may appear, and contrary to widespread belief, credit for the current state \\n  of development of numerical methods does not go to the computer manufacturer \\n  alone.\\n The invention and analysis of the numerical methods is, in the first place, \\n  the result of the research of scientists, who originate from the most diverse \\n  backgrounds, usually Mathematics, Engineering or Physics. Moreover, the production \\n  of more and better algorithms, and the corresponding numerical analysis to theoretically \\n  justify their application to extremely complex situations, must continue to \\n  be an active area of investment for further research. \\nBack to top\\n\\n \\nNumerical methods are today a key technology to industrial and social progress \\n  in a modern society. The use of numerical methods to solve mathematical problems \\n  arising from a wide variety of applications has exploded, in the last two decades \\n  or so. \\nA distinctive feature of numerical methods as a discipline is its highly multidisciplinary \\n  character; it involves research workers from a wide spectrum of interests, professional \\n  numerical analysts, computer scientists, mathematicians, physicists, chemists, \\n  engineers, financial modellers and specialists from other fields. As discussed \\n  above, a key feature of numerical methods is that it makes it possible to theoretically \\n  model and simulate phenomena of interest. \\nFor instance, \\n\\n the flight of a passenger aircraft at cruising conditions may be simulated \\n    on the computer. That means that key features of the physical phenomena involved \\n    may be studied under a wide range of conditions by performing the numerical \\n    simulations for a corresponding range of parameter values, such as cruising \\n    speed. \\n The re-entry of a space vehicle into the atmosphere may be studied numerically \\n    for a wide range of conditions; the design engineer might be interested in \\n    predicting the temperature distribution on the surface of the vehicle. \\n A nuclear explosion, intended or accidental, may also be simulated numerically. \\n  \\n Government agencies might be interested in predicting the consequences \\n    of the accidental rupture of a large dam situated close to populated areas \\n    so as to put in place emergency services accordingly; such phenomena can also \\n    be simulated numerically. Traditionally, most studies of the above problems, \\n    would have been performed through physical experiments. \\n\\nFor some phenomena, experimentation might in fact not be feasible, as the likely \\n  physical conditions, such as temperature, are bound to severely restrict what \\n  is actually possible in the laboratory and what is not. For example, the peak \\n  temperature values when a space vehicle re-enters the atmosphere can be larger \\n  than that of the temperature on the surface of the sun. Numerical methods can \\n  be an alternative to experiments that are expensive and dangerous, and are the \\n  only useful way of studying phenomena for which experiments are impossible. \\n  Numerical methods have indeed made a very substantial contribution to scientific \\n  and industrial development, including the design and manufacturing of the computer, \\n  the twin brother of numerical algorithms. \\nEnvironmental pollution, a consequence of rapid industrialisation, is a reality \\n  that modern society is only beginning to acknowledge. Social awareness of environmental \\n  problems has resulted in an increased research activity requiring the massive \\n  use of numerical methods. In addition to the well-established activities in \\n  weather and climate prediction by numerical means, there are at present new \\n  demands from a fast-moving manufacturing industry to operate under increasing \\n  environmental constraints. \\nNumerical methods are also becoming an increasingly vital resource in Economics, \\n  Financial Modelling and other related disciplines. Finite difference methods \\n  are being used today for the computation of option prices; Monte Carlo methods \\n  are applied to the simulation of the market; stochastic differential equations \\n  in finance are solved numerically as are partial differential equations that \\n  are related to portfolio management strategies. \\nBack to top\\n\\n\\n\\nGreat care is required in correctly interpreting the explosive success of numerical \\n  methods and their use to solve practical problems. Before a numerical simulation \\n  is carried out, a mathematical model of the problem at hand is required. Then, \\n  an algorithm to solve the equations must be carefully selected. \\nThe adopted mathematical model is expected to be based on a thorough understanding \\n  of the problem in the first place, and this points us to a major limitation \\n  of numerical methods as a discipline. The numerical simulation of a problem \\n  is, at best, as good as the adopted mathematical model for the problem. If this \\n  is poor the numerical simulation will be poor, or even poorer; recall that numerical \\n  methods are approximate solution methods to solve the mathematical problem. \\n  This leads us to a second limitation of numerical methods. \\nIn spite of the tremendous advances one needs to bear in mind that the theoretical \\n  analysis of numerical methods lags far behind the actual use of numerical methods. \\n  For instance, we routinely use numerical methods to solve systems of non-linear \\n  partial differential equations in several space dimensions and for general initial \\n  and boundary conditions, with the only theoretical justification that when applied \\n  to the simplest of model problems, usually one-dimensional scalar and linear, \\n  the schemes are convergent. Allied disciplines such as experimental methods \\n  and analytical methods will continue to play an important supportive role of \\n  numerical methods, as will further progress in theoretical numerical analysis. \\n\\nThis state of affairs poses serious challenges at the level of research on \\n  numerical methods and mathematical modelling, training of numerical analysts \\n  and numerical practitioners, and numerical technology transfer. Our activities, \\n  as an organisation concerned with numerical methods, are strongly motivated \\n  by the challenges identified above. \\nOur products and services are aimed at: teaching and research on numerical \\n  methods, the application of specialised algorithms for demanding technological \\n  applications, the dissemination of related information and numerical technology \\n  transfer. \\nBack to top\\n\\n \\n\\n\\n',\n",
       " ' Como vai? (Port. How are you going) A letter or diacritic used to modify the sound associated with a letter. e, a, and h are often used as markers in English orthography. Marker letters are silent. mit/mite resume/resum e\\' caio/c h ow /p/ = signal or phoneme (refers to a sound) [p] = symbol or particular letter or letter combination, <particular spelling> *word = word reference as in *eel alt. \"eel\" Taging particular categories in a document, e.g., chapter head, break, bold,... A silent e used to mark a vowel that operates at a distance [fat/fat e ]. Most markers are next to the vowel they modify [mo a t]. The letter e after a consonant, or consonants, which lengthens a preceding vowel. A meaningful unit of a language that cannot be further divided. English morphemes include s (for plural) /dogz/ & /cats/ and [ed] for past tense /started, finisht/. Consistent morphemic spelling often misrepresents pronunciation as in checked/chekt Referring to a morpheme NES New Spelling notation Nu Folik New Follick (1934) Nue Speling New Spelling (ca. 1910) New English Spelling - refers to an update of New Spelling A simple consistent close-to-TO spelling scheme (see below) The representation of sounds by symbols. A means of transcribing spoken language into visual symbols. A variation on the spelling scheme developed by Mont Follick, MP. What English might look like if written in Spanish or Portuguese orthography. See also Spanglish . The spelling scheme, suggested by Ellis, that is the foundation for most phonemic spelling notations based on the principle of least disturbance (of traditional English spelling conventions) Extended vowels are written AE[ei] EE[i:] IE [ai] OE[ou] UE[ju] (NS[ipa]) orthographic orthography From The Devil\\'s Dictionary by Ambrose Bierce Literally \"right markings\" or \"right writng\" The study or practice of correct spelling or writing. Can include the study of various rules and regularities and some grammar. That part of grammar that treats of the way a given language is written. The orthography is distinct from spoken language. English can be written in the orthography designed for Spanish. The problem with hitchhiking orthographies is mostly a problem of having sounds without associated symbols. Spanish has no symbol for the checked or short [i] or the initial sound in [the]. Orthography, n. The science of spelling by the eye instead of by the ear. Advocated with more heat than light by the outmates of every asylum for the insane... [ more ] palaeography palate (anat) parsing parts of speech pasigraphy phone phoneme phonemicity phonetic phonetic transcription phonemic transcription phonics phonogram plural polysemy polyvalent positional spelling pronunciation guide PV - Personal View The study of ancient writing and inscriptions The hard & soft bony structure forming the roof the mouth Parting, break into parts. The analysis and labeling of the gramatical elements of sentences also callled diagramming. A classification of words into groups displaying the same formal features - esp. related to inflection and distribution: verb, noun, adj. etc. Very large morphemic classes. Void of semantic content. The use of symbols between languages a particular occurence of a vowel or consonant A range of sounds that language users interpret as an instance of a discrete sound segment. The smallest contrastive unit of sound that distinguishes one word (meaning) from another. The extent to which spelling is a guide to pronunciation. TO has a low level of phonemicity compared to the Spanish orthographic system When the spelling of a word corresponds to its pronunciation A narrow detailed transcription that can distinguish regional accents A broad transcription that preserves only those distinctions that have semantic value for native languge users. Teaching reading by first introducing letter sounds (see whole word) A sound sign. e.g., a letter or word representing a sound. In an alphabetical system the phonograms represent letters. In a syllabary, the sound signs represent syllables. see hotsuma . The form of a word denoting more than one in number Many meanings, e.g. words such as fair, bar Many sound values. English (TO) contains letters associated with 14 different sounds. When the sound value of letters varies according to position in a word. See John Reilly on positional spelling. A guide often given in English dictionaries to show the sound of words. Most spelling reformers want to use a consistent spelling guide as for the writing system instead of TO. An SSS publication relating to a notational scheme or spelling reform proposal. The scheme is not necessarily endorsed by the society. It represents an individual\\'s views. PV-7 Nu Folik RP - received pronunciation redundant reformed spelling regularity r-ending Roman Alphabet Standard English accent of the BBC and of Southern Britain A letter that does not contribute to the sound of a word Any spelling scheme that is more regular than TO The extent to which a sound is always represented by a spelling The use of r at the end of a word to indicate a vowel sound The 26 letters commonly used in Western Europe Rhotic A dialect or accent in which [r] is pronounced when it occurs bfore a consonant or a pause. Saund Spell schwa schwapostrophe script Shaw Alphabet short vowel silent silent e simplified spelling sound-symbol correspondence A strictly phonetic spelling scheme by Ian Ascott The indistinct unstressed vowel sound common in English An apostrophe used to represent a schwa A word with the same or similar meaning The visible part of a writing system. Another word for alphabet A novel alphabet designed for phonetic English writing The vowel sounds as in pat pet pit pot putt A letter used in the spelling of a word which is not pronounced Some silent e\\'s are markers (fate) others are just hold overs (give) from a time when they had a function and were pronounced. The American name for the magic e Any spelling scheme that streamlines and simplifies TO The match, or lack of it, between sounds and letters a = /ae/, /a:/, /ei/, / \\' /, ... The alphabetic ideal is to have one letter correspondent for each sound and one (and only one) sound (phoneme) for each letter (grapheme). Semite, Semitic spanglish speech recognition spell checker spelling SR-1 SS SSS stress surplus cut syllabic syllable symbol symbol-sound correspondence synonym A linguistic, not a racial, classification. The languages spoken in the Middle East since ancient times. Ancient Egyptian was a blend of west african (Hamitic) and Semitic. The early writing systems for these languages did not contain letters for vowels. chart Spanglish is a systematic and highly phonemic way of transcribing English speech. It is based on the ancient Saxon alphabet [900-1066] and the old practice of inserting trailing double consonants to mark short stressed vowels. The augmented Latin alphabet [sound-symbol correspondences] was used until the 15th c. when the pronunciaiton of the long vowels in many words changed. There was no corresponding change in spelling. This led to a divergence between spelling and pronunciation. There are two ways to represent each vowel in Spanglish: stressed or unstressed. Most one syllable words therefor have two possible spellings. e.g. aet or att, ej and edj. Spanglish is a parallel pronunciation guide spelling. Respelling is limited to words that cannot be understood when pronounced according to the values assigned in the Saxon alphabet. enough becomes enuff or enof. A computer program that checks spelling. It matches a letter configuration to letter configurations in memory and locates letter combinations that do not match. \" Eye have a knew checker... \" would not be corrected. The choice of which letters or symbols to combine to represent a word Spelling Reform - 1st Stage For Lindgren, e = /e/ head=hed Simpl Spel \\x96 The newsletter published by the SSS Simplified Spelling Society, UK (Ashton University) Some syllables in spoken English are stressed louder & clearer A spelling scheme by Valerie Yule that clips silent letters from TO Referring to a syllable Unit of pronunciation forming the whole or part of a word A letter or character or sign or combination used in writing The match, or lack of it, between letters and sounds in a word (see grapheme-phoneme correspondence) Two or more words with the same meaning or reference tense [vs. lax] grammar [past present-future] thesaurus TO -Traditional Orthography trigraph turned c turned a turned e [ \\'] typology Uni fon Unigraph Articulated with muscle tension or effort. Cf. fortis Contrast term: lax Gramatical time: variation in the morphological form of the verb [eg. -ed] Book that lists words in groups of synonyms The spelling system standardized about 1755 for English A combination of three letters denoting one sound, e.g. sch A spelling scheme by Tom Zurinskas based on New Spelling but using double consonants to show stress. The IPA symbol No. 5 for the sound in pot The IPA symbol for the schwa (unstressed central vowel, e.g. uh) Classification by type Unifon - an augmented alphabet devised by John Malone. 40 sound signs. Unifon means one sound and implies one sound per symbol A single letter or symbol denoting one sound. The name of a notational scheme that uses the upper case letters as new sound signs: A=ei, E=i: I,Y=ai, O=ou, U=ju. and eliminates most digraphs ( eg, Q=oi, T=th, G=ng) vowel semi-vowel vowel diagram A speech sound produced without any obstruction - different vowels are produced by altering the size and shape of various cavities through which air passes. A speech sound produced without any friction but similar to a consonant in marking syllable boundaries. English Y & W. Serb R. (see syllabic) A quadrilateral or trapezoid representing jaw, mouth, and tongue positions involved in the production of vowel sounds. Devised by Jones. WES World English Spelling whole word method word sign ( logogram ) A spelling scheme A way of teaching reading without reference to letter sounds See also look say method An irregular spelling such as the, me, to, of which is retained in a reformed spelling scheme. A symbol without a consistent sound association. In TO, the words [ the, of, and a ] are considered word signs due to their irregularity. They are often retained in otherwise phonemic and consistent notational systems such as anglic . Phonics Phonics does more than teach reading by the `sound of a word\\': it is a method of teaching reading and spelling by teaching all sound-symbol correspondences, from simple to complex, systematically and directly. It is important to insist that phonics relates sounds to written symbols - it needs to be distinguished from the current bandwagon of training in `phonemic awareness\\' BEFORE teaching letters. sitemap-linguistics The sitemap provides links to pages with detailed disucssions of many many of these terms This site of the Spelling Reform Ring is owned by steve bett [ Previo us 5 Sites | Previou s | Next | Next 5 Sites | | Random Site | List Sites ] To add to, comment on, or critique an entry in this glossary, contact Steve Bett This site of the Saundspel Ring is owned by steve bett [ Previo us 5 Sites | Previou s | Next | Next 5 Sites | | Random Site | List Sites ] COMMENTS I developed a more technical glossary of linguistics several years ago that few people ever used. The problem with narrow technical definitions such as \"allophone --two more more forms of the same phoneme\" is that they are clear only to those who already have the concept. Damian came up with a much better selection of terms and supplied his own down to earth definitions. His emailed glossary was the foundation for the current glossary. Since I added to, edited, and changed half of the entries that Damian supplied, I take responsibility for the errors. If you have something good to say about this list, directo those comments to Damian. -- Steve Bett sbett@mailcity.com I inadvertently failed to copy the names of the people who supplied these comments on the SSS Forum: -SB .............. I think allophone - An alternative, similar sound for the same spelling pattern - is wrong: it\\'s nothing to do with spelling so not: \"allophone An alternative, similar sound for the same spelling pattern\" it\\'s nearer to \"a variation in the way in which a particular sound is pronounced, which carries no function in terms of distinguishing it from other sounds; for example \\'l\\' in \\'look\\' and \\'little\\'\". The Chambers dictionary definition is not really suitable for general discussion (\"one of two or more forms of the same phoneme\") ............... I\\'m not sure that a \"free vowel\" is always the same as a \"long vowel\"; it\\'s more to do with syllable boundaries than length. In some languages it could well be the same thing, but I don\\'t think it\\'s automatic. ............... \"inflection\" need not be at the end of a word. Welsh for example changes the sound at the beginning of a word. ............... \"ligature\" I\\'d add \"... to make a single character\" .............. \"logogram\" could include as a more immediate example the ampersand \"&\" and all the numerals. .............. \"Roman alphabet\" - a possibly dodgy definition, since only English uses this set of 26! A more roundabout definition (ie one less vulnerable to pedants) might be \"The 26 letters which form the basis of the alphabet used for Western European languages\"? ............. perhaps you could add: \"glide\" - a sound that appears between two others as a side effect of the pronunciation, for example \"lawR and order\" (This is an important phenomenon that figures prominently in Taam\\'s spellings, so could be worth including) See Truespel From John Reilly\\'s website. Jonh is the new editor of the JSSS. http://pages.prodigy.net/aesir/iln.htm -------------------------------------- From \"The Devil\\'s Dictionary\" by Ambrose Bierce (1842-1913?) Orthography, n. The science of spelling by the eye instead of by the ear. Advocated with more heat than light by the outmates of every asylum for the insane. They have had to concede a few things since Chaucer\\'s day, but are none the less hot in defense of those to be conceded hereafter. \"A spelling reformer indicted For fudge was before the court ci c ted. The judge said: \\'Enough-- His candle we\\'ll sn o u gh , And his sepulchre shall not be whi c ted.\\'\" In Spanglish: What is the last word? \"A spelling reformer indaited For fudj waz befor the cort saited The judj sed: \"enuff-- Hiz candl wiel snuff, And his sepulkr shaal not bi hwaited.\" RITE has a different way of handling /ai/ A better example of this design latin-1 Steve web hosting • domain names • web design online games • online dating • long distance digital cameras • advertising online \\n\\t\\n\\n-->\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGlossary of Applied Linguistics\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nweb hosting\\ndomain names\\nemail addresses\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n.alpha-index\\n\\n\\n\\nTerms\\nused in the discussions of spelling & orthographic reform\\nInspired\\nby\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nDamian Bonsall\\xa0\\xa0 URL\\xa0\\nhttp://www.unifon.org/glos-spel.html\\nFormatted\\nby\\xa0\\xa0\\xa0\\xa0 Steve Bett\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nwebpages at sitemap-linguistics\\nUpdated\\nby\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 Saundspel\\n& SSSlist [simplified spelling] discussion groups\\nTo\\nJoin\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nRegister at the SpelreformRing or email\\nglos-spel.html\\n\\n\\n\\nTerm\\nto be defiined\\n|A|B|\\nC|D|\\nE|\\nF|G|H| I\\n|J|K| L|\\nM|\\nN|O|\\nP|Q|R|S|T|U|V|W|X|Y|Z|\\n\\n\\naccent\\xa0\\n\\xa0\\naccusative\\xa0\\n\\xa0\\nALC\\nfonetic\\xa0\\n212-781-0099\\n\\xa0\\n\\xa0\\n\\xa0\\nallophone*\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nalphabet\\nA\\ncollection of\\nsound\\nsigns linking\\xa0\\nvisible\\nmarks\\xa0\\nto\\nspeech sounds\\n\\xa0\\nRoman alphabet\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nAdoption of the\\xa0\\nRoman letters\\n\\xa0\\n\\xa0\\n\\xa0\\nalphabetic\\xa0\\nThe mode of utterance peculiar to an individual or locality,\\nindlcuing stress, tone and pitch. [see dialect]\\nthe grammatical case with expresses the destination of\\nthe ation signified by a verb.\\nA spelling scheme promoted\\nby the American Literacy Council.\\xa0\\nFonetic is derived from WES and related to\\nNew Spelling.\\xa0\\nElectronic text can be easily converted to fonetic using the BTRSPL\\nconverter which is available on the web. There are more than 10,000\\ndigitized books on the web and any one of them can be quickly converted\\nto ALC\\'s American Spelling using the converter. (See Rondthaller &\\nLias. 1986. The Dictionary of Simplified American Spelling)\\nA variation in pronunciation that is still close enough\\nto be interpreted as an instance of a particular phoneme.\\xa0 An insignificant\\nvariation.\\xa0 A difference in pronunciation such as the sound of L\\nin \"look\" and \"little\" which is ignored by language users.\\xa0\\nAn ordered set of symbols used as sound signs. The alphabet\\nwas invented only once and was spread by the Phoenician traders and others\\nthroughout the world. The Semites were inspired by the Egyptian writing\\nsystem (esp. middle Egyptian, 1200 bc)\\xa0\\nwhich made limited use of\\xa0 sound signs (phonograms) mostly for proper\\nnames. Egyptian sound signs were not ordered.\\xa0 The standardizing the\\narrangement of 22 semitic symbols allowed it to be used as a number system.\\nThe alphabet doubled as a number system for the Phoenicians and the Greeks\\n- the 8th letter in classical Greek,\\xa0 eta (eituh), is associated with\\neight (eit) and H (eich), originally a squared off 8.\\nThe 26 letters which form the basis of the alphabet used\\nfor most Western European languages. Late arrivals include\\xa0 j, k,\\nu, and w.\\xa0 J used to be an alternate i and y was a slighty different\\nsound.\\xa0 v and u were used for the same sound until the 16th century.\\xa0\\nThe distinction between f and v also occured at this time.\\nThe Roman letters were adopted for English in the 8th\\ncentury.\\xa0 By the 10th century, English had a consistent augmented\\nLatin alphabet. 25% of the consistency was lost after the conquest of the\\nNorman French.\\xa0 Another 30% was lost with the great vowel shift in\\nthe 15th century. Modern spelling dates from about 1755.\\xa0 By this\\ntime only 40% of the writing system was consistent with the alphabet.\\xa0\\nA writing system is alphabetic when the graphemes (shapes\\nor letters) refer to specific phonemes (sounds).\\xa0 The traditional\\nEnglish writing system, which was standardized at the word level around\\n1755, is about 40% alphabetic, 10% morphemic, and 50% chaotic.\\xa0 See\\nsyllabary\\nand logogram.\\xa0 English (circa 1000 AD) was once about\\n90% alphabetic and on par with other roman based writing systems that have\\nundergone several reforms to keep spelling aligned with pronunciation.\\nIn unreformed writing systems, words are often spelled as they were pronounced\\na 1000 years ago.\\xa0 Greek, English, and Icelandic are examples.\\xa0\\xa0\\xa0\\nhave=hah-vuh\\n\\n\\nalphabetic order\\xa0\\n\\xa0\\n\\xa0\\nalternation\\xa0\\naltscript\\xa0\\n\\xa0\\nanglic\\xa0\\nASCII(Am\\nStd. Code)\\nA traditional order or arrangement\\xa0 of symbols (letters)\\nonce used as a numbering system by the Semites and Greeks. This conventional\\norder has changed very little over the past 3,000 years.\\xa0\\nSubstitution of one letter for another, e.g. i for y\\xa0\\nA spelling scheme using positional spelling\\xa0 by John\\nReilly\\xa0\\n[hyly] for\\n/haili/ is an example.\\xa0 See also snapscript\\nand OGD.\\nA variation of New Spelling published by Zachrissen (ca.\\n1950)\\nThe standard character set used in computer text files\\nand email containing 26 lower case and 26 upper case letters without diacritics.\\nASCII also contains\\xa0 punctuation marks and numerals.\\n(See Latin 1 for any special\\ncharacters used in other orthographies)\\n\\n\\nBTRSPL\\n\\xa0\\n\\xa0\\nBroad\\nromic\\n\\xa0\\n\\xa0\\n\\xa0\\nBroad transcription\\n\\xa0\\nA program that converts text from TO to reformed orthography.\\xa0\\nThe name, betrspl, is an abbreviation for better spelling.\\xa0\\nAlan Mole is the principle author of BTRSPL.\\xa0\\nAn IPA based phonemic notational system that uses the\\nroman character set plus a few special characters (such as the turned e,\\nc, and a or digraphs)\\xa0 to represent 41 to 46 phonemes.\\xa0 Sweet\\'s\\nbroad romic used traditional continental sound values.\\xa0 Such a\\ntranscription is not quite precise enough to fully distinguish dialects\\n(see diaphonic)\\nA phonetic transcription of a particular dialect is a\\nnarrow transcription. A broad transcription would tend to ignore\\nregional pronunciation differences. (see phonemic)\\n\\n\\ncategory\\xa0\\n\\xa0\\n\\xa0\\ncharacter\\xa0\\ncharacter set\\xa0\\nChecked Vowel\\n\\xa0\\n\\xa0\\n\\xa0\\nChecked Clipped Spelling\\xa0\\nChekt Klipt Speling CKS\\n\\n\\ncognate\\xa0\\nconsonant\\xa0\\n\\xa0\\ncorpus\\xa0\\ncreole\\n\\xa0\\ncut speling\\xa0\\nA range of instances treated as equivalent for some purpose.\\xa0\\nA phoneme is a category - different sounds that are treated as equivalent\\nby a language community. A grapheme is a category.\\xa0 Categories are\\nabstractions.\\xa0 One can only see or hear instances.\\xa0 You cannot\\nhear a phoneme.\\nA letter or digit or symbol\\xa0\\nA collection of letters and symbols - usually restricted\\xa0\\nA vowel that is always followed by a consonant and is\\ntherefore short. When a free vowel comes before a consonant, its duration\\nis about as short as a checked (short) vowel (eg, mad, made).\\xa0\\nIn TO, checked vowels are often marked with a double\\nconsonant. (madden)\\nA space efficient IPA based spelling scheme by Steve Bett.\\xa0\\nCKS\\nis an attempt to build an ASCII pronunciation guide for English which can\\nbe read by anyone versed in TO.\\xa0 (Cf. free vowel)\\nChekt speling iz an a\\'tempt\\ntu bild an ASCII pro.nu\\'nsie5n gyd for ingli5 wi.ch kan bi red by eniwa\\'n\\nv\\'rst in TO.\\xa0 Spanglish iz an atempt tu bild a pronunciacion gaid\\nwich\\nHaving the same linguistic family or derivation\\xa0\\nA speech sound produced by partial obstruction of the\\nair stream\\xa0\\nwith one or more of the mobile articulaters in the mouth:\\nlips, tongue, ...\\na body of writings referred to by lexicographers\\n\\xa0a pidgin which has been adopted as a mother tongue.\\nNew Guinea tok pidgin English is the best known example.\\xa0\\nAn unsystematic or non-phonemic spelling scheme by Chris\\xa0\\nUpward which fixes 75% of the problems with English spelling\\nby removing silent, redundant, and superfluous letters. Word patterns are\\nlargely undisturbed. See Valerie Yule\\'s surplus\\ncut spelling\\n\\n\\ndiacritic\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\ndiaphone\\n\\xa0\\ndiaphonic\\xa0\\ndictionary\\xa0 (lexicon)\\xa0\\n\\xa0\\ndigraph\\xa0\\ndiphthong\\xa0\\ndyslexia\\xa0\\ndyslexic\\nAn accent mark\\xa0 indicating a change in a letter\\'s\\nsound value or stress\\xa0\\nAn effective way to deal with the shortage of sound signs\\nin ASCII.\\nLatin 1 options include diresis, ring,\\ngrave, and acute markers.\\na mark added to a letter or symbolindicating a change\\nin its usual pronunciation. e.g. e e e\\nall the different forms of a phoneme that collectively\\noccur in all the dialects of a lnaguage.\\nA broad (as opposed to a narrow) transcription of sounds\\xa0\\nAn alphabetical list of words and meanings.\\xa0 TO dictionaries\\nalso include a pronunciation guide which is absent in dictionaries in most\\nother languages\\nTwo letter combinaions representing one sound, e.g. sh,\\xa0\\nph (phone), ee\\xa0\\nA blend of two vowel phonemes. e.g. oi\\xa0 (see glide)\\nProfound difficulty in acquisition of literacy\\xa0\\nA person with dyslexia\\xa0\\n\\n\\nelide, elision\\xa0\\netymological\\xa0\\netymology\\nextended vowel\\nFanetik\\nfont\\xa0\\nfree vowel\\xa0\\nThe omission of a spoken vowel or syllable\\xa0\\nReferring to the origin of words\\xa0\\nThe study of the origin\\nof words\\xa0\\nA long vowel sound, e.g. ee, aa or oo.\\xa0 Often includes\\ndiphthongs.\\nA phonemic spelling scheme\\xa0\\nA typeface in a particular size and weight - e.g. Arial\\n8\\xa0 pt. italic\\xa0\\nA vowel that can occur at the end of a word.\\xa0 (Cf.\\nchecked\\nvowel)\\xa0\\n\\xa0\\n\\n\\ngrapheme\\n\\xa0\\n\\xa0\\nglottal\\nglottal stop [ipa ?]\\nglottis\\nA category that may include several\\ndifferent shapes but referencing but one phoneme (e.g., G, g, g\\n) One of several shapes understood as being equivalent in terms of reference.\\xa0\\n(see phoneme)\\xa0 Smallest contrastive unit in a writing\\nsystem\\nsounds made in the larynx by narrowing the glottis, whisper\\nThe audible release of a closure of the glottis: bottle-bot\\'l-bo?l\\nThe aperture between the vocal folds (cords) deep in\\nthe throat (larynx)\\n\\n\\nheterograph\\xa0\\nheterophone\\xa0\\nhomograph\\xa0\\nhomonym\\xa0\\nhomophone\\xa0\\nWords with the same sound, spelt differently\\xa0\\nWords that sound differently, but are spelt the same\\xa0\\nWords spelt the same, but different meaning or sound\\nor both\\xa0\\nWords that sound or appear the same, but differ in meaning\\xa0\\nWords that sound the same, but differ in meaning or spelling\\xa0\\n\\n\\nilliteracy\\xa0\\ninflection\\nirregular\\nThe inability to read and write functionally\\xa0\\nAn affix that signals a chanage in a grammatical relationship\\nsuch as tense, mood, etc.\\xa0 Usually a change in a word ending [-s,\\n-ed, -ing].\\xa0 (gram)\\n2. A change in voice pitch (phonetics).\\na linguistic form that is an exception to a pattern or\\nstated rule\\n\\n\\nInitial Teaching Alphabet\\xa0\\nInternational Phonetic\\xa0\\nAlphabet (IPA, IFA)\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nInterspel\\xa0\\nIPA\\nITA\\xa0\\nA system devised to help children to read (paedography).\\nPitman\\'s augmented roman alphabet used as an initial teaching medium. (see\\nITA)\\nA notational scheme developed in 1890\\'s by Passy and Jones\\nfor use by linguists as a standard means of graphically representing spoken\\nlanguages.\\xa0 IPA uses the roman alphabet as a starting point and augments\\nit as needed with rotated characters and special symbols.\\xa0 IPA can\\nbe used for a broad or narrow transcription of a language.\\xa0\\nA pragmatic spelling scheme by Valerie Yule\\xa0\\nInternational Phonetic Alphabet\\xa0\\nInitial Teaching Alphabet, a system\\xa0devised\\nby Sir James Pitman. It was heavily used in the schools in the late 1960\\'s\\nparticularly in the UK.\\xa0 ITA is based on New Spelling and uses a special\\nfont to augment the roman character set.\\xa0 ITA\\nuses 40+ graphemes.\\nReference:\\xa0 Pitman, James\\nand St. John, Reading & Alphabets\\n\\n\\nJSSS\\xa0\\nJournal of the Simplified Spelling Society - a scholarly\\njournal that publishes research on orthographic change and spelling reform.\\xa0\\n\\n\\nLatin 1\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nletter\\nlexical\\xa0\\nlexicon\\nlexicography\\xa0\\nligature\\xa0\\n\\xa0\\nlinguistic(s)\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n.\\xa0\\nliteracy\\xa0\\n\\xa0\\nliteral\\nlogogram\\xa0\\n\\xa0\\n\\xa0\\nlogographic\\nAn augmented character set that includes every letter\\nused in the orthographies of any European language.\\xa0 Available in\\nHTML.\\næ\\xa0 å\\xa0\\nà\\xa0 ì\\xa0\\xa0 î\\xa0\\xa0 è\\xa0\\né\\xa0 ò\\xa0 ù\\xa0\\xa0\\xa0 ñ are\\nsome of the characters available in the Latin\\n1 character set.\\na mark or glyph used in an alphabetic writing system to\\nindicate a sound.\\nReferring to words\\xa0\\nThe vocabulary of a language, esp. in dictionary form.\\nThe compiling of dictionaries\\xa0\\na written or printed symbol in which 2 or more letters\\nare joined (e.g., ae)\\nITA used ligatures for digraphs that\\ndesignate a sound distinct from the component letters. ch does not = c\\n+ h. (see illustration)\\nPertaining to language or the study of languages (tongues)\\nlingua\\nThe ability to read and write functionally.\\xa0 The\\nability to read and comprehend a newspaper is considered the threshold\\nof literacy.\\xa0\\nThe usual or conventional meaning of a word.\\xa0 cf.\\nfigurative\\nA word mark or word sign. Can be comprehended without\\na particular phonographic rendering.\\xa0 4\\nis a logogram.\\xa0\\xa0 The phonetic interpretation of a logogram will\\nbe different in different languages. 4-SALE\\nwould be read as \"quatro sahleh\" in Spanish.\\xa0 (see numbers\\nas sound signs)\\nChinese is often referred to as a logographic writing\\nsystem.\\xa0 Chinese ideograms usually refer to words or concepts although\\nthey often contain some phonographic clues. [ref: deFrancis, Visible\\nLanguage]\\n\\n\\nLojicon -logical\\nconsonants\\xa0\\nLojikon\\xa0\\nlong vowel\\xa0\\nA spelling scheme\\nbased on consistent use of consonants\\xa0\\nLogical Icons - A pictographic script\\xa0\\nSince northern European languages usually have 12 pure\\nvowels and use a character set containing only 5 vowel letters, the same\\nletter is used to denote at least two vowel sounds: a long and short sound.\\xa0\\nLinguists refer to the long vowels as a:, e:\\xa0 i:\\xa0 o:\\xa0 and\\nu:.\\xa0 Reading teachers often use a set that includes diphthongs:\\xa0\\n/ei/ ay, /i:/ ee, /ai/ eye, /ou/ owe, and /ju/ you.\\xa0\\n\\n\\nmachine\\ntranslation\\n\\xa0\\n\\xa0\\nmarker\\n\\xa0\\nmarking conventions\\n\\xa0\\n\\xa0\\nmarkup\\nmagic e\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nmorpheme\\xa0\\n\\xa0\\n\\xa0\\nmorphemic\\xa0\\nAutomated (computer aided) translation. Paste\\nin the text in one language, get back equivalent text in another language.\\xa0\\nAvailable on the web at Alta Vista.\\xa0 How are\\nyou? --> Como vai? (Port. How are you going)\\nA letter or diacritic used to modify the sound associated\\nwith a letter. e, a, and h are often used as markers in English orthography.\\nMarker letters are silent.\\xa0 mit/mite\\xa0 resume/resume\\'\\xa0\\ncaio/chow\\xa0\\n\\xa0/p/ = signal or phoneme (refers to a sound)\\xa0\\n\\xa0[p] = symbol or particular letter or\\nletter combination, <particular spelling>\\n*word = word reference as in *eel\\xa0\\xa0\\nalt.\\xa0 \"eel\"\\nTaging particular categories in a document, e.g., chapter\\nhead, break, bold,...\\nA silent e used to mark a vowel that operates at a distance\\n[fat/fate].\\xa0 Most markers are next to the\\nvowel they modify [moat].\\xa0 The letter e after a consonant,\\nor consonants,\\xa0 which\\xa0 lengthens a preceding vowel.\\xa0\\nA meaningful unit of a language that cannot be further\\ndivided.\\xa0 English morphemes include s (for plural) /dogz/ & /cats/\\nand [ed] for past tense /started, finisht/.\\xa0 Consistent morphemic\\nspelling often misrepresents pronunciation as in checked/chekt\\xa0\\nReferring to a morpheme\\n\\n\\nNES\\xa0\\nNew Spelling\\xa0\\nnotation\\xa0\\n\\xa0\\nNu Folik\\xa0\\nNew Follick (1934)\\n\\xa0\\nNue Speling\\xa0\\nNew Spelling (ca. 1910)\\xa0\\n\\xa0\\nNew English Spelling - refers to an update of New Spelling\\nA simple consistent close-to-TO spelling scheme\\xa0\\n(see below)\\nThe representation of sounds by symbols.\\xa0 A means\\nof transcribing spoken language into visual symbols.\\xa0\\nA variation on the spelling scheme developed by Mont Follick,\\nMP. What English might look like if written in Spanish or Portuguese orthography.\\nSee also Spanglish.\\nThe spelling scheme, suggested by Ellis, that is the foundation\\nfor most phonemic spelling notations based on the principle of least disturbance\\n(of traditional English spelling conventions)\\xa0 Extended vowels are\\nwritten\\xa0 AE[ei]\\xa0 EE[i:]\\xa0 IE [ai]\\xa0 OE[ou]\\xa0 UE[ju]\\xa0\\xa0\\xa0\\n(NS[ipa])\\n\\n\\northographic\\xa0\\northography\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nFrom The Devil\\'s Dictionary\\nby Ambrose Bierce\\xa0\\n\\xa0\\nLiterally \"right markings\" or \"right writng\"\\xa0\\nThe study or practice of correct spelling or writing.\\xa0\\nCan include the study of various rules and regularities and some grammar.\\xa0\\nThat part of grammar that treats of the way a given language is written.\\nThe orthography is distinct from spoken language.\\xa0 English can be\\nwritten in the orthography designed for Spanish.\\xa0 The problem with\\nhitchhiking\\northographies is mostly a problem of having sounds without associated symbols.\\nSpanish has no symbol for the checked or short [i] or the initial sound\\nin [the].\\xa0\\nOrthography, n. The science of spelling by the eye instead\\nof by the ear. Advocated with more heat than light by the outmates of every\\nasylum for the insane... [more]\\n\\n\\npalaeography\\npalate\\xa0 (anat)\\nparsing\\n\\xa0\\nparts of speech\\n\\xa0\\npasigraphy\\nphone\\nphoneme\\xa0\\n\\xa0\\n\\xa0\\nphonemicity\\xa0\\n\\xa0\\nphonetic\\xa0\\nphonetic transcription\\nphonemic transcription\\nphonics\\nphonogram\\n\\xa0\\n\\xa0\\nplural\\xa0\\npolysemy\\xa0\\npolyvalent\\xa0\\n\\xa0\\npositional spelling\\xa0\\n\\xa0\\npronunciation guide\\xa0\\n\\xa0\\n\\xa0\\nPV\\xa0 - Personal View\\xa0\\nThe study of ancient writing and inscriptions\\nThe hard & soft bony structure forming the roof the\\nmouth\\nParting, break into parts. The analysis and labeling of\\nthe gramatical elements of sentences also callled diagramming.\\nA classification of words into groups displaying the same\\nformal features - esp. related to inflection and distribution: verb, noun,\\nadj. etc. Very large morphemic classes. Void of semantic content.\\nThe use of symbols between languages\\na particular occurence of a vowel or consonant\\nA range of sounds that language users interpret as an\\ninstance of a discrete sound segment. The smallest contrastive unit of\\nsound that distinguishes one word (meaning) from another.\\xa0\\nThe extent to which spelling is a guide to pronunciation.\\nTO has a low level of phonemicity compared to the Spanish orthographic\\nsystem\\nWhen the spelling of a word corresponds to its pronunciation\\xa0\\nA narrow detailed transcription that can distinguish regional\\naccents\\nA broad transcription that preserves only those distinctions\\nthat have semantic value for native languge users.\\xa0\\nTeaching reading by first introducing letter sounds (see\\nwhole word)\\nA sound sign.\\xa0 e.g., a letter or word representing\\na sound.\\xa0 In an alphabetical system the phonograms represent letters.\\xa0\\nIn a syllabary, the sound signs represent syllables.\\xa0 see hotsuma.\\nThe form of a word denoting more than one in number\\xa0\\nMany meanings, e.g. words such as fair, bar\\xa0\\nMany sound values.\\xa0 English (TO) contains letters\\nassociated with 14 different sounds.\\xa0\\nWhen the sound value of letters varies according to\\xa0\\nposition in a word.\\xa0 See John Reilly on positional\\nspelling.\\xa0\\nA guide often given in English dictionaries to show the\\nsound of words.\\xa0 Most spelling reformers want to use a consistent\\nspelling guide as for the writing system instead of\\xa0 TO.\\xa0\\nAn SSS publication relating to a notational scheme or\\nspelling reform proposal. The scheme is not necessarily endorsed by the\\nsociety. It\\xa0 represents an individual\\'s views.\\xa0 PV-7\\nNu Folik\\n\\xa0\\n\\n\\nRP - received pronunciation\\xa0\\nredundant\\xa0\\nreformed spelling\\xa0\\nregularity\\xa0\\nr-ending\\xa0\\nRoman Alphabet\\xa0\\nStandard English accent of the BBC and of Southern Britain\\xa0\\nA letter that does not contribute to the sound of a word\\xa0\\nAny spelling scheme that is more regular than TO\\xa0\\nThe extent to which a sound is always represented by\\na spelling\\xa0\\nThe use of r at the end of a word to indicate a vowel\\nsound\\xa0\\nThe 26 letters commonly used in Western Europe\\xa0\\nRhotic A dialect or accent in which [r] is pronounced when it occurs\\nbfore a consonant or a pause.\\n\\n\\nSaund Spell\\xa0\\nschwa\\nschwapostrophe\\xa0\\nscript\\xa0\\nShaw Alphabet\\xa0\\nshort vowel\\xa0\\nsilent\\xa0\\nsilent e\\xa0\\n\\xa0\\n\\xa0\\nsimplified spelling\\xa0\\nsound-symbol correspondence\\xa0\\nA strictly phonetic spelling scheme by Ian Ascott\\xa0\\nThe indistinct unstressed vowel sound common in English\\xa0\\nAn apostrophe used to represent a schwa\\xa0\\nA word with the same or similar meaning\\xa0\\nThe visible part of a writing system.\\xa0 Another word\\nfor alphabet\\xa0\\nA novel alphabet designed for phonetic English writing\\xa0\\nThe vowel sounds as in pat pet pit pot putt\\xa0\\nA letter used in the spelling of a word which is not\\npronounced\\xa0\\nSome silent e\\'s are markers (fate) others are just hold\\novers (give) from a time when they had a function and were pronounced.\\nThe American name for the magic e\\xa0\\nAny spelling scheme that streamlines and simplifies TO\\nThe match, or lack of it, between sounds and letters\\xa0\\na =\\xa0 /ae/, /a:/, /ei/, /\\'/,\\n... The alphabetic ideal is to have one letter correspondent for each sound\\nand one (and only one) sound (phoneme) for each letter (grapheme).\\n\\n\\nSemite, Semitic\\n\\xa0\\n\\xa0\\n\\xa0\\nspanglish\\n\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nspeech recognition\\n\\xa0\\nspell checker\\xa0\\n\\xa0\\nspelling\\xa0\\nSR-1\\xa0\\nSS\\xa0\\nSSS\\xa0\\nstress\\xa0\\nsurplus cut\\xa0\\nsyllabic\\xa0\\nsyllable\\xa0\\nsymbol\\xa0\\nsymbol-sound\\xa0\\n\\xa0\\xa0 correspondence\\xa0\\nsynonym\\xa0\\nA linguistic, not a racial, classification.\\xa0 The\\nlanguages spoken in the Middle East since ancient times.\\xa0 Ancient\\nEgyptian was a blend of west african (Hamitic) and Semitic. The early writing\\nsystems for these languages did not contain letters for vowels.\\xa0 chart\\nSpanglish\\nis a systematic and highly phonemic way of\\xa0 transcribing English speech.\\xa0\\nIt is based on the ancient Saxon alphabet [900-1066] and the old practice\\nof inserting trailing double consonants to mark short stressed vowels.\\xa0\\nThe augmented Latin alphabet [sound-symbol correspondences] was used until\\nthe 15th c. when the pronunciaiton of the long vowels in many words changed.\\xa0\\nThere was no corresponding change in spelling.\\xa0 This led to a divergence\\nbetween spelling and pronunciation.\\xa0 There are two ways to represent\\neach vowel in Spanglish: stressed or unstressed.\\xa0 Most one syllable\\nwords therefor have two possible spellings.\\xa0 e.g. aet or att,\\xa0\\nej and edj.\\xa0 Spanglish is a parallel pronunciation guide spelling.\\xa0\\nRespelling is limited to words that cannot be understood when\\xa0 pronounced\\naccording to the values assigned in the Saxon alphabet.\\xa0 enough\\nbecomes enuff or enof.\\nA computer program that checks spelling.\\xa0 It matches\\na letter configuration to letter configurations in memory and locates letter\\ncombinations that do not match.\\xa0 \" Eye\\nhave a knew checker...\"\\nwould\\nnot be corrected.\\nThe choice of which letters or symbols to combine to represent\\na word\\xa0\\nSpelling Reform - 1st Stage\\xa0 For Lindgren, e = /e/\\xa0\\nhead=hed\\nSimpl Spel \\x96 The newsletter published by the SSS\\xa0\\nSimplified Spelling Society, UK (Ashton University)\\xa0\\nSome syllables in spoken English are stressed louder\\n& clearer\\xa0\\nA spelling scheme by\\xa0 Valerie Yule that clips silent\\nletters from TO\\xa0\\nReferring to a syllable\\xa0\\nUnit of pronunciation forming the whole or part of a\\nword\\xa0\\nA letter or character or sign or combination used in\\nwriting\\xa0\\nThe match, or lack of it, between letters and sounds in\\na word\\xa0\\n(see grapheme-phoneme correspondence)\\xa0\\nTwo or more words with the same meaning or reference\\n\\n\\ntense [vs. lax]\\ngrammar [past present-future]\\nthesaurus\\xa0\\nTO -Traditional Orthography\\ntrigraph\\n\\nturned c\\xa0 turned a\\nturned e [\\']\\ntypology\\xa0\\nUnifon\\n\\n\\nUnigraph\\nArticulated with muscle tension or effort. Cf. fortis\\xa0\\nContrast term: lax\\nGramatical time: variation in the morphological form\\nof the verb [eg. -ed]\\n\\xa0\\nBook that lists words in groups of synonyms\\xa0\\nThe spelling system standardized about 1755 for\\xa0\\nEnglish\\xa0\\nA combination of three letters denoting one sound, e.g.\\nsch\\nA spelling scheme by Tom Zurinskas based on New Spelling\\nbut using double consonants to show stress.\\xa0\\nThe IPA symbol No. 5\\xa0 for the sound in pot\\nThe IPA symbol for the schwa (unstressed central\\nvowel, e.g. uh)\\xa0\\nClassification by type\\xa0\\nUnifon - an augmented alphabet devised by John Malone.\\n40 sound signs.\\nUnifon means one sound and implies one sound per symbol\\nA single letter or symbol denoting one sound.\\xa0 The\\nname of a notational scheme that uses the upper case letters as new sound\\nsigns: A=ei, E=i: I,Y=ai, O=ou, U=ju. and eliminates most digraphs ( eg,\\nQ=oi,\\xa0 T=th, G=ng)\\n\\xa0\\n\\n\\nvowel\\xa0\\n\\xa0\\n\\xa0\\nsemi-vowel\\n\\xa0\\nvowel diagram\\xa0\\n\\xa0\\nA speech sound produced without any obstruction - different\\nvowels are produced by altering the size and shape of various cavities\\nthrough which air passes.\\nA speech sound produced without any friction but similar\\nto a consonant in marking syllable boundaries. English Y & W.\\xa0\\nSerb R.\\xa0 (see syllabic)\\nA quadrilateral or trapezoid\\nrepresenting jaw, mouth, and tongue positions involved in the production\\nof vowel sounds.\\xa0 Devised by Jones.\\n\\n\\nWES World English Spelling\\nwhole word method\\xa0\\n\\xa0\\nword sign (logogram)\\nA spelling scheme\\xa0\\nA way of teaching reading without reference to letter\\nsounds\\xa0\\nSee also look say method\\nAn irregular spelling such as the, me, to, of\\xa0\\nwhich is retained in a reformed spelling scheme.\\xa0\\nA symbol without a consistent sound association.\\xa0\\nIn TO, the words [the, of, and\\na ] are considered word signs due to their irregularity.\\nThey are often retained in otherwise phonemic and consistent notational\\nsystems such as anglic.\\n\\n\\nPhonics\\n\\n\\xa0\\nPhonics does more than teach reading by the `sound of\\na word\\': it is a method of teaching reading and spelling by teaching all\\nsound-symbol correspondences, from simple to complex, systematically and\\ndirectly.\\xa0\\nIt is important to insist that phonics relates sounds\\nto written symbols - it needs to be distinguished from the current\\nbandwagon of training in `phonemic awareness\\' BEFORE teaching letters.\\n\\n\\n\\xa0sitemap-linguisticsThe\\nsitemap provides links to pages with detailed disucssions of many many\\nof these terms\\n\\n\\n\\n\\n\\n\\n\\nThis site of the\\xa0\\nSpelling\\nReform Ring\\nis owned by steve\\nbett\\n\\n\\n[ Previo\\nus 5 Sites | Previou\\ns | Next\\n| Next\\n5 Sites |\\n| Random\\nSite | List\\nSites ]\\n\\n\\n\\nTo\\nadd to, comment on, or critique an entry in this glossary, contactSteve\\nBett\\n\\xa0\\n\\xa0\\n\\n\\n\\n\\n\\n\\nThis site of the\\xa0\\nSaundspel\\xa0\\nRing\\nis owned by steve\\nbett\\n\\n\\n[ Previo\\nus 5 Sites | Previou\\ns | Next\\n| Next\\n5 Sites |\\n| Random\\nSite | List\\nSites ]\\n\\n\\n\\n\\n\\n\\xa0\\n\\n\\nCOMMENTS\\nI\\ndeveloped a more technical glossary of linguistics several years ago that\\nfew people ever used.\\xa0 The problem with narrow technical definitions\\nsuch as \"allophone --two more more forms\\nof the same phoneme\" is that they are clear\\nonly to those who already have the concept. Damian came up with a much\\nbetter selection of terms and supplied his own down to earth definitions.\\xa0\\nHis emailed glossary was the foundation for the current glossary.\\xa0\\nSince I added to, edited, and changed half of the entries that Damian supplied,\\nI take responsibility for the errors.\\xa0 If you have something good\\nto say about this list, directo those comments to Damian. -- Steve Bett\\xa0\\nsbett@mailcity.com\\nI inadvertently failed to copy the\\nnames of the people who supplied these comments on the SSS Forum:\\xa0\\n-SB\\n..............\\nI think allophone -\\xa0 An alternative, similar sound\\nfor the same spelling pattern - is wrong: it\\'s nothing to do with spelling\\nso not: \"allophone An alternative, similar sound for the same spelling\\npattern\" it\\'s nearer to \"a variation in the way in which a particular sound\\nis pronounced, which carries no function in terms of distinguishing it\\nfrom other sounds; for example \\'l\\' in \\'look\\' and \\'little\\'\". The Chambers\\ndictionary definition is not really suitable for general discussion (\"one\\nof two or more forms of the same phoneme\")\\n...............\\nI\\'m not sure that a \"free vowel\" is always the same as\\na \"long vowel\"; it\\'s more to do with syllable boundaries than length. In\\nsome languages it could well be the same thing, but I don\\'t think it\\'s\\nautomatic.\\n...............\\n\"inflection\" need not be at the end of a word. Welsh\\nfor example changes the sound at the beginning of a word.\\n...............\\n\"ligature\" I\\'d add \"... to make a single character\"\\n..............\\n\"logogram\" could include as a more immediate example\\nthe ampersand \"&\" and all the numerals.\\n..............\\n\"Roman alphabet\" - a possibly dodgy definition, since\\nonly English uses this set of 26! A more roundabout definition (ie one\\nless vulnerable to pedants) might be \"The 26 letters which form the basis\\nof the alphabet used for Western European languages\"?\\n.............\\nperhaps you could add: \"glide\" - a sound that appears\\nbetween two others as a side effect of\\nthe pronunciation, for example \"lawR and order\"\\n(This is an important phenomenon that figures prominently\\nin Taam\\'s spellings, so could be worth including)\\xa0 See Truespel\\n\\nFrom John Reilly\\'s website.\\nJonh is the new editor of the JSSS.\\nhttp://pages.prodigy.net/aesir/iln.htm\\n--------------------------------------\\nFrom \"The Devil\\'s Dictionary\"\\nby Ambrose Bierce (1842-1913?)\\xa0\\n\\xa0\\xa0\\xa0\\xa0 Orthography, n. The science\\nof spelling by the eye instead of by the ear. Advocated with more heat\\nthan light by the outmates of every asylum for the insane.\\xa0\\nThey have had to concede a few things since Chaucer\\'s\\nday, but are none the less hot\\xa0\\nin defense of those to be conceded hereafter.\\xa0\\n\\xa0\\xa0\\xa0\\xa0 \"A spelling reformer\\nindicted\\n\\xa0\\xa0\\xa0\\xa0 For fudge was before\\nthe court cicted.\\n\\xa0\\xa0\\xa0\\xa0 The judge said: \\'Enough--\\n\\xa0\\xa0\\xa0\\xa0 His candle we\\'ll snough,\\n\\xa0\\xa0\\xa0\\xa0 And his sepulchre shall\\nnot be whicted.\\'\"\\xa0\\n\\xa0\\nIn Spanglish: What is the last word?\\n\"A spelling reformer indaited\\nFor fudj waz befor the cort saited\\nThe judj sed: \"enuff--\\nHiz candl wiel snuff,\\nAnd his sepulkr shaal not bi hwaited.\"\\nRITE has a different way of handling /ai/\\nA better example of this design\\xa0\\xa0 latin-1\\xa0\\nSteve\\n\\xa0\\n\\n\\n\\xa0\\n\\n\\n\\nweb hosting • domain names • web designonline games • online dating  • long distancedigital cameras  • advertising online\\n\\n\\n\\n\\n\\n',\n",
       " '\\n\\n\\nA Graphemic-Phonemic Analysis of the Reading Errors of Inner City Children\\n\\n\\nA Graphemic-Phonemic Analysis of the Reading Errors of Inner City Children\\n\\nA Graphemic-Phonemic Analysis\\nof the Reading Errors of Inner City Children\\n1. The stability of the reading problem. \\n2. Data gathering\\n3. Graphemic/phonemic analysis\\n4. The onset\\n5. The vowel nucleus.\\n6. The coda\\n7. Inflectional elements\\n\\n7.1. The plural. \\n7.2. The -ing suffix.\\n7.3. The past tense -ed. \\n7.4. The possessive -\\'s. \\n\\n8. Pedagogical implications.\\nReferences\\n\\n\\nDraft\\nMay 26, 1998\\n\\nA Graphemic-Phonemic Analysis\\n\\nof the Reading Errors of Inner City Children\\n\\nW. Labov, B. Baker, S Bullock, L. Ross, M. Brown\\nUniversity of Pennsylvania\\n\\tThis report is an analysis of the reading errors made by African American\\nchildren in a Philadelphia elementary school.[1]\\nIt is designed to provide the linguistic basis for the improvement of methods\\nof teaching reading, which will address the specific limitations in the reading\\nability of children who speak African American Vernacular English [AAVE]. This\\nresearch effort at the Linguistics Laboratory is part of the long-term\\nengagement of the University of Pennsylvania in the educational problems of the\\nWest Philadelphia community, under the Center for Community Partnerships. These\\nare the first results of one year\\'s cooperative work involving Penn students\\nand faculty, West Philadelphia high school students, and teachers in West\\nPhiladelphia elementary schools.\\n\\tThe research reported here has the goal of developing methods for the teaching\\nof reading to take into account (1) the home language and (2) the culture of\\nAfrican American children. This report will present a linguistic analysis of\\nreading difficulties which should be useful in any method of teaching reading.\\nReports to follow will evaluate specific methods for correcting the problems\\noutlined here, and the ways in which these methods can be inserted into a\\ncultural framework that is strongly motivating for African American children in\\nthe inner cities. \\n\\tThe primary site of the research to be reported here will be referred to as\\nthe Woodruff Elementary School, ranging from Kindergarten to the fifth grade.\\nOur first contact with the reading problems of Woodruff students was in the\\nfall of 1997, when Penn students in Linguistics/AFAM 160 acted as teaching\\nassistants in 4th and 5th grade classes. We found many children whose reading\\nskills were one to two years behind grade level and could not read for content\\nat their current grade level. In the spring of 1998, members of the seminar\\nLinguisticsAFAM 161 engaged in extensive tutoring of 2nd and 3rd grade children\\nin the Extended Day Program organized by B. Baker. This program involved a\\nrandom sample of 40 children in the 2rd, 3rd, 4th and 5th grades who were one\\nto two years behind in reading grade level. Almost 100% of the children\\nselected for the sample volunteered for the after school program, and therefore\\nprovided a representative sample of the reading difficulties of children in the\\nWoodruff School.\\n\\n1. The stability of the reading problem. \\n\\nA low level of reading achievement in the inner city has been recognized for\\nmore than three decades. What is remarkable is the stability of the problem, in\\nthe face of many large scale efforts at remediation. The over-all view of the\\nsituation is best provided by the NAEP data, reporting average reading\\nproficiency scores by race/ethnicity. Figure 1 shows the figures for 9 year\\nolds, the age group closest to those we are concerned with. The figure shows\\nthat despite a slight improvement from 1971 to 1980, both black and white\\nscores and the black/white differential are essentially stable from 1980. The\\ninterpretation of this difference in average scores is important here: Score\\nlevel 200 is required for \"partial skills and undersetanding,\" and the black\\naverage has not reached that level, while the whites are past that level on the\\nway to score level 250, which \"Interrelates ideas and makes generalizations.\"\\nOn the whole, the average reading proficiency of the black 9 year olds through\\n1992 was at the level of \"simple discrete reading tasks.\"\\nFigure 1. NAEP average reading proficiency scores by race for 9 year olds, \\n1971-1992\\n\\n\\n\\tThe more detailed study of particular groups of African American students\\nshows that these average figures understate the nature of the problem for inner\\ncity children. Work in South Harlem in 1966-1968 addressed the question as to\\nwhether the dialect spoken by inner city African-American children was\\nresponsible for the failure to teach reading in inner city schools (Labov,\\nCohen, Robins and Lewis 1968). A review of school records showed that 32\\nisolated individuals from various backgrounds showed the expected lag of one to\\ntwo years behind reading grade level, while the problem was far more severe for\\nthose who were integrated into organized forms of street culture. For them\\nreading scores showed an absolute ceiling of 4.9 on the Metropolitan\\nAchievementTest, with no improvement visible from the 6th to the 10th grade.\\n\\tThroughout the United States, the difference between minority and majority\\nperformance in reading increases from the third grade onward. In 1976, the\\nCalifornia Achievement Test scores reported in Philadelphia showed such a\\ntypical decline for schools that had 100% African American enrollment. Figure 2\\nshows the percent in the lowest 16th percentile for all elementary, junior high\\nand high schools, and for three typical schools with 100% African American\\nenrollment (Philadelphia Inquirer 7/25/76).\\nFigure 2. Percent of Philadelphia students in the lowest 16th percentile in\\nCalifornia Achievement Test reading scores, 1976\\n\\nIt can readily be seen that the over-all reading failure is even greater than\\nthat shown in Figure 2, since the high school percentages do not include those\\nwho have dropped out along the way. \\n\\tThe most recent scores published by the Philadelphia School District, using\\nthe PSSA test, show that there has been no improvement. These scores show the\\npercentage of school students whose test scores are in the lowest quartile for\\nthe state. For Cooke, 58% of the students\\' scores were in the lowest quartile\\nand for Franklin High School, 80%. For Cooke, only 12% of the students\\' scores\\nwere in the highest quartile, and for Franklin, 0%. This acceleration of the\\nfailure rate reflects a low level of reading for almost all schools from the\\noutset. Table 1 and Figure 3 show the distribution of reading scores by\\nquintile for all of the 5th grades in Philadelphia.\\n\\tTable 1 and Figure 3. Distribution of reading scores by quintile for all 5th\\ngrades in Philadelphia by the Pennsylvania System of School Assessment (PSSA)\\nfor February 1997 (Philadelphia Inquirer 4/17/98).\\nQuintile\\tNo. of schools\\n\\t1\\t1\\n\\t2\\t0\\n\\t3\\t0\\n\\t4\\t14\\n\\t5\\t141\\n\\n\\nThe Woodruff Elementary School was in the lowest quintile for these PSSA\\nreading scores. The distribution of students\\' reading scores by quartile was:\\n1st, 6%; 2nd, 10%; third, 15% and 4th 69%, typical of most of the schools in\\nthe district.\\n\\tA similar situation is found in schools with high minority enrollment\\nthroughout the nation. The figures as normally reported do not differentiate\\nthe effect of race and ethnicity from socioeconomic factors. Minority status\\nand poverty are highly correlated, and socioeconomic status is highly\\ncorrelated with reading performance. In the 1997 data of Table 1, the\\ncorrelation between the percent of students\\' scoring in the lowest quartile and\\nthe percent of low income students was .75. \\n\\tThis report deals with the reading of African American students from low\\nincome families. Some of the results show a correlation with the specific\\nfeatures of African American Vernacular English (AAVE), but many reflect\\nproblems that appear to be general for all speakers of English. Further studies\\nwill undertake the analysis of reading difficulties for comparable\\nsocioeconomic groups with different cultural backgrounds.\\n\\n2. Data gathering\\n\\nThe Extended Day Program gave the 40 elementary students involved a great deal\\nof individual attention. Children engaged in many activities with the Penn\\nstudents, volunteers from West Philadelphia High School and Woodruff staff. The\\nmost systematic tutoring in reading was done by the Woodruff staff and Penn\\nstudents. The researchers who obtained the data in this report selected\\nillustrated books from a wide range that had been determined to be at the\\nstudents\\' grade level.[2] Children read the\\ntexts aloud; when there were errors or difficulties, tutors supplied the word\\nonly when the child had made several wrong efforts, paused for at least five\\nseconds, or indicated that they had no idea what the word was. \\n\\tTwo types of data were recorded: words where no effort was made to pronounce a\\nguess or a version, and words that were read aloud in a form that could not be\\ninterpreted as corresponding to the sense intended in the text. Throughout this\\nproceeding we distinguished carefully between differences in pronunciation and\\nmistakes in reading (Labov 1965, Goodman 1969). It has been well established\\nthat speakers of AAVE frequently do not pronounce the second element of apical\\nconsonant clusters (in past, passed, etc.), although there is good\\nevidence that these clusters are intact in their underlying mental\\ndictionaries.[3] When students read words like\\nworked as work, this was not rated as a mistake in reading unless\\nthere was clear evidence that they had failed to grasp the past tense\\nmeaning.\\n\\tThe main body of findings to follow is the result of the analysis of the\\nsecond type of data, where readers gave some version of the word aloud. These\\ndata permit us to locate the source of the reading difficulty more precisely\\nthan the simple inability to read the word. However, an examination of the\\nwords that were not read at all is consistent with the findings of the analysis\\nof attempted reading errors. A total of 450 reading errors from 20 children\\nform the body of data to be examined in the sections that follow.\\n\\n3. Graphemic/phonemic analysis\\n\\nOur first contact with the reading problems of second and third grade children\\nshowed that they suffered a high level of frustration in attempting to read the\\nbooks that were designed for 2nd and 3rd grade readers. One might point to many\\nof the sources of reading problems identified in the literature: a lack of\\nfamiliarity with the vocabulary or the subject matter, poor strategies for\\ndeducing the meanings of unfamiliar words from context, lack of attention to\\nthe main ideas, and so on. However, the first priority was given to the\\nanalysis of their ability to use sound/letter correspondences to identify words\\nthat are included in their speaking and hearing competence. A great deal of\\nrecent discussion of children\\'s decoding abilities has focused on predictors of\\nsuccess or failure in reading, and the development of phonemic or phonological\\nawareness has played a major role (NRC 1998: 110-112). Our focus is not on\\nprediction but upon the actual description of children\\'s ability to use the\\nregularities of grapheme-to-phoneme correspondence to identify words. In this\\nrespect, we continue the tradition of Calfee and Venezky (1968) who examined\\nchildren\\'s abilities to decode specific alphabetic elements and relations; we\\nextend this approach to the reading of inner city African American children. We\\nhave reason to believe that there are specific weaknesses in this population\\'s\\ngrapheme-to-phoneme processing. Earlier studies of the reading errors of AAVE\\nspeaking youth indicated that the alphabet was used consistently for the first\\nconsonant and vowel, but that it was frequently ignored for the following\\nmaterial (Labov, Cohen, Robins and Lewis 1968). \\n\\tIf it should turn out that there were little relationship between the\\nfrequency of errors and the phonemic structure of words, one would conclude\\nthat weakness in decoding skills was not an important part of the causes of\\nreading failure, and turn to other aspects of reading. But if there is a high\\ncorrelation between frequency of reading errors and the complexity of the word\\nand syllable, we can conclude that there is a great deal to gain by reinforcing\\nchildren\\'s knowledge of these relations.\\n\\tThe analysis must necessarily combine graphic and phonemic relations. Our\\nfirst question on any letter is whether the reader paid attention to it and\\nrecognized it for what it is. The second question is whether it was interpreted\\nproperly as an individual phoneme, as a part of a digraph indicating an\\nindividual phoneme, or as a part of a cluster of phonemes which function\\njointly as the onset, nucleus, and coda of the syllable.[4] A third question is whether it was combined with other\\nelements to produce a recognizable version of the intended word.\\n\\tThis analysis of reading errors is consistent with findings from studies of\\nchildren\\'s abilities to use sound/letter correspondences in reading nonsense\\nwords (Venezky 1972; Venezky and Johnson 1972; Calfee, Venezky and Chapman\\n1972). However, the pronunciation of nonsense words introduces factors which\\nunderestimate the actual reading competence of children, since rules of\\nsound/letter correspondence are not equally productive, and the role of\\nexceptions in weakening the person\\'s confidence in the validity of rules is not\\nyet understood. The study of meaningful reading in context has the advantage of\\neliminating the artifactual nature of nonsense words. Yet it introduces the\\nopposing problem of overestimating decoding skills by mixing the process of\\ndecoding with the recognition of sight words. By focusing on the distribution\\nof letter/sound correspondences in reading errors, where sight words are least\\nlikely to appear, we should come closer to a valid estimate of decoding skills.\\n\\n4. The onset\\n\\nTable 2 and Figure 4 show the distribution of reading errors for the onsets of\\nthe first syllable--all of the consonants that precede the first vowel.\\nTable 2 and Figure 4. Distribution of reading errors for onsets of first\\nsyllables.\\n\\n\\n\\n\\nN\\n\\nTotal\\nerrors\\n\\n\\nPercent\\nerrors\\n\\n\\nInverse\\n\\nL/R\\ntransfer\\n\\n\\nR/L\\ntransfer\\n\\n\\n\\nFirst\\nconsonant\\n\\n\\n414\\n\\n17\\n\\n4\\n\\n0\\n\\n0\\n\\n0\\n\\n\\nSoft\\nc\\n\\n\\n20\\n\\n5\\n\\n25\\n\\n1\\n\\n1\\n\\n0\\n\\n\\nDigraph\\n\\n42\\n\\n14\\n\\n33\\n\\n3\\n\\n0\\n\\n0\\n\\n\\nsC\\ncluster\\n\\n\\n39\\n\\n14\\n\\n36\\n\\n3\\n\\n0\\n\\n2\\n\\n\\nCr/l\\ncluster\\n\\n\\n93\\n\\n32\\n\\n34\\n\\n5\\n\\n9\\n\\n3\\n\\n\\nsCr\\ncluster\\n\\n\\n9\\n\\n7\\n\\n78\\n\\n0\\n\\n0\\n\\n3\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPerhaps the most important finding of this study is shown in the first column\\non the left: errors in reading the initial consonant. This measure indicates\\nwhether or not the reader accurately detected what letter was present, but not\\nthe application of contextual rules of sound/letter correspondences. For\\nexample, the word ceiling was read as \"killing,\" showing a failure to\\napply the rule that c is pronounced as /s/ before a front vowel e\\nor i. However, it was recognized as c. The percent of errors\\nis very low: 3%. The 17 errors include four which indicate problems in spatial\\nlocation rather than identification of letters: the familiar reversal was\\nfor saw, yes for eyes, and two misreadings of them\\nwhich began with m. \\n\\tThus it is clear that the children\\'s reading problems are not the result of\\nchildren\\'s failure to learn values of the letters of the alphabet. Consistent\\nwith earlier studies of Harlem adolescents (Labov et al. 1968), we do not find\\n2nd and 3rd graders in West Philadelphia who fail to use the alphabet\\naccurately for the initial consonant. A comparison can be made with a study\\nwhich used the standard methodology of testing children\\'s ability to give\\nplausible readings of nonsense words: the Wisconsin study of Venezky, Chapman\\nand Calfee (as cited in Venezky 1972). The West Philadelphia reading of lone\\ninitial consonants is comparable to the decoding skills of the highest quartile\\nof Wisconsin children tested at the end of grade 2. For the recognition of the\\ninvariant consonants m, d, l, b, the range was 97 to 99% correct in\\ninitial position. Since our measure looks only at words that were read\\nincorrectly, the actual rate of success in reading all m,d,l,b would be\\nmuch greater; yet for the 75 words in our list beginning with these consonants,\\nthe West Philadelphians made only one error in the initial consonant. The West\\nPhiladelphia children are far superior to the Wisconsin lowest quartile, whose\\nerror rates were close to 10% for all four consonants.[5] \\n\\tThis situation contrasts sharply with error patterns for any words in which\\nthe onset has more than a single consonant. The third row of Table 2 and third\\ncolumn of Figure 4 shows reading errors associated with initial digraphs. The\\nlist of errors includes not only cases of digraphs read as a single grapheme\\n(scenes -> sense, thump -> jump, their -> her, three ->\\ntree), and as another digraph (that -> what), but there were also\\ninverse errors, where a phoneme corresponding to a digraph was used when only a\\nsingle consonant was printed (tank -> think, trout -> throat,\\nsuggested -> shoulder). Such inverse errors are shown as black bars in\\nthe error column for all items in Figure 2 and figures to follow.\\n\\tThe column labeled sC- cluster designates all words that have consonant\\nclusters or \"blends\" beginning with s-. Here we find an\\nadditional type of error--readings in which the second element of the cluster\\nis transferred to the right, on the other side of the vowel nucleus. Thus we\\nhave stall -> /sæt´l/ steel -> settle, strong ->\\nshort. There are also reverse, right-to-left locational errors, with a\\nsegment moving from the coda of the syllable to the onset, as in settling\\n-> stealing. \\n\\tThe next category, Cr/l clusters, involves onset clusters where the\\nsecond consonant is a grapheme indicating a liquid phoneme-- an /r/ or an /l/.\\nThe first consonant is a grapheme corresponding to an obstruent:\\nphoneme: that is, either a stop (p,t,k,b,d,g) or a fricative\\n(f,s).[6], Errors with clusters of\\nthis type are more numerous and show the same pattern. In addition to the loss\\nof the second grapheme (dragons -> danger, crops -> coops, shrubs\\n-> shubs, drawn -> down), there are frequent re-assignments of the\\nsecond element to the syllable coda, as in cloud -> cold, tried ->\\ntired, friendly -> fire, slimy -> smelly). In some cases, an element\\nof the coda is moved into the onset: dirtying -> drying, peddlars ->\\nplatters, fell -> flew). There are also a good share of inverse errors,\\nwhere a single consonant is read as a cluster: tack -> track, double\\n-> trouble. \\n\\tThe distribution of errors among the various types of complex onsets is\\nremarkably stable, close to 35%. In the smaller subset of words with\\nthree-member onsets, the combination of sC- and Cr- problems is\\nuniformly responsible for errors: strong -> short, screamed -> scare,\\nstruggle -> /s/. The fact that the spatial location of the second\\nsegment is uncertain is not surprising. Thus Adams (1990) notes:\\n\\tAlthough the visual system is quite fast and accurate at processing item\\ninformation (such as the identities of the individual letters of a word), it is\\nboth slow and sloppy about processing their spatial locations. --113.\\nAdams found (1979) that a major difference between less skilled and more\\nskilled readers was in their ability to view and report ordered pairs of\\nletters, particularly in their sensitivity to the frequency with which a pair\\nof consonants occurred in printed English.\\n In our data, the contrast between simple and complex onsets is almost\\ncategorical: when words are read wrong, it is rare to find that a lone initial\\nconsonant is involved; but if a complex onset was present in the word, it is\\nvery likely responsible for the error. The pedagogical implications of this\\nfact will be developed in section 8.\\n\\n5. The vowel nucleus.\\n\\nThe view of reading errors for the vowel nucleus shows a much higher level of\\nerrors than the onset. By far the lowest value is shown in the column for the\\nfirst vowel. In the majority of the reading errors, the first vowel was\\nrecognized for what it is; yet the percent of errors is seven times as great as\\nfor the first consonant.\\n\\tOne might attribute this higher rate for the first vowel to the fact that it\\nis not as salient in the word, but comes after an initial consonant. However,\\nwhen we compare in the next row the error rate for words read wrongly with\\ninitial vowels, it is 22%, not far behind the overall rate of 29% for the first\\nvowel, and much greater than the 4% for the first consonant: uses ->\\nsees, united -> /´nayt´d/, over -> /Uv´r/, even ->\\nany, etc.\\nTable 3 and Figure 5. Distribution of reading errors for the vowel nucleus.\\n\\n\\n\\n\\nN\\n\\nTotal\\nerrors\\n\\n\\nPercent\\nerrors\\n\\n\\nInverse\\n\\nExceptions\\n\\n\\nFirst\\nvowel\\n\\n\\n436\\n\\n128\\n\\n29\\n\\n0\\n\\n0\\n\\n\\nInitial\\nvowel\\n\\n\\n36\\n\\n8\\n\\n22\\n\\n0\\n\\n0\\n\\n\\nSilent\\ne\\n\\n\\n61\\n\\n55\\n\\n90\\n\\n8\\n\\n3\\n\\n\\nDouble\\nvowel\\n\\n\\n118\\n\\n70\\n\\n59\\n\\n8\\n\\n7\\n\\n\\nr-controlled\\nvowel\\n\\n\\n55\\n\\n39\\n\\n71\\n\\n18\\n\\n0\\n\\n\\nBisyllabic\\nshortening\\n\\n\\n27\\n\\n36\\n\\n75\\n\\n1\\n\\n1\\n\\n\\nUnstressed\\nvowel\\n\\n\\n127\\n\\n97\\n\\n76\\n\\n5\\n\\n0\\n\\n\\n\\n\\tComplications in the nucleus are responsible for a much higher rate of the\\nreading errors than complications in the consonantal onset; the rates in Figure\\n3 are all well above 50%. The first of these columns shows the distribution of\\nerrors for words containing the \"silent e\" pattern, that is, a final\\northographic e in the second syllable after a lone consonant. Of all\\nregularities in English vowel system, this is one of the most reliable. It\\nconsists of two parts:\\n\\t(1) An e after a lone consonant is not pronounced. (Only exceptions:\\nNike, adobe).\\n\\t(2) The vowel of the first syllable has its appropriate long sound.\\n(Exceptions: (a) have, give, live; (b) love, glove, shove, hover,\\noven, above, cover, dove; (c) move, prove.\\n\\tThe exceptions to the second part of this rule are heavily concentrated in the\\nspecific environment of -ov-[7]\\nand there are a vast number of words which follow the pattern regularly.\\nNevertheless, the West Philadelphia students have plainly not mastered the\\nsilent e rule. We observe a large number of simple errors. Some are\\nsimply a preservation of the short sound of the vowel in defiance of the silent\\ne: plane -> plan, globes -> globs, aside -> acid, ate ->\\nafter, Pete -> pet, fluoride -> /flohrid/, concrete -> /kaNkret/.\\nIn other cases, a vowel is supplied that has no relation to the\\nbasic rule: device -> /divoys/, stated -> started, arrive ->\\n/riyv/, mice -> /miys/. Some errors are plainly motivated by the\\nexceptions noted above: moves -> more, moves -> most, moved ->\\nmost. \\n A most significant set of errors are the inverse errors, where a long\\nvowel is supplied when a short vowel is dictated by the spelling: slid ->\\nslide, crops -> coops, hymns -> /haymz/, children -> chide, listen\\n-> lice, twenty -> twice, suppertime -> supertime, hopscotch ->\\nhopescotch. These indicate that the mastery of the simple regular CVC\\npattern is not complete. The fact that slid can be realized as slide\\nshows that the reader does not control either the rule that i is\\nshort /i/ in the environment C(C)VC (Exceptions: none), or the rule that i\\nis long /ay/ in the environment C(C)VCe (Exceptions noted above). \\n\\tThe third column in Figure 5 indicates that when an error does occur in a word\\nsubject to the CVCe regularity, in nine out of ten cases the silent /e/ rule is\\nnot being used accurately.\\n\\tThere is no simple regularity governing sequences of two vowels: the often\\nquoted rule that the first vowel says its own name applies only to AY, AI,\\nEE, OA, UE but not to AU, AW, EY, OO, OY, OU, OW, UY. The analysis\\nof the individual patterns would require a larger data set, based on frequency\\nof all words in the texts read. However, Table 4 gives some indication of the\\npatterns involved in the 108 reading errors with double vowels in the nucleus.\\nThe words in which the errors occur are classified as \"regular\" if they follow\\nthe major regularity that can be described by a rule. For example, the word\\nlooked is considered regular since all oo combinations before /k/\\nare pronounced with short /u/ (Exception: kook); the word dew is\\nregular since it shows ew -> /uw/, while sew is irregular. \\n\\tIn Table 4, there are very few errors involving the most regular double vowel\\ncombinations: AY, AI, OA. But the most transparent double vowel of all, EE, is\\nwell represented among the list of errors. The irregular combination EA is also\\nwell represented, with threatened -> /t´rt´nd/, earth ->\\neach, dean -> Dianne, etc. The totals for all vowels indicate a relation\\nbetween irregularity and reading errors (Chi-square = 3.8, p < .05).\\nTable 4. Errors and successes with double vowels in words with reading errors.\\n(Upper figure: reading of VV correct; lower figure; reading of VV\\nincorrect).\\n\\t\\tRegular\\tIrregular\\tInverse\\n\\tee\\t3/3\\t\\t\\t2\\n\\tei\\t\\t1/4\\t\\n\\tea\\t16/6\\t1/3\\t1\\n\\tew\\t1/5\\t\\t\\n\\teo\\t1/0\\t\\t\\n\\tey\\t\\t1/1\\n\\tie\\t1/2\\t\\t\\n\\tai\\t0/1\\t\\t\\t2\\n\\tay\\t4/1  \\n\\toi/y\\t2/0\\t\\t\\t1\\n\\toa\\t0/1\\t\\t\\n\\too\\t4/1\\t \\t\\t2\\n\\tou\\t2/6\\t4/9\\t1\\n\\tow\\t\\t0/3\\t\\n\\taw\\t2/5\\t\\t\\t3\\n\\tue\\t0/2\\t\\t\\n\\tTotal\\t36/33\\t7/20\\t12\\n\\n\\tThe next category, r-controlled vowels, refers to vowels followed by\\n/r/ in the same closed syllable: here, card, hardware, store, board\\netc., but not merry, orange or arrest. This category seems to\\nshow the effect of the phonetic pattern of AAVE. In Philadelphia, speakers of\\nAAVE show a wide range of variation in the realization of postvocalic /r/, with\\nmost speakers averaging about 50% consonantal and 50% vocalized /r/.[8] In this data set, there are only three examples\\nof an orthographic r not being registered in the interpretation of the\\nword: earth -> each, return -> runting and Missouri ->\\nmesses. It is more common to find that the vowel supplied is not the one\\nindicated in the spelling: star -> store, stare -> store, war ->\\n/wiyr/, where -> worry, their -> her, Hampshire -> hemisphere.\\nMore often, we find the /r/ pronounced as if it occurred in a different\\nlocation in the word: assorted -> across, strong -> short, threaten\\n-> /Q´rt´nd/, tried -> tired, dirtying -> drying, Corvette\\n-> /kohrvend´r/. But the most common type of error is inverse: an\\n/r/ is supplied although there is no indication in the spelling that an\\nr is present. Thus we have\\n\\tanother \\t->\\tArthur\\n\\tstated \\t-> \\tstarted\\n\\teconomy\\t-> \\t/´kohrn´mi/\\n\\tcousins \\t->\\t/kohrzinz/\\n\\tagent\\t-> \\t/ohrgid/\\n\\tAlbert \\t-> \\t/ahrb´rt/\\n\\tocean \\t-> \\t/owkohrn/\\n\\tmoves\\t-> \\tmore\\n\\tthey\\t->\\ttheir\\n\\twhile\\t->\\twhere\\n\\tglide\\t->\\tgirlies\\n\\nThis pattern of errors indicates that postvocalic /r/ is a stumbling block in\\nthe reading patterns of the West Philadelphia speakers of AAVE. Since /r/ does\\nnot have a stable representation in the spoken language, the step from\\northographic representation to phonemic interpretation appears to be\\nobstructed.\\n\\tProblems with the interpretation of bisyllabic words are too frequent and\\ncomplex to be dealt with here. One of the problems lies with the readers\\nassigning long vowel status to the vowel in a second syllable that is \\n\\tstorage \\t->\\t /stohreyj&/\\n\\tnotice\\t->\\t/natays/\\n\\tcarrot\\t->\\t/kærowt/\\n\\tpirate\\t->\\t/p´riyt/\\n\\tocean\\t->\\t/owkohrn\\n\\taphid\\t->\\t/æp´hiyd/\\n\\tmodels\\t->\\t/midey/\\n\\tmoment\\t->\\tmommy\\nThere are many other difficulties involved in bisyllabic words, but it is clear\\nthat rules for assigning stress must be incorporated to advance one\\'s reading\\nlevel. This problem is highlighted by a question that students have asked more\\nthan once: why is comfortable not pronounced \"comfort-table\"?\\n\\tThe last column in Figure 5 concerns unstressed vowels. As in the previous\\ncase, the number and variety of errors involving unstressed vowels is too great\\nto allow a simple analysis. But there is a remarkable tendency to simply omit a\\nfinal syllable, which is the converse of the tendency discussed above to assign\\na full value to unstressed vowels. Thus we have a second syllable simply\\nomitted:\\n\\tcloset\\t->\\t/klowz/\\n\\tseconds\\t->\\tsix\\n\\tdizzy\\t->\\tdiz\\n\\tbroomy\\t-> \\tbroom\\n\\tmorning\\t->\\t moon\\n\\tnasty\\t->\\tnast\\n\\tgrouchy\\t->\\tgrouch\\n\\tdaydreamers\\t-> \\tdaydreams\\n\\tstrangest \\t->\\tstrange\\n\\nEven greater problems are created by trisyllabic words with unstressed vowels\\nin a middle syllable: separate -> sport, incident -> /inhent/.\\nThis is not surprising, since all of the problems outlined so far are\\nmultiplied in such multisyllabic words.\\t\\n\\n6. The coda\\n\\nIn previous discussions of how one might apply knowledge of AAVE to the\\nteaching of reading, it was emphasized that the relation between the written\\nand the spoken language was far more complex at the ends of words (or\\nsyllables) than at the beginning. This is particularly true for AAVE, where the\\ntendency to simplify final consonants is more extreme than in other dialects,\\nparticularly in absolute final position, or \"citation form.\" It was therefore\\nsuggested that in the teaching of reading to speakers of AAVE, more attention\\nbe given to the ends of words than the beginnings (Labov 1983, 1995). So far,\\nthe special phonological character of AAVE has been reflected in reading errors\\nonly in the r-controlled vowels. Further evidence will appear in this section\\nof the analysis.\\n\\tThe error rate for the lone post-vocalic consonant shown in Table 5 and Figure\\n4 comes close to the error rate for the first vowel. While it is higher than\\nfor the initial consonant, it is considerably lower than any of the more\\ncomplex codas.[9] Again, this figure reflects\\nthe fact that even readers who are operating at one or two years behind grade\\nhave the ability to recognize and integrate individual letters into the reading\\nprocess. The errors that do occur are the result of a wide variety of causes\\nwhich are often connected with the problems reflected at the end of the last\\nsection: the segmentation of syllables.\\n\\tFinal geminates seem at first glance to be no different from final lone\\nconsonants. In speech, English phonological rules automatically reduce them to\\nsingle consonants. Yet they produce a distinctly higher rate of errors in the\\nwords involved here. It is not clear that the errors of planning -> play\\nand passing -> pages have anything to do with the -nn- and\\n-ss- involved here. It is even less clear how the -tt- in\\nletting is involved in the misreading lesting. But the overall\\npicture is that any complication in the coda will be a focus of difficulty in\\ndecoding. \\n\\tThe final digraphs -ch, -th, -ng, -ck lead to a further increase in the\\nlikelihood that this will be the site of a reading error. It is the set of\\nwords with -gh spellings that are responsible for this increase, marked\\nin the table and figure as exceptions: thoughts -> things, thoughts ->\\nthough, right -> ridge. This is hardly surprising.\\n\\tThe fourth and fifth categories in Table 5 and Figure 6 deal with clusters of\\ntwo different consonants. In speech, English clusters are frequently\\nsimplified; this happens in different ways, depending on whether the second\\nelement is a tongue-tip or apical consonant (/t, d/) or a consonant of a\\ndifferent type formed with the lips (/p, b/) or the back of the tongue(/k,g/).\\nIn speech, the apical consonants are frequently deleted by speakers of all\\ndialects, yielding forms like good ol\\' boy or las\\' month.\\nClusters ending in non-apical consonants follow a different pattern. If the\\nfirst element is nasal consonant (/m,n/) it is often vocalized, and the second\\nelement is rarely dropped. If the first consonant is an /s/, we find that\\nsecond element in words like wasp or ask is occasionally deleted,\\nbut not as often as with apical consonants.[10]\\nThese patterns of simplification are found in all English dialects, but for\\nAAVE, it occurs at a relatively high rate, and simplification in absolute final\\nposition is very high.[11] This means that\\nwhen a word is explicitly introduced to a learner (\"This is gold.\"), the\\nfinal /d/ is in a position where it is least likely to be pronounced or\\nheard.\\n\\tThe most likely focus for reading errors in the final set of consonants is\\ntherefore in the group of clusters that are most reduced in AAVE speech: the\\napical clusters -st, -nd, -nt, -ld, -lt. Of the 38 words with such final\\nclusters, 26 are the site of a reading problem.\\n\\tIt is sometimes thought that AAVE does not simplify clusters with\\nheterogeneous voicing like -lt in belt or dealt, and\\n-nt in sent and went. The rate of simplification is lower\\nfor such clusters than homogeneous clusters, but there is a regular rate of\\nreduction for all speakers (Labov et al. 1968). The reading problems are\\ncomparable:\\n\\tHomogeneous\\tHeterogeneous\\n-ld\\t\\t-lt\\n\\tbald\\t->\\tblad\\tsalty\\t->\\tsailintli\\n\\tmold \\t-> \\tmolid\\tsplit\\t->\\tspilt\\t\\t\\n\\tchildren\\t->\\tchide\\n\\tmold\\t->\\tmiles\\n\\tfieldlets\\t->\\tfindlets\\n-nd\\t-nt\\t\\n\\tbehind\\t->\\tbeginning\\ttwenty\\t->\\ttwice\\n\\tround\\t->\\tright\\tmoment\\t->\\tmommy\\n\\tseconds\\t->\\tsix\\tmoment\\t->\\tmeat\\n\\t\\t\\t\\twanted\\t->\\twater\\n\\t\\t\\t\\tagent\\t-> \\t/ohrgid/\\nAmong the highest rates of simplification in speech are the clusters in -st,\\nexemplified here by roasted -> rose, suggested -> shoulder,\\nstrangest -> strange. It is not an accident that the word posts\\nis read as pots, since the cluster -sts is categorically\\nunpronounceable in AAVE (Labov et al. 1968:131), and this is one of the ways of\\nreducing it to a speakable form.\\n There are also significant rates of inverse errors involved with\\nconsonant clusters: Audrey -> Andrew, decided -> dances, disease ->\\ndistance. It is interesting to note that these additions of clusters where\\nno errors were found in the spelling all involve the insertion of an apical\\nnasal consonant /n/.\\n\\tOnly 10 non-apical clusters are found in the list of reading errors, and of\\nthese, only are the site of an error: Hampshire -> hemisphere,\\nclumping-> climbing, Ralph -> /ripU/, and the inverse hiding ->\\nhanking. [12]\\nThe orthographic problems posed by non-apical clusters are no different from\\nthose posed by apical clusters, and this difference must therefore be\\nattributed to the intersection of speech patterns and phonological perception\\nwith decoding of the visual signal.\\nTable 5 and Figure 6. Distribution of errors in the consonantal coda.\\n\\n\\n\\n\\nN\\n\\nTotal\\nerrors\\n\\n\\nInverse\\n\\nR/L\\ntransfer\\n\\n\\nExceptions\\n\\n\\nLone\\nconsonant\\n\\n\\n225\\n\\n56\\n\\n0\\n\\n0\\n\\n0\\n\\n\\nFinal\\ngeminate\\n\\n\\n48\\n\\n18\\n\\n0\\n\\n0\\n\\n0\\n\\n\\nFinal\\ndigraph\\n\\n\\n34\\n\\n14\\n\\n0\\n\\n0\\n\\n4\\n\\n\\nFinal\\napical cluster\\n\\n\\n58\\n\\n46\\n\\n10\\n\\n1\\n\\n0\\n\\n\\nFinal\\nnon-apical cluster\\n\\n\\n11\\n\\n5\\n\\n1\\n\\n0\\n\\n0\\n\\n\\n\\n7. Inflectional elements\\n\\nAppended to the coda are the English series of grammatical inflections: the\\nplural -s, the possessive -\\'s, the third singular -s on\\nthe present tense of the verb, the comparative -er and superlative\\n-est, the regular past tense -ed, and the present participle\\n-ing. Here one would expect to see the most specific effects of the home\\nlanguage of the West Philadelphia students, since AAVE treats the various\\nsuffixes quite differently. In the body of reading errors, there is a fair\\namount of data on three of these: the plural -s, past -ed, and\\nparticiple -ing. We have a small set of possessives, sufficient to give\\na bare indication of how readers treat this suffix.\\n Though the inflections represented by a single consonant form part of\\nthe coda of the preceding syllable, they are treated differently in speech from\\nother members of the coda. For example, the word goes usually shows the\\nlength and phonetic position of the free vowel in go, as opposed to the\\nchecked syllable of rose. In reading, we would also expect that those\\nwho are familiar with the functions of these particles will treat them\\ndifferently from other members of the coda.\\n \\nTable 6 and Figure 7. Distribution of errors for grammatical inflections.\\n\\tN\\tTotal\\tInverse\\t% errors\\t\\n\\t\\terrors\\nPlural -s\\t89\\t47\\t12\\t53\\t\\t\\nParticiple -ing\\t34\\t19\\t8\\t56\\nPreterit -ed\\t25\\t21\\t0\\t84\\nPossessive -\\'s\\t3\\t3\\t0\\t100\\n\\n\\n\\n7.1. The plural. \\n\\nEvery study of AAVE shows that the plural inflection is intact. There are only\\ntwo points of difference from other dialects to be noted:\\n\\t(a) Like many other Southern dialects, AAVE does not use the plural inflection\\nwith nouns of measure: e.g., three books but three cent, five\\nyear.\\n (b) The -s is generalized to nouns that take a zero plural in\\nother dialects: deers, sheeps, fishes, and nouns with the -en\\nplural: mens, womens. \\n Some pre-adolescents will omit the plural inflection, but Torrey\\'s\\nstudy of second graders in Harlem in 1967 showed over 95% use of the plural in\\nspontaneous speech, and no difficulty in semantic interpretation. Ball\\'s\\nreplication in Michigan in 1983 showed similar results. Because the plural is\\nregularly pronounced, we count every omission of it as a probable error in\\nreading. \\n\\tTable 6 and Figure 7 show a sizeable error rate for plural -s, but by\\nno means as high as the other elements in this section, and quite moderate when\\ncompared with the coda in Figure 6. Furthermore, the pattern of inverse errors\\nindicates a consciousness of the use of the plural as a grammatical inflection.\\nIn changing -> changes, thinking -> things and passing ->\\npages the reader is plain substituting one inflection for another. It is\\ninteresting to note that many of the simple omissions involve the variant\\n-es, used after sibilants and i (from y): dresses ->\\ndress (twice), witches -> witch (twice), houses -> house,\\nbodies -> body . The actual percentage of simple errors of the plural\\n-s is then reduced to a small number.\\n\\tIt follows that the West Philadelphia readers have a better understanding of\\nthe consonantal representation of the plural as -s than the alternate\\nspelling -es. The ability to read the -s inflection is shown in\\nsuch errors as treaters -> treats, where the -er on the\\nunusual word treaters is omitted but not the plural itself.\\n\\n7.2. The -ing suffix.\\n\\nSo far, no observations of AAVE have indicated any difference from other\\ndialects in the use of the -ing suffix.[13] At first glance, it seems surprising that there is such a\\nhigh error rate in the reading errors: 61% of the words involving -ing show\\nerrors. But a second glance shows that most of these are inverse errors:\\n-ing being supplied where it did not exist in the original text. The\\nsignificance of such inverse errors for a suffix is quite different for the\\nsilent -e rule. Instead of showing a lack of knowledge, it indicates that\\nreaders are aware of the -ing suffix and are willing to supply it to\\nmake sense of the text. In order to explore this topic more deeply, we would\\nneed a different type of data, examining the reader\\'s efforts to interpret the\\nentire sentence. At the moment, it seems clear that the treatment of -ing\\nis comparable to the treatment of plural -s, both suffixes that are\\nwell known and recognized in AAVE.\\n\\n7.3. The past tense -ed. \\n\\nAs noted above, many studies of the simplification of past tense clusters show\\nthat AAVE speakers have the same variable behavior as in all other dialects of\\nEnglish. The second element of clusters ending in -t or -d is\\ndeleted, less often when the following word begins with a vowel and less often\\nwith stressed syllables. Most importantly for this section, the deletion occurs\\nless often when the -t or -d represents a separate morpheme,\\neither the regular past -ed or the participle -ed used with the\\npassive or the perfect. It is important to note that this has nothing to do\\nwith knowledge of the past tense, for the irregular verbs like told and\\ngave are used consistently for the past in the same way as other\\ndialects. The fact that this is a phonological rule is underlined even more\\nsharply by the fact that the /´d/ form used after apical stops is never\\ndeleted: the suffix is quite regular in wanted, interested, demanded.\\n\\n\\tAt the same time, it should be noted that the grammatical constraint is weaker\\nin AAVE than in other dialects, and experimental evidence shows that core\\nspeakers of AAVE are not able to use the information of the -ed suffix\\nin a written text to obtain past tense information.[14] The rate of errors shown in Table 6 and Figure 7 is very\\nhigh, comparable to the monomorphemic apical clusters in Table 5 and Figure 6.\\nThis similarity in error rates suggest that the readers are behaving as if the\\npast tense clusters do not contain any special information, consistent with the\\nexperimental results on semantic interpretation.\\n\\tBecause the realization of -ed is variable in speech, the absence of\\n-ed was never counted as an error when it was omitted except after\\napical consonants -t and -d. Thus tricked -> trick was\\nnot included as an error, since we must follow the basic principle of\\ndistinguishing (possible) differences in pronunciation from mistakes in\\nreading. The case of acted -> act is counted as an error, since this\\nfull syllable is never omitted in spontaneous speech. The error list does\\ninclude 7 cases from several readers where the -ed is pronounced as\\n[´d] after non-apical consonants: tricked -> /trik´d/, forced\\n-> /fohrk´d/, winked -> /wiNk´d/, tucked -> /tUk´d/,\\nwatched -> / wac&´d/ (twice), screeched ->\\n/skriyc&´d/, looked -> /luk´d/. In these cases, we can\\nconclude that the reader has not detected the past tense information, since the\\nsuffixes would never be pronounced in this way in speech.[15]\\n\\tThe high error rate for the -ed form, as compared with the plural,\\ntherefore reflects the high rate of variation in speech and the unreliability\\nof the printed signal for deriving past tense information for speakers of AAVE.\\n\\n7.4. The possessive -\\'s. \\n\\nEvery study of spontaneous speech in AAVE shows that the possessive suffix is\\nabsent in attributive position. Although it appears regularly in absolute\\nposition (This is hers, that is mines), it is close to 100% absent in\\nthe basilectal vernacular in such constructions as my brother house, the\\ndude old lady. Furthermore, the experiments of Torrey and Ball show that\\nsecond grade children cannot assign a semantic interpretation to -s ro\\ndistinguish pairs like the duck nurse vs. the duck\\'s nurse. There\\nare only three occurrences of the possessive -s in the reading errors,\\nall involving the form witch\\'s. As Table 6 shows, the -s does is\\nabsent from the reading in all three cases.\\n\\tIn spite of the limitations of the data set on grammatical affixes, the\\nover-all picture that emerges from Table 6 and Figure 7 is quite clear. The\\ninflections that are stable in AAVE show only a moderate percent of reading\\nerrors, including a high proportion of inverse errors. The significance of\\nthese inverse errors is the opposite of inverse errors in phonology, as in the\\nreading of CVC syllables with long vowels. Inverse grammatical errors indicate\\nthat the suffix is part of the inventory of forms that the reader is willing to\\nsupply in the effort to make sense of a sentence when other difficulties\\narise.\\n\\tOn the other hand, suffixes that do not have a stable position in the\\nunderlying grammar will show a very high rate of reading errors, and will not\\nbe supplied when they are not present in the text.\\n\\n8. Pedagogical implications.\\n\\nIn assessing the pedagogical implications of this research, the first step is\\nto sum up the strengths and weaknesses of the Woodruff 2nd and 3rd graders in\\ndecoding skills. One clear strength is in the accuracy of their recognition of\\nconsonants. For initial consonants, it is very high: at least 96%. In this\\nrespect, the goals of the initial phase of their phonics program can be said to\\nhave succeeded. This level of accuracy is less for the first vowel, but it is\\nstill moderately high, and even when a lone consonant is imbedded in the middle\\nof a word, it is recognized correctly 80% of the time. The next steps in\\nadvancing the level of reading can build on these skills.\\n\\tThe findings show that this level of accuracy contrasts sharply with a very\\ngreat degree of difficulty the Woodruff students have with any syllable\\nstructure other than CVC. Complexities in the consonantal onset are responsible\\nfor more than 30% of the errors in words read incorrectly, and when more than\\none consonant occurs , this rate jumps to 40, 50 and 80%, depending on the\\nstructures involved. When more than one vowel occurs in a word, this is\\nresponsible for the mistake in an even higher proportion of reading errors:\\nfrom 60 to 90%. It is startling to find that the most reliable of all the sound\\nto letter correspondences in the choice of long or short vowels--the silent\\ne rule--is not learned at all.\\n\\tWe can couple these findings with what we know about the teaching of phonics\\nin the Woodruff school. The most commonly used phonics book is the Steck-Vaughn\\nseries (York 1995). The approach to phonics is typical of many other phonics\\nbooks. Table 7 shows the distribution of topics, the ordering, and the amount\\nof effort devoted to each phonics area. It is clear that the lion\\'s share of\\nthe attention goes to the first, lone consonant and the first (short) vowel;\\n62% of the lessons are devoted to these topics. If we add in the lone consonant\\nafter the vowel, we find that 72% is devoted to the CVC structure. It is not\\nsimply the proportion of lessons that is involved here, but their ordering. It\\nis not until page 157 that long vowels are introduced.\\nTable 7. Topics of lessons for Year 1 of Steck-Vaughn Phonics Series.\\n\\n\\nPages\\n\\nLetter\\n\\nConsonants\\n\\n\\nVowels\\n\\n\\n\\n\\n\\nrecognition\\n\\nInitial\\n\\nMed/final\\n\\nBlends\\n\\nDigraphs\\n\\nShort\\n\\nLong\\n\\nY\\n\\n0-\\n\\n10\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n10-\\n\\n10\\n\\n\\n\\n\\n\\n\\n\\n\\n20-\\n\\n8\\n\\n2\\n\\n\\n\\n\\n\\n\\n\\n30-\\n\\n\\n7\\n\\n2\\n\\n\\n\\n2\\n\\n\\n\\n40-\\n\\n\\n4\\n\\n\\n\\n\\n8\\n\\n\\n\\n50-\\n\\n\\n10\\n\\n1\\n\\n\\n\\n2\\n\\n\\n\\n60-\\n\\n\\n4\\n\\n2\\n\\n\\n\\n7\\n\\n\\n\\n70-\\n\\n\\n6\\n\\n1\\n\\n\\n\\n6\\n\\n\\n\\n80-\\n\\n\\n5\\n\\n2\\n\\n\\n\\n7\\n\\n\\n\\n90-\\n\\n\\n4\\n\\n\\n\\n\\n10\\n\\n\\n\\n100-\\n\\n\\n8\\n\\n1\\n\\n\\n\\n1\\n\\n\\n\\n110-\\n\\n\\n6\\n\\n\\n\\n\\n10\\n\\n\\n\\n120-\\n\\n\\n5\\n\\n\\n\\n\\n6\\n\\n\\n\\n130-\\n\\n\\n4\\n\\n3\\n\\n\\n\\n4\\n\\n\\n\\n140-\\n\\n\\n4\\n\\n4\\n\\n\\n\\n6\\n\\n\\n\\n150-\\n\\n\\n\\n6\\n\\n\\n\\n5\\n\\n\\n\\n160-\\n\\n\\n\\n\\n\\n\\n\\n10\\n\\n\\n170-\\n\\n\\n\\n\\n\\n\\n\\n10\\n\\n\\n180-\\n\\n\\n\\n\\n\\n\\n\\n10\\n\\n\\n190-\\n\\n\\n\\n\\n\\n\\n\\n8\\n\\n2\\n\\n200-\\n\\n\\n\\n\\n2\\n\\n\\n\\n8\\n\\n6\\n\\n210-\\n\\n\\n\\n\\n10\\n\\n\\n\\n\\n\\n22-0\\n\\n\\n\\n\\n\\n10\\n\\n\\n\\n\\n230-\\n\\n\\n\\n\\n\\n8\\n\\n\\n\\n\\nTotal\\n\\n28\\n\\n69\\n\\n22\\n\\n12\\n\\n18\\n\\n74\\n\\n46\\n\\n8\\n\\n\\tGrammatical inflections are introduced at the end of the first year: three\\npages devoted to plurals, 5 to verbal -s and -ing. Discussion of\\nvowel digraphs or r-controlled vowels, diphthongs or shwa is relegated to the\\nsecond year. This is not an unreasonable procedure, if we follow the\\nhierarchical principle of beginning with the simplest objects and relations and\\nadding more complex ones only when they have been mastered. It is an open\\nquestion whether or not this level of success in identifying initial consonants\\ncould be achieved with less time and effort. The question to be considered here\\nis whether the grave defects in decoding more complex structures which persist\\neven into the fourth grade might be corrected by following a different\\nstrategy.\\n\\tAt present, we do not know how much time is devoted to the whole phonics\\nseries, either in individual or group sessions. But it is clear that no school\\nchild is likely to work through all of the lessons of the Steck-Vaughn or any\\nother series. The long vowels are not introduced until page 160 of the texts.\\nThere are 129 lessons in the first year; to keep pace would mean 3 or 4 lessons\\nevery week: the long vowels are not introduced until lesson 79, and one would\\nhave to guess from the end result that the children had not been exposed for\\nany length of time to this topic.[16]\\n\\tThe implications of this report suggest strategies that are radically\\ndifferent from those most widely used in the field. One general strategy is\\nobvious: that more time should be devoted to the \"rhyme\": the vowels and\\nconsonants which follow the consonantal onset, and especially to the consonants\\nat the ends of words. The lessons devoted to the initial lone consonant should\\nbe accompanied by lessons that consider the same consonant in final and perhaps\\nmedial position in the syllable. \\n\\tThe second general strategy that emerges from this report is that the CVC\\nsyllable should not dominate the introductory phonics lessons to the extent\\nthat it does. The high prevalence of inverse errors in the long/short vowel\\npattern shows that teaching CVC patterns in isolation does not lead to an\\nunderstanding of the basic relations involved. The fact that children learn to\\nread the word hop accurately does not mean that they understand when a\\ngiven spelling regularly indicates a short vowel, since in other occasions they\\nread hope as hop. Knowing that hop is /hap/ is not\\nsufficient; they must also know that hope cannot be /hap. By introducing\\nCVCe words earlier, in conjunction with CVC, one would allow the child to learn\\nnot only that bit is /bit/, and bite is /bayt/, but also\\nthat bit is not /bayt/ and that bite is not /bit/; that hope\\nis not /hap/. It is this contrastive relation which would solidify the\\nchild\\'s understanding of the CVC structure.\\n\\tThe same argument applies to vowel combinations. The more stable vowel\\ndigraphs, like OA and AI, may be introduced early to solidify the contrast\\nbetween cot and coat, rod and road, pan and pain, mad\\nand maid.\\n These are particular strategies. The general approach that seems\\nnecessary is to increase learners\\' awareness of the structure of words\\nvery soon after they have achieved accurate letter recognition. This can be\\ndone by many means, consistent with any explicit or implicit approach to\\nphonics that controls the reading vocabulary. We are developing and testing\\nmethods that will increase the accuracy of students\\' early perception of the\\nnumber of graphemes in the onset, a nucleus and coda, so that they are ready to\\napply decoding strategies to structures that are recognized as CVC, CCVC, CVVC,\\nand so on.\\n This research into methods accompanies the gathering a larger body of\\ndata to confirm or revise the findings presented here. The suggestions put\\nforward here are designed to contribute to the thinking of the many educators\\nwho are engaged in the long and difficult process of improving the teaching of\\nreading. \\n\\nReferences\\nAdams,\\nMarilyn Jager 1990. Beginning to Read: Thinking and Learning about\\nPrintCambridge, MA: MIT Press.\\nBaugh, John. 1983. Black Street Speech: its history, structure and\\nsurvival. Austin: University of Texas Press.\\nCalfee, Robert C. and Richard L. Venezky 1968. Component skills in beginning\\nreading. Technical Report No. 60. Madison, WI: Wisconsin Research and\\nDevelopment Center for Cognitive Learning, U. of Wisconsin.\\nClymer, T.. 1963. The utility of phonic generalizations in the primary grades.\\nReading Teacher16:252-258.\\nFasold, Ralph.. 1972. Tense Marking in Black English. Arlington, VA.:\\nCenter for Applied Linguistics.\\nGoodman, Kenneth S. 1969. Dialect barriers to reading comprehension. In J.\\nBaratz & R. W. Shuy (eds), Language Differences: Do they\\ninterfere?Newark: DE, International Reading Association. Pp. 14-28.\\nLabov, William 1995. Can reading failure be reversed: a linguistic approach to\\nthe question. In V. Gadsden and D. Wagner (eds.), Literacy Among\\nAfrican-American Youth: Issues in Learning, Teaching and\\nSchooling.Cresskill, NJ: Hampton Press. Pp. 39-68.\\nLabov, William, P. Cohen, C. Robins and J. Lewis.. 1968. A study of the\\nnon-standard English of Negro and Puerto Rican Speakers in New York City.\\nCooperative Research Report 3288. Vols I and II. Philadelphia: U.S. Regional\\nSurvey (Linguistics Laboratory, U. of Pa.)\\nLabov, William. 1972. Language in the Inner City. Philadelphia: Univ. of\\nPenn.\\nLabov, William. 1983. Recognizing Black English in the classroom. In J.\\nChambers (ed.), Black English: Educational Equity and theLaw. Ann Arbor:\\nKaroma Press. Pp. 29-55.\\nNAEP 1994. National Assessment of Educational Progress: Achievement of U.S.\\nStudenets in Science 1969-1992, Math 1973-1992, Reading 1971-1992, Writing\\n1984-1992. Washington: Department of Education.\\nTorrey, Jane. 1983. Black children\\'s knowledge of standard English. American\\nEducational Research Journal20:627-643.\\nVenezky, Richard L. 1972. Language and Cognition in Reading. Technical Report\\nNo. 188. Madison, WI: Wisconsin Research and Development Center for Cognitive\\nLearning, U. of Wisconsin.\\nWolfram, Walt. 1969. A Sociolinguistic Description of Detroit Negro\\nSpeech.Arlington, VA: Center for Applied Linguistics.\\nYork, Barbara 1995. Phonics: Book A. Austin, TX: Steck-Vaughn. \\n\\n[1] This report is the product of a seminar\\nheld in the spring of 1998, Linguistics/AFAM 161, \"The Sociolinguistics of\\nReading.\" The organization and planning of the Extended Day Program is the work\\nof the Woodruff staff and B. Baker; data on reading errors were collected by\\nBaker, Bullock, Ross and Brown. We gratefully acknowledge the support of the\\nCenter for Community Partnerships, directed by I. Harkavy, and the Kellogg\\nFoundation for the organization and development of this activity. We are\\nparticularly indebted to the staff of the Woodruff School.\\n[2] The books read included Come to My\\nIsland, Math Workbook, Martin Luther King, , History of Jazz, Spelling Words,\\nV-Tech game, Molly\\'s Monsters, Five Little Monkeys, Maya Angelou, Alexander and\\nthe Terrible, Horrible, NO Good, Very Bad Day, Lil\\'s Purple Plastic Purse, I\\nLike Me, Little Witch\\'s Big Night, The Grouchy Lady Bug, Magic School Bus at\\nthe Waterworks,  The Witch Baby, Kyla\\'s Big Day, Daydreamers. \\n[3] The main evidence for this fact is that the\\nclusters are realized in their full form much more frequently when the next\\nword begins with a vowel, and that there is little or no hypercorrection\\n(Labov, Cohen, Robins and Lewis 1968, Labov 1972, Baugh 1983).\\n[4] The onset of a syllable consists of\\nall the consonants that precede the first vowel. The nucleus consists of\\nthe vowel and glide that form the peak of sonority of the syllable. The\\ncoda is the consonant or consonants that follow the nucleus. Thus in\\ncat the onset is /c/, the nucleus is /æ/, and the coda is /t/.  In\\nstrengths, the onset is /str/, the nucleus is /e/, and the coda is\\n/NQs/.\\n[5] \\tA certain number of errors are to be\\nexpected for the rule for the softening of /c/ before front vowels /i/ and /e/.\\nHere again, the West Philadelphia children are not markedly different from the\\nWisconsin group who had just finished the second grade. The average per cent of\\nerrors for such words was 30%, with quartile scores ranging from 19 to 40%\\n(Venezky 1972: Figure 1). For those words read wrong in West Philadelphia which\\ninvolved a syllable initial c, one third did not apply this rule:\\nceiling -> killing, forced -> /fohrk´d/, ocean -> /okohrn/.\\n This included one inverse example, where softening was applied before an\\nunstressed vowel, spelled u:: circus -> /s´rsis/. On the\\nother hand, the softening rule was correctly applied in the majority of cases,\\neven when other errors were made: process -> /prowsiys/, voice ->\\nvice, ceiling -> /sæniy/, etc.\\n[6] In some clusters, the first element is a\\ndigraph, as in three or shrewd. In this case, the word is counted\\nin both the digraph category and the Cr/l category.\\n[7] Though the scope of this set of exceptions\\nis limited, its regularity was plainly observed by the student who read over\\nas /Uv´r/.\\n[8] In a city like New York, where the\\nsurrounding vernacular of the mainstream community shows uniform vocalization\\nof /r/, speakers of AAVE use 100% vocalized /r/ (Labov et al. 1968).\\n[9] The lone consonant here is assigned to the\\ncoda, but in many cases it actually occupies an \"ambisyllabic\" position,\\nsharing membership in the preceding and following consonant.\\n[10] When the cluster is complicated by an\\nadditional final /s/, as in wasps, desks, ghosts the rate of\\nsimplification is much higher, and for speakers of AAVE, it is categorical, as\\nindicated above.\\n[11] For most English dialects, absolute final\\nposition is the least likely place for simplification to occur. Guy 1981 shows\\nthat white speakers in New York City also have a high rate of simplification in\\nfinal position, but this is not typical of most mainstream dialects.\\n[12] The difference between the rate of errors\\nin the apical clusters and the non-apical clusters is significant with a\\nchi-square of 5.9.\\n[13] It has been observed that the constraint\\non using the progressive with stative verbs is weakened in AAVE.\\n[14] In reading a sentence like Last month,\\nI read the sign, subjects can transfer the past tense information in\\nlast month to derive the past tense pronunciation of read. But in\\nthe sentence, When I passed by, I read the sign, results were random.\\n[15] These are not to be identified with the\\nduplicated plurals that are commonly heard in speech, where /´d/ is added\\nafter the regular form, as in pickted, lookted.[16]\\n \\tThis situation can be generalized to the phonics programs as a whole. In\\n1963, Clymer examined the teachers\\' manuals of four widely used basal programs,\\nand extracted 121 phonics generalizations; of these, he found that 45 were\\nclearly stated in all four programs (1963; cited in Adams 1990). It is\\ngenerally assumed that no reading program can actually teach all of these\\nrelations, but that some initial exposure to the general principles will help\\nchildren to continue to abstract the patterns themselves as they continue to\\nread. The initial configuration of phonics teaching is therefore of great\\nimportance. From our examination of reading errors, some suggestions emerge\\nwhich might be helpful in reforming this initial configuration.\\n\\n\\n',\n",
       " ' 3.1.2 ChannelList The <channelList> element lists all data channels that the device is capable of reporting. Channels include: X coordinate (horizontal pen position, relative or absolute) Y coordinate (up/down or vertical pen position, relative or absolute) Z coordinate (height of pen above paper/digitizer, relative or absolute) Force (pen tip force) [NOTE: this is often referred to as \"pressure\" by manufacturers] Tip switch state (touching, not touching digitizer) Side switches and Buttons (for example, bezel buttons, cursor buttons...) Tilt angle in X dimension Tilt angle in Y dimension Pen Azimuth (alternative to tilt) Pen Elevation (alternative to tilt) Pen Rotation (around the pen axis) Syntax: <channelList id=\"foo\"> <channel name=\"X\"> ... </channel> </channelList> Attributes: id A unique identifier for this channelList element Examples: --> In addition, each channel may specify any of the following when known and appropriate: Value representation - for example, Boolean, integer, or decimal Range - the range of possible values that may be reported Threshold - (for binary channels) - e.g. the threshold force at which the tip switch is activated For continuous channels, like X, Y and Z, and Force, these additional characteristics may be specified: Resolution - the scale of the values recorded, expressed as \"fraction units\", e.g. \"1/1000 inch\") or \"decimal units\", e.g. \"0.1 mm\" or \"1 degrees\" Note that if decimal values are recorded, the quantization of the data may be smaller than the \"resolution\" Quantization - the unit of smallest change in the reported values. If the value is reported as integer, this is assumed to be the same as the resolution Noise - the RMS value of noise typically observed on the channel. This is distinct from accuracy! It is an indication of the difference observed in the data from the device when the same path is traced out multiple times (e.g. by a robot). Accuracy - the typical accuracy of the data on the channel (e.g. \"0.5 mm\", \"10 degrees\" or \"0.1 newton\") This is the typical difference between the reported position and the actual position of the pen tip (or tilt ...) Cross-coupling - the distortion in the data from one channel due to changes in another channel. For example, the X and Y coordinates in an electromagnetic digitizer are influenced by the tilt of the pen. This would be specified by dX/dTx = ... ??? or max delta X vs. Tx = ... ??? If the influencing channels are also recorded, and the cross-couplings are accurately specified, it may be possible to compensate for the cross-coupling by subtracting the influence, at the expense of higher noise. The cross-coupling is always expressed in the units of the two channels, e.g. if X mm and Tx is in degrees, then cross-coupling is in mm/deg Skew - the temporal skew of this channel relative to the basic device latency, if any. For example, some devices actually sample X and Y at different points in time, so one might have a skew of -5 msec, and the other +5 msec. Minimum bandwidth (in Hz) - the minimum bandwidth of the channel, in Hz (not samples/sec), i.e., the frequency of input motion up to which the signal is accurate to within 3dB. Peak rate - the maximum speed at which the device can accurately track motion Dynamic distortion, e.g., how velocity affects position accuracy. This is expressed in inverse seconds, e.g. 0.01 mm / mm / sec. This kind of distortion is often cross channel, but this spec only allows a generic, channel independent specification. Syntax: <channel name=\"X\"> <representation value=\"INTEGER\"/> <range min=\"0\" max=\"8191\"/> <threshold value=\"0.1\" units=\"newtons\"/> <resolution value=\"0.1\" units=\"mm\"/> <quantization value=\"0.01\" units=\"mm\"/> <noise value=\"0.05\" units=\"mm\"/> <accuracy value=\"0.5\" units=\"mm\"/> <crossCoupling otherChannel=\"Tx\" value=\"0.1\"/> <crossCoupling otherChannel=\"Ty\" value=\"0.01\"/> <skew value=\"2\" units=\"msec\"/> <minBandwidth value=\"15.0\"/> <distortion value=\".001\"/> </channel> Attributes: name The name of the channel described by this channel element Examples: <channel name=\"S\"> <representation value=\"BOOLEAN\"/> <threshold value=\"0.1\" units=\"newtons\"/> <skew value=\"5\" units=\"msec\"/> </channel> <channel name=\"X\"> <representation value=\"INTEGER\"/> <range min=\"0\" max=\"8191\"/> <resolution value=\"0.1\" units=\"mm\"/> <quantization value=\"0.01\" units=\"mm\"/> <noise value=\"0.05\" units=\"mm\"/> <accuracy value=\"0.5\" units=\"mm\"/> <crossCoupling otherChannel=\"Tx\" value=\"0.1\"/> <crossCoupling otherChannel=\"Ty\" value=\"0.01\"/> <skew value=\"2\" units=\"msec\"/> <minBandwidth value=\"15.0\"/> <distortion value=\".001\"/> </channel> 3.1.4 Error Calculations This Error Calculations section is informative. The following are some suggestions for how error estimates might be derived from the basic fidelity information in a spatial channel (x or y): Total position error is the sum of {absolute accuracy + velocity*(dynamic distortion) + noise + quantization error} for identical path (in all channels). Repeatability is also the sum of {noise + quantization error} for a repeated, identical physical trajectory across the digitizer. Relative position error is the minimum of {linearity*delta, absolute accuracy). This effects the ability to accurately measure the length and orientation of a short stroke. Maximum error including skew (by assuming that all channels are in sync) is equal to the sum of {absolute accuracy + velocity*dynamic distortion + cross-coupling + velocity*(skew) + noise + quantization error}. All errors are subject to additional distortion from a signal exceeding the channel bandwidth. Open Issues The attribute for identifying the capture device info block has not been incorporated into the Context section. There should be a \"time\" channel. We recently noticed that it is missing, and it will be incorporated in the next draft. There have been last minute additions to try to flesh out the syntax and examples. These are preliminary, and may be changed. 3.2 Brushes Along with trace data, it is often necessary to record certain attributes of the pen during ink capture. For example, in a notetaking application, it is important to be able to distinguish between traces captured while writing as opposed to those which represent erasures. Because these attributes will often be application specific, this specification does not attempt to enumerate the brush attributes which can be associated with a trace. It also does not provide a language for describing brush attributes, since it is possible to imagine attributes which are described using complex functions parameterized by time, pressure, or other factors. Instead, the specification allows for capturing the fact that a given trace was recorded in a particular brush context, leaving the details of precisely specifying that context to a higher-level, application specific layer. Depending on the application, brush attributes may change frequently. Accordingly, there should be a concise mechanism to assign the attributes for an individual trace. On the other hand, it is likely that many traces will be recorded using the same sets of attributes; therefore, it should not be necessary to explicitly state the attributes of every trace (again, for reasons of conciseness). Furthermore, it should be possible to define entities which encompass these attribute sets and refer to them rather than listing the entire set each time. Since many attribute sets will be similar to one another, it should also be possible to inherit attributes from a prior set while overriding some of the attributes in the set. In the ink markup, brush attributes are described by the <brush> element. This element allows for the definition of reusable sets of brush attributes which may be associated with traces. For reference purposes, a brush specifies an identifier which can be used to refer to the brush. A brush can inherit the attributes of another <brush> element by including a brushRef attribute which contains the referenced brush\\'s id. Brush attributes are associated with traces using the brushRef attribute. When it appears as an attribute of an individual <trace> , the brushRef specifies the brush attributes for that trace. When it appears as an attribute of a <traceGroup> element, the brushRef specifies the common brush attributes for all traces enclosed in the <traceGroup> . Within the <traceGroup> , an individual trace may still override the traceGroup\\'s brush attributes using a brushRef attribute. Brush attributes can also be associated with a context by including the brushRef attribute on a <context> element. Any traces which reference the context using a contextRef attribute are assigned the brush attributes defined by the context. If a trace includes both brushRef and contextRef attributes, the brushRef overrides any brush attributes given by the contextRef. In streaming ink markup, brushes are assigned to a trace according to the current brush, which can be set using the <context> and <brush> elements. See section 4.2 for a detailed description of streaming mode. 3.3 Context This section describes the <context> element and its attributes: canvas , mapping traceFormatRef , and brushRef . The context element both defines the shared context (canvas) and serves as a convenient agglomeration of contextual attributes. It is used by the <traceGroup> (Section 2.3) element to define the complete shared context of a group of traces or may be referred to as part of a context change in streaming mode. In either mode, individual attributes may be overridden at time of use. Additionally, individual traces may refer to a previously defined context (again optionally overriding its attributes) to describe a context change that persists only for the duration of that trace. Although the use of the <context> element and attributes is strongly encouraged, default interpretations are provided so that they are not required in an ink markup file if all trace data is recorded in the same virtual coordinate system, and its relationship to digitizer coordinates is either not needed or unknown. A shared context, called a canvas , is needed for the ink markup to support screen sharing amongst multiple devices, each of which might have a different set of capture characteristics. For example, a single ink markup stream or file may contain traces that are captured on a tablet computer, a PDA device, and an opaque graphics tablet attached to a desktop computer. The size of these traces on each capture device and corresponding display might differ, yet it may be necessary to relate these traces to one another. They could represent scribbles on a shared electronic whiteboard, annotations of a common document, or the markings of two players in a distributed tic-tac-toe game. The trace data for these different ink sessions could be recorded using the same set of virtual coordinates; however, it is often useful and occasionally may even be necessary to record the data in the capture device coordinates, in order to more precisely represent the original capture conditions, for compactness, or to avoid round-off errors that might be associated with the use of a common coordinate system. Thus the mapping ; (section 3.3.2) from trace coordinates to the shared canvas coordinates may vary from device to device. The <traceFormat> (Section 2.1) used to record trace data may also vary, therefore the <context> element also contains a traceFormatRef attribute. Finally, the <context> element provides a brushRef attribute to record the attributes of the pen during the capture of the digital ink, for a particular context. 3.3.1 canvas Attribute In order to render data from a participant in a multi-party ink app, it is necessary to know how to transform trace data to screen coordinates. Each party may have a different coordinate system for their traces. Each party will need a mapping to their display that allows scrolling and zooming. Call this S[k] . Party k still needs to determine the meaning of the traces from party i . This is most simply accomplished by having each party define the relationship between their trace coordinate system, and an arbitrary reference coordinate system. This virtual coordinate system does not have any physical dimensions, because each party will render it differently, and each person will draw onto it differently, with arbitrary zoom and scrolling. Thus the virtual coordinate system is arbitrary. This virtual coordinate system is provided by the canvas, declared via the canvas attribute. This uniquely identifies a shared virtual coordinate system for cooperating ink applications. Together with the trace-to-canvas coordinate mapping (discussed below), it provides a common frame of reference for ink collected in multiple sessions on different devices. In the example above, trace data collected from the tablet computer can be combined with trace data collected from the PDA by specifying a common canvas and describing the relationships between each device\\'s trace data and the common canvas coordinate system. In the ink markup, the canvas is an unbounded space oriented so that x and y coordinates increase as one moves to the right and down, respectively. Specifying a standard handedness for the canvas coordinate system allows each device to orient and display ink from every other device. Canvas Math To collaborate in the multi-party ink exchange, party k needs to know the orientation and handedness of the virtual coordinate system (in order to determine their own local S[k] ), and the mapping of each other party\\'s data to that virtual coordinate system. Call these mappings T[i] To map from trace coordinates to screen coordinates, we compose the transform from party i to virtual space with my transform from virtual space to screen space, S[k] . This is M = T * S . This matrix is used to transform all points from that traceGroup. When the display is zoomed or scrolled, S[k] changes, and M is recomputed. When a new traceGroup with a different T[i] is encountered, it is composed with S[k] , and rendering continues. The S[k] matrix is not part of the inkML file, but is determined locally during capture or rendering. T and S are the minimum necessary information to be able to render some data. However, in order to determine S or T, it is also necessary to make a decision about the orientation of the virtual space. If everyone makes this determination independently, there is no common virtual space. Consequently, the virtual space, or canvas is defined to have a specific orientation. The orientation of this canvas does not effect anyone, as it disappears when T and S are composed. It simply provides a common intermediate space that everyone uses when computing T (which goes into the xml) and S (which is used only to display the data). The default canvas Since a canvas identifier is a simple string, the id of the default canvas is defined to be \"default\". This is sufficient to allow simple single-canvas sharing without further action on the part of devices or applications. 3.3.2 mapping Attribute The trace-to-canvas coordinate system mapping, declared via the mapping attribute, defines the transformation from trace coordinates to the shared canvas coordinate system. The trace-to-canvas coordinate system mapping is expressed as a standard 2x3 2D transformation matrix (at this time, we ignore the additional complication of nonlinearity in the digitizing device\\'s coordinate system). The default mapping is the identity matrix (with a zero offset). The format of the trace data--both the mapping from digitizer to trace coordinates and the channels and channel formats present in the data--for a given context is specified via the traceFormatRef attribute, which refers to a <traceFormat> element (Section 3.x). Note: As it is primarily intended as an input specification, the ink markup language does not provide a mechanism for representing the transformations to screen or view coordinates, which relate to ink display and are typically transient. 3.3.3 traceFormatRef Attribute The trace format to associate with the context being defined is specified with a traceFormatRef attribute, which refers to a <traceFormat> element (Section 2.1). 3.3.4 brushRef Attribute The brush to associate with the context being defined is specified with a brushRef attribute, which refers to a <brush> element (Section 3.3). 3.3.5 Context The <context> element consolidates all salient characteristics of one or more ink traces. It may be specified by declaring all non-default attributes, or by referring to a previously defined context and overriding specific attributes. Syntax: <context id=\"\" contextRef=\"\" canvas=\"\" mapping=\"\" traceFormatRef=\"\" brushRef=\"\"/> Attributes: id A unique identifier for this context. contextRef A previously defined context upon which this context is to be based. canvas The unique identifier of the canvas for this context. mapping The standard 2x3 matrix representation of the transformation from the trace data coordinates to the canvas; expressed as the six values of the transformation matrix in row order xx xy x0 yx yy y0 . traceFormatRef A reference to the traceFormat for this context. brushRef A reference to the brush for this context. Examples: <context id=\"context1\" canvas=\"canvas1\" traceFormatRef=\"format1\" brushRef=\"brush1\"/> <context id=\"context2\" contextRef=\"context1\" brushRef=\"brush2\"/> <context id=\"context3\" canvas=\"canvas1\" mapping=\"2 0 0 0 2 0\" traceFormatRef=\"format2\" brushRef=\"brush3\"/> The first example is a hypothetical device #1, using a previously defined format1 and brush1, and indicating that it can share trace data using canvas1. Its trace coordinates are mapped to this shared canvas using the default identity matrix with zero offset. The second example is the same device #1, using a different brush: brush2. The third example is a hypothetical device #2, using previously defined format2 and brush3, and sharing trace data with the first device by using the common canvas1. Its trace coordinates require a scale factor of 2 to map to the canvas. 3.4 Defs The <defs> element is a container which is used to define reusable content. The definitions within a <defs> block can be referenced by other elements using the appropriate syntax. Content within a <defs> has no impact on the interpretation of traces, unless referenced from outside the <defs> . In order to allow them to be referenced, elements within a <defs> block must include an id ; attribute. Therefore, an element which is defined inside a <defs> without an id , or that is never referenced, serves no purpose. The three elements which can be defined inside a <defs> are: <context> , <brush> and <traceFormat> . The attributes which are used to reference these definitions are the associated contextRef , brushRef and traceFormatRef attributes. The following simple example illustrates usage of the <defs> element. <ink> <defs> <brush id=\"redPen\"/> <brush id=\"bluePen\"/> <traceFormat id=\"normal\"/> <traceFormat id=\"noForce\"/> <context id=\"context1\" brushRef=\"redPen\" traceFormatRef=\"normal\"/> <context id=\"context2\" contextRef=\"context1\" brushRef=\"bluePen\"/> </defs> <context contextRef=\"context2\" traceFormatRef=\"noForce\"/> <context id=\"context3\"/> </ink> More details on the usage of the <defs> element are provided in section 4. 4 Streams and Archives The ink markup is expected to be utilized in many different scenarios. Ink markup data may be transmitted in substantially real time while exchanging ink messages, or ink documents may be archived for later retrieval or processing. These examples illustrate two different styles of ink generation and usage. In the former, the markup must facilitate the incremental transmission of a stream of ink data, while in the latter, the markup should provide the structure necessary for operations such as search and interpretation. In order to support both cases, InkML provides archival and streaming modes of usage. 4.1 Archival Applications In archival usage, contextual elements are defined within a <defs> element and assigned identifiers using the id attribute. References to defined elements are made using the corresponding brushRef , traceFormatRef , and contextRef attributes. The following example: <defs> <brush id=\"penA\"/> <brush id=\"penB\"/> <traceFormat id=\"fmt1\"> <regularChannels> <channel name=\"X\" type=\"integer\"> <channel name=\"Y\" type=\"integer\"> <channel name=\"Z\" type=\"integer\"> </regularChannles> </traceFormat> <context id=\"context1\" canvas=\"canvasA\" mapping=\"1 0 0 0 1 0\" traceFormatRef=\"fmt1\" brushRef=\"penA\"/> <context id=\"context2\" canvas=\"canvasA\" mapping=\"2 0 0 0 2 0\" traceFormatRef=\"fmt1\" brushRef=\"penB\"/> </defs> defines two brushes (\"penA\" and \"penB\"), a traceFormat (\"fmt1\"), and two contexts (\"context1\" and \"context2\") which both refer to the same canvas (\"canvasA\") and traceFormat (\"fmt1\"), but with different mappings and brushes. Note the use of the brushRef and traceFormatRef attributes to refer to the previously defined <brush> and <traceFormat> . Within the scope of a <defs> element, unspecified attributes of a <context> element are assumed to have their default values. This <defs> block: <defs> <brush id=\"penA\"> <context id=\"context1\" canvas=\"canvasA\" brushRef=\"penA\"/> </defs> defines \"context1\", which is comprised of \"canvasA\" with the default mapping and traceFormat (the identity mapping and a traceFormat consisting of decimal X-Y coordinate pairs), and \"penA\". A <context> element can inherit and override the values of a previously defined context by including a contextRef attribute, so: <defs> <brush id=\"penA\"/> <context id=\"context1\" canvas=\"canvasA\" mapping=\"1 0 0 0 1 0\"/> <context id=\"context2\" contextRef=\"context1\" mapping=\"2 0 0 0 2 0\" brushRef=\"penA\"/> </defs> defines \"context2\" which shares the same canvas (\"canvasA\") and traceFormat (the default format) as \"context1\", but has a different mapping and brush. Within archival ink markup, traces can either explicitly specify their context through the use of contextRef and brushRef attributes, or they can have their context provided by an enclosing traceGroup. In the following: <trace id=\"t001\" contextRef=\"context1\"/>...</trace> <trace id=\"t002\" brushRef=\"penA\"/>...</trace> <traceGroup contextRef=\"context1\"> <trace id=\"t003\">...</trace> </traceGroup> traces \"t001\" and \"t003\" have the context defined by \"context1\", while trace \"t002\" has a context consisting of the default canvas, mapping and traceFormat, and \"penA\". Traces within a <traceGroup> element can also override the context or brush specified by the traceGroup. In this example: <traceGroup contextRef=\"context1\"> <trace id=\"t001\">...</trace> <trace id=\"t002\" brushRef=\"penA\">...</trace> <trace id=\"t003\">...</trace> </traceGroup> traces \"t001\" and \"t003\" have their context specified by \"context1\" while trace \"t002\" overrides the default brush of \"context1\" with \"penA\". A trace or traceGroup can both reference a context and override its brush, as in the following: <trace id=\"t001\" contextRef=\"context1\" brushRef=\"penA\">...</trace> <traceGroup contextRef=\"context1\" brushRef=\"penA\"> <trace id=\"t002\">...</trace> </traceGroup> which assigns the context specified by \"context1\" to traces \"t001\" and \"t002\", but with \"penA\" instead of the default brush. In archival mode, the ink markup processor can straightforwardly determine the context for a given trace by examining only the <defs> blocks within the markup and the enclosing traceGroup for the trace. 4.2 Streaming Applications In streaming ink markup, changes to trace context are expressed directly using the <brush> , <traceFormat> , and <context> elements. This corresponds to an event-driven model of ink generation, where events which result in contextual changes map directly to elements in the markup. In the streaming case, the current context consists of the set of canvas, mapping, traceFormat and brush which are associated with subsequent traces in the ink markup. Initially, the current context contains the default canvas, an identity mapping, the default traceFormat, and a brush with no attributes. Each <brush> , <traceFormat> , and <context> element which appears outside of a <defs> element changes the current context accordingly (elements appearing within a <defs> block have no effect on the current context, and behave as described above in the archival section). The appearance of a <brush> element in the ink markup sets the current brush attributes, leaving all other contextual values the same. Likewise, the appearance of a <traceFormat> element sets the current traceFormat, and the appearance of a <context> element sets the current context. Outside of a <defs> block, any values which are not specified within a <context> element are taken from the current context. For instance, the <context> element in the following example changes the current brush from \"penB\" to \"penA\", leaving the canvas, mapping, and traceFormat unchanged from trace \"t001\" to trace \"t002\". <brush id=\"penA\"/> <brush id=\"penB\"/> <trace id=\"t001\">...</trace> <context brushRef=\"penA\"/> <trace id=\"t002\">...</trace> In order to change a contextual value back to its default value, its attribute can be specified with the value \"\". In the following: <context canvas=\"canvasA\" brushRef=\"penA\"/> <trace id=\"t001\">...</trace> <context canvas=\"\" brushRef=\"\"/> <trace id=\"t002\">...</trace> trace \"t001\" is on \"canvasA\" and has the brush specified by \"penA\", while trace \"t002\" is on the default canvas and has the default brush. Brushes, traceFormats, and contexts which appear outside of a <defs> block and contain an id attribute both set the current context and define contextual elements which can be reused (as shown above for the brushes \"penA\" and \"penB\"). This example: <context id=\"context1\" canvas=\"canvasA\" mapping=\"2 0 0 0 2 0\" traceFormatRef=\"fmt1\" brushRef=\"penA\"/> defines a context which can be referred to by its identifier \"context1\". It also sets the current context to the values specified in the <context> element. A previously defined context is referenced using the contextRef attribute of the <context> element. For example: <context contextRef=\"context1\"/> sets the current context to have the values specified by \"context1\". A <context> element can also override values of a previously defined context by including both a contextRef attribute and canvas , mapping , traceFormatRef or brushRef attributes. The following: <context contextRef=\"context1\" brushRef=\"penB\"/> sets the current context to the values specified by \"context1\", except that the current brush is set to \"penB\" instead of \"penA\". A <context> element which inherits and overrides values from a previous context can itself be reused, so the element: <context id=\"context2\" contextRef=\"context1\" brushRef=\"penB\"/> defines \"context2\" which has the same context values as \"context1\" except for the brush. Finally, a <context> element with only an id has the effect of taking a \"snapshot\" of the current context which can then be reused. The element: <context id=\"context3\"/> defines \"context3\", whose values consist of the current canvas, mapping, traceFormat, and brush at the point where the element occurs (note that since \"context3\" does not specify any values, the element has no effect on the current context). An advantage of the streaming style is that it is easier to express overlapping changes to the individual elements of the context. However, determining the context for a particular trace can require more computation from the ink markup processor, since the entire file may need to be scanned from the beginning in order to establish the current context at the point of the <trace> element. 4.3 Archival and Streaming Equivalence The following examples of archival and streaming ink markup data are equivalent, but they highlight the differences between the two styles: Archival <ink> ... <defs> <brush id=\"penA\"/> <brush id=\"penB\"/> <context id=\"context1\" canvas=\"canvas1\" mapping=\"1 0 0 0 1 0\" traceFormatRef=\"format1\"/> <context id=\"context2\" contextRef=\"context1\" mapping=\"2 0 50 0 2 50\"/> </defs> <traceGroup contextRef=\"context1\"> <trace>...</trace> ... </traceGroup> <traceGroup contextRef=\"context2\"> <trace>...</trace> ... </traceGroup> <traceGroup contextRef=\"context2\" brushRef=\"penB\"> <trace>...</trace> ... </traceGroup> <traceGroup contextRef=\"context1\" brushRef=\"penB\"> <trace>...</trace> ... </traceGroup> <traceGroup contextRef=\"context1\" brushRef=\"penA\"> <trace>...</trace> ... </traceGroup> </ink> Streaming <ink> ... <defs> <brush id=\"penA\"/> <brush id=\"penB\"/> </defs> <context id=\"context1\" canvas=\"canvas1\" mapping=\"1 0 0 0 1 0\" traceFormatRef=\"format1\"/> <trace>...</trace> ... <context id=\"context2\" contextRef=\"context1\" mapping=\"2 0 50 0 2 50\"/> <trace>...</trace> ... <context brushRef=\"penB\"/> <trace>...</trace> ... <context contextRef=\"context1\"/> <trace>...</trace> ... <context brushRef=\"penA\"/> <trace>...</trace> ... </ink> In the archival case, the context for each trace is simply determined by the <trace> element, its enclosing traceGroup, and contextual elements defined in the <defs> block, while in the streaming case, the context for a trace can depend on the entire sequence of context changes up to the point of the <trace> element. However, the streaming case more simply expresses the changes of context involving \"penB\", \"context1\", and \"penA\", whereas the archival case requires the restatement of the unchanged values in the successive traceGroups. The two styles of ink markup are equally expressive, but impose different requirements on the ink markup processor and generator. The working group is considering the usefulness of additional mechanisms for distinguishing between the two forms, such as separate profiles for archival and streaming ink markup. Tools to translate from streaming to archival style might also be of use to applications which work on stored ink markup. 5 Semantic Labelling and traceRefGroup The <traceRefGroup> element provides the basis for most semantic labelling of groups of traces. It should be used as the base class for all application specific elements that identify collections of traces. The <traceRefGroup> element has the following syntax: <traceRefGroup id=\"\" contentCategory=\"\"> <traceref xpath=\"\"> <traceref xpath=\"\" from=\"\" to=\"\"> <traceRefGroup id=\"\"> <!-- a nested traceRefGroup, which has attributes of all parent traceRefGroups --> ... </traceRefGroup> </traceRefGroup> Traces listed within a <traceRefGroup> are included by reference only. The xpath attribute of the <traceRef> element is used to refer to traces within the current document, or from external documents. The from and to attributes can be used to reference a (contiguous) subset of the points within a given trace. <traceRefGroup> elements may also include other <traceRefGroup> elements by reference. A <traceRefGroup> element may be overlapping, i.e., a trace may be referenced in multiple groups. <traceRefGroup> elements will typically be used either to tag a group of traces for further processing, to tag a group of traces with some metadata, or to provide a concise reference to a group of traces for external use. Open Issues TODO: we intend to add a paragraph with more detail about using XPATH to identify groups of traces. 5.1 contentCategory attribute One of the common attributes of <traceRefGroup> will be contentCategory , which describes at a basic level the category of content that the traces represent; e.g., \"Text/English\", \"Drawing\", \"Math\", \"Music\". Such categories are useful for general data identification purposes, and may be essential for selecting data to train handwriting recognizers in different problem domains. A number of likely, common categories are suggested below. However, since this attribute: is largely application-specific may take on values that are difficult or impossible to predict may be a conjunction of more than one primitive type (e.g., \"Text/English and Graphics\") it is defined as a general-purpose string, to be used as necessary by applications. If, however, the data fits conveniently into one of the following basic categories, it is recommended that the appropriate suggested category (and optional sub-category) be used. Suggested categories: Text/<language>[/<script>][/<sub-category>] (e.g., Text/jpn/Kanji, Text/en/SSN) Drawing[/<sub-category>] (e.g., Drawing/Sketch, Drawing/Diagram) Math Music Chemistry[<sub-category>] The language specification may be made using any of the language identifiers specified in ISO 639 , using 2-letter codes, 3-letter codes, or country names. Some text may also require a script specification (such as Kanji, Katakana, or Hiragana) in addition to the language. For some applications it may be useful to provide additional sub-categories defining the type of the data. Suggested sub-categories for Text: SSN (Social Security Number) Phone Date Time Money URL Suggested sub-categories for Drawing: Sketch (Not suitable for geometric clean-up) Diagram (Suitable for geometric clean-up) \\n\\t\\n\\n-->\\n\\n\\n\\n\\n\\nInk Markup Language\\n\\n\\n\\n\\n\\n\\nInk Markup Language\\nW3C Working Draft 6 August 2003\\n\\nThis version:\\n\\nhttp://www.w3.org/TR/2003/WD-InkML-20030806\\nLatest version:\\n\\nhttp://www.w3.org/TR/InkML\\nPrevious version:\\nThis is the first Working Draft\\nAuthors:\\nGregory Russell, IBM (editor)\\nYi-Min Chee, IBM\\nGiovanni Seni, Motorola\\nLarry Yaeger, Apple\\nChristopher Tremblay, Corel\\nKatrin Franke, Fraunhofer Gesellschaft\\nSriganesh Madhvanath, HP\\nMax Froumentin, W3C\\n\\n\\nCopyright\\n© 2003\\n\\nW3C®\\n(\\nMIT,\\n\\nERCIM, Keio),\\nAll Rights Reserved. W3C\\n\\nliability,\\n\\ntrademark,\\ndocument\\nuse and\\nsoftware\\nlicensing rules apply.\\n\\n\\n\\nAbstract\\nThis document describes the syntax and semantics for the Ink\\nMarkup Language for use in the\\nW3C Multimodal\\nInteraction Framework as proposed by the\\nW3C Multimodal Interaction\\nActivity. The Ink Markup Language serves as the data format\\nfor representing ink entered with an electronic pen or stylus.\\nThe markup allows for the input and processing of handwriting,\\ngestures, sketches, music and other notational languages in\\nWeb-based  applications. It provides a common format for the exchange of ink data between components such as handwriting and gesture recognizers, signature verifiers, and other ink-aware modules.\\n\\nStatus of this document\\nThis section describes the status of this document at the time\\nof its publication. Other documents may supersede this document. A\\nlist of current W3C publications and the latest revision of this\\ntechnical report can be found in the W3C technical reports index at\\nhttp://www.w3.org/TR/.\\nThis document is a public W3C Working Draft for review by W3C\\nmembers and other interested parties. Publication as a Working Draft\\ndoes not imply endorsement by the W3C Membership. This is a draft\\ndocument and may be updated, replaced or obsoleted by other documents\\nat any time. It is inappropriate to cite this document as other than\\n\\'work in progress.\\nThis specification describes the syntax and semantics for ink\\nmarkup, as a basis for a common format for the exchange of ink data\\nbetween components such as handwriting and gesture recognizers,\\nsignature verifiers, and other ink-aware modules.\\nThis document has been produced as part of the\\nW3C Multimodal Interaction\\nActivity,\\nfollowing\\nthe procedures set out for the\\nW3C Process.\\nThe authors of this document are members of the\\nMultimodal Interaction\\nWorking Group\\n(W3C\\nMembers only).\\nPatent disclosures relevant to this specification may be found\\non the Working Group\\'s\\npatent disclosure\\npage in conformance with W3C policy.\\nThis document is for public review, and comments and discussion are\\nwelcomed on the (archived)\\npublic mailing list <www-multimodal@w3.org>.\\n\\n\\n\\n\\nTable of contents\\n\\n i.  Abstract              \\n ii. Document Status       \\n1 Overview              \\n2 Traces and Trace Formatting \\n\\n2.1 Trace Formats  \\n2.2 Traces     \\n2.3 Trace Groups \\n\\n\\n3    The Context Elements \\n\\n3.1  Capture Device\\n3.2  Brushes         \\n3.3  context element\\n3.4  defs element\\n\\n\\n4 Streaming Applications and Archives\\n\\n4.1 Archival Apps\\n4.2 Streaming Apps\\n4.3 Archival and Streaming Equivalence\\n\\n5 Semantic Labelling and traceRefGroup\\n\\n\\n\\n1 Overview\\nAs more electronic devices with pen interfaces have and continue to become \\navailable for entering and manipulating information, applications need to be \\nmore effective at leveraging this method of input. Handwriting is an input modality that \\nis very familiar for most users since everyone learns to write in school. Hence, \\nusers will tend to use this as a mode of input and control when available.\\nA pen-based interface consists of a transducer device and a pen so that \\nthe movement of the pen is captured as digital ink. Digital ink can be passed \\non to recognition software that will convert the pen input into appropriate \\ncomputer actions. Alternatively, the handwritten input can be organized into ink \\ndocuments, notes or messages that can be stored for later retrieval or exchanged \\nthrough telecommunications means. Such ink documents are appealing because they \\ncapture information as the user composed it, including text in any mix of \\nlanguages and drawings such as equations and graphs.\\nHardware and software vendors have typically stored and represented digital \\nink using proprietary or restrictive formats. The lack of a public and \\ncomprehensive digital ink format has severely limited the capture, transmission, \\nprocessing, and presentation of digital ink across heterogeneous devices \\ndeveloped by multiple vendors. In response to this need, the  Ink Markup \\nLanguage (InkML) provides a simple and platform-neutral data format to promote \\nthe interchange of digital ink between software applications.\\nInkML supports a complete and accurate representation of hand-drawn ink. \\nFor instance, in addition to the pen position over time, InkML allows \\nrecording of information about transducer device characteristics and \\ndetailed dynamic behavior to support applications such as \\nhandwriting recognition and authentication. For example, there is support \\nfor recording additional channels such as pen tilt, or pen tip \\nforce (commonly referred to as pressure in manufacturers\\' documentation).\\nInkML provides means for extension. By virtue of being an XML-based\\nlanguage, users may easily add application-specific information to ink\\nfiles to suit the needs of the application at hand.\\nNote: A media type will be registered for InkML instances.\\nIt is expected that this media type will be application/inkml+xml as\\nrecommended by RFC3023.\\nThis specification was developed to\\nfulfill the \\nW3C requirements for the Ink Markup Language.\\n\\nOpen Issues\\nThe question of whether this specification will use the term\\n\"pressure\" or \"force\" has not been decided yet. The Working Group\\nwelcomes feedback from the public on this issue.\\n\\n1.1 Uses of InkML\\nWith the establishment of a non-proprietary ink standard, a number of \\napplications, old and new, are expanded where the pen can be used as a very \\nconvenient and natural form of input. Here are a few examples.\\n\\nInk Messaging\\nTwo-way transmission of digital ink, possibly wireless, offers mobile-device \\nusers a compelling new way to communicate. Users can draw or write with a pen\\non the device\\'s screen to compose a note in their own handwriting. Such an ink \\nnote can then be addressed and delivered to other mobile users, desktop users, \\nor fax machines. The recipient views the message as the sender composed it, \\nincluding text in any mix of languages and drawings.\\n\\nInk and SMIL\\nA photo taken with a digital camera can be annotated with a pen; the digital \\nink can be coordinated with a spoken commentary. The ink annotation could be \\nused for indexing the photo (for example, one could assign different handwritten \\nglyphs to different categories of pictures).\\n\\nInk Archiving and Retrieval\\nA software application may allow users to archive handwritten notes and \\nretrieve them using either the time of creation of the handwritten notes or the \\ntags associated with keywords. The tags are typically text strings created using \\na handwriting recognition system.\\n\\nElectronic Form-Filling\\nIn support of natural and robust data entry for electronic forms on a wide-\\nspectrum of keyboardless devices, a handwriting recognition engine developer \\nmay define an API that takes InkML as input.\\n\\nPen Input and Multimodal Systems\\nRobust and flexible user interfaces can be created that integrate the pen with \\nother input modalities such as speech. Higher robustness is achievable because \\ncross-modal redundancy can be used to compensate for imperfect recognition on \\neach individual mode. Higher flexibility is possible because users can choose \\nthe most appropriate from among various modes for achieving a task or issuing \\ncommands. This choice might be based on user preferences, suitability for the \\ntask, or external conditions. For instance, when noise in the environment or \\nprivacy is a concern, the pen modality is preferred over voice.\\n\\n\\n1.2 Elements\\nThe current InkML specification defines a set of primitive elements\\nsufficient for all basic ink applications. Few semantics are attached\\nto these elements. All content of an InkML document is contained\\nwithin a single <ink> element.  The fundamental\\ndata element in an InkML file is the <trace>. A\\ntrace represents a sequence of contiguous ink points -- e.g., the X\\nand Y coordinates of the pen\\'s position. A sequence of traces\\naccumulates to meaningful units, such as characters and words. The\\n<traceFormat> element is used to define the format\\nof data within a trace. \\nIn its simplest form, an InkML file with its enclosed traces looks like this:\\n\\n<ink>\\n  <trace>\\n    10 0 9 14 8 28 7 42 6 56 6 70 8 84 8 98 8 112 9 126 10 140\\n    13 154 14 168 17 182 18 188 23 174 30 160 38 147 49 135\\n    58 124 72 121 77 135 80 149 82 163 84 177 87 191 93 205\\n  </trace>\\n  <trace>\\n    130 155 144 159 158 160 170 154 179 143 179 129 166 125\\n    152 128 140 136 131 149 126 163 124 177 128 190 137 200\\n    150 208 163 210 178 208 192 201 205 192 214 180\\n  </trace>\\n  <trace>\\n    227 50 226 64 225 78 227 92 228 106 228 120 229 134\\n    230 148 234 162 235 176 238 190 241 204\\n  </trace>\\n  <trace>\\n    282 45 281 59 284 73 285 87 287 101 288 115 290 129\\n    291 143 294 157 294 171 294 185 296 199 300 213\\n  </trace>\\n  <trace>\\n    366 130 359 143 354 157 349 171 352 185 359 197\\n    371 204 385 205 398 202 408 191 413 177 413 163\\n    405 150 392 143 378 141 365 150\\n  </trace>\\n</ink>\\n\\nThese traces consist simply of alternating X and Y values, and may look like this when rendered:\\n\\nFigure 1: example trace rendering\\nFigure 1 shows a trace of a sampled handwriting signal\\nrepresenting. The dots mark the sampling positions which were\\ninterpolated by the blue line. Green points represent pen-downs\\nwhereas red dots indicate pen-ups.\\nInformation about the transducer device used to collect the ink\\n(e.g., the sampling rate and resolution) is specified with the <captureDevice>\\nelement. The Multimodal Interaction Working Group is currently working\\nwith the Device Independence Working Group to make sure that transducer\\ncharacteristics are also represented as a CC/PP profile that can be\\nincluded inside an ink document by reference. See \\n\"Composite Capability/Preference Profiles (CC/PP): Structure and Vocabularies\"\\nInk traces can have certain attributes such as color and width. \\nThese attributes are captured in the <brush> \\nelement. Traces that share the same characteristics, such as being \\nwritten with the same brush, can be grouped together with the \\n<traceGroup> element.\\nFor applications that require ink sharing, such as collaborative whiteboards, \\nwhere ink coming from different devices is drawn on a common canvas, the \\n<context> element allows representation and grouping of the pertinent \\ninformation, such as the trace format, brush, and canvas. \\nThe <traceRefGroup> element is provided as a building block for semantic \\nlabelling of groups of traces. It includes a generic contentCategory attribute \\nthat can be used by applications to describe at a basic level the category of \\ncontent that the traces represent (e.g., \"handwritten text\", \"drawing\", \\netc.).\\nIn all appropriate cases, the InkML specification defines default values for \\nelements that are not specified, and rules that establish the scope of a given \\nattribute.\\nApplication-specific elements are expected to be \\ndefined to provide a higher-level description of the digital ink captured in the \\nprimitive elements. Some application-specific elements would reference the \\nprimitive elements. For example, a page tag may be useful in a document \\nmanagement application to indicate groups of traces belonging to a particular \\npage. In a form processing application, a field tag might indicate a group of \\ntraces belonging to a particular field. Another example of an \\napplication-specific element is <writerInfo> which \\ncould be used to record information about \\nthe age and handedness of the writer. \\n\\nWhen combining InkML and other XML elements within applications, elements\\nfrom different namespaces may be disambiguated by use of the namespace qualifier.\\nInkML element names are defined within the InkML namespace, specifically \\nhttp://www.w3.org/2003/InkML\\n\\nFinally, the InkML specification is currently restricted to fixed \\nCartesian coordinate systems. Similarly, it does not support detailed timestamp \\nhandling, events (although these could be handled via application-specific \\nelements), or sophisticated compression of trace data.\\n1.3 Exchange Modes\\nMost ink-related applications fall into two broad categories: Streaming and Archival. \\nArchival ink applications capture and store digital ink for later processing, \\nsuch as document storage/retrieval applications and remote on-line forms \\nprocessing (where forms are filled on electronic tablet computers and processed \\nremotely). In these applications, all primitive elements are written prior to \\nprocessing. For ease of processing, it is recommended that, in \\narchival mode, referenced elements be defined inside of a declaration block \\nusing the <defs> element.\\nStreaming ink applications, on the other hand, capture and transmit digital \\nink in essentially real time, such as in the electronic whiteboard example \\nmentioned above. In order to support a streaming style of ink markup generation, \\nthe InkML language supports the notion of a \"current\" state (e.g., the current \\nbrush) and allows for incremental changes to this state.\\n\\n 2  Traces and Trace Formatting\\nTraces are the basic element used to record the trajectory of the pen as the \\nuser writes digital ink. More specifically, these recordings describe sequences\\nof connected points. On most devices, these sequences of points will be bounded \\nby pen contact change events (pen-up and pen-down), although some  application simply \\nrecord proximity and force data without providing an interpretation of pen-up or pen-down \\nstate.\\nThe simplest form of encoding specifies the X and Y coordinates\\nof each sample point. For compactness, it may be desirable to\\nspecify absolute coordinates only for the first point in the trace\\nand use delta-x and delta-y values to encode subsequent points.\\nSome devices record acceleration rather than absolute or relative\\nposition; some provide additional data that may be encoded in the\\ntrace, including Z coordinates or tip force (pressure), or the state of side\\nswitches or buttons.\\nThese variations in the information available from different\\ncapture devices, or needed by different applications, are supported in\\nInkML through the <traceFormat> and\\n<trace> elements. The\\n<traceFormat> element specifies the encoding format\\nfor each sample of a recorded trace, while <trace>\\nelements are used to represent the actual trace data. If no\\n<traceFormat> is specified, a default encoding\\nformat of X and Y coordinates is assumed.\\n2.1 Trace Formats\\nTraces generated by differing devices, or used in differing\\napplications, may contain different types of information. InkML\\ndefines channels to describe the data that may be encoded\\nin a trace.\\nA channel can be characterized as either regular--meaning that\\nits value is recorded for every sample point of the trace, or\\nintermittent--meaning that its value may change infrequently and\\nthus will not necessarily be recorded for every sample point. X and\\nY coordinates are examples of likely regular channels, while the\\nstate of a pen button is likely to be an intermittent channel.\\nThe <traceFormat> element describes the\\nformat used to encode points within <trace>\\nelements. In particular, it defines the sequence of channel values\\nthat occurs within <traceFormat> elements. The\\norder of declaration of channels in the\\n<traceFormat> element determines the order of\\nappearance of their values within <trace>\\n\\nelements. X and Y should be the first two channels of the\\n<traceFormat> if they are used.\\nRegular channels appear first in the <trace>,\\nfollowed by any intermittent channels. Correspondingly, the\\n<traceFormat> element contains a\\n<regularChannels> section followed by a\\n<intermittentChannels> section. The\\n\\n<regularChannels> element lists those channels\\nwhose value must be recorded for each sample point, while the\\n<intermittentChannels> lists those channels\\nwhose value may optionally be recorded for each sample point. If no\\nchannels of either type exist, the corresponding element may be\\nomitted.\\nWithin a <regularChannels> or\\n<intermittentChannels> element, channels are\\ndescribed using the empty element <channel>,\\nwith name, type, default, and mapping\\nattributes.\\nThe required name attribute specifies the interpretation\\nof the channel in the trace data. The following channel names, with\\ntheir specified meanings, are reserved:\\n\\n\\n\\nchannel name\\ninterpretation\\n\\n\\nX\\nX coordinate (horizontal pen position)\\n\\n\\nY\\nY coordinate (vertical pen position)\\n\\n\\nZ\\nZ coordinate (height of pen above paper/digitizer)\\n\\n\\nF\\npen tip force (tablet pressure)\\n\\n\\nS\\ntip switch state (touching/not touching the digitizer)\\n\\n\\nB1...Bn\\nside button states\\n\\n\\nTx\\ntilt along the x-axis\\n\\n\\nTy\\ntilt along the y-axis\\n\\n\\nAz\\nazimuth angle of the pen (yaw)\\n\\n\\nEl\\nelevation angle of the pen (pitch)\\n\\n\\nR\\nrotation (rotation about pen axis - i.e., like the roll axis of an airplane)\\n\\n\\n\\nOrientation Channels\\nThere are 5 channels defined for recording of pen orientation data.  \\nImplementers may choose to use either Azimuth and Elevation, or tilt angles. The latter are the angles of projections of the pen axis onto the XZ and YZ planes, measured from the vertical. It is often useful to record the sine of this angle, rather than the angle itself, as this is usually more useful in calculations involving angles. The specification does not yet include a mechanism for distinguishing these two.\\nThe third degree of freedom in orientation is generally defined as the rotation of the pen about its axis. This is potentially useful (in combination with tilt) in application such as illustration or calligraphy, and signature verification.\\n\\n\\nFigure 2: (a) azimuth and elevation angles, (b) tilt angles\\n\\n\\n Figure 3: (a) pen orientation decomposition, (b) pen rotation\\n\\nFigure 2a displays the pen orientation using Azimuth and\\nElevation. The origin of the Azimuth is at the Y-axis. Azimuth\\nincreases anticlockwise up to 360 degrees.  The origin of Elevation is\\nlocated within the XY-plane.  Elevation increases up to 90 degrees, at\\nwhich point the pen is perpendicular to the XY-plane.\\nFigure 2b explains the definition of the Tilt-X and the Tilt-Y\\nangles. For both the origin is along the Z-axis. Tilt-X increases up\\nto +90 degrees for inclinations along the positive X-axis and\\ndecreases up to -90 degrees for inclinations along the negative\\nX-axis. Respectively, Tilt-Y is defined for pen inclinations along the\\nY-axis.\\nFigure 3a displays the pen orientation decomposition as functions of\\nAzimuth/Elevation or alternatively as function of Tilt-X/Tilt-Y. Thereby,\\nElevations of the pen which are mapped to the XZ- and to the YZ- plane lead\\nto Tilt-X and Tilt-Y.\\nFigure 3b shows the Rotation of the pen along its longitudinal\\naxis.\\nUser Defined Channels\\nIn addition, user-defined channels are allowed, although their\\ninterpretation is not required by conforming ink markup processors.\\nThe type attribute defines the encoding type for the\\nchannel (either boolean, decimal, or integer). If type is\\nnot specified, it defaults to decimal.\\nA default value can be specified for the channel using the\\ndefault attribute; the use of default values within a trace\\nis described in the next section. If no default is\\nspecified, it is assumed to be zero for integer and decimal-valued\\nchannels, and false for boolean channels.\\nTypically, a channel in the <traceFormat> will map\\ndirectly to a corresponding channel provided by the digitizing\\ndevice, and its values as recorded in the trace data will be the\\noriginal channel values recorded by the device. However, for some\\napplications, it may be useful to store normalized channel values\\ninstead, or even to remap the channels provided by the digitizing\\ndevice to different channels in the trace data. This correspondence\\nbetween the trace data and the device channels is recorded using\\nthe mapping attribute of the <channel> element.\\nThe mapping attribute has three forms. Identity mappings\\nfrom device channels are described using a mapping value of \"*\".\\nThe following example defines a channel in the trace data which\\nrecords the values obtained directly from the X coordinate channel\\nprovided by the device:\\n\\n<channel name=\"X\" type=\"decimal\" mapping=\"*\"/>\\n\\nSimple mappings such as scaling, and translation, can be specified\\nusing a mapping value of the form \"formula(...)\", where the expression\\nenclosed in the parentheses contains only channel names (from the\\ndevice element), integer and decimal values, mathematical operators +,\\n-, *, /, and boolean operators !, &, |. Formulae syntax is defined\\nto be standard ANSI-C expression syntax, including use of integer and\\ndecimal values, restricted to the listed operators. The examples below\\ndefine a channel for Y coordinates which is derived from the original\\ndevice y-coordinate channel by scaling by 2 and translating by 10\\nunits, and another channel which normalizes the device\\'s tip force\\nvalues from the range 0..1024 to 0..128:\\n\\n<channel name=\"Y\" type=\"decimal\" mapping=\"formula(2*Y+10)\"/>\\n<channel name=\"F\" type=\"decimal\" mapping=\"formula(F*.125)\"/>\\n\\nMore complex relations can be described using a mapping value of\\nthe form \"uri(...)\", where the URI enclosed within the parentheses\\ncan refer to a resource such as a MathML document. The following\\nelement defines a force channel in the trace data whose values were\\nobtained by some mapping of device channels specified in a separate\\nresource called fxform:\\n\\n<channel name=\"F\" type=\"decimal\"\\n         mapping=\"uri(\\'http://www.example.org/fxform\\')\"/>\\n\\nIf no mapping is specified for a channel, it is assumed to be\\nunknown.\\nThe following example defines a\\n<traceFormat> which reports decimal-valued X and\\nY coordinates for each point, and intermittent boolean values for\\nthe states of two buttons B1 and B2, which have default values of\\n\"false\":\\n\\n<traceFormat id=\"xyb1b2\">\\n  <regularChannels>\\n     <channel name=\"X\" type=\"decimal\" mapping=\"*\"/>\\n     <channel name=\"Y\" type=\"decimal\" mapping=\"*\"/>\\n  </regularChannels>\\n  <intermittentChannels>\\n     <channel name=\"B1\" type=\"boolean\" default=\"F\" mapping=\"*\"/>\\n     <channel name=\"B2\" type=\"boolean\" default=\"F\" mapping=\"*\"/>\\n  </intermittentChannels>\\n</traceFormat>\\n\\nThe appearance of a\\n<traceFormat> element in an ink markup file both\\ndefines the format and installs it as the current format for\\nsubsequent traces (except within a <defs> block,\\ndiscussed later in section 3.4). The id attribute of a\\n<traceFormat> allows the format to be reused by\\nmultiple contexts (section 3.2). If no\\n<traceFormat> is specified, the following default\\nformat is assumed for all traces:\\n\\n<traceFormat id=\"default\">\\n  <regularChannels>\\n     <channel name=\"X\" type=\"decimal\"/>\\n     <channel name=\"Y\" type=\"decimal\"/>\\n  </regularChannels>\\n</traceFormat>\\n\\nThus, in the simplest case, an ink markup file need only contain\\ntraces.\\n\\nOpen Issues\\nShould a <traceFormat> be allowed to reference another\\n<traceFormat>? If so, what is the nature of the modifications\\nwhich would be allowed? One possibility is to allow extension only;\\ni.e. the channels defined in the <traceFormat> are added in\\norder after the ones in the referenced <traceFormat>. Another\\nis to allow overriding of the attributes of channels in the\\nreferenced <traceFormat>; e.g. any channel whose name matches\\nthat of a channel in the referenced <traceFormat> replaces\\nits definition.\\nAdditional detail about formula syntax is still open. Lookup tables, < > == operators, ...\\n\\n 2.2 Traces\\nThe <trace> element is used to record the data\\ncaptured by the digitizer. It contains a sequence of points encoded\\naccording to the specification given by the\\n<traceFormat> element.\\nThe type attribute of a <trace> indicates the pen\\ncontact state (either \"pen-up\" or \"pen-down\") during its recording.\\nA value of \"indeterminate\" is used if the contact-state is neither pen-up \\nnor pen-down, and may be either unknown or variable within the trace.\\nFor example, a signature may be captured as a single indeterminate trace \\ncontaining both the actual writing and the trajectory of the pen between strokes.\\nA value of \"continuation\" means both that\\nthe pen contact state is retained from the previous trace element\\nand that the points of the current trace element are a temporally\\ncontiguous continuation of (and thus should be connected to) the\\nprevious trace element. This allows a trace to be spread across\\nseveral elements for purposes such as streaming.\\nRegular channels may be reported as explicit values,\\ndifferences, or second differences. Prefix symbols are used to\\nindicate the interpretation of a value. A preceding exclamation\\npoint indicates an explicit value, a single quote indicates a\\nsingle difference, and a double quote prefix indicates a second\\ndifference. If there is no prefix, then the channel value is\\ninterpreted as explicit, difference, or second difference based on\\nthe last prefix for the channel. If there is no last prefix, the\\nvalue is interpreted as explicit.\\nA second difference encoding must be preceded by a single\\ndifference representation; which, in turn, must be preceded with an\\nexplicit encoding.\\nNOTE: All traces must begin with an explicit value, not with a first or second difference. This is true of continuation traces as well. This allows the location and velocity state information to be discarded at the end of each trace, simplifying parser design.Intermittent channels are always encoded explicitly, and\\nprefixes are not allowed.\\nBoth regular and intermittent channels may be encoded with a\\nwildcard character *. The wildcard character means either that the\\nvalue of the channel remains at the previous channel value (if\\nexplicit), or that the channel continues integrating the previous\\nvelocity and acceleration values.\\nBooleans are encoded as \"T\" or \"F\".\\nFor each point in the trace, regular channel values are reported\\nfirst in the order given by the <traceFormat>. If any\\nintermittent values are reported for the point, the set of\\nintermittent values is preceded by a colon and ended with a\\nsemicolon. Within these delimiters, the intermittent channels are\\nrepresented in the order given by the <traceFormat>. The list\\nmay be terminated early with the semicolon, and the unreported\\nintermittent channels are interpreted with wildcards.\\nHere is an example of a trace of 11\\npoints, using the following traceFormat:\\n\\n<traceFormat>\\n  <regularChannels>\\n     <channel name=\"X\" type=\"decimal\">\\n     <channel name=\"Y\" type=\"decimal\">\\n  </regularChannels>\\n  <intermittentChannels>\\n     <channel name=\"B1\" type=\"boolean\" default=\"F\"/>\\n     <channel name=\"B2\" type=\"boolean\" default=\"F\"/>\\n  </intermittentChannels>\\n</traceFormat>\\n\\n<trace id = \"id4525abc\">\\n1125 18432\\'23\\'43\"7\"-8 3-5+7  -3+6+2+6 8+3+6:T;+2+4:*T;+3+6+3-6:FF;\\n</trace>\\n\\nThe trace is interpreted as follows:\\n\\n\\n\\nTrace\\nX\\nY\\nvx\\nvy\\nB1\\nB2\\nComments\\n\\n\\n1125 18432\\n1125\\n18432\\n?\\n?\\nF\\nF\\nbutton default values\\n\\n\\n\\'23\\'43\\n1148\\n18475\\n23\\n43\\nF\\nF\\nvelocity values\\n\\n\\n\"7\"-8\\n1178\\n18510\\n30\\n35\\nF\\nF\\nacceleration Values\\n\\n\\n3-5\\n1211\\n18540\\n33\\n30\\nF\\nF\\nimplicit acceleration\\n whitespace token sep\\n\\n\\n+7 -3\\n1251\\n18567\\n40\\n27\\nF\\nF\\noptional whitespace\\n\\n\\n+6+2\\n1297\\n18596\\n46\\n29\\nF\\nF\\n \\n\\n\\n+6 8\\n1349\\n18633\\n52\\n37\\nF\\nF\\nspace instead of +\\n\\n\\n+3+6:T;\\n1404\\n18676\\n55\\n43\\nT\\nF\\nan optional value\\n\\n\\n+2+4:*T;\\n1461\\n18723\\n57\\n47\\nT\\nT\\nwildcard\\n\\n\\n+3+6\\n1521\\n18776\\n60\\n53\\nT\\nT\\noptional keep last\\n\\n\\n+3-6:FF;\\n1584\\n18823\\n63\\n47\\nF\\nF\\noptionals\\n\\n\\n\\nOne would not typically see both a \"+\"and a \"space\" used as a\\nseparator in the same trace or document, but it is legal.\\nAn ink markup generator might also include additional whitespace\\nformatting for clarity. The following trace specification is\\nidentical in meaning to the more compact version shown above:\\n\\n<trace id = \"id4525abc\">\\n1125  18432\\n\\'23  \\'43\\n\"7  \"-8\\n3  -5\\n7  -3\\n6  2\\n6  8\\n3  6  :T;\\n2  4  :  *T;\\n3  6\\n3-6  :F  F;\\n</trace>\\n\\nIn addition, the alphabetic characters may be used to encode\\nsmall negative and positive integer values. These may be substituted\\nanywhere for an integer value between -25 and +25.\\n\\nThe characters \"a\" to \"y\" are interpreted as -1 through\\n-25.\\nThe characters \"A\" to \"Y\" are interpreted as 1 through 25.\\n\"z\" and \"Z\" are interpreted as zero.\\n\\nUsing these shorthand codes, the above trace could be encoded: \\n\\n\\n<trace id=\"4525BCD\">\\n1125 18432\\'W\\'43\"G\"hCeGcFBFHCF:T;BD:*T;CFCf:FF;\\n</trace>Note that the true and false values for the side buttons use\\nsymbols that are also used to encode numbers. However, they are\\nunambiguous because of their location.\\n2.2.1 Grammar\\nThe grammar for trace encoding is described in Backus-Naur Form\\n(BNF) using the following notation:\\n\\n*: 0 or more\\n+: 1 or more\\n?: 0 or 1\\n(): grouping\\n|: separates alternatives\\ndouble quotes surround literals\\n#x precedes hex character codes\\n\\n\\nThe grammar is as follows:\\n\\ntrace ::=\\n    wsp* point+\\n\\npoint ::=\\n    regularPart intermittentPart?\\n\\nregularPart ::=\\n    regularValue+\\n\\nintermittentPart ::=\\n    \":\" wsp* intermittentValue* \";\" wsp* \\n\\nregularValue ::=\\n    qualifier? value wsp*\\n\\nintermittentValue ::=\\n    value wsp*\\n\\nvalue ::=\\n    integer | decimal | code\\n\\ninteger ::=\\n    sign? digit+\\n\\ndecimal ::=\\n    sign? digit+ \".\" digit+\\n\\ncode ::=\\n    \"a\" | \"b\" | \"c\" | \"d\" | \"e\" | \"f\" | \"g\" | \"h\" | \"i\" |\\n    \"j\" | \"k\" | \"l\" | \"m\" | \"n\" | \"o\" | \"p\" | \"q\" | \"r\" |\\n    \"s\" | \"t\" | \"u\" | \"v\" | \"w\" | \"x\" | \"y\" | \"z\" | \"A\" |\\n    \"B\" | \"C\" | \"D\" | \"E\" | \"F\" | \"G\" | \"H\" | \"I\" | \"J\" |\\n    \"K\" | \"L\" | \"M\" | \"N\" | \"O\" | \"P\" | \"Q\" | \"R\" | \"S\" |\\n    \"T\" | \"U\" | \"V\" | \"W\" | \"X\" | \"Y\" | \"Z\" | \"*\"\\n\\ndigit ::=\\n    \"0\" | \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\" | \"7\" | \"8\" | \"9\"\\n\\nsign ::=\\n    \"+\" | \"-\"\\n\\nqualifier ::=\\n    \"!\" | \"\\'\" | \"\"\"\\n\\nwsp ::=\\n    #x20 | #x9 | #xD | #xA\\n\\nThe number of regularValue\\ntokens appearing within a trace must match the number of regular\\nchannels specified in the <traceFormat>, and the number of\\nintermittentValue tokens must\\nbe no greater than the number of intermittent channels.\\nWhitespace is optional before and after\\nregularValue and\\nintermittentValue tokens\\n(unless required to separate two adjacent positive integer or\\ndecimal tokens values without + signs).\\n\\nOpen Issues\\nSince many sources of digital ink are temporal, many digital ink\\nrecords will have significant time information. The \"current\" or\\n\"cumulative\" time may be expressed in several ways, depending on\\nwhat is available at the time of capture. The most explicit\\nexpression of time is by the use of a startTime attribute in\\nany element. This is not an ideal solution and should be considered\\nmore carefully by the working group.\\nThere is currently some discussion about whether to make \\ncontinuation a separate attribute, rather than a \\ntype. This would allow specification of whether a continuation \\ntrace was pen-up, pen-down, or indeterminate in addition to the\\nfact that it is a continuation.\\n\\n 2.3 Trace Groups\\nThe <traceGroup> element is used to group successive traces which share common characteristics, \\nsuch as the same <traceFormat>. The brush and context sections describe other contextual \\nvalues that can be specified for a <traceGroup>. In the following example the two traces enclosed \\nin the <traceGroup> share the same brush (see section 3.2 for a description of brushes).\\n\\n\\n<traceGroup brushRef=\"penA\">\\n  <trace>...</trace>\\n  <trace>...</trace>\\n</traceGroup>\\n\\nThe use of <traceGroup> is reserved for the\\ncontainment of traces according to their properties at the time of\\ncapture. The element may not be nested, and it is not meant to be a\\ngeneric grouping mechanism for the semantic labelling of traces. For\\nthat purpose, InkML provides the <traceRefGroup>\\nelement, which is described in section 5.\\nTrace groups are the primary mechanism for assigning\\n<context> to traces in archival ink markup.  For\\nadditional details about this usage, see section 4.1.\\nOpen Issues\\nWe recently clarified that <traceGroup> elements\\nmay not be nested. <traceRefGroup> provides most of\\nthe functionality for which this would be desireable.\\nIs there any use case we have overlooked that would require nested\\ntraceGroups?\\n\\n3 Context Elements\\nA number of device, data format, and coordinate system details \\ncomprise the context in which ink is written and recorded. These \\ncontextual details need to be captured by the ink markup language in \\norder to fully characterize the recorded ink data.\\nThe <context> element (section 3.3) provides various \\nattributes such as canvas and mapping by which \\nInkML addresses this need.  In addition, the \\n<captureDevice> element (section 3.1) describes how InkML allows \\naccurate recording of the hardware characteristics relevant during the \\ncapture of the ink traces.\\nDifferent pen tips (e.g. eraser vs. writing end) or \\nentirely different pens, physical or virtual, may be used on the same \\ninput device.  These details are captured by the <brush> \\nelement (Section 3.2).\\nThe following sections describe the elements which are used to capture the context in\\nwhich the ink data was recorded.\\n3.1 Capture Device\\nOne of the important requirements for the ink format is to allow \\naccurate recording of meta-data about the hardware that was used to \\nacquire the ink contained in a file. This is accomplished in the \\n<captureDevice> block, which may contain either very basic information, \\nor very detailed information about a number of device \\ncharacteristics.\\nSome of these characteristics are already commonly used in \\ndigitizer specifications, while others are somewhat more esoteric, but \\nnonetheless potentially very useful. Most digitizer manufacturers do \\nnot spec them, and many are not able to measure them. However, these \\ndevice characteristics influence signal fidelity and impose some \\nlimits on how the data can be used. Hopefully by beginning to \\nstandardize the recording of these characteristics, we can raise \\nawareness and encourage device manufacturers to take them into \\nconsideration.\\nThe <captureDevice> block, including\\n<channelList>, will often be specified by reference\\nto a separate xml document, either local or at some remote\\nURI. Ideally, <captureDevice> blocks for common\\ndevices will become publicly available.\\n3.1.1 captureDevice Element\\nThe <captureDevice> element will allow specification of:\\n\\nManufacturer and model\\nBasic sampling rate - samples/sec\\nSampling uniformity: must be designated non-uniform if any pen-down points are\\n    skipped or if the sampling is irregular\\nLatency: latency of the real-time channel, in msec, from physical action to the API time\\n    stamp. This is typically specified at the device level, since all channels often are\\n    subject to a common processing and communications latency.\\nChannel List\\n\\nSyntax:\\n\\n<captureDevice id=\"foo\"\\n               manufacturer=\"AcmePen\"\\n               model=\"FooBar 2000 USB\" \\n               sampleRate=\"100\"\\n               uniform=\"TRUE\"\\n               latency=\"50\">\\n\\n  <channelList>\\n     ...\\n\\n  </channelList>\\n</captureDevice>\\n\\n\\nAttributes:\\n\\n\\n\\n\\nid\\nA unique identifier for this captureDevice element\\n\\n\\nmanufacturer\\nString identifying the digitizer device manufacturer\\n\\n\\nmodel\\nString identifying the digitizer model\\n\\n\\nsampleRate\\nThe basic sample rate in samples/sec. May be \"unknown\"\\n\\n\\nuniform\\nTRUE or FALSE indication of whether sample rate\\n                  is consistent, with no dropped points\\n\\n\\nlatency\\nThe basic device latency that applies to all channels, in msec\\n\\n\\n\\n\\n\\n3.1.2 ChannelList\\nThe <channelList> element lists all data channels that the device is capable of\\nreporting. Channels include:\\n\\nX coordinate (horizontal pen position, relative or absolute)\\nY coordinate (up/down or vertical pen position, relative or absolute)\\nZ coordinate (height of pen above paper/digitizer, relative or absolute)\\nForce (pen tip force) [NOTE: this is often referred to as \"pressure\" by\\n    manufacturers]\\nTip switch state (touching, not touching digitizer)\\nSide switches and Buttons (for example, bezel buttons, cursor buttons...)\\nTilt angle in X dimension\\nTilt angle in Y dimension\\nPen Azimuth (alternative to tilt)\\nPen Elevation (alternative to tilt)\\nPen Rotation (around the pen axis)\\n\\nSyntax:\\n\\n<channelList id=\"foo\">\\n  <channel name=\"X\">\\n     ...\\n\\n  </channel>\\n</channelList>\\n\\n\\nAttributes:\\n\\n\\n\\n\\nid\\nA unique identifier for this channelList element\\n\\n\\n\\n\\n\\nIn addition, each channel may specify any of the following when known and appropriate:\\n\\nValue representation - for example, Boolean, integer, or decimal\\nRange - the range of possible values that may be reported\\nThreshold - (for binary channels) - e.g. the threshold force at which the tip switch is\\n    activated\\n\\nFor continuous channels, like X, Y and Z, and Force, these additional characteristics\\nmay be specified:\\n\\nResolution - the scale of the values recorded, expressed as \"fraction units\", e.g. \"1/1000 inch\") or \"decimal units\", e.g. \"0.1 mm\" or\\n    \"1 degrees\" Note that if decimal values are recorded, the quantization of the data may be smaller than the \"resolution\"\\nQuantization - the unit of smallest change in the reported values. If the value is reported as integer, this is assumed to be the same as the resolution\\nNoise - the RMS value of noise typically observed on the channel. This is distinct from\\n    accuracy! It is an indication of the difference observed in the data from the device when\\n    the same path is traced out multiple times (e.g. by a robot).\\nAccuracy - the typical accuracy of the data on the channel (e.g. \"0.5 mm\",\\n    \"10 degrees\" or \"0.1 newton\") This is the typical difference between\\n    the reported position and the actual position of the pen tip (or tilt ...)\\nCross-coupling - the distortion in the data from one channel due to changes in another\\n    channel. For example, the X and Y coordinates in an electromagnetic digitizer are\\n    influenced by the tilt of the pen. This would be specified by dX/dTx = ... ??? or max\\n    delta X vs. Tx = ... ??? If the influencing channels are also recorded, and the\\n    cross-couplings are accurately specified, it may be possible to compensate for the\\n    cross-coupling by subtracting the influence, at the expense of higher noise. The cross-coupling is always expressed in the units of the two channels, e.g. if X mm and Tx is in degrees, then cross-coupling is in mm/deg\\nSkew - the temporal skew of this channel relative to the basic device latency, if any.\\n    For example, some devices actually sample X and Y at different points in time, so one\\n    might have a skew of -5 msec, and the other +5 msec.\\nMinimum bandwidth (in Hz) - the minimum bandwidth of the channel, in Hz (not\\n    samples/sec), i.e., the frequency of input motion up to which the signal is accurate to\\n    within 3dB.\\nPeak rate - the maximum speed at which the device can accurately track motion\\nDynamic distortion, e.g., how velocity affects position accuracy. This is expressed in\\n    inverse seconds, e.g. 0.01 mm / mm / sec. This kind of distortion is often cross channel,\\n    but this spec only allows a generic, channel independent specification.\\n\\nSyntax:\\n\\n<channel name=\"X\">\\n  <representation  value=\"INTEGER\"/>\\n  <range           min=\"0\" max=\"8191\"/>\\n  <threshold       value=\"0.1\" units=\"newtons\"/>\\n  <resolution    value=\"0.1\"  units=\"mm\"/>\\n  <quantization  value=\"0.01\" units=\"mm\"/>\\n  <noise         value=\"0.05\" units=\"mm\"/>\\n  <accuracy      value=\"0.5\"  units=\"mm\"/>\\n  <crossCoupling otherChannel=\"Tx\" value=\"0.1\"/>\\n  <crossCoupling otherChannel=\"Ty\" value=\"0.01\"/>\\n  <skew          value=\"2\" units=\"msec\"/>\\n  <minBandwidth  value=\"15.0\"/>\\n  <distortion    value=\".001\"/>\\n\\n</channel>\\n\\n\\nAttributes:\\n\\n\\n\\n\\nname\\nThe  name of the channel described by this channel element\\n\\n\\n\\n\\nExamples:\\n<channel name=\"S\">\\n  <representation  value=\"BOOLEAN\"/>\\n  <threshold       value=\"0.1\" units=\"newtons\"/>\\n  <skew          value=\"5\" units=\"msec\"/>\\n</channel>\\n \\n<channel name=\"X\">\\n  <representation  value=\"INTEGER\"/>\\n  <range           min=\"0\" max=\"8191\"/>\\n  <resolution    value=\"0.1\"  units=\"mm\"/>\\n  <quantization  value=\"0.01\" units=\"mm\"/>\\n  <noise         value=\"0.05\" units=\"mm\"/>\\n  <accuracy      value=\"0.5\"  units=\"mm\"/>\\n  <crossCoupling otherChannel=\"Tx\" value=\"0.1\"/>\\n  <crossCoupling otherChannel=\"Ty\" value=\"0.01\"/>\\n  <skew          value=\"2\" units=\"msec\"/>\\n  <minBandwidth  value=\"15.0\"/>\\n  <distortion    value=\".001\"/>\\n\\n</channel>\\n\\n3.1.4 Error Calculations\\nThis Error Calculations section is informative.\\nThe following are some suggestions for how error estimates might be derived from the\\nbasic fidelity information in a spatial channel (x or y):\\n\\nTotal position error is the sum of {absolute accuracy + velocity*(dynamic distortion) +\\n    noise + quantization error} for identical path (in all channels).\\nRepeatability is also the sum of {noise + quantization error} for a repeated, identical\\n    physical trajectory across the digitizer.\\nRelative position error is the minimum of {linearity*delta, absolute accuracy). This\\n    effects the ability to accurately measure the length and orientation of a short stroke.\\nMaximum error including skew (by assuming that all channels are in sync) is equal to the\\n    sum of {absolute accuracy + velocity*dynamic distortion + cross-coupling + velocity*(skew)\\n    + noise + quantization error}.\\n\\nAll errors are subject to additional distortion from a signal exceeding the channel\\nbandwidth.\\n\\nOpen Issues\\nThe attribute for identifying the capture device info\\nblock has not been incorporated into the Context section.\\n\\nThere should be a \"time\" channel.  We recently noticed that it is missing,\\nand it will be incorporated in the next draft.\\n\\nThere have been last minute additions to try to flesh out the syntax and examples. These are preliminary, and may be changed.\\n\\n3.2 Brushes\\nAlong with trace data, it is often necessary to record certain \\nattributes of the pen during ink capture. For example, in a notetaking \\napplication, it is important to be able to distinguish between traces \\ncaptured while writing as opposed to those which represent erasures. \\nBecause these attributes will often be application specific, this \\nspecification does not attempt to enumerate the brush attributes which \\ncan be associated with a trace. It also does not provide a language \\nfor describing brush attributes, since it is possible to imagine \\nattributes which are described using complex functions parameterized \\nby time, pressure, or other factors. Instead, the specification allows \\nfor capturing the fact that a given trace was recorded in a particular \\nbrush context, leaving the details of precisely specifying that \\ncontext to a higher-level, application specific layer.\\nDepending on the application, brush attributes may change \\nfrequently. Accordingly, there should be a concise mechanism to assign \\nthe attributes for an individual trace. On the other hand, it is \\nlikely that many traces will be recorded using the same sets of \\nattributes; therefore, it should not be necessary to explicitly state \\nthe attributes of every trace (again, for reasons of conciseness). \\nFurthermore, it should be possible to define entities which encompass \\nthese attribute sets and refer to them rather than listing the entire \\nset each time. Since many attribute sets will be similar to one \\nanother, it should also be possible to inherit attributes from a prior \\nset while overriding some of the attributes in the set.\\nIn the ink markup, brush attributes are described by the \\n<brush> element. This element allows for the definition of \\nreusable sets of brush attributes which may be associated with traces. \\nFor reference purposes, a brush specifies an identifier which can be \\nused to refer to the brush. A brush can inherit the attributes of \\nanother <brush> element by including a brushRef attribute which \\ncontains the referenced brush\\'s id.\\nBrush attributes are associated with traces using the brushRef \\nattribute. When it appears as an attribute of an individual \\n<trace>, the brushRef specifies the brush attributes for that \\ntrace. When it appears as an attribute of a <traceGroup> \\nelement, the brushRef specifies the common brush attributes for all \\ntraces enclosed in the <traceGroup>. Within the \\n<traceGroup>, an individual trace may still override the \\ntraceGroup\\'s brush attributes using a brushRef attribute.\\nBrush attributes can also be associated with a context by including \\nthe brushRef attribute on a <context> element. Any traces which \\nreference the context using a contextRef attribute are assigned the \\nbrush attributes defined by the context. If a trace includes both \\nbrushRef and contextRef attributes, the brushRef overrides any brush \\nattributes given by the contextRef.\\nIn streaming ink markup, brushes are assigned to a trace according\\nto the current brush, which can be set using the\\n<context> and <brush>\\nelements. See section 4.2 for a detailed description of streaming\\nmode. \\n3.3 Context\\nThis section describes the <context> element and its \\nattributes: canvas, mapping\\ntraceFormatRef, and brushRef.  The \\ncontext element both defines the shared context (canvas) and serves as \\na convenient agglomeration of contextual attributes. It is used by the \\n<traceGroup> (Section 2.3) element to define the complete \\nshared context of a group of traces or may be referred to as part of a \\ncontext change in streaming mode. In either mode, individual \\nattributes may be overridden at time of use. Additionally, individual \\ntraces may refer to a previously defined context (again optionally \\noverriding its attributes) to describe a context change that persists \\nonly for the duration of that trace.\\nAlthough the use of the <context> element and attributes \\nis strongly encouraged, default interpretations are provided so that \\nthey are not required in an ink markup file if all trace data is \\nrecorded in the same virtual coordinate system, and its relationship \\nto digitizer coordinates is either not needed or unknown.\\nA shared context, called a canvas, is needed for the \\nink markup to support screen sharing amongst multiple devices, each of \\nwhich might have a different set of capture characteristics. For \\nexample, a single ink markup stream or file may contain traces that \\nare captured on a tablet computer, a PDA device, and an opaque \\ngraphics tablet attached to a desktop computer. The size of these \\ntraces on each capture device and corresponding display might differ, \\nyet it may be necessary to relate these traces to one another. They \\ncould represent scribbles on a shared electronic whiteboard, \\nannotations of a common document, or the markings of two players in a \\ndistributed tic-tac-toe game.\\nThe trace data for these different ink sessions could be recorded\\nusing the same set of virtual coordinates; however, it is often useful\\nand occasionally may even be necessary to record the data in the\\ncapture device coordinates, in order to more precisely represent the\\noriginal capture conditions, for compactness, or to avoid round-off\\nerrors that might be associated with the use of a common coordinate\\nsystem. Thus the mapping; (section 3.3.2) from trace\\ncoordinates to the shared canvas coordinates may vary from device to\\ndevice.  The <traceFormat> (Section 2.1) used to\\nrecord trace data may also vary, therefore the\\n<context> element also contains a\\ntraceFormatRef attribute.\\nFinally, the <context> element provides a \\nbrushRef attribute to record the attributes of the pen \\nduring the capture of the digital ink, for a particular \\ncontext.\\n3.3.1 canvas Attribute\\nIn order to render data from a participant in a multi-party ink \\napp, it is necessary to know how to transform trace data to screen \\ncoordinates. \\nEach party may have a different coordinate system for their traces. \\nEach party will need a mapping to their display that allows scrolling \\nand zooming. Call this S[k]. \\nParty k still needs to determine the meaning of the traces from \\nparty i. This is most simply accomplished by having each party define \\nthe relationship between their trace coordinate system, and an \\narbitrary reference coordinate system.\\nThis virtual coordinate system does not have any physical \\ndimensions, because each party will render it differently, and each \\nperson will draw onto it differently, with arbitrary zoom and \\nscrolling. Thus the virtual coordinate system is arbitrary.\\nThis virtual coordinate system is provided by the \\ncanvas, declared via the canvas attribute. \\nThis uniquely identifies a shared virtual coordinate system for \\ncooperating ink applications. \\n\\nTogether with the trace-to-canvas coordinate mapping (discussed \\nbelow), it provides a common frame of reference for ink collected in \\nmultiple sessions on different devices. In the example above, trace \\ndata collected from the tablet computer can be combined with trace \\ndata collected from the PDA by specifying a common canvas and \\ndescribing the relationships between each device\\'s trace data and the \\ncommon canvas coordinate system.\\nIn the ink markup, the canvas is an unbounded space oriented so \\nthat x and y coordinates increase as one moves to the right and down, \\nrespectively. Specifying a standard handedness for the canvas \\ncoordinate system allows each device to orient and display ink from \\nevery other device.\\nCanvas Math \\nTo collaborate in the multi-party ink exchange, party k needs to \\nknow the orientation and handedness of the virtual coordinate system \\n(in order to determine their own local S[k]), and the mapping of each \\nother party\\'s data to that virtual coordinate system. Call these \\nmappings T[i]\\nTo map from trace coordinates to screen coordinates, we compose the \\ntransform from party i to virtual space with my transform from virtual \\nspace to screen space, S[k]. This is M = T * S. This matrix is used to \\ntransform all points from that traceGroup.\\nWhen the display is zoomed or scrolled, S[k] changes, and M is recomputed. \\nWhen a new traceGroup with a different T[i] is encountered, it is \\ncomposed with S[k], and rendering continues.\\n\\nThe S[k] matrix is not part of the inkML file, but is determined locally\\nduring capture or rendering.\\n\\nT and S are the minimum necessary information to be able to render \\nsome data. However, in order to determine S or T, it is also necessary to make \\na decision about the orientation of the virtual space. If everyone \\nmakes this determination independently, there is no common virtual \\nspace. Consequently, the virtual space, or canvas is \\ndefined to have a specific orientation.\\nThe orientation of this canvas does not effect anyone, as it \\ndisappears when T and S are composed. It simply provides a common \\nintermediate space that everyone uses when computing T (which goes \\ninto the xml) and S (which is used only to display the data).\\nThe default canvas \\nSince a canvas identifier is a simple string, the id of the default canvas is defined to be \"default\".  This is sufficient to allow simple single-canvas sharing without further action on the part of devices or applications.\\n3.3.2 mapping Attribute\\nThe trace-to-canvas coordinate system mapping, declared via the \\nmapping attribute, defines the transformation from trace \\ncoordinates to the shared canvas coordinate system.\\nThe trace-to-canvas coordinate system mapping is expressed as a \\nstandard 2x3 2D transformation matrix (at this time, we ignore the \\nadditional complication of nonlinearity in the digitizing device\\'s \\ncoordinate system). The default mapping is the identity matrix (with a \\nzero offset).\\nThe format of the trace data--both the mapping from digitizer to \\ntrace coordinates and the channels and channel formats present in the \\ndata--for a given context is specified via the \\ntraceFormatRef attribute, which refers to a \\n<traceFormat> element (Section 3.x).\\nNote: As it is primarily intended as an input specification, the \\nink markup language does not provide a mechanism for representing the \\ntransformations to screen or view coordinates, which relate to ink \\ndisplay and are typically transient.\\n3.3.3 traceFormatRef Attribute\\nThe trace format to associate with the context being defined is\\nspecified with a traceFormatRef attribute, which refers\\nto a <traceFormat> element (Section 2.1).\\n3.3.4 brushRef Attribute\\nThe brush to associate with the context being defined is specified with a\\nbrushRef attribute, which refers to a <brush> element (Section 3.3).\\n3.3.5 Context\\nThe <context> \\nelement consolidates all salient characteristics of one or more ink \\ntraces. It may be specified by declaring all non-default attributes, \\nor by referring to a previously defined context and overriding \\nspecific attributes.\\nSyntax:\\n\\n<context id=\"\" contextRef=\"\" canvas=\"\"\\n         mapping=\"\" traceFormatRef=\"\" brushRef=\"\"/>\\n\\nAttributes:\\n\\n\\n\\n\\nid\\nA unique identifier for this context.\\n\\n\\ncontextRef\\nA previously defined context upon which this context is to be based.\\n\\n\\ncanvas\\nThe unique identifier of the canvas for this context.\\n\\n\\nmapping\\nThe standard 2x3 matrix representation of the transformation from the trace data\\n    coordinates to the canvas; expressed as the six values of the transformation matrix in row\\n    order xx xy x0 yx yy y0. \\n\\n\\ntraceFormatRef\\nA reference to the traceFormat for this context.\\n\\n\\nbrushRef\\nA reference to the brush for this context.\\n\\n\\n\\nExamples:\\n\\n<context id=\"context1\" canvas=\"canvas1\"\\n         traceFormatRef=\"format1\" brushRef=\"brush1\"/>\\n<context id=\"context2\" contextRef=\"context1\" brushRef=\"brush2\"/>\\n<context id=\"context3\" canvas=\"canvas1\" mapping=\"2 0 0 0 2 0\" \\n         traceFormatRef=\"format2\" brushRef=\"brush3\"/>\\n\\nThe first example is a hypothetical device #1, using a previously \\ndefined format1 and brush1, and indicating that it can share trace \\ndata using canvas1. Its trace coordinates are mapped to this shared \\ncanvas using the default identity matrix with zero offset.\\nThe second example is the same device #1, using a different brush: brush2.\\nThe third example is a hypothetical device #2, using previously \\ndefined format2 and brush3, and sharing trace data with the first \\ndevice by using the common canvas1. Its trace coordinates require a \\nscale factor of 2 to map to the canvas. \\n3.4 Defs\\nThe <defs> element is a container which is used\\nto define reusable content.   The definitions within a\\n<defs> block can be referenced by other elements\\nusing the appropriate syntax.  Content within a\\n<defs> has no impact on the interpretation of\\ntraces, unless referenced from outside the\\n<defs>.  In order to allow them to be\\nreferenced, elements within a <defs> block must\\ninclude an id; attribute.  Therefore, an element which is\\ndefined inside a <defs> without an id, or\\nthat is never referenced, serves no purpose.   \\nThe three elements which can be defined inside a\\n<defs> are: <context>,\\n<brush> and <traceFormat>. \\nThe attributes which are used to reference these definitions are the\\nassociated contextRef, brushRef and\\ntraceFormatRef attributes.  The following simple example\\nillustrates usage of the <defs> element.\\n\\n<ink>\\n  <defs>\\n    <brush id=\"redPen\"/>\\n    <brush id=\"bluePen\"/>\\n    <traceFormat id=\"normal\"/>\\n    <traceFormat id=\"noForce\"/>\\n    <context id=\"context1\"\\n             brushRef=\"redPen\"\\n             traceFormatRef=\"normal\"/>\\n    <context id=\"context2\"\\n             contextRef=\"context1\"\\n      brushRef=\"bluePen\"/>\\n  </defs>\\n  <context contextRef=\"context2\"\\n           traceFormatRef=\"noForce\"/>      \\n  <context id=\"context3\"/>\\n</ink>\\n\\nMore details on the usage of the <defs> element are provided in section 4.\\n\\n4 Streams and Archives\\nThe ink markup is expected to be utilized in many different\\nscenarios. Ink markup data may be transmitted in substantially real\\ntime while exchanging ink messages, or ink documents may be archived\\nfor later retrieval or processing. \\nThese examples illustrate two\\ndifferent styles of ink generation and usage. In the former, the\\nmarkup must facilitate the incremental transmission of a stream of ink\\ndata, while in the latter, the markup should provide the structure\\nnecessary for operations such as search and interpretation. In order\\nto support both cases, InkML provides archival and streaming\\nmodes of usage.\\n4.1 Archival Applications\\nIn archival usage, contextual elements are defined within a <defs>\\nelement and assigned identifiers using the id attribute. References to\\ndefined elements are made using the corresponding brushRef,\\ntraceFormatRef, and contextRef attributes. The following example:\\n\\n<defs>\\n  <brush id=\"penA\"/>\\n  <brush id=\"penB\"/>\\n  <traceFormat id=\"fmt1\">\\n    <regularChannels>\\n      <channel name=\"X\" type=\"integer\">\\n      <channel name=\"Y\" type=\"integer\">\\n      <channel name=\"Z\" type=\"integer\">\\n    </regularChannles>\\n  </traceFormat>\\n  <context id=\"context1\" canvas=\"canvasA\"\\n           mapping=\"1 0 0 0 1 0\" traceFormatRef=\"fmt1\" brushRef=\"penA\"/>\\n  <context id=\"context2\" canvas=\"canvasA\"\\n           mapping=\"2 0 0 0 2 0\" traceFormatRef=\"fmt1\" brushRef=\"penB\"/>\\n</defs>\\n\\ndefines two brushes (\"penA\" and \"penB\"), a traceFormat (\"fmt1\"), and\\ntwo contexts (\"context1\" and \"context2\") which both refer to the same\\ncanvas (\"canvasA\") and traceFormat (\"fmt1\"), but with different\\nmappings and brushes.  Note the use of the brushRef and traceFormatRef\\nattributes to refer to the previously defined <brush> and <traceFormat>.\\nWithin the scope of a <defs> element, unspecified attributes of a\\n<context> element are assumed to have their default values. This\\n<defs> block:\\n\\n<defs>\\n  <brush id=\"penA\">\\n  <context id=\"context1\" canvas=\"canvasA\" brushRef=\"penA\"/>\\n</defs>\\n\\ndefines \"context1\", which is comprised of \"canvasA\" with the default\\nmapping and traceFormat (the identity mapping and a traceFormat\\nconsisting of decimal X-Y coordinate pairs), and \"penA\".\\nA <context> element can inherit and override the values of a\\npreviously defined context by including a contextRef attribute, so:\\n\\n<defs>\\n  <brush id=\"penA\"/>\\n  <context id=\"context1\" canvas=\"canvasA\"\\n           mapping=\"1 0 0 0 1 0\"/>\\n  <context id=\"context2\" contextRef=\"context1\"\\n           mapping=\"2 0 0 0 2 0\" brushRef=\"penA\"/>\\n</defs>\\n\\ndefines \"context2\" which shares the same canvas (\"canvasA\") and\\ntraceFormat (the default format) as \"context1\", but has a different\\nmapping and brush.\\nWithin archival ink markup, traces can either explicitly specify their\\ncontext through the use of contextRef and brushRef attributes, or they\\ncan have their context provided by an enclosing traceGroup. In the\\nfollowing:\\n\\n<trace id=\"t001\" contextRef=\"context1\"/>...</trace>\\n<trace id=\"t002\" brushRef=\"penA\"/>...</trace>\\n<traceGroup contextRef=\"context1\">\\n  <trace id=\"t003\">...</trace>\\n</traceGroup>\\n\\ntraces \"t001\" and \"t003\" have the context defined by \"context1\", while\\ntrace \"t002\" has a context consisting of the default canvas, mapping and\\ntraceFormat, and \"penA\".\\nTraces within a <traceGroup> element can also override the\\ncontext or brush specified by the traceGroup. In this example:\\n\\n<traceGroup contextRef=\"context1\">\\n  <trace id=\"t001\">...</trace>\\n  <trace id=\"t002\" brushRef=\"penA\">...</trace>\\n  <trace id=\"t003\">...</trace>\\n</traceGroup>\\n\\ntraces \"t001\" and \"t003\" have their context specified by \"context1\"\\nwhile trace \"t002\" overrides the default brush of \"context1\" with\\n\"penA\".\\nA trace or traceGroup can both reference a context and override its\\nbrush, as in the following:\\n\\n<trace id=\"t001\" contextRef=\"context1\" brushRef=\"penA\">...</trace>\\n<traceGroup contextRef=\"context1\" brushRef=\"penA\">\\n  <trace id=\"t002\">...</trace>\\n</traceGroup>\\nwhich assigns the context specified by \"context1\" to traces \"t001\"\\nand \"t002\", but with \"penA\" instead of the default brush.\\nIn archival mode, the ink markup processor can straightforwardly\\ndetermine the context for a given trace by examining only the\\n<defs> blocks within the markup and the enclosing traceGroup for\\nthe trace.\\n4.2 Streaming Applications\\nIn streaming ink markup, changes to trace context are expressed\\ndirectly using the <brush>, <traceFormat>, and\\n<context> elements. This corresponds to an event-driven model of\\nink generation, where events which result in contextual changes map\\ndirectly to elements in the markup.\\nIn the streaming case, the current context consists of the set of\\ncanvas, mapping, traceFormat and brush which are associated with\\nsubsequent traces in the ink markup. Initially, the current context\\ncontains the default canvas, an identity mapping, the default\\ntraceFormat, and a brush with no attributes. Each <brush>,\\n<traceFormat>, and <context> element which appears outside\\nof a <defs> element changes the current context accordingly\\n(elements appearing within a <defs> block have no effect on the\\ncurrent context, and behave as described above in the archival\\nsection).\\nThe appearance of a <brush> element in the ink markup sets\\nthe current brush attributes, leaving all other contextual values the\\nsame. Likewise, the appearance of a <traceFormat> element sets\\nthe current traceFormat, and the appearance of a <context>\\nelement sets the current context.\\nOutside of a <defs> block, any values which are not specified\\nwithin a <context> element are taken from the current context.\\nFor instance, the <context> element in the following example\\nchanges the current brush from \"penB\" to \"penA\", leaving the canvas,\\nmapping, and traceFormat unchanged from trace \"t001\" to trace\\n\"t002\".\\n\\n<brush id=\"penA\"/>\\n<brush id=\"penB\"/>\\n<trace id=\"t001\">...</trace>\\n<context brushRef=\"penA\"/>\\n<trace id=\"t002\">...</trace>\\n\\nIn order to change a contextual value back to its default value,\\nits attribute can be specified with the value \"\".  In the following:\\n\\n<context canvas=\"canvasA\" brushRef=\"penA\"/>\\n<trace id=\"t001\">...</trace>\\n<context canvas=\"\" brushRef=\"\"/>\\n<trace id=\"t002\">...</trace>\\n\\n trace \"t001\" is on \"canvasA\" and has the brush specified by\\n\"penA\", while trace \"t002\" is on the default canvas and has the\\ndefault brush.\\nBrushes, traceFormats, and contexts which appear outside of a\\n<defs> block and contain an id attribute both set the current\\ncontext and define contextual elements which can be reused (as shown\\nabove for the brushes \"penA\" and \"penB\"). This example:\\n\\n<context id=\"context1\" canvas=\"canvasA\" mapping=\"2 0 0 0 2 0\"\\ntraceFormatRef=\"fmt1\" brushRef=\"penA\"/>\\n\\ndefines a context which can be referred to by its identifier\\n\"context1\". It also sets the current context to the values specified\\nin the <context> element.\\nA previously defined context is referenced using the contextRef\\nattribute of the <context> element. For example:\\n\\n<context contextRef=\"context1\"/>\\n\\nsets the current context to have the values specified by\\n\"context1\". A <context> element can also override values of a\\npreviously defined context by including both a contextRef attribute\\nand canvas, mapping, traceFormatRef or brushRef attributes.  The\\nfollowing:\\n\\n<context contextRef=\"context1\" brushRef=\"penB\"/>\\n\\nsets the current context to the values specified by \"context1\",\\nexcept that the current brush is set to \"penB\" instead of \"penA\".\\nA <context> element which inherits and overrides values from\\na previous context can itself be reused, so the element:\\n\\n<context id=\"context2\" contextRef=\"context1\" brushRef=\"penB\"/>\\n\\ndefines \"context2\" which has the same context values as \"context1\"\\nexcept for the brush.\\nFinally, a <context> element with only an id has the effect\\nof taking a \"snapshot\" of the current context which can then be\\nreused. The element:\\n\\n<context id=\"context3\"/>\\n\\ndefines \"context3\", whose values consist of the current canvas,\\nmapping, traceFormat, and brush at the point where the element occurs\\n(note that since \"context3\" does not specify any values, the element\\nhas no effect on the current context).\\nAn advantage of the streaming style is that it is easier\\nto express overlapping changes to the individual elements of the\\ncontext. However, determining the context for a particular trace can\\nrequire more computation from the ink markup processor, since the\\nentire file may need to be scanned from the beginning in order\\nto establish the current context at the point of the <trace>\\nelement.\\n4.3 Archival and Streaming Equivalence\\nThe following examples of archival and streaming ink markup data are\\nequivalent, but they highlight the differences between the two styles:\\nArchival\\n\\n<ink>\\n  ...\\n  <defs>\\n    <brush id=\"penA\"/>\\n    <brush id=\"penB\"/>\\n    <context id=\"context1\" canvas=\"canvas1\"\\n             mapping=\"1 0 0 0 1 0\" traceFormatRef=\"format1\"/>\\n    <context id=\"context2\" contextRef=\"context1\"\\n             mapping=\"2 0 50 0 2 50\"/>\\n  </defs>\\n  <traceGroup contextRef=\"context1\">\\n    <trace>...</trace>\\n    ...\\n  </traceGroup>\\n  <traceGroup contextRef=\"context2\">\\n    <trace>...</trace>\\n    ...\\n  </traceGroup>\\n  <traceGroup contextRef=\"context2\" brushRef=\"penB\">\\n    <trace>...</trace>\\n    ...\\n  </traceGroup>\\n  <traceGroup contextRef=\"context1\" brushRef=\"penB\">\\n    <trace>...</trace>\\n    ...\\n  </traceGroup>\\n  <traceGroup contextRef=\"context1\" brushRef=\"penA\">\\n    <trace>...</trace>\\n    ...\\n </traceGroup>\\n</ink>\\n\\nStreaming\\n\\n<ink>\\n  ...\\n  <defs>\\n    <brush id=\"penA\"/>\\n    <brush id=\"penB\"/>\\n  </defs>\\n  <context id=\"context1\" canvas=\"canvas1\"\\n           mapping=\"1 0 0 0 1 0\" traceFormatRef=\"format1\"/>\\n  <trace>...</trace>\\n  ...\\n  <context id=\"context2\" contextRef=\"context1\"\\n           mapping=\"2 0 50 0 2 50\"/>\\n  <trace>...</trace>\\n  ...\\n  <context brushRef=\"penB\"/>\\n  <trace>...</trace>\\n  ...\\n  <context contextRef=\"context1\"/>\\n  <trace>...</trace>\\n  ...\\n  <context brushRef=\"penA\"/>\\n  <trace>...</trace>\\n  ...\\n</ink>\\n\\nIn the archival case, the context for each trace is simply\\ndetermined by the <trace> element, its enclosing traceGroup, and\\ncontextual elements defined in the <defs> block, while in the\\nstreaming case, the context for a trace can depend on the entire\\nsequence of context changes up to the point of the <trace>\\nelement.\\nHowever, the streaming case more simply expresses the changes of\\ncontext involving \"penB\", \"context1\", and \"penA\", whereas the archival\\ncase requires the restatement of the unchanged values in the\\nsuccessive traceGroups.\\nThe two styles of ink markup are equally expressive, but impose\\ndifferent requirements on the ink markup processor and generator. The\\nworking group is considering the usefulness of additional mechanisms\\nfor distinguishing between the two forms, such as separate profiles\\nfor archival and streaming ink markup. Tools to translate from\\nstreaming to archival style might also be of use to applications\\nwhich work on stored ink markup.\\n\\n5  Semantic Labelling and traceRefGroup\\nThe <traceRefGroup> element provides the basis for most\\nsemantic labelling of groups of traces. It should be used as the\\nbase class for all application specific elements that identify\\ncollections of traces.\\nThe <traceRefGroup> element has the\\nfollowing syntax:\\n\\n<traceRefGroup id=\"\" contentCategory=\"\">\\n    <traceref xpath=\"\">\\n    <traceref xpath=\"\" from=\"\" to=\"\">\\n    <traceRefGroup id=\"\">\\n        <!-- a nested traceRefGroup, which has\\n         attributes of all parent traceRefGroups -->\\n       ...\\n    </traceRefGroup>\\n</traceRefGroup>\\n\\nTraces listed within a <traceRefGroup> are included by\\nreference only. The xpath attribute of the <traceRef>\\nelement is used to refer to traces within the current document, or\\nfrom external documents. The from and to attributes can be used\\nto reference a (contiguous) subset of the points within a given\\ntrace.\\n<traceRefGroup> elements may also include other\\n<traceRefGroup> elements by reference. A\\n<traceRefGroup> element may be overlapping, i.e., a\\ntrace may be referenced in multiple groups.\\n<traceRefGroup> elements will typically be used\\neither to tag a group of traces for further processing, to tag a group\\nof traces with some metadata, or to provide a concise reference to a\\ngroup of traces for external use.\\n Open Issues\\nTODO: we intend to add a paragraph with more detail about using XPATH to identify groups of traces.\\n\\n5.1 contentCategory attribute\\nOne of the common attributes of <traceRefGroup>\\nwill be contentCategory, which describes at a basic level the\\ncategory of content that the traces represent; e.g., \"Text/English\",\\n\"Drawing\", \"Math\", \"Music\". Such categories are useful for general\\ndata identification purposes, and may be essential for selecting data\\nto train handwriting recognizers in different problem domains.\\nA number of likely, common categories are suggested below.\\nHowever, since this attribute:\\n\\nis largely application-specific\\nmay take on values that are difficult or impossible to predict\\nmay be a conjunction of more than one primitive type (e.g., \"Text/English and Graphics\")\\n\\nit is defined as a general-purpose string, to be used as necessary\\nby applications. If, however, the data fits conveniently into one\\nof the following basic categories, it is recommended that the\\nappropriate suggested category (and optional sub-category) be\\nused.\\nSuggested categories:\\n\\nText/<language>[/<script>][/<sub-category>] (e.g., Text/jpn/Kanji, Text/en/SSN)\\nDrawing[/<sub-category>] (e.g., Drawing/Sketch, Drawing/Diagram)\\nMath\\nMusic\\nChemistry[<sub-category>]\\n\\nThe language specification may be made using any of the language\\nidentifiers specified in\\nISO 639,\\nusing 2-letter codes, 3-letter codes, or country names. Some text\\nmay also require a script specification (such as Kanji, Katakana,\\nor Hiragana) in addition to the language.\\nFor some applications it may be useful to provide additional\\nsub-categories defining the type of the data.\\nSuggested sub-categories for Text:\\n\\nSSN (Social Security Number)\\nPhone\\nDate\\nTime\\nMoney\\nURL\\n\\nSuggested sub-categories for Drawing:\\n\\nSketch (Not suitable for geometric clean-up)\\nDiagram (Suitable for geometric clean-up)\\n\\n\\n\\n',\n",
       " \"\\n\\n\\n\\n\\nOptimal Selection of Partners in Agile Manufacturing\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n Next:  Background\\n  \\n\\n Optimal Selection of Partners in Agile Manufacturing\\n\\nPrincipal Investigator: Ioannis Minis\\nDepartment of Mechanical Engineering and \\nInstitute for Systems Research \\nUniversity of Maryland, College Park, Maryland 20742\\n\\nProject Team:\\nArun Candadai, M.E. and ISR\\nJeffrey W. Herrmann, M.E. and ISR \\nGiang Lam, M.E. and ISR\\nEdward Lin, ISR\\nRakesh Nagi, Dept. of Industrial Engineering, SUNY at Buffalo\\nVijay Ramanchandran, M.E. and ISR\\n\\n Abstract\\n\\n\\nThe Optimal Selection of Partners in Agile Manufacturing (OSPAM) project focuses on design evaluation and partner selection in distributed manufacturing.  It is being conducted at the Institute for Systems Research of the University of Maryland in cooperation with the State University of New York at Buffalo, Westinghouse ESG, Martin Marietta, Orlando and The National Institute of Standards and Technology.   The work is funded by the U.S. Army TACOM.\\n\\nThe focus of this project is the design for distributed manufacture of electronic products, such as microwave modules and hybrid assemblies.   The project is developing methods for \\ncritiquing the design of such products with respect to the production capabilities of potential partners,\\nalerting the designer to elements of the design that correspond to costly operations, and\\nselecting a good combination of partners that contribute to the manufacture of the design.\\nThus, we are developing a design advisor that integrates partnering (including make or buy) decisions in the design phase, and facilitates design improvements with respect to partner capabilities.\\n\\nThe research contributions of this project include the following:\\n\\n Information infrastructure:\\n\\n Developed a STEP-based product information model for Microwave Modules, and implemented of this model in an Object Oriented database.\\n Developed Object Oriented Group Technology (OOGT) codes for design retrieval and variant design critiquing.\\nDeveloped a manufacturing resource model to represent the process capabilities and historical performance of manufacturing firms.\\n\\n Decision support:\\n\\n Automated generation of a product's OOGT from its STEP information model.\\n Methods for variant design critiquing.\\n Methods for high-level process planning in distributed manufacturing.\\n Methods for plan-based design critiquing in distributed manufacturing.\\n Methods for multi-objective partner selection in distributed manufacturing.\\n\\n\\nThe following functional diagram will navigate the user through the OSPAM architecture.  Hypertext within the blocks representing the major modules of the system allows the user to access more detailed descriptions of these modules. (This functionality is still under construction and will be available soon.)\\n\\n\\n\\n\\n\\n\\n\\nIn the following, hypertext links pointing to the major modules of the system allow the user to access more detailed description of these modules.\\n\\n \\n\\n  Background\\n  Information Requirements\\n  Design Processing and Retrieval\\n  High-level Process Planning and Manufacturability Evaluation\\n  Selection of Manufacturing Partners\\n  Summary\\n References\\n    About this document ... \\n\\n \\nNote: Most of the contents on the OSPAM Web pages have been submitted to IIE Transaction for publishing.\\n \\n\\nEdward Lin \\nWednesday Nov. 14 15:01:45 EDT 1995\\n\\n\\n\",\n",
       " '\\n\\n\\n\\nCOMPUTING MACHINERY AND INTELLIGENCE\\n\\n\\n\\n\\n\\nCOMPUTING MACHINERY AND INTELLIGENCE\\n\\nBy A. M. Turing\\n\\n\\n1. The Imitation Game\\n\\nI propose to consider the question, \"Can machines\\nthink?\" This should begin with definitions of the meaning of the terms\\n\"machine\" and \"think.\" The definitions might be framed\\nso as to reflect so far as possible the normal use of the words, but this\\nattitude is dangerous, If the meaning of the words \"machine\"\\nand \"think\" are to be found by examining how they are commonly\\nused it is difficult to escape the conclusion that the meaning and the\\nanswer to the question, \"Can machines think?\" is to be sought\\nin a statistical survey such as a Gallup poll. But this is absurd. Instead\\nof attempting such a definition I shall replace the question by another,\\nwhich is closely related to it and is expressed in relatively unambiguous\\nwords.\\n\\nThe new form of the problem can be described in terms\\nof a game which we call the \\'imitation game.\" It is played with three\\npeople, a man (A), a woman (B), and an interrogator (C) who may be of either\\nsex. The interrogator stays in a room apart front the other two. The object\\nof the game for the interrogator is to determine which of the other two\\nis the man and which is the woman. He knows them by labels X and Y, and\\nat the end of the game he says either \"X is A and Y is B\" or\\n\"X is B and Y is A.\" The interrogator is allowed to put questions\\nto A and B thus:\\n\\nC: Will X please tell me the length of his or her hair?\\n\\nNow suppose X is actually A, then A must answer. It is\\nA\\'s object in the game to try and cause C to make the wrong identification.\\nHis answer might therefore be:\\n\\n\"My hair is shingled, and the longest strands are\\nabout nine inches long.\"\\n\\nIn order that tones of voice may not help the interrogator\\nthe answers should be written, or better still, typewritten. The ideal\\narrangement is to have a teleprinter communicating between the two rooms.\\nAlternatively the question and answers can be repeated by an intermediary.\\nThe object of the game for the third player (B) is to help the interrogator.\\nThe best strategy for her is probably to give truthful answers. She can\\nadd such things as \"I am the woman, don\\'t listen to him!\" to\\nher answers, but it will avail nothing as the man can make similar remarks.\\n\\n\\nWe now ask the question, \"What will happen when a\\nmachine takes the part of A in this game?\" Will the interrogator decide\\nwrongly as often when the game is played like this as he does when the\\ngame is played between a man and a woman? These questions replace our original,\\n\"Can machines think?\" \\n\\n2. Critique of the New Problem\\n\\nAs well as asking, \"What is the answer to this new\\nform of the question,\" one may ask, \"Is this new question a worthy\\none to investigate?\" This latter question we investigate without further\\nado, thereby cutting short an infinite regress.\\n\\nThe new problem has the advantage of drawing a fairly\\nsharp line between the physical and the intellectual capacities of a man.\\nNo engineer or chemist claims to be able to produce a material which is\\nindistinguishable from the human skin. It is possible that at some time\\nthis might be done, but even supposing this invention available we should\\nfeel there was little point in trying to make a \"thinking machine\"\\nmore human by dressing it up in such artificial flesh. The form in which\\nwe have set the problem reflects this fact in the condition which prevents\\nthe interrogator from seeing or touching the other competitors, or hearing\\n-their voices. Some other advantages of the proposed criterion may be shown\\nup by specimen questions and answers. Thus:\\n\\nQ: Please write me a sonnet on the subject of the Forth\\nBridge.\\n\\nA : Count me out on this one. I never could write poetry.\\n\\nQ: Add 34957 to 70764.\\n\\nA: (Pause about 30 seconds and then give as answer) 105621.\\n\\nQ: Do you play chess?\\n\\nA: Yes.\\n\\nQ: I have K at my K1, and no other pieces. You have only\\nK at K6 and R at R1. It is your move. What do you play? \\n\\nA: (After a pause of 15 seconds) R-R8 mate.\\n\\nThe question and answer method seems to be suitable for\\nintroducing almost any one of the fields of human endeavour that we wish\\nto include. We do not wish to penalise the machine for its inability to\\nshine in beauty competitions, nor to penalise a man for losing in a race\\nagainst an aeroplane. The conditions of our game make these disabilities\\nirrelevant. The \"witnesses\" can brag, if they consider it advisable,\\nas much as they please about their charms, strength or heroism, but the\\ninterrogator cannot demand practical demonstrations.\\n\\nThe game may perhaps be criticised on the ground that\\nthe odds are weighted too heavily against the machine. If the man were\\nto try and pretend to be the machine he would clearly make a very poor\\nshowing. He would be given away at once by slowness and inaccuracy in arithmetic.\\nMay not machines carry out something which ought to be described as thinking\\nbut which is very different from what a man does? This objection is a very\\nstrong one, but at least we can say that if, nevertheless, a machine can\\nbe constructed to play the imitation game satisfactorily, we need not be\\ntroubled by this objection.\\n\\nIt might be urged that when playing the \"imitation\\ngame\" the best strategy for the machine may possibly be something\\nother than imitation of the behaviour of a man. This may be, but I think\\nit is unlikely that there is any great effect of this kind. In any case\\nthere is no intention to investigate here the theory of the game, and it\\nwill be assumed that the best strategy is to try to provide answers that\\nwould naturally be given by a man.\\n\\n3. The Machines Concerned in the Game\\n\\nThe question which we put in 1 will not be quite definite\\nuntil we have specified what we mean by the word \"machine.\" It\\nis natural that we should wish to permit every kind of engineering technique\\nto be used in our machines. We also wish to allow the possibility than\\nan engineer or team of engineers may construct a machine which works, but\\nwhose manner of operation cannot be satisfactorily described by its constructors\\nbecause they have applied a method which is largely experimental. Finally,\\nwe wish to exclude from the machines men born in the usual manner. It is\\ndifficult to frame the definitions so as to satisfy these three conditions.\\nOne might for instance insist that the team of engineers should be all\\nof one sex, but this would not really be satisfactory, for it is probably\\npossible to rear a complete individual from a single cell of the skin (say)\\nof a man. To do so would be a feat of biological technique deserving of\\nthe very highest praise, but we would not be inclined to regard it as a\\ncase of \"constructing a thinking machine.\" This prompts us to\\nabandon the requirement that every kind of technique should be permitted.\\nWe are the more ready to do so in view of the fact that the present interest\\nin \"thinking machines\" has been aroused by a particular kind\\nof machine, usually called an \"electronic computer\" or \"digital\\ncomputer.\" Following this suggestion we only permit digital computers\\nto take part in our game.\\n\\nThis restriction appears at first sight to be a very drastic\\none. I shall attempt to show that it is not so in reality. To do this necessitates\\na short account of the nature and properties of these computers.\\n\\nIt may also be said that this identification of machines\\nwith digital computers, like our criterion for \"thinking,\" will\\nonly be unsatisfactory if (contrary to my belief), it turns out that digital\\ncomputers are unable to give a good showing in the game.\\n\\nThere are already a number of digital computers in working\\norder, and it may be asked, \"Why not try the experiment straight away?\\nIt would be easy to satisfy the conditions of the game. A number of interrogators\\ncould be used, and statistics compiled to show how often the right identification\\nwas given.\" The short answer is that we are not asking whether all\\ndigital computers would do well in the game nor whether the computers at\\npresent available would do well, but whether there are imaginable computers\\nwhich would do well. But this is only the short answer. We shall see this\\nquestion in a different light later.\\n\\n4. Digital Computers\\n\\nThe idea behind digital computers may be explained by\\nsaying that these machines are intended to carry out any operations which\\ncould be done by a human computer. The human computer is supposed to be\\nfollowing fixed rules; he has no authority to deviate from them in any\\ndetail. We may suppose that these rules are supplied in a book, which is\\naltered whenever he is put on to a new job. He has also an unlimited supply\\nof paper on which he does his calculations. He may also do his multiplications\\nand additions on a \"desk machine,\" but this is not important.\\n\\nIf we use the above explanation as a definition we shall\\nbe in danger of circularity of argument. We avoid this by giving an outline.\\nof the means by which the desired effect is achieved. A digital computer\\ncan usually be regarded as consisting of three parts:\\n\\n(i) Store. \\n(ii) Executive unit. \\n(iii) Control.\\n\\nThe store is a store of information, and corresponds to\\nthe human computer\\'s paper, whether this is the paper on which he does\\nhis calculations or that on which his book of rules is printed. In so far\\nas the human computer does calculations in his bead a part of the store\\nwill correspond to his memory.\\n\\nThe executive unit is the part which carries out the various\\nindividual operations involved in a calculation. What these individual\\noperations are will vary from machine to machine. Usually fairly lengthy\\noperations can be done such as \"Multiply 3540675445 by 7076345687\"\\nbut in some machines only very simple ones such as \"Write down 0\"\\nare possible. \\n\\nWe have mentioned that the \"book of rules\" supplied\\nto the computer is replaced in the machine by a part of the store. It is\\nthen called the \"table of instructions.\" It is the duty of the\\ncontrol to see that these instructions are obeyed correctly and in the\\nright order. The control is so constructed that this necessarily happens.\\n\\nThe information in the store is usually broken up into\\npackets of moderately small size. In one machine, for instance, a packet\\nmight consist of ten decimal digits. Numbers are assigned to the parts\\nof the store in which the various packets of information are stored, in\\nsome systematic manner. A typical instruction might say-\\n\\n\"Add the number stored in position 6809 to that in\\n4302 and put the result back into the latter storage position.\" \\n\\nNeedless to say it would not occur in the machine expressed\\nin English. It would more likely be coded in a form such as 6809430217.\\nHere 17 says which of various possible operations is to be performed on\\nthe two numbers. In this case the)e operation is that described above,\\nviz., \"Add the number. . . .\" It will be noticed that the instruction\\ntakes up 10 digits and so forms one packet of information, very conveniently.\\nThe control will normally take the instructions to be obeyed in the order\\nof the positions in which they are stored, but occasionally an instruction\\nsuch as\\n\\n\"Now obey the instruction stored in position 5606,\\nand continue from there\"\\n\\nmay be encountered, or again\\n\\n\"If position 4505 contains 0 obey next the instruction\\nstored in 6707, otherwise continue straight on.\"\\n\\nInstructions of these latter types are very important\\nbecause they make it possible for a sequence of operations to be replaced\\nover and over again until some condition is fulfilled, but in doing so\\nto obey, not fresh instructions on each repetition, but the same ones over\\nand over again. To take a domestic analogy. Suppose Mother wants Tommy\\nto call at the cobbler\\'s every morning on his way to school to see if her\\nshoes are done, she can ask him afresh every morning. Alternatively she\\ncan stick up a notice once and for all in the hall which he will see when\\nhe leaves for school and which tells him to call for the shoes, and also\\nto destroy the notice when he comes back if he has the shoes with him.\\n\\nThe reader must accept it as a fact that digital computers\\ncan be constructed, and indeed have been constructed, according to the\\nprinciples we have described, and that they can in fact mimic the actions\\nof a human computer very closely.\\n\\nThe book of rules which we have described our human computer\\nas using is of course a convenient fiction. Actual human computers really\\nremember what they have got to do. If one wants to make a machine mimic\\nthe behaviour of the human computer in some complex operation one has to\\nask him how it is done, and then translate the answer into the form of\\nan instruction table. Constructing instruction tables is usually described\\nas \"programming.\" To \"programme a machine to carry out the\\noperation A\" means to put the appropriate instruction table into the\\nmachine so that it will do A.\\n\\nAn interesting variant on the idea of a digital computer\\nis a \"digital computer with a random element.\" These have instructions\\ninvolving the throwing of a die or some equivalent electronic process;\\none such instruction might for instance be, \"Throw the die and put\\nthe-resulting number into store 1000.\" Sometimes such a machine is\\ndescribed as having free will (though I would not use this phrase myself),\\nIt is not normally possible to determine from observing a machine whether\\nit has a random element, for a similar effect can be produced by such devices\\nas making the choices depend on the digits of the decimal for .\\n\\nMost actual digital computers have only a finite store.\\nThere is no theoretical difficulty in the idea of a computer with an unlimited\\nstore. Of course only a finite part can have been used at any one time.\\nLikewise only a finite amount can have been constructed, but we can imagine\\nmore and more being added as required. Such computers have special theoretical\\ninterest and will be called infinitive capacity computers.\\n\\nThe idea of a digital computer is an old one. Charles\\nBabbage, Lucasian Professor of Mathematics at Cambridge from 1828 to 1839,\\nplanned such a machine, called the Analytical Engine, but it was never\\ncompleted. Although Babbage had all the essential ideas, his machine was\\nnot at that time such a very attractive prospect. The speed which would\\nhave been available would be definitely faster than a human computer but\\nsomething like I 00 times slower than the Manchester machine, itself one\\nof the slower of the modern machines, The storage was to be purely mechanical,\\nusing wheels and cards.\\n\\nThe fact that Babbage\\'s Analytical Engine was to be entirely\\nmechanical will help us to rid ourselves of a superstition. Importance\\nis often attached to the fact that modern digital computers are electrical,\\nand that the nervous system also is electrical. Since Babbage\\'s machine\\nwas not electrical, and since all digital computers are in a sense equivalent,\\nwe see that this use of electricity cannot be of theoretical importance.\\nOf course electricity usually comes in where fast signalling is concerned,\\nso that it is not surprising that we find it in both these connections.\\nIn the nervous system chemical phenomena are at least as important as electrical.\\nIn certain computers the storage system is mainly acoustic. The feature\\nof using electricity is thus seen to be only a very superficial similarity.\\nIf we wish to find such similarities we should took rather for mathematical\\nanalogies of function. \\n\\n5. Universality of Digital Computers\\n\\nThe digital computers considered in the last section may\\nbe classified amongst the \"discrete-state machines.\" These are\\nthe machines which move by sudden jumps or clicks from one quite definite\\nstate to another. These states are sufficiently different for the possibility\\nof confusion between them to be ignored. Strictly speaking there, are no\\nsuch machines. Everything really moves continuously. But there are many\\nkinds of machine which can profitably be thought of as being discrete-state\\nmachines. For instance in considering the switches for a lighting system\\nit is a convenient fiction that each switch must be definitely on or definitely\\noff. There must be intermediate positions, but for most purposes we can\\nforget about them. As an example of a discrete-state machine we might consider\\na wheel which clicks round through 120 once a second, but may be stopped\\nby a ]ever which can be operated from outside; in addition a lamp is to\\nlight in one of the positions of the wheel. This machine could be described\\nabstractly as follows. The internal state of the machine (which is described\\nby the position of the wheel) may be q1, q2 or q3.\\nThere is an input signal i0. or i1 (position of ]ever). The internal state\\nat any moment is determined by the last state and input signal according\\nto the table\\n\\n(TABLE DELETED)\\n\\nThe output signals, the only externally visible indication\\nof the internal state (the light) are described by the table\\n\\nState q1 q2 q3\\n\\noutput o0 o0 o1\\n\\nThis example is typical of discrete-state machines. They\\ncan be described by such tables provided they have only a finite number\\nof possible states.\\n\\nIt will seem that given the initial state of the machine\\nand the input signals it is always possible to predict all future states,\\nThis is reminiscent of Laplace\\'s view that from the complete state of the\\nuniverse at one moment of time, as described by the positions and velocities\\nof all particles, it should be possible to predict all future states. The\\nprediction which we are considering is, however, rather nearer to practicability\\nthan that considered by Laplace. The system of the \"universe as a\\nwhole\" is such that quite small errors in the initial conditions can\\nhave an overwhelming effect at a later time. The displacement of a single\\nelectron by a billionth of a centimetre at one moment might make the difference\\nbetween a man being killed by an avalanche a year later, or escaping. It\\nis an essential property of the mechanical systems which we have called\\n\"discrete-state machines\" that this phenomenon does not occur.\\nEven when we consider the actual physical machines instead of the idealised\\nmachines, reasonably accurate knowledge of the state at one moment yields\\nreasonably accurate knowledge any number of steps later. \\n\\nAs we have mentioned, digital computers fall within the\\nclass of discrete-state machines. But the number of states of which such\\na machine is capable is usually enormously large. For instance, the number\\nfor the machine now working at Manchester is about 2 165,000,\\ni.e., about 10 50,000. Compare this with our example of the\\nclicking wheel described above, which had three states. It is not difficult\\nto see why the number of states should be so immense. The computer includes\\na store corresponding to the paper used by a human computer. It must be\\npossible to write into the store any one of the combinations of symbols\\nwhich might have been written on the paper. For simplicity suppose that\\nonly digits from 0 to 9 are used as symbols. Variations in handwriting\\nare ignored. Suppose the computer is allowed 100 sheets of paper each containing\\n50 lines each with room for 30 digits. Then the number of states is 10\\n100x50x30 i.e., 10 150,000 . This is about the number\\nof states of three Manchester machines put together. The logarithm to the\\nbase two of the number of states is usually called the \"storage capacity\"\\nof the machine. Thus the Manchester machine has a storage capacity of about\\n165,000 and the wheel machine of our example about 1.6. If two machines\\nare put together their capacities must be added to obtain the capacity\\nof the resultant machine. This leads to the possibility of statements such\\nas \"The Manchester machine contains 64 magnetic tracks each with a\\ncapacity of 2560, eight electronic tubes with a capacity of 1280. Miscellaneous\\nstorage amounts to about 300 making a total of 174,380.\" \\n\\nGiven the table corresponding to a discrete-state machine\\nit is possible to predict what it will do. There is no reason why this\\ncalculation should not be carried out by means of a digital computer. Provided\\nit could be carried out sufficiently quickly the digital computer could\\nmimic the behavior of any discrete-state machine. The imitation game could\\nthen be played with the machine in question (as B) and the mimicking digital\\ncomputer (as A) and the interrogator would be unable to distinguish them.\\nOf course the digital computer must have an adequate storage capacity as\\nwell as working sufficiently fast. Moreover, it must be programmed afresh\\nfor each new machine which it is desired to mimic.\\n\\nThis special property of digital computers, that they\\ncan mimic any discrete-state machine, is described by saying that they\\nare universal machines. The existence of machines with this property has\\nthe important consequence that, considerations of speed apart, it is unnecessary\\nto design various new machines to do various computing processes. They\\ncan all be done with one digital computer, suitably programmed for each\\ncase. It \\'ill be seen that as a consequence of this all digital computers\\nare in a sense equivalent.\\n\\nWe may now consider again the point raised at the end\\nof §3. It was suggested tentatively that the question, \"Can machines\\nthink?\" should be replaced by \"Are there imaginable digital computers\\nwhich would do well in the imitation game?\" If we wish we can make\\nthis superficially more general and ask \"Are there discrete-state\\nmachines which would do well?\" But in view of the universality property\\nwe see that either of these questions is equivalent to this, \"Let\\nus fix our attention on one particular digital computer C. Is it true that\\nby modifying this computer to have an adequate storage, suitably increasing\\nits speed of action, and providing it with an appropriate programme, C\\ncan be made to play satisfactorily the part of A in the imitation game,\\nthe part of B being taken by a man?\"\\n\\n6. Contrary Views on the Main Question\\n\\nWe may now consider the ground to have been cleared and\\nwe are ready to proceed to the debate on our question, \"Can machines\\nthink?\" and the variant of it quoted at the end of the last section.\\nWe cannot altogether abandon the original form of the problem, for opinions\\nwill differ as to the appropriateness of the substitution and we must at\\nleast listen to what has to be said in this connexion.\\n\\nIt will simplify matters for the reader if I explain first\\nmy own beliefs in the matter. Consider first the more accurate form of\\nthe question. I believe that in about fifty years\\' time it will be possible,\\nto programme computers, with a storage capacity of about 109, to make them\\nplay the imitation game so well that an average interrogator will not have\\nmore than 70 per cent chance of making the right identification after five\\nminutes of questioning. The original question, \"Can machines think?\"\\nI believe to be too meaningless to deserve discussion. Nevertheless I believe\\nthat at the end of the century the use of words and general educated opinion\\nwill have altered so much that one will be able to speak of machines thinking\\nwithout expecting to be contradicted. I believe further that no useful\\npurpose is served by concealing these beliefs. The popular view that scientists\\nproceed inexorably from well-established fact to well-established fact,\\nnever being influenced by any improved conjecture, is quite mistaken. Provided\\nit is made clear which are proved facts and which are conjectures, no harm\\ncan result. Conjectures are of great importance since they suggest useful\\nlines of research. \\n\\nI now proceed to consider opinions opposed to my own.\\n\\n(1) The Theological Objection\\n\\nThinking is a function of man\\'s immortal soul. God has\\ngiven an immortal soul to every man and woman, but not to any other animal\\nor to machines. Hence no animal or machine can think.\\n\\nI am unable to accept any part of this, but will attempt\\nto reply in theological terms. I should find the argument more convincing\\nif animals were classed with men, for there is a greater difference, to\\nmy mind, between the typical animate and the inanimate than there is between\\nman and the other animals. The arbitrary character of the orthodox view\\nbecomes clearer if we consider how it might appear to a member of some\\nother religious community. How do Christians regard the Moslem view that\\nwomen have no souls? But let us leave this point aside and return to the\\nmain argument. It appears to me that the argument quoted above implies\\na serious restriction of the omnipotence of the Almighty. It is admitted\\nthat there are certain things that He cannot do such as making one equal\\nto two, but should we not believe that He has freedom to confer a soul\\non an elephant if He sees fit? We might expect that He would only exercise\\nthis power in conjunction with a mutation which provided the elephant with\\nan appropriately improved brain to minister to the needs of this sort[.\\nAn argument of exactly similar form may be made for the case of machines.\\nIt may seem different because it is more difficult to \"swallow.\"\\nBut this really only means that we think it would be less likely that He\\nwould consider the circumstances suitable for conferring a soul. The circumstances\\nin question are discussed in the rest of this paper. In attempting to construct\\nsuch machines we should not be irreverently usurping His power of creating\\nsouls, any more than we are in the procreation of children: rather we are,\\nin either case, instruments of His will providing .mansions for the souls\\nthat He creates.\\n\\nHowever, this is mere speculation. I am not very impressed\\nwith theological arguments whatever they may be used to support. Such arguments\\nhave often been found unsatisfactory in the past. In the time of Galileo\\nit was argued that the texts, \"And the sun stood still . . . and hasted\\nnot to go down about a whole day\" (Joshua x. 13) and \"He laid\\nthe foundations of the earth, that it should not move at any time\"\\n(Psalm cv. 5) were an adequate refutation of the Copernican theory. With\\nour present knowledge such an argument appears futile. When that knowledge\\nwas not available it made a quite different impression. \\n\\n(2) The \"Heads in the Sand\" Objection \\n\\nThe consequences of machines thinking would be too dreadful.\\nLet us hope and believe that they cannot do so.\" \\n\\nThis argument is seldom expressed quite so openly as in\\nthe form above. But it affects most of us who think about it at all. We\\nlike to believe that Man is in some subtle way superior to the rest of\\ncreation. It is best if he can be shown to be necessarily superior, for\\nthen there is no danger of him losing his commanding position. The popularity\\nof the theological argument is clearly connected with this feeling. It\\nis likely to be quite strong in intellectual people, since they value the\\npower of thinking more highly than others, and are more inclined to base\\ntheir belief in the superiority of Man on this power. \\n\\nI do not think that this argument is sufficiently substantial\\nto require refutation. Consolation would be more appropriate: perhaps this\\nshould be sought in the transmigration of souls.\\n\\n(3) The Mathematical Objection\\n\\nThere are a number of results of mathematical logic which\\ncan be used to show that there are limitations to the powers of discrete-state\\nmachines. The best known of these results is known as Godel\\'s theorem (\\n1931 ) and shows that in any sufficiently powerful logical system statements\\ncan be formulated which can neither be proved nor disproved within the\\nsystem, unless possibly the system itself is inconsistent. There are other,\\nin some respects similar, results due to Church (1936), Kleene (1935),\\nRosser, and Turing (1937). The latter result is the most convenient to\\nconsider, since it refers directly to machines, whereas the others can\\nonly be used in a comparatively indirect argument: for instance if Godel\\'s\\ntheorem is to be used we need in addition to have some means of describing\\nlogical systems in terms of machines, and machines in terms of logical\\nsystems. The result in question refers to a type of machine which is essentially\\na digital computer with an infinite capacity. It states that there are\\ncertain things that such a machine cannot do. If it is rigged up to give\\nanswers to questions as in the imitation game, there will be some questions\\nto which it will either give a wrong answer, or fail to give an answer\\nat all however much time is allowed for a reply. There may, of course,\\nbe many such questions, and questions which cannot be answered by one machine\\nmay be satisfactorily answered by another. We are of course supposing for\\nthe present that the questions are of the kind to which an answer \"Yes\"\\nor \"No\" is appropriate, rather than questions such as \"What\\ndo you think of Picasso?\" The questions that we know the machines\\nmust fail on are of this type, \"Consider the machine specified as\\nfollows. . . . Will this machine ever answer \\'Yes\\' to any question?\"\\nThe dots are to be replaced by a description of some machine in a standard\\nform, which could be something like that used in §5. When the machine\\ndescribed bears a certain comparatively simple relation to the machine\\nwhich is under interrogation, it can be shown that the answer is either\\nwrong or not forthcoming. This is the mathematical result: it is argued\\nthat it proves a disability of machines to which the human intellect is\\nnot subject.\\n\\nThe short answer to this argument is that although it\\nis established that there are limitations to the Powers If any particular\\nmachine, it has only been stated, without any sort of proof, that no such\\nlimitations apply to the human intellect. But I do not think this view\\ncan be dismissed quite so lightly. Whenever one of these machines is asked\\nthe appropriate critical question, and gives a definite answer, we know\\nthat this answer must be wrong, and this gives us a certain feeling of\\nsuperiority. Is this feeling illusory? It is no doubt quite genuine, but\\nI do not think too much importance should be attached to it. We too often\\ngive wrong answers to questions ourselves to be justified in being very\\npleased at such evidence of fallibility on the part of the machines. Further,\\nour superiority can only be felt on such an occasion in relation to the\\none machine over which we have scored our petty triumph. There would be\\nno question of triumphing simultaneously over all machines. In short, then,\\nthere might be men cleverer than any given machine, but then again there\\nmight be other machines cleverer again, and so on. \\nThose who hold to the mathematical argument would, I think,\\nmostly he willing to accept the imitation game as a basis for discussion,\\nThose who believe in the two previous objections would probably not be\\ninterested in any criteria.\\n\\n(4) The Argument from Consciousness\\n\\nThis argument is very, well expressed in Professor Jefferson\\'s\\nLister Oration for 1949, from which I quote. \"Not until a machine\\ncan write a sonnet or compose a concerto because of thoughts and emotions\\nfelt, and not by the chance fall of symbols, could we agree that machine\\nequals brain-that is, not only write it but know that it had written it.\\nNo mechanism could feel (and not merely artificially signal, an easy contrivance)\\npleasure at its successes, grief when its valves fuse, be warmed by flattery,\\nbe made miserable by its mistakes, be charmed by sex, be angry or depressed\\nwhen it cannot get what it wants.\"\\n\\nThis argument appears to be a denial of the validity of\\nour test. According to the most extreme form of this view the only way\\nby which one could be sure that machine thinks is to be the machine and\\nto feel oneself thinking. One could then describe these feelings to the\\nworld, but of course no one would be justified in taking any notice. Likewise\\naccording to this view the only way to know that a man thinks is to be\\nthat particular man. It is in fact the solipsist point of view. It may\\nbe the most logical view to hold but it makes communication of ideas difficult.\\nA is liable to believe \"A thinks but B does not\" whilst B believes\\n\"B thinks but A does not.\" instead of arguing continually over\\nthis point it is usual to have the polite convention that everyone thinks.\\n\\nI am sure that Professor Jefferson does not wish to adopt\\nthe extreme and solipsist point of view. Probably he would be quite willing\\nto accept the imitation game as a test. The game (with the player B omitted)\\nis frequently used in practice under the name of viva voce to discover\\nwhether some one really understands something or has \"learnt it parrot\\nfashion.\" Let us listen in to a part of such a viva voce:\\n\\nInterrogator: In the first line of your sonnet which reads\\n\"Shall I compare thee to a summer\\'s day,\" would not \"a spring\\nday\" do as well or better?\\n\\nWitness: It wouldn\\'t scan.\\n\\nInterrogator: How about \"a winter\\'s day,\" That\\nwould scan all right.\\n\\nWitness: Yes, but nobody wants to be compared to a winter\\'s\\nday.\\n\\nInterrogator: Would you say Mr. Pickwick reminded you\\nof Christmas?\\n\\nWitness: In a way.\\n\\nInterrogator: Yet Christmas is a winter\\'s day, and I do\\nnot think Mr. Pickwick would mind the comparison.\\n\\nWitness: I don\\'t think you\\'re serious. By a winter\\'s day\\none means a typical winter\\'s day, rather than a special one like Christmas.\\n\\nAnd so on, What would Professor Jefferson say if the sonnet-writing\\nmachine was able to answer like this in the viva voce? I do not\\nknow whether he would regard the machine as \"merely artificially signalling\"\\nthese answers, but if the answers were as satisfactory and sustained as\\nin the above passage I do not think he would describe it as \"an easy\\ncontrivance.\" This phrase is, I think, intended to cover such devices\\nas the inclusion in the machine of a record of someone reading a sonnet,\\nwith appropriate switching to turn it on from time to time.\\n\\nIn short then, I think that most of those who support\\nthe argument from consciousness could be persuaded to abandon it rather\\nthan be forced into the solipsist position. They will then probably be\\nwilling to accept our test.\\n\\nI do not wish to give the impression that I think there\\nis no mystery about consciousness. There is, for instance, something of\\na paradox connected with any attempt to localise it. But I do not think\\nthese mysteries necessarily need to be solved before we can answer the\\nquestion with which we are concerned in this paper.\\n\\n(5) Arguments from Various Disabilities\\n\\nThese arguments take the form, \"I grant you that\\nyou can make machines do all the things you have mentioned but you will\\nnever be able to make one to do X.\" Numerous features X are suggested\\nin this connexion I offer a selection: \\n\\nBe kind, resourceful, beautiful, friendly, have initiative,\\nhave a sense of humour, tell right from wrong, make mistakes, fall in love,\\nenjoy strawberries and cream, make some one fall in love with it, learn\\nfrom experience, use words properly, be the subject of its own thought,\\nhave as much diversity of behaviour as a man, do something really new.\\n\\nNo support is usually offered for these statements. I\\nbelieve they are mostly founded on the principle of scientific induction.\\nA man has seen thousands of machines in his lifetime. From what he sees\\nof them he draws a number of general conclusions. They are ugly, each is\\ndesigned for a very limited purpose, when required for a minutely different\\npurpose they are useless, the variety of behaviour of any one of them is\\nvery small, etc., etc. Naturally he concludes that these are necessary\\nproperties of machines in general. Many of these limitations are associated\\nwith the very small storage capacity of most machines. (I am assuming that\\nthe idea of storage capacity is extended in some way to cover machines\\nother than discrete-state machines. The exact definition does not matter\\nas no mathematical accuracy is claimed in the present discussion,) A few\\nyears ago, when very little had been heard of digital computers, it was\\npossible to elicit much incredulity concerning them, if one mentioned their\\nproperties without describing their construction. That was presumably due\\nto a similar application of the principle of scientific induction. These\\napplications of the principle are of course largely unconscious. When a\\nburnt child fears the fire and shows that he fears it by avoiding it, f\\nshould say that he was applying scientific induction. (I could of course\\nalso describe his behaviour in many other ways.) The works and customs\\nof mankind do not seem to be very suitable material to which to apply scientific\\ninduction. A very large part of space-time must be investigated, if reliable\\nresults are to be obtained. Otherwise we may (as most English \\'Children\\ndo) decide that everybody speaks English, and that it is silly to learn\\nFrench. \\n\\nThere are, however, special remarks to be made about many\\nof the disabilities that have been mentioned. The inability to enjoy strawberries\\nand cream may have struck the reader as frivolous. Possibly a machine might\\nbe made to enjoy this delicious dish, but any attempt to make one do so\\nwould be idiotic. What is important about this disability is that it contributes\\nto some of the other disabilities, e.g., to the difficulty of the same\\nkind of friendliness occurring between man and machine as between white\\nman and white man, or between black man and black man.\\n\\nThe claim that \"machines cannot make mistakes\"\\nseems a curious one. One is tempted to retort, \"Are they any the worse\\nfor that?\" But let us adopt a more sympathetic attitude, and try to\\nsee what is really meant. I think this criticism can be explained in terms\\nof the imitation game. It is claimed that the interrogator could distinguish\\nthe machine from the man simply by setting them a number of problems in\\narithmetic. The machine would be unmasked because of its deadly accuracy.\\nThe reply to this is simple. The machine (programmed for playing the game)\\nwould not attempt to give the right answers to the arithmetic problems.\\nIt would deliberately introduce mistakes in a manner calculated to confuse\\nthe interrogator. A mechanical fault would probably show itself through\\nan unsuitable decision as to what sort of a mistake to make in the arithmetic.\\nEven this interpretation of the criticism is not sufficiently sympathetic.\\nBut we cannot afford the space to go into it much further. It seems to\\nme that this criticism depends on a confusion between two kinds of mistake,\\nWe may call them \"errors of functioning\" and \"errors of\\nconclusion.\" Errors of functioning are due to some mechanical or electrical\\nfault which causes the machine to behave otherwise than it was designed\\nto do. In philosophical discussions one likes to ignore the possibility\\nof such errors; one is therefore discussing \"abstract machines.\"\\nThese abstract machines are mathematical fictions rather than physical\\nobjects. By definition they are incapable of errors of functioning. In\\nthis sense we can truly say that \"machines can never make mistakes.\"\\nErrors of conclusion can only arise when some meaning is attached to the\\noutput signals from the machine. The machine might, for instance, type\\nout mathematical equations, or sentences in English. When a false proposition\\nis typed we say that the machine has committed an error of conclusion.\\nThere is clearly no reason at all for saying that a machine cannot make\\nthis kind of mistake. It might do nothing but type out repeatedly \"O\\n= I.\" To take a less perverse example, it might have some method for\\ndrawing conclusions by scientific induction. We must expect such a method\\nto lead occasionally to erroneous results.\\n\\nThe claim that a machine cannot be the subject of its\\nown thought can of course only be answered if it can be shown that the\\nmachine has some thought with some subject matter. Nevertheless, \"the\\nsubject matter of a machine\\'s operations\" does seem to mean something,\\nat least to the people who deal with it. If, for instance, the machine\\nwas trying to find a solution of the equation x2 - 40x - 11 = 0 one would\\nbe tempted to describe this equation as part of the machine\\'s subject matter\\nat that moment. In this sort of sense a machine undoubtedly can be its\\nown subject matter. It may be used to help in making up its own programmes,\\nor to predict the effect of alterations in its own structure. By observing\\nthe results of its own behaviour it can modify its own programmes so as\\nto achieve some purpose more effectively. These are possibilities of the\\nnear future, rather than Utopian dreams.\\n\\nThe criticism that a machine cannot have much diversity\\nof behaviour is just a way of saying that it cannot have much storage capacity.\\nUntil fairly recently a storage capacity of even a thousand digits was\\nvery rare. \\n\\nThe criticisms that we are considering here are often\\ndisguised forms of the argument from consciousness, Usually if one maintains\\nthat a machine can do one of these things, and describes the kind of method\\nthat the machine could use, one will not make much of an impression. It\\nis thought that tile method (whatever it may be, for it must be mechanical)\\nis really rather base. Compare the parentheses in Jefferson\\'s statement\\nquoted on page 22.\\n\\n(6) Lady Lovelace\\'s Objection\\n\\nOur most detailed information of Babbage\\'s Analytical\\nEngine comes from a memoir by Lady Lovelace ( 1842). In it she states,\\n\"The Analytical Engine has no pretensions to originate anything.\\nIt can do whatever we know how to order it to perform\" (her\\nitalics). This statement is quoted by Hartree ( 1949) who adds: \"This\\ndoes not imply that it may not be possible to construct electronic equipment\\nwhich will \\'think for itself,\\' or in which, in biological terms, one could\\nset up a conditioned reflex, which would serve as a basis for \\'learning.\\'\\nWhether this is possible in principle or not is a stimulating and exciting\\nquestion, suggested by some of these recent developments But it did not\\nseem that the machines constructed or projected at the time had this property.\"\\n\\nI am in thorough agreement with Hartree over this. It\\nwill be noticed that he does not assert that the machines in question had\\nnot got the property, but rather that the evidence available to Lady Lovelace\\ndid not encourage her to believe that they had it. It is quite possible\\nthat the machines in question had in a sense got this property. For suppose\\nthat some discrete-state machine has the property. The Analytical Engine\\nwas a universal digital computer, so that, if its storage capacity and\\nspeed were adequate, it could by suitable programming be made to mimic\\nthe machine in question. Probably this argument did not occur to the Countess\\nor to Babbage. In any case there was no obligation on them to claim all\\nthat could be claimed.\\n\\nThis whole question will be considered again under the\\nheading of learning machines.\\n\\nA variant of Lady Lovelace\\'s objection states that a machine\\ncan \"never do anything really new.\" This may be parried for a\\nmoment with the saw, \"There is nothing new under the sun.\" Who\\ncan be certain that \"original work\" that he has done was not\\nsimply the growth of the seed planted in him by teaching, or the effect\\nof following well-known general principles. A better variant of the objection\\nsays that a machine can never \"take us by surprise.\" This statement\\nis a more direct challenge and can be met directly. Machines take me by\\nsurprise with great frequency. This is largely because I do not do sufficient\\ncalculation to decide what to expect them to do, or rather because, although\\nI do a calculation, I do it in a hurried, slipshod fashion, taking risks.\\nPerhaps I say to myself, \"I suppose the Voltage here ought to he the\\nsame as there: anyway let\\'s assume it is.\" Naturally I am often wrong,\\nand the result is a surprise for me for by the time the experiment is done\\nthese assumptions have been forgotten. These admissions lay me open to\\nlectures on the subject of my vicious ways, but do not throw any doubt\\non my credibility when I testify to the surprises I experience.\\n\\nI do not expect this reply to silence my critic. He will\\nprobably say that h surprises are due to some creative mental act on my\\npart, and reflect no credit on the machine. This leads us back to the argument\\nfrom consciousness, and far from the idea of surprise. It is a line of\\nargument we must consider closed, but it is perhaps worth remarking that\\nthe appreciation of something as surprising requires as much of a \"creative\\nmental act\" whether the surprising event originates from a man, a\\nbook, a machine or anything else.\\n\\nThe view that machines cannot give rise to surprises is\\ndue, I believe, to a fallacy to which philosophers and mathematicians are\\nparticularly subject. This is the assumption that as soon as a fact is\\npresented to a mind all consequences of that fact spring into the mind\\nsimultaneously with it. It is a very useful assumption under many circumstances,\\nbut one too easily forgets that it is false. A natural consequence of doing\\nso is that one then assumes that there is no virtue in the mere working\\nout of consequences from data and general principles.\\n\\n(7) Argument from Continuity in the Nervous System\\n\\n\\nThe nervous system is certainly not a discrete-state machine.\\nA small error in the information about the size of a nervous impulse impinging\\non a neuron, may make a large difference to the size of the outgoing impulse.\\nIt may be argued that, this being so, one cannot expect to be able to mimic\\nthe behaviour of the nervous system with a discrete-state system.\\n\\nIt is true that a discrete-state machine must be different\\nfrom a continuous machine. But if we adhere to the conditions of the imitation\\ngame, the interrogator will not be able to take any advantage of this difference.\\nThe situation can be made clearer if we consider sonic other simpler continuous\\nmachine. A differential analyser will do very well. (A differential analyser\\nis a certain kind of machine not of the discrete-state type used for some\\nkinds of calculation.) Some of these provide their answers in a typed form,\\nand so are suitable for taking part in the game. It would not be possible\\nfor a digital computer to predict exactly what answers the differential\\nanalyser would give to a problem, but it would be quite capable of giving\\nthe right sort of answer. For instance, if asked to give the value of (actually\\nabout 3.1416) it would be reasonable to choose at random between the values\\n3.12, 3.13, 3.14, 3.15, 3.16 with the probabilities of 0.05, 0.15, 0.55,\\n0.19, 0.06 (say). Under these circumstances it would be very difficult\\nfor the interrogator to distinguish the differential analyser from the\\ndigital computer. \\n\\n(8) The Argument from Informality of Behaviour \\n\\nIt is not possible to produce a set of rules purporting\\nto describe what a man should do in every conceivable set of circumstances.\\nOne might for instance have a rule that one is to stop when one sees a\\nred traffic light, and to go if one sees a green one, but what if by some\\nfault both appear together? One may perhaps decide that it is safest to\\nstop. But some further difficulty may well arise from this decision later.\\nTo attempt to provide rules of conduct to cover every eventuality, even\\nthose arising from traffic lights, appears to be impossible. With all this\\nI agree.\\n\\nFrom this it is argued that we cannot be machines. I shall\\ntry to reproduce the argument, but I fear I shall hardly do it justice.\\nIt seems to run something like this. \"if each man had a definite set\\nof rules of conduct by which he regulated his life he would be no better\\nthan a machine. But there are no such rules, so men cannot be machines.\"\\nThe undistributed middle is glaring. I do not think the argument is ever\\nput quite like this, but I believe this is the argument used nevertheless.\\nThere may however be a certain confusion between \"rules of conduct\"\\nand \"laws of behaviour\" to cloud the issue. By \"rules of\\nconduct\" I mean precepts such as \"Stop if you see red lights,\"\\non which one can act, and of which one can be conscious. By \"laws\\nof behaviour\" I mean laws of nature as applied to a man\\'s body such\\nas \"if you pinch him he will squeak.\" If we substitute \"laws\\nof behaviour which regulate his life\" for \"laws of conduct by\\nwhich he regulates his life\" in the argument quoted the undistributed\\nmiddle is no longer insuperable. For we believe that it is not only true\\nthat being regulated by laws of behaviour implies being some sort of machine\\n(though not necessarily a discrete-state machine), but that conversely\\nbeing such a machine implies being regulated by such laws. However, we\\ncannot so easily convince ourselves of the absence of complete laws of\\nbehaviour as of complete rules of conduct. The only way we know of for\\nfinding such laws is scientific observation, and we certainly know of no\\ncircumstances under which we could say, \"We have searched enough.\\nThere are no such laws.\"\\n\\nWe can demonstrate more forcibly that any such statement\\nwould be unjustified. For suppose we could be sure of finding such laws\\nif they existed. Then given a discrete-state machine it should certainly\\nbe possible to discover by observation sufficient about it to predict its\\nfuture behaviour, and this within a reasonable time, say a thousand years.\\nBut this does not seem to be the case. I have set up on the Manchester\\ncomputer a small programme using only 1,000 units of storage, whereby the\\nmachine supplied with one sixteen-figure number replies with another within\\ntwo seconds. I would defy anyone to learn from these replies sufficient\\nabout the programme to be able to predict any replies to untried values.\\n\\n(9) The Argument from Extrasensory Perception \\n\\nI assume that the reader is familiar with the idea of\\nextrasensory perception, and the meaning of the four items of it, viz.,\\ntelepathy, clairvoyance, precognition and psychokinesis. These disturbing\\nphenomena seem to deny all our usual scientific ideas. How we should like\\nto discredit them! Unfortunately the statistical evidence, at least for\\ntelepathy, is overwhelming. It is very difficult to rearrange one\\'s ideas\\nso as to fit these new facts in. Once one has accepted them it does not\\nseem a very big step to believe in ghosts and bogies. The idea that our\\nbodies move simply according to the known laws of physics, together with\\nsome others not yet discovered but somewhat similar, would be one of the\\nfirst to go.\\n\\nThis argument is to my mind quite a strong one. One can\\nsay in reply that many scientific theories seem to remain workable in practice,\\nin spite of clashing with ESP; that in fact one can get along very nicely\\nif one forgets about it. This is rather cold comfort, and one fears that\\nthinking is just the kind of phenomenon where ESP may be especially relevant.\\n\\nA more specific argument based on ESP might run as follows:\\n\"Let us play the imitation game, using as witnesses a man who is good\\nas a telepathic receiver, and a digital computer. The interrogator can\\nask such questions as \\'What suit does the card in my right hand belong\\nto?\\' The man by telepathy or clairvoyance gives the right answer 130 times\\nout of 400 cards. The machine can only guess at random, and perhaps gets\\n104 right, so the interrogator makes the right identification.\" There\\nis an interesting possibility which opens here. Suppose the digital computer\\ncontains a random number generator. Then it will be natural to use this\\nto decide what answer to give. But then the random number generator will\\nbe subject to the psychokinetic powers of the interrogator. Perhaps this\\npsychokinesis might cause the machine to guess right more often than would\\nbe expected on a probability calculation, so that the interrogator might\\nstill be unable to make the right identification. On the other hand, he\\nmight be able to guess right without any questioning, by clairvoyance.\\nWith ESP anything may happen.\\n\\nIf telepathy is admitted it will be necessary to tighten\\nour test up. The situation could be regarded as analogous to that which\\nwould occur if the interrogator were talking to himself and one of the\\ncompetitors was listening with his ear to the wall. To put the competitors\\ninto a \"telepathy-proof room\" would satisfy all requirements.\\n\\n7. Learning Machines\\n\\nThe reader will have anticipated that I have no very convincing\\narguments of a positive nature to support my views. If I had I should not\\nhave taken such pains to point out the fallacies in contrary views. Such\\nevidence as I have I shall now give.\\n\\nLet us return for a moment to Lady Lovelace\\'s objection,\\nwhich stated that the machine can only do what we tell it to do. One could\\nsay that a man can \"inject\" an idea into the machine, and that\\nit will respond to a certain extent and then drop into quiescence, like\\na piano string struck by a hammer. Another simile would be an atomic pile\\nof less than critical size: an injected idea is to correspond to a neutron\\nentering the pile from without. Each such neutron will cause a certain\\ndisturbance which eventually dies away. If, however, the size of the pile\\nis sufficiently increased, tire disturbance caused by such an incoming\\nneutron will very likely go on and on increasing until the whole pile is\\ndestroyed. Is there a corresponding phenomenon for minds, and is there\\none for machines? There does seem to be one for the human mind. The majority\\nof them seem to be \"subcritical,\" i.e., to correspond in this\\nanalogy to piles of subcritical size. An idea presented to such a mind\\nwill on average give rise to less than one idea in reply. A smallish proportion\\nare supercritical. An idea presented to such a mind that may give rise\\nto a whole \"theory\" consisting of secondary, tertiary and more\\nremote ideas. Animals minds seem to be very definitely subcritical. Adhering\\nto this analogy we ask, \"Can a machine be made to be supercritical?\"\\n\\nThe \"skin-of-an-onion\" analogy is also helpful.\\nIn considering the functions of the mind or the brain we find certain operations\\nwhich we can explain in purely mechanical terms. This we say does not correspond\\nto the real mind: it is a sort of skin which we must strip off if we are\\nto find the real mind. But then in what remains we find a further skin\\nto be stripped off, and so on. Proceeding in this way do we ever come to\\nthe \"real\" mind, or do we eventually come to the skin which has\\nnothing in it? In the latter case the whole mind is mechanical. (It would\\nnot be a discrete-state machine however. We have discussed this.)\\n\\nThese last two paragraphs do not claim to be convincing\\narguments. They should rather be described as \"recitations tending\\nto produce belief.\"\\n\\nThe only really satisfactory support that can be given\\nfor the view expressed at the beginning of §6, will be that provided\\nby waiting for the end of the century and then doing the experiment described.\\nBut what can we say in the meantime? What steps should be taken now if\\nthe experiment is to be successful? \\n\\nAs I have explained, the problem is mainly one of programming.\\nAdvances in engineering will have to be made too, but it seems unlikely\\nthat these will not be adequate for the requirements. Estimates of the\\nstorage capacity of the brain vary from 1010 to 1015\\nbinary digits. I incline to the lower values and believe that only a very\\nsmall fraction is used for the higher types of thinking. Most of it is\\nprobably used for the retention of visual impressions, I should be surprised\\nif more than 109 was required for satisfactory playing of the\\nimitation game, at any rate against a blind man. (Note: The capacity of\\nthe Encyclopaedia Britannica, 11th edition, is 2 X 109)\\nA storage capacity of 107, would be a very practicable possibility\\neven by present techniques. It is probably not necessary to increase the\\nspeed of operations of the machines at all. Parts of modern machines which\\ncan be regarded as analogs of nerve cells work about a thousand times faster\\nthan the latter. This should provide a \"margin of safety\" which\\ncould cover losses of speed arising in many ways, Our problem then is to\\nfind out how to programme these machines to play the game. At my present\\nrate of working I produce about a thousand digits of progratiirne a day,\\nso that about sixty workers, working steadily through the fifty years might\\naccomplish the job, if nothing went into the wastepaper basket. Some more\\nexpeditious method seems desirable.\\n\\nIn the process of trying to imitate an adult human mind\\nwe are bound to think a good deal about the process which has brought it\\nto the state that it is in. We may notice three components.\\n\\n(a) The initial state of the mind, say at birth, \\n\\n(b) The education to which it has been subjected, \\n\\n(c) Other experience, not to be described as education,\\nto which it has been subjected.\\n\\nInstead of trying to produce a programme to simulate the\\nadult mind, why not rather try to produce one which simulates the child\\'s?\\nIf this were then subjected to an appropriate course of education one would\\nobtain the adult brain. Presumably the child brain is something like a\\nnotebook as one buys it from the stationer\\'s. Rather little mechanism,\\nand lots of blank sheets. (Mechanism and writing are from our point of\\nview almost synonymous.) Our hope is that there is so little mechanism\\nin the child brain that something like it can be easily programmed. The\\namount of work in the education we can assume, as a first approximation,\\nto be much the same as for the human child.\\n\\nWe have thus divided our problem into two parts. The child\\nprogramme and the education process. These two remain very closely connected.\\nWe cannot expect to find a good child machine at the first attempt. One\\nmust experiment with teaching one such machine and see how well it learns.\\nOne can then try another and see if it is better or worse. There is an\\nobvious connection between this process and evolution, by the identifications\\n\\n\\nStructure of the child machine = hereditary material \\n\\nChanges of the child machine = mutation,\\n\\nNatural selection = judgment of the experimenter \\n\\nOne may hope, however, that this process will be more\\nexpeditious than evolution. The survival of the fittest is a slow method\\nfor measuring advantages. The experimenter, by the exercise of intelligence,\\nshould he able to speed it up. Equally important is the fact that he is\\nnot restricted to random mutations. If he can trace a cause for some weakness\\nhe can probably think of the kind of mutation which will improve it.\\n\\nIt will not be possible to apply exactly the same teaching\\nprocess to the machine as to a normal child. It will not, for instance,\\nbe provided with legs, so that it could not be asked to go out and fill\\nthe coal scuttle. Possibly it might not have eyes. But however well these\\ndeficiencies might be overcome by clever engineering, one could not send\\nthe creature to school without the other children making excessive fun\\nof it. It must be given some tuition. We need not be too concerned about\\nthe legs, eyes, etc. The example of Miss Helen Keller shows that education\\ncan take place provided that communication in both directions between teacher\\nand pupil can take place by some means or other. \\n\\nWe normally associate punishments and rewards with the\\nteaching process. Some simple child machines can be constructed or programmed\\non this sort of principle. The machine has to be so constructed that events\\nwhich shortly preceded the occurrence of a punishment signal are unlikely\\nto be repeated, whereas a reward signal increased the probability of repetition\\nof the events which led up to it. These definitions do not presuppose any\\nfeelings on the part of the machine, I have done some experiments with\\none such child machine, and succeeded in teaching it a few things, but\\nthe teaching method was too unorthodox for the experiment to be considered\\nreally successful.\\n\\nThe use of punishments and rewards can at best be a part\\nof the teaching process. Roughly speaking, if the teacher has no other\\nmeans of communicating to the pupil, the amount of information which can\\nreach him does not exceed the total number of rewards and punishments applied.\\nBy the time a child has learnt to repeat \"Casabianca\" he would\\nprobably feel very sore indeed, if the text could only be discovered by\\na \"Twenty Questions\" technique, every \"NO\" taking the\\nform of a blow. It is necessary therefore to have some other \"unemotional\"\\nchannels of communication. If these are available it is possible to teach\\na machine by punishments and rewards to obey orders given in some language,\\ne.g., a symbolic language. These orders are to be transmitted through the\\n\"unemotional\" channels. The use of this language will diminish\\ngreatly the number of punishments and rewards required.\\n\\nOpinions may vary as to the complexity which is suitable\\nin the child machine. One might try to make it as simple as possible consistently\\nwith the general principles. Alternatively one might have a complete system\\nof logical inference \"built in.\"\\' In the latter case the store\\nwould be largely occupied with definitions and propositions. The propositions\\nwould have various kinds of status, e.g., well-established facts, conjectures,\\nmathematically proved theorems, statements given by an authority, expressions\\nhaving the logical form of proposition but not belief-value. Certain propositions\\nmay be described as \"imperatives.\" The machine should be so constructed\\nthat as soon as an imperative is classed as \"well established\"\\nthe appropriate action automatically takes place. To illustrate this, suppose\\nthe teacher says to the machine, \"Do your homework now.\" This\\nmay cause \"Teacher says \\'Do your homework now\\' \" to be included\\namongst the well-established facts. Another such fact might be, \"Everything\\nthat teacher says is true.\" Combining these may eventually lead to\\nthe imperative, \"Do your homework now,\" being included amongst\\nthe well-established facts, and this, by the construction of the machine,\\nwill mean that the homework actually gets started, but the effect is very\\nsatisfactory. The processes of inference used by the machine need not be\\nsuch as would satisfy the most exacting logicians. There might for instance\\nbe no hierarchy of types. But this need not mean that type fallacies will\\noccur, any more than we are bound to fall over unfenced cliffs. Suitable\\nimperatives (expressed within the systems, not forming part of the rules\\nof the system) such as \"Do not use a class unless it is a subclass\\nof one which has been mentioned by teacher\" can have a similar effect\\nto \"Do not go too near the edge.\"\\n\\nThe imperatives that can be obeyed by a machine that has\\nno limbs are bound to be of a rather intellectual character, as in the\\nexample (doing homework) given above. important amongst such imperatives\\nwill be ones which regulate the order in which the rules of the logical\\nsystem concerned are to be applied, For at each stage when one is using\\na logical system, there is a very large number of alternative steps, any\\nof which one is permitted to apply, so far as obedience to the rules of\\nthe logical system is concerned. These choices make the difference between\\na brilliant and a footling reasoner, not the difference between a sound\\nand a fallacious one. Propositions leading to imperatives of this kind\\nmight be \"When Socrates is mentioned, use the syllogism in Barbara\"\\nor \"If one method has been proved to be quicker than another, do not\\nuse the slower method.\" Some of these may be \"given by authority,\"\\nbut others may be produced by the machine itself, e.g. by scientific induction.\\n\\n\\nThe idea of a learning machine may appear paradoxical\\nto some readers. How can the rules of operation of the machine change?\\nThey should describe completely how the machine will react whatever its\\nhistory might be, whatever changes it might undergo. The rules are thus\\nquite time-invariant. This is quite true. The explanation of the paradox\\nis that the rules which get changed in the learning process are of a rather\\nless pretentious kind, claiming only an ephemeral validity. The reader\\nmay draw a parallel with the Constitution of the United States.\\n\\nAn important feature of a learning machine is that its\\nteacher will often be very largely ignorant of quite what is going on inside,\\nalthough he may still be able to some extent to predict his pupil\\'s behavior.\\nThis should apply most strongly to the later education of a machine arising\\nfrom a child machine of well-tried design (or programme). This is in clear\\ncontrast with normal procedure when using a machine to do computations\\none\\'s object is then to have a clear mental picture of the state of the\\nmachine at each moment in the computation. This object can only be achieved\\nwith a struggle. The view that \"the machine can only do what we know\\nhow to order it to do,\"\\' appears strange in face of this. Most of\\nthe programmes which we can put into the machine will result in its doing\\nsomething that we cannot make sense (if at all, or which we regard as completely\\nrandom behaviour. Intelligent behaviour presumably consists in a departure\\nfrom the completely disciplined behaviour involved in computation, but\\na rather slight one, which does not give rise to random behaviour, or to\\npointless repetitive loops. Another important result of preparing our machine\\nfor its part in the imitation game by a process of teaching and learning\\nis that \"human fallibility\" is likely to be omitted in a rather\\nnatural way, i.e., without special \"coaching.\" (The reader should\\nreconcile this with the point of view on pages 23 and 24.) Processes that\\nare learnt do not produce a hundred per cent certainty of result; if they\\ndid they could not be unlearnt.\\n\\nIt is probably wise to include a random element in a learning\\nmachine. A random element is rather useful when we are searching for a\\nsolution of some problem. Suppose for instance we wanted to find a number\\nbetween 50 and 200 which was equal to the square of the sum of its digits,\\nwe might start at 51 then try 52 and go on until we got a number that worked.\\nAlternatively we might choose numbers at random until we got a good one.\\nThis method has the advantage that it is unnecessary to keep track of the\\nvalues that have been tried, but the disadvantage that one may try the\\nsame one twice, but this is not very important if there are several solutions.\\nThe systematic method has the disadvantage that there may be an enormous\\nblock without any solutions in the region which has to be investigated\\nfirst, Now the learning process may be regarded as a search for a form\\nof behaviour which will satisfy the teacher (or some other criterion).\\nSince there is probably a very large number of satisfactory solutions the\\nrandom method seems to be better than the systematic. It should be noticed\\nthat it is used in the analogous process of evolution. But there the systematic\\nmethod is not possible. How could one keep track of the different genetical\\ncombinations that had been tried, so as to avoid trying them again?\\n\\nWe may hope that machines will eventually compete with\\nmen in all purely intellectual fields. But which are the best ones to start\\nwith? Even this is a difficult decision. Many people think that a very\\nabstract activity, like the playing of chess, would be best. It can also\\nbe maintained that it is best to provide the machine with the best sense\\norgans that money can buy, and then teach it to understand and speak English.\\nThis process could follow the normal teaching of a child. Things would\\nbe pointed out and named, etc. Again I do not know what the right answer\\nis, but I think both approaches should be tried.\\n\\nWe can only see a short distance ahead, but we can see\\nplenty there that needs to be done.\\n\\n\\n\\n\\nReturn to Writing That Defined Computing\\n\\n\\n',\n",
       " '\\n\\n\\nProblem Solving and Truth Maintenance Systems\\n\\n\\nProblem Solving and Truth Maintenance Systems\\n\\nLast Modified: \\nThursday, 06-Apr-1995 15:01:43 EDT\\n\\n\\nIntroduction\\nJustification-Based Truth Maintenance\\nAssumption-Based Truth Maintenance\\nConclusions\\nReferences\\nAppendix: The ABC example\\n\\n\\nIntroduction\\n\\nTruth Maintenance Systems (TMS), also called Reason Maintenance Systems, \\nare used within Problem Solving Systems,\\nin conjunction with Inference Engines (IE) such as rule-based \\ninference systems, to manage as a Dependency Network\\nthe inference engine\\'s beliefs in given sentences.\\n\\n\\n\\n\\t\\t    PROBLEM SOLVING SYSTEM\\n\\t+--------------------------------------------+\\n\\t|\\t\\t\\t\\t\\t     |\\n\\t|\\t+----+\\t                +-----+\\t     |\\n\\t|\\t| IE |<---------------->| TMS |\\t     |\\n\\t|\\t+----+\\t                +-----+\\t     |\\n\\t|\\t\\t\\t\\t\\t     |\\n\\t+--------------------------------------------+\\n\\n\\n\\nA TMS is intended to satisfy a number of goals:\\n\\nProvide justifications for conclusions\\nWhen a problem solving system gives an answer to a user\\'s query,\\nan explanation of the answer is usually required. If the advice \\nto a stockbroker\\nis to invest millions of dollars, an explanation of the reasons for\\nthat advice can help the broker reach a reasonable decision.\\nAn explanantion can be constructed by the IE by tracing the justification\\nof the assertion.\\nRecognise inconsistencies\\nThe IE may tell the TMS that some sentences are\\ncontradictory. Then, if on the basis of other IE commands and of inferences\\nwe find that all those sentences are believed true, then the TMS reports\\nto the IE that a contradiction has arisen.\\nFor instance, in the ABC example the statement\\nthat either Abbott, or Babbitt, or Cabot is guilty together with the statements\\nthat Abbott is not guilty, Babbitt is not guilty, and Cabot is not guilty,\\nform a contradiction.\\nThe IE can eliminate an inconsistency by determining the assumptions\\nused and changing them appropriately, or by presenting the contradictory\\nset of sentences to the users and asking them to choose which \\nsentence(s) to retract.\\n\\n\\nSupport default reasoning\\nIn many situations we want, in the absence of firmer knowledge,\\nto reason from default assumptions. If Tweety is a bird, until told\\notherwise, we will assume that Tweety flies and use as justification the fact\\nthat Tweety is a bird and the assumption that birds fly.\\nRemember derivations computed previously\\nIn the process of determining what is responsible for a network problem,\\nwe may have derived, while examining the performance of a name server, that \\nMs.Doe is an expert on e-mail systems. That conclusion will not need to \\nbe derived again when the name server ceases to be the potential culprit \\nfor the problem and we examine instead the routers.\\nSupport dependency driven backtracking\\nThe justification of a sentence, as maintained by the TMS, provides \\nthe natural indication of what assumptions need to be changed if we want to\\ninvalidate that sentence.\\n\\n\\nOur belief [by \"our belief\" we mean the \"inference-engine\\'s belief\"]\\nabout a sentence can be:\\n\\nfalse, the sentence is believed to be unconditionally false;\\nthis is also called a contradiction\\ntrue, the sentence is believed unconditionally true; this is also\\ncalled a premise\\nassumed-true, the sentence is assumed true [we may change our belief\\nlater]; this is also called an enabled assumption\\nassumed-false, the sentence is assumed false [we may change our \\nbelief later]; this is also called a retracted assumption\\nassumed, the sentence is believed by inference from other sentences\\ndon\\'t-care.\\n\\n\\nWe say a sentence in in if true or assumed-true; it is out if false,\\nassumed-false, or don\\'t-care.\\n\\nA TMS maintains a Dependency Network. The network is bipartite,\\nwith nodes representing sentences and justifications.\\n\\nA sentence may correspond to a fact, such as \"Socrates is a man\",\\nor a rule, such as \"if ?x is a man then ?x is mortal\".\\nWe will say that a sentence node is a premise if its sentence is true, \\nis a contradiction if its sentence is false, is an assumption \\nif its sentence is assumed-true or assumed-false or assumed.\\nA sentence node receives arcs from  justification nodes. Each such \\njustification node provides an argument for believing the given sentence node.\\nIn turn a sentence node has arcs to the justification nodes that use it\\nas a premise.\\n\\nA justification node has inputs from sentence nodes, its premises or justifiers,\\nand has output to a sentence node, its conclusion or justificand. \\nA justification node\\nrepresents the inference of the conclusion from the conjunction\\nof the stated premises.\\n\\nThe conventions used in representing graphically dependency networks are \\nsummarized here.\\n\\nThe TMS maintains the following information with each sentence node:\\n\\na sentence\\na label expressing the current belief in the sentence;\\nit is IN for sentences that are believed, and OUT for sentences that are\\nnot believed.\\na list of the justification nodes that support it\\na list of the justification nodes supported by it\\nan indication if the node is an assumption, contradiction, or premise.\\n\\nThe TMS maintains very little information with justification nodes. Only a \\nlabel with value IN or OUT depending if we believe the justification\\nvalid or not.\\n\\n\\nThe IE can give at least the following orders to the TMS:\\n\\ncreate a sentence node with specific properties\\ncreate a justification with specific premises and conclusions\\nattach rules to sentence nodes and execute them when\\nspecific beliefs hold at those nodes [these are like callbacks or triggers;\\na standard callback informs the IE when a contradiction becomes true]\\nretrieve the properties of a sentence node or its justifications and\\nconsequences.\\n\\n\\n\\nTruth Maintenance Systems can have different characteristics:\\n\\n\\nJustification-Based Truth Maintenance System (JTMS)\\nIt is a simple TMS where one can examine the consequences of the\\ncurrent set of assumptions. The meaning of sentences is not known.\\n\\nAssumption-Based Truth Maintenance System (ATMS)\\nIt allows to maintain and reason with a number of simultaneous,\\npossibly incompatible, current sets of assumption. Otherwise it is\\nsimilar to JTMS, i.e. it does not recognise the meaning of sentences.\\nLogical-Based Truth Maintenance System (LTMS)\\nLike JTMS in that it reasons with only one set of current \\nassumptions at a time. More powerful than JTMS in that it recognises\\nthe propositional semantics of sentences, i.e. understands the relations\\nbetween p and ~p, p and q and p&q, and so on.\\nWe will not discuss further LTMSs.\\n\\n\\n\\nJustification-Based Truth Maintenance\\n\\nA Justification-based truth maintenance system (JTMS)\\nis a simple TMS where one can examine the consequences of the\\ncurrent set of assumptions. In JTMS labels are attacched to arcs from \\nsentence nodes to justification nodes. This label is either \"+\" or \"-\".\\nThen, for a justification node we can talk of its in-list,\\nthe list of its inputs with \"+\" label, and of its out-list,\\nthe list of its inputs with \"-\" label.\\n\\n\\nThe meaning of sentences is not known.\\nWe can have a node representing a sentence p and one representing ~p\\nand the two will be totally unrelated, unless relations are \\nestablished between them by justifications. For example, we can write:\\n\\n\\n\\t\\t~p^p Contradiction Node\\n\\t\\t  o\\n\\t\\t  |\\n\\t\\t  x\\t\\t   \\'x\\' denotes a justification node\\n\\t\\t/   \\\\\\t\\t   \\'o\\' denotes a sentence node\\n\\t      +/     \\\\+\\n\\t      o       o\\n\\t      p      ~p\\n\\n\\nwhich says that if both p and ~p are IN we have a contradiction.\\n\\nThe association of IN or OUT labels with the nodes in a dependency network\\ndefines an in-out-labeling function. This function is \\nconsistent if:\\n\\nThe label of a junctification node is IN iff the labels of all the sentence \\nnodes in its in-list are all IN and the labels of all the sentence nodes in\\nits out-list are OUT.\\nThe label of a sentence node is IN iff it is a premise, or an enabled\\nassumption node, or it has an input from a justification node with label IN.\\n\\n\\nHere are examples of JTMS operations (see Forbus&deKleer):\\n\\ncreate-jtms (node-string contradiction-handler enqueue-procedure)\\ncreates a dependency network. The parameters are:\\n\\nnode-string, a function to be used by the JTMS, which, given a node,\\nreturns its description\\ncontradiction-handler, a function to be invoked by the JTMS when it \\nrecognises a contradiction\\nenqueue-procedure, a function to be called by the JTMS when it changes the\\nlabel of a node to IN.\\n\\ntms-create-node (jtms datum assumptionp contradictoryp)\\ncreates a sentence  node. The parameters are:\\n\\njtms, the jtms within which the sentence node is to be created\\ndatum, the sentence\\nassumptionp, true iff this is an assumption node; initially the \\nassumption is retracted\\ncontradictoryp, true iff this is a contradictory node\\n\\nNote that premise nodes are recognised because they are given a \\njustification without any premises.\\nenable-assumption(node), \\nretract-assumption(node).\\nThese functions respectively enable and retract an existing assumption node.\\njustify-node (informant conclusion-node list-of-premise-nodes),\\ncreates a justification node. The parameters are:\\n\\nThe informant, that is who/what is entering this justification\\nThe conslusion-node and list-of-premise-nodes that specify respectively\\nthe conslusion and the premises of this justification.\\n\\n\\nHere is a simple dependency network:\\n\\n\\n\\t     contradiction\\n\\t\\t   |\\n\\t\\t   x\\n\\t\\t   |\\n\\t\\t g o     o h\\n\\t\\t   |     | \\n\\t\\t   x     x \\n\\t\\t  / \\\\   / \\\\\\n\\t\\t /   \\\\ /   \\\\\\n\\t        o     o     o\\n\\t\\tA     C\\t    E\\n\\nThis network is created:\\n\\t(setq *jtms* (create-jtms \"Forbus-deKleer example on page 181\"))\\nThe nodes A, C, E are created and enabled:\\n\\t(setq assumption-a (tms-create-node *jtms* \"A\" :assumptionp t)\\n\\t      assumption-c (tms-create-node *jtms* \"C\" :assumptionp t)\\n\\t      assumption-e (tms-create-node *jtms* \"E\" :assumptionp t))\\n\\t(enable-assumption assumption-a)\\n\\t(enable-assumption assumption-c)\\n\\t(enable-assumption assumption-e)\\nThe node h is created and justified:\\n\\t(setq node-h (tms-create-node *jtms* \"h\"))\\n\\t(justify-node \"R1\" node-h (list assumption-c assumption-a))\\n\\t\\t\\t\\t;; R1 is the informant for this \\n\\t\\t\\t        ;; justification\\nThe node g is created, justified, and established as a contradiction:\\n\\t(setq node-g (tms-create-node *jtms* \"g\"))\\n\\t(justify-node \"R2\" node-g (list assumption-a assumption-c))\\n\\t(setq contradiction (tms-create-node *jtms* \\'contra :contradictoryp t))\\n\\t(justify-node \"R3\" contradiction (list node-g))\\n\\nThis last command will recognise that a contradiction exists and will\\ninvoke a contradiction handler that should state that the node \\n\"contradiction\" is a contradiction and that some of the assumptions \\nin {A, C} should be retracted.\\n\\nHere should be a larger dependency network as used in a\\nJTMS.\\n\\nAlgorithms are available to propage changes in assumptions, either asserting a sentence, or \\nretracting it. Further, algorithms are available to determine the maximally\\nconsistent sets of  assumptions\\nsupporting a given sentence.\\n\\n\\nAssumption-Based Truth Maintenance\\n\\nIn a JTMS, we change assumptions by relabeling the nodes of a \\ndependency network. Any time we switch among alternative sets of assumptions, \\nwe re-label the net. This may lead to considerable work.  \\nAnother possibility is to maintain information at each node\\nwithin a single dependency network for\\nall the alternative sets of assumptions that we may hold, also called\\nworlds or environments. \\nThat is, we can use as label of a node \\nthe set consisting of all the environments that justify this node.\\nThis is the path taken in Assumption-Based Truth Maintenance Systems (ATMS).\\nSince all environments are simultaneously available, \\nit is not necessary to enable\\nor retract sentences, and thus re-label the network. It is only necessary \\nto switch from one environment to another.\\n\\nThe problem of the ATMS approach is that if we are given n assumptions,\\nthere are 2**n possible  selections of all these assumptions, that is\\n2**n environments. \\nThis, except in trivial cases, is more than we can work with.\\n\\nTwo observations simplify this problem:\\n\\nIf we find out that a set A of assertions is inconsistent, in which\\ncase we say that A is a no-good set,\\nthen any set B of assertions such that A is contained in B will also\\nbe inconsistent. [Notice that we need to label a node only with consistent\\nenvironments.]\\nIf we find that a node has a justification based on \\na set of assertions A, in which case we say that the node holds in A,\\nthen we do not need to consider any context B, where\\nB contains A.\\n\\nWe define the context of a set A of assumptions to be the set\\nconsisting of all the nodes that hold in A, i.e. that have a \\njustification based on A.\\n\\nHere are examples of ATMS operations (see Forbus&deKleer, pg440):\\n\\ncreate-atms (title node-string enqueue-procedure)\\ncreates a dependency network with the same parameters as create-jtms,\\nexcept that now we do not have a contradiction-handler, since the \\nrelevant information is recorded by the label.\\ntrue-node? (node)\\nrecognises if node is a premise\\nwhy-node(node)\\nreturns the label of node\\natms-assumptions(an-atms)\\nreturns a list of all the assumptions enabled for this atms\\ninterpretations(atms choice-sets defaults)\\nreturns all the environments that are consistent with \\ndefaults and include at least an element from choice-sets.\\n\\n\\ntms-create-node and justify-node are for ATMS as defined for JTMS.\\n\\nHere is the same example considered for JTMS:\\n\\n\\n    (stq *atms* (create-atms \"Example from Forbus-deKleer pg.442\")\\n    (setq assumption-a (tms-create-node *atms* \"A\" :assumptionp t)\\n\\t  assumption-c (tms-create-node *atms* \"C\" :assumptionp t)\\n\\t  assumption-e (tms-create-node *atms* \"E\" :assumptionp t))\\n    (setq node-h (tms-create-node *atms* \"h\"))\\n    (justify-node \"R1\" node-h (list assumption-c assumption-e))\\n\\nNow the query (why-node node-h) returns the label (h {{C,E}}). Continuing:\\n\\n    (setq node-g (tms-create-node *atms* \"g\"))\\n    (justify-node \"R2\" node-g (list assumption-a assumption-c))\\n    (setq contradiction (tms-create-node *atms* \\n\\t\\t\\t\\t\\'contradiction :contradictionp t))\\n    (justify-node \"R3\" contradiction (list node-g))\\n\\nAfter this last statement there is no call-back from ATMS to IE. Yet\\nwe can determine the maximally consistent sets of assumptions that are\\ncompatible with what we know by calling:\\n\\n    (mapc \\'print-env \\n\\t (interpretations *atms* nil (atms-assumptions *atms*)))\\n\\nwith result:\\n\\n\\t{A,E}\\n\\t{C,E}\\n\\n\\nConclusions\\n\\nTruth Maintenance Systems are significant as a mechanism for implementing dependency \\ndirected backtracking during search. \\n\\nSoftware is available on JTMS and \\nATMS in Forbus.\\nReferences\\n\\nForbus,K.,deKleer,J.: Building Problem Solvers\\nMIT Press, 1993\\nForbus is the most comprehensive reference on Truth Maintenance in \\nproblem solving systems. The programs described in the book are available\\non line.\\nShoham,Y.: Artificial Intelligence: Techniques in Prolog\\nMorgan-Kauffman, 1994\\nBrief introduction and programs\\nin Prolog showing basic algorithms in TMS.\\nRich,E.,Knight,K.: Artificial Intelligence, Third Edition\\nMcGraw-Hill 1991\\nGood, Brief introduction and the ABC example. No programs.\\n\\nAppendix: The ABC example\\n\\nAn example by Quine and Ullian, as reported by Rich&Knight,\\nshows typical circumstances under which people operate. We would like\\na problem solving system to be able to reason about cases such as this.\\n\\n\\n\\n   We know that:\\n\\n\\tAbbott, Babbitt, Cabot are suspects in a murder.\\n\\n\\tAbbott has an alibi, the registration at a good hotel in\\n\\tAlbany at the time of the crime\\n\\n\\tBabbitt has an alibi, his brother-in-law says they were\\n\\ttogether at the time of the crime\\n\\n\\tCabot says he was at a Catskills ski resort, but with no\\n\\twitnesses.\\n\\n   Our other premises (i.e. assumed unconditionally) are:\\n\\n\\tThe hotel in Albany is reliable\\n\\n\\tBabbitt\\'s bother-in-law is reliable\\n\\n   We have as current assumptions:\\n\\n\\t(1) Abbott did not commit crime\\n\\n\\t(2) Babbitt did not commit crime\\n\\n\\t(3) Abbott, Babbitt, or Cabot committed the crime.\\n\\n   We then find a newsreel showing Cabot at the ski resort:\\n\\n\\t(4) Cabott did not commit the crime.\\n\\n   Thus:\\n\\n\\t(1) & (2) & (3) & (4) are contradictory beliefs\\n\\n\\n\\ningargiola@cis.temple.edu\\n\\n\\n',\n",
       " '\\n\\n\\nMultivariate Statistics: Introduction\\n\\n\\nMultivariate Statistics: An Introduction\\nWelcome to this introduction to the family of data analysis techniques often grouped\\ntogether under the name, \"multivariate statistics.\" The word multivariate should\\nsay it all -- these techniques look at the pattern of relationships between several\\nvariables simultaneously. This may sound scary, but fear not -- you do not need training\\nin highly advanced statistics to follow the explanation in these pages. We will look at\\nthree types of multivariate methods -- factor analysis, multidimensional scaling, and cluster analysis.\\n\\nMultivariate statistics help the researcher to summarize data and reduce the number of\\nvariables necessary to describe it.\\nHow are these techniques used?\\n\\nMost commonly multivariate statistics are employed: \\n\\n\\nfor developing taxonomies or systems of classification \\nto investigate useful ways to conceptualize or group items \\nto generate hypotheses \\nto test hypotheses \\n\\n\\nOne researcher has this to say about factor analysis, a comment that could apply to all\\n  three techniques: \\n\\nWhen I think of factor analysis, two words come to mind: \"curiosity\" and\\n    \"parsimony.\" This seems a rather strange pair -- but not in relation to factor\\n    analysis. Curiosity means wanting to know what is there, how it works, and why it is there\\n    and why it works ... Scientists are curious. They want to know what\\'s there and why. They\\n    want to know what is behind things. And they want to do this in as parsimonious a fashion\\n    as possible. They do not want an elaborate explanation when it is not needed ... This\\n    ideal we can call the principle of parsimony (Kerlinger, 1979).\\n\\n\\nHow do these techniques differ from regression?\\n\\nIn multiple regression and analysis of variance, several variables are used, however\\n  one -- a dependent variable -- is generally predicted or explained by means of the\\n  other(s) -- independent variables and covariates. These are called dependence\\n  methods.\\n\\n\\nFactor analysis, multidimensional scaling (MDS) and cluster analysis look at interrelationships\\n  among variables. They are not generally used in prediction, there is no p-value,\\n  and the researcher interprets the output of the analysis and determines the best model.\\n  This can be frustrating! (See cautions for novice researchers.)\\n\\nWhat are the assumptions of multivariate analyses?\\n\\nAll of the models require that input data be in the form of interrelationships -- this\\n  means correlations for factor analysis. MDS and cluster analysis can use a variety of\\n  different input data -- distances, or measures of similarity or proximity. This means that\\n  MDS and cluster analysis can be somewhat more flexible than factor analysis.\\nA big assumption of these methods is that the data itself is valid\\n  . (See Trochim\\'s Knowledge\\n  Base for a discussion of validity, especially construct validity.) Because\\n  these methods do not use the same logic of statistical inference that dependence methods\\n  do, there are no robust measures that can overcome problems in the data. So, these methods\\n  are only as good as the input you have. The \"garbage in-garbage out\" rule\\n  definately applies.\\n\\nWhat does the output look like?\\nIn each case, the output will look somewhat different, but in all of the techniques,\\nthe researcher is required to look at the results and make some determination of how many\\nfactors, dimensions or clusters to use in further analysis in order to represent the data.\\nWhat the researcher should not forget is that each case or variable used in the analysis\\nis simultaneously classified on all the dimensions. While this is most apparent in\\nmultidimensional scaling, it applies equally well to the other techniques.\\nA recommendation\\nI recommend that visitors to this site begin their journey into the world of\\nmultivariate classification and measurement with a look at the Factor Analysis page. While\\nyou may be more interested in MDS or cluster analysis, those methods involve many of the\\nsame decision-making processes as factor analysis. In the interest of parsimony (always a\\ngood thing!), I will assume that visitors to the MDS and cluster analysis pages have\\nskimmed the factor analysis page.\\nNow, armed with this information, you are now ready to look at:\\n\\nForward to the Factor Analysis page \\nForward to the Multidimensional Scaling (MDS) page \\nForward to the Cluster Analysis page \\n\\n\\nOther links\\n\\nCautions! -- computer programs and other things to think\\n    about before trying out these techniques \\nReferences -- an annotated list \\nMultidimensional Analysis\\n    -- an overview of some of the math behind these methods, from Prof. George W Hart. Not for\\n    the faint of heart. \\n\\n \\n\\n\\n  Got a beef with anything written here?\\n  Send me your comments and suggestions:\\nColleen Flynn Thapalia\\n\\n\\n\\n\\n',\n",
       " '\\n\\n\\n\\nDBMS - August 1996 - Defining Data Mining\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\nDBMS\\nDefining Data Mining\\n\\nBy Bruce Moxon\\nDBMS Data Warehouse Supplement, August 1996\\n\\nThe Hows and Whys of Data Mining, and How It Differs From Other Analytical Techniques\\n\\nData mining is one of the hottest topics in information technology. This article is an introduction to data mining: what it is, why it\\'s important, and how it can be used to provide increased understanding of critical relationships in rapidly expanding corporate data warehouses.\\n\\nWhat is Data Mining?\\nThere are probably as many definitions of the term data mining as there are software analytical tool vendors in the market today. As with OLAP, which could mean almost anything, vendors and industry analysts have adopted the term \"data mining\" somewhat indiscriminately. The result is a blanket definition that includes all tools employed to help users analyze and understand their data. In this article, I explore a more narrow definition. Data mining is a set of techniques used in an automated approach to exhaustively explore and bring to the surface complex relationships in very large datasets. \\n\\nI discuss only datasets that are largely tabular in nature, having most likely been implemented in relational database management technology. However, these techniques can be, have been, and will be applied to other data representations, including spatial data domains, text-based domains, and multimedia (image) domains.\\n\\nA significant distinction between data mining and other analytical tools is in the approach they use in exploring the data interrelationships. Many of the analytical tools available support a verification-based approach, in which the user hypothesizes about specific data interrelationships and then uses the tools to verify or refute those hypotheses. This approach relies on the intuition of the analyst to pose the original question and refine the analysis based on the results of potentially complex queries against a database. The effectiveness of this verification-based analysis is limited by a number of factors, including the ability of the analyst to pose appropriate questions and quickly return results, manage the complexity of the attribute space, and think \"out of the box.\"\\n\\nMost available analytical tools have been optimized to address some of these issues. Query and reporting tools address ease of use, letting users develop SQL queries through point-and-click interfaces. Statistical analysis packages provide the ability to explore relationships among a few variables and determine statistical significance against a population. Multidimensional and relational OLAP tools precompute hierarchies of aggregations along various dimensions in order to respond quickly to users\\' inquiries. New visualization tools let users explore higher dimensionality relationships by combining spatial and non-spatial attributes (location, size, color, and so on). \\n\\nData mining, in contrast to these analytical tools, uses discovery-based approaches in which pattern-matching and other algorithms are employed to determine the key relationships in the data. Data mining algorithms can look at numerous multidimensional data relationships concurrently, highlighting those that are dominant or exceptional. \\n\\nThe Need and Opportunity for Data Mining\\nMany of the techniques used by today\\'s data mining tools have been around for many years, having originated in the artificial intelligence research of the 1980s and early 1990s. Yet these tools are only now being applied to large-scale database systems. The confluence of several key trends is responsible for this new usage.\\n\\nWidespread Deployment of High-Volume Transactional Systems. Over the past 15 to 20 years, computers have been used to capture detailed transaction information in a variety of corporate enterprises. Retail sales, telecommunications, banking, and credit card operations are examples of transaction-intensive industries.\\n\\nThese transactional systems are designed to capture detailed information about every aspect of business. Only five years ago, database vendors were struggling to provide systems that could deliver several hundred transactions per minute. Now we routinely see TPC-C numbers (tpmC\\'s) for large multiprocessor servers in excess of 10,000 per minute, with some clustered SMPs as high as 30,000. This growth has been accompanied by an equally impressive reduction in the cost per tpmC, which is now well under $500. Recent developments in \"low-end\" four- and eight-way Pentium-based SMPs and the commoditization of clustering technology promise to make this high transaction-rate technology more affordable and easier to integrate into businesses, leading to an even greater proliferation of transaction-based information.\\n\\nInformation as a Key Corporate Asset. The need for information has resulted in the proliferation of data warehouses that integrate information from multiple, disparate operational systems to support decision making. In addition, they often include data from external sources, such as customer demographics and household information.\\n\\nWidespread Availability of Scalable Information Technology. Recently, there has been widespread adoption of scalable, open systems-based information technology. This includes database management systems, analytical tools, and, most recently, information exchange and publishing through Intranet services. \\n\\nThese factors put tremendous pressure on the information \"value chain.\" At the source side, the amount of raw data stored in corporate data warehouses is growing rapidly. The \"decision space\" is too complex; there is too much data and complexity that might be relevant to a specific problem. And at the sink side, the knowledge required by decision makers to chart the course of a business places tremendous stress on traditional decision-support systems. Data mining promises to bridge the analytical gap by giving knowledgeworkers the tools to navigate this complex analytical space.\\n\\nData Mining Tools and Techniques\\nData mining applications can be described in terms of a three-level application architecture. These layers include applications, approaches, and algorithms and models. These three layers sit on top of a data repository. I discuss these three levels in the following sections; the characteristics of the data repository are addressed in the implementation section that follows.\\n\\nApplications. You can classify data mining applications into sets of problems that have similar characteristics across different application domains. The parameterization of the application is distinct from industry to industry and application to application. The same approaches and underlying models used to develop a fraud-detection capability for a bank can be used to develop medical insurance fraud detection applications. The difference is how the models are parameterized - for example, which of the domain-specific attributes in the data repository are used in the analysis and how they are used.\\n\\nApproaches. Each data mining application class is supported by a set of algorithmic approaches used to extract the relevant relationships in the data: association, sequence-based analysis, clustering, classification, and estimation. These approaches differ in the classes of problems they are able to solve.\\n\\n association: Association approaches address a class of problems typified by a market-basket analysis. Classic market-basket analysis treats the purchase of a number of items (for example, the contents of a shopping basket) as a single transaction. The goal is to find trends across large numbers of transactions that can be used to understand and exploit natural buying patterns. This information can be used to adjust inventories, modify floor or shelf layouts, or introduce targeted promotional activities to increase overall sales or move specific products. While these approaches had their origins in the retail industry, they can be applied equally well to services that develop targeted marketing campaigns or determine common (or uncommon) practices. In the financial sector, association approaches can be used to analyze customers\\' account portfolios and identify sets of financial services that people often purchase together. They may be used, for example, to create a service \"bundle\" as part of a promotional sales campaign. \\n\\nAssociation approaches often express the resultant item affinities in terms of confidence-rated rules, such as, \"80 percent of all transactions in which beer was purchased also included potato chips.\" Confidence thresholds can typically be set to eliminate all but the most common trends. The results of the association analysis (for example, the attributes involved in the rules themselves) may trigger additional analysis.\\n\\n sequence-based analysis: Traditional market-basket analysis deals with a collection of items as part of a point-in-time transaction. A variant of this problem occurs when there is additional information to tie together a sequence of purchases (for example, an account number, a credit card, or a frequent buyer/flyer number) in a time series. In this situation, not only may the coexistence of items within a transaction be important, but also the order in which those items appear across ordered transactions and the amount of time between transactions.\\n\\nRules that capture these relationships can be used, for example, to identify a typical set of precursor purchases that might predict the subsequent purchase of a specific item. In health care, such methods can be used to identify both routine and exceptional courses of treatment, such as multiple procedures over time.\\n\\n clustering: Clustering approaches address segmentation problems. These approaches assign records with a large number of attributes into a relatively small set of groups or \"segments.\" This assignment process is performed automatically by clustering algorithms that identify the distinguishing characteristics of the dataset and then partition the n-dimensional space defined by the dataset attributes along natural cleaving boundaries. There is no need to identify the groupings desired or the attributes that should be used to segment the dataset.\\n\\nClustering is often one of the first steps in data mining analysis. It identifies groups of related records that can be used as a starting point for exploring further relationships. This technique supports the development of population segmentation models, such as demographic-based customer segmentation. Additional analyses using standard analytical and other data mining techniques can determine the characteristics of these segments with respect to some desired outcome. For example, the buying habits of multiple population segments might be compared to determine which segments to target for a new sales campaign.\\n\\n classification: Classification, perhaps the most commonly applied data mining technique, employs a set of preclassified examples to develop a model that can classify the population of records at large. Fraud detection and credit-risk applications are particularly well suited to this type of analysis. This approach frequently employs decision tree or neural network-based classification algorithms. The use of classification algorithms begins with a training set of preclassified example transactions. For a fraud detection application, this would include complete records of both fraudulent and valid activities, determined on a record-by-record basis. The classifier training algorithm uses these preclassified examples to determine the set of parameters required for proper discrimination. The algorithm then encodes these parameters into a model called a classifier.\\n\\nThe approach affects the explanation capability of the system. Once an effective classifier is developed, it is used in a predictive mode to classify new records into these same predefined classes. For example, a classifier capable of identifying risky loans could be used to aid in the decision of whether to grant a loan to an individual.\\n\\n estimation: A variation on the classification problem involves the generation of scores along various dimensions in the data. Rather than employing a binary classifier to determine whether a loan applicant is a good or bad risk, this approach generates a credit-worthiness \"score\" based on a prescored training set. \\n\\n other techniques: Additional approaches used in conjunction with these and other analytical techniques include case-based reasoning, fuzzy logic, genetic algorithms, and fractal-based transforms. Fractal-based transforms (relatively new as data analysis tools) are interesting in that they are also used as aggressive, lossless data compression algorithms. Hence, there is the possibility that pattern-matching approaches based on these techniques could exploit substantially reduced dataset sizes to increase performance. Each of these has its own strengths and weaknesses in terms of the problem characteristics best addressed, discrimination capabilities, performance, and training requirements. The algorithms are often tunable using a variety of parameters aimed at providing the right balance of fidelity and performance. \\n\\nAlgorithms and Models. The promise of data mining is attractive for executives and IS professionals looking to make sense out of large volumes of complex business data. The promise that programs can analyze an entire data warehouse and identify the key relationships relevant to the business is being pushed as a panacea for all data analysis woes. Yet this image is far from reality.\\n\\nToday\\'s data mining tools have typically evolved out of the pattern recognition and artificial intelligence research efforts of both small and large software companies. These tools have a heavy algorithmic component and are often rather \"bare\" with respect to user interfaces, execution control, and model parameterization. They typically ingest and generate Unix flatfiles (both control and data files) and are implemented using a single-threaded computational model.\\n\\nThis state of affairs presents challenges to users that can be summed up in a sort of \"tools gap.\" (See Figure 1) The gap, caused by a number of factors, requires significant pre- and post-processing of data to get the most out of a data mining application. Pre-processing activities include the selection of appropriate data subsets for performance and consistency reasons, as well as complex data transformations to bridge the representational gap. Post-processing often involves subselection of voluminous results and the application of visualization techniques to provide added understanding. These activities are critical to effectively address key implementation issues such as:\\n\\n\\n susceptibility to \"dirty\" data: Data mining tools have no higher-level model of the data on which they operate. They have no application-oriented (semantic) structure and, as such, they simply take everything they are given as factual and draw the resulting conclusions. Users must take the necessary precautions to ensure that the data being analyzed is \"clean.\" This may require significant analysis of the attribute values being fed to the discovery tools. However, if the company has a good data cleansing process that cleans up data going into a data warehouse, then data mining tools benefit from this cleansing effort.\\n\\n\\n inability to \"explain\" results in human terms: Many of the tools employed in data mining analysis use complex mathematical algorithms that are not easily mapped into human terms; for example, they don\\'t always generate \"if-then\" rules that use the original data\\'s attributes by name, so the ability of these systems to \"explain\" their results is minimal. Even with approaches such as decision trees and rule induction that are capable of generating information about the underlying attributes, the volume and format of the information may be unusable without additional post-processing and/or visualization.\\n\\n\\n the data representation gap: Most of the source data for today\\'s data mining applications resides in large, parallel relational database systems. The information is typically somewhat normalized and the attributes being used in a data mining application may span multiple tables. The data mining engines typically operate over a set of attribute \"vectors\" presented through a Unix flatfile. Conditioning code must be used to provide the denormalized representation the tools need. Large central fact tables in data warehouses designed using star schema often combine denormalized data into one flat table.\\n\\nMany of the tools are constrained in terms of the types of data elements with which they can work. Users may have to categorize continuous variables or remap categorical variables. Time-series information may need to be remapped as well. For example, you might need to derive counts of the number of times a particular criterion was met in a historical database.\\n\\nAlthough the 2GB file is becoming less important with the advance of 64-bit operating systems, many Unix implementations still have 2GB file limitations. For flatfile-based data mining tools, this limits the size of the datasets they can analyze, making sampling a necessity.\\n\\nParallel relational database systems store data that is spread across many disks and accessed by many CPUs. Current database architectures are such that result sets generated by the database engine are eventually routed through a single query coordinator process. This can cause a significant bottleneck in using parallel database resources efficiently. Because data mining applications are typically single-threaded implementations operating off Unix flat files, the process requires potentially large result sets to be extracted from the database.\\n\\nEven if you\\'re able to extract large datasets, processing them can be compute-intensive. Although most data mining tools are intended to operate against data coming from a parallel database system, most have not been parallelized themselves.\\n\\nThis performance issue is mitigated by \"sampling\" the input dataset, which poses issues of its own. Users must be careful to ensure that they capture a \"representative\" set of records, lest they bias the discovery algorithms. Because the algorithms themselves determine which attributes are important in the pattern matching, this presents a chicken-and-egg scenario that may require an iterative solution\\n\\nFor algorithms that require training sets (classification problems), the training sets must adequately cover the population at large. Again, this may lead to iterative approaches, as users strive to find reasonably sized training sets that ensure adequate population coverage.\\n\\nPresent-day tools are algorithmically strong but require significant expertise to implement effectively. Nevertheless, these tools can produce results that are an invaluable addition to a business\\' corporate information assets. As these tools mature, advances in server-side connectivity, the development of business-based models, and user interface improvements will bring data mining into the mainstream of decision-support efforts.\\n\\n\\n\\n\\nBruce Moxon is a senior consultant at Emergent Corp., a San Mateo, California-based consultancy that specializes in implementing IT solutions based on commercial parallel processing systems. You can reach Bruce at 415-372-5800 or email him at bmoxon@emergent.com.\\n\\n\\n\\nFigure 1\\n\\nThe data mining \"tools gap.\" Users, data mining tools, and SQL-based relational databases each \"speak their own language\" when it comes to describing the fundamental aspects of data mining applications.\\n\\n\\n\\nTable of Contents - August 1996 | Home Page\\n\\nCopyright ©  1996 Miller Freeman, Inc. ALL RIGHTS RESERVED\\nRedistribution without permission is prohibited.\\n\\nPlease send questions or comments to mfrank@mfi.com\\nUpdated Friday, August 2, 1996\\n\\n\\n',\n",
       " '\\n\\nWas ist das Usenet\\nWas ist das Usenet\\nAutor: kris@toppoint.de\\n\\nrenner@ame.zer.sub.org writes:\\nsuche Informationen ueber das Usenet und das Internet...\\n\\nBeim UNIX Betriebssystem ist seit langer Zeit ein Paket dabei,\\ndas UUCP heisst. UUCP ist eine Abkuerzung und steht fuer Unix\\nto Unix CoPy. Leider sagt diese Abkuerzung rein gar nichts\\nueber die Leistungsfaehigkeit und die Moeglichkeiten von UUCP\\naus.\\n\\nUUCP\\n\\nUUCP ist ein Programmpaket, mit dem es moeglich ist, auf Daten\\nund Kommandos im Batchbetrieb auf einen anderen Rechner zu\\nuebertragen und dort ablaufen zu lassen. UUCP ist also im\\nPrinzip eine Methode des RJE, des Remote Job Entry. Dabei\\nwerden die Daten zunaechst lokal in einem Spooldirectory\\nzwischengelagert und spaeter gesammelt uebertragen. Nach der\\nUebertragung werden die Daten dann auf dem entfernten Rechner\\nden angegebenen Programmen zum Frasse vorgeworfen.\\n\\nMit diesem Verfahren ist vor vielen Jahren schon ein Netzwerk\\nentstanden, dann zwischen den einzelnen Maschinen oeffentliche\\nNachrichten, die News, und private Nachrichten, die Mail,\\naustauschen konnte. Da in diesen News sehr interessante\\nInformationen zu finden sind, wollten immer mehr Leute an\\ndiesen Netzwerk teilnehmen und so wuchs das Netz ueber die\\nUNIX-Welt und auch ueber die Verbreitung von UUCP als\\nTransportprotokoll hinaus.\\n\\nHeute ist es sehr schwer zu definieren, was USENET eigentlich\\nist. Einige Leute meinen, USENET sei die Menge alle der\\nRechner, die einige USENET-Bretter, die Newsgroups, empfangen\\nkoennen. Andere meinen, \"einige\" reicht nicht, \"alle\" sei\\nkorrekt. Wieder andere meinen, eine Mail-Anbindung gehoere auch\\ndazu. Wieder andere meinen, es sei einfacher zu sagen, was\\nUSENET nicht sei, obwohl das manchmal seltsame Blueten treibt.\\n(\" USENET is not your mother. \")\\n\\nWie dem auch sei, die Leute, die USENET kennen, wissen auch, ob\\nsie im USENET sind.\\n\\nWas kann man also im USENET bekommen?\\n\\nNun, man bekommt die News. \"Die News\" sind etwa 250 MB Text in\\n14 Tagen, unterteilt in etwa 1700 Newsgroups - dem USENET\\nAequivalent von Brettern.\\n\\nAber lassen wir USENET selbst zu Wort kommen:\\n\\n\\nSubject: Total traffic through uunet for the last 2 weeks\\nDate: 10 Sep 91 15:37:25 GMT\\n\\n\\n124208 articles, totaling 245.649645 Mbytes (301.578027 including headers),\\nwere submitted from 13503 different Usenet sites by 33411 different\\nusers to 1657 different newsgroups for an average of 17.546403 Mbytes\\n(21.541288 including headers) per day.\\n\\n\\n                                  Article               Total\\n      Category          Count  Mbytes       Percent     Mbytes\\n      ----------------------------------------------------------\\n      alt               19520  61.810154       25%     70.716713\\n      comp              27677  58.003429       23%     70.340852\\n      rec               36922  56.160507       22%     72.292561\\n      soc               13642  25.087772       10%     31.550280\\n      talk               6326  12.777823        5%     15.943504\\n      misc               6539  10.348264        4%     13.274056\\n      sci                5086   9.326363        3%     11.734640\\n      news               2115   6.317744        2%      7.293113\\n\\n\\nUSENET ist schnell:   95% aller Artikel erreichen den zentralen\\nKnoten uunet noch am Tage der Entstehung.\\n\\n\"Propagation Delay to uunet: Average delay per article is 0.2 days\"\\n\\nDie \"Big 7\"\\n\\nUSENET ist unterteilt in verschiedene Hierarchien von Newsgroups.\\n7 dieser Hierarchien, die \"Big 7\" bilden das eigentliche USENET:\\n\\ncomp, rec, soc, talk, misc, sci und news.\\n\\nDiese Gruppen werden nach denselben festen und demokratischen\\nRegeln verwaltet.\\n\\nDie achte grosse Hierarchie, alt, entspricht in etwa dem /T-NETZ.\\nHier herrscht totale Anarchie: Wer eine Newsgroup haben moechte,\\nkuendigt das in alt.config an und richtet sie ein.\\n\\nAber wieder sagt USENET selber etwas ueber sich aus:\\n\\n\\nSubject: List of Active Newsgroups\\nDate: 9 Sep 91 00:15:02 GMT\\n\\n\\nThe following is a list of currently active USENET newsgroups as of\\n8 Sep 1991.  The groups distributed worldwide are divided\\ninto seven broad classifications:  \"news\", \"soc\", \"talk\", \"misc\",\\n\"sci\", \"comp\" and \"rec\".  Each of these classifications is organized\\ninto groups and subgroups according to topic.\\n\\n\\n\"comp\"  Topics of interest to both computer professionals and\\n      hobbyists, including topics in computer science, software\\n      source, and information on hardware and software systems.\\n\\n\"sci\"   Discussions marked by special and usually practical knowledge,\\n      relating to research in or application of the established\\n      sciences.\\n\"misc\"  Groups addressing themes not easily classified under any of the\\n      other headings or which incorporate themes from multiple\\n      categories.\\n\\n\"soc\" Groups primarily addressing social issues and socializing.\\n\\n\"talk\" Groups largely debate-oriented and tending to feature long\\n      discussions without resolution and without appreciable amounts\\n      of generally useful information.\\n\\n\"news\" Groups concerned with the news network and software themselves.\\n\\n\"rec\" Groups oriented towards hobbies and recreational activities.\\n\\n\\nThese \"world\" newsgroups are (usually) circulated around the entire\\nUSENET -- this implies world-wide distribution.  Not all groups\\nactually enjoy such wide distribution, however.  The European Usenet\\nand Eunet sites take only a selected subset of the more \"technical\"\\ngroups, and controversial \"noise\" groups are often not carried by many\\nsites in the US and Canada (these groups are often under the \"talk\"\\nand \"soc\" classifications).  Many sites do not carry some or all of\\nthe comp.binaries groups.\\n[ ... Rest weggelassen ... ]\\n\\nAuch ueber die \"alt\"-Gruppen findet man etwas im Netz selber:\\n\\n\\nSubject: Alternative Newsgroup Hierarchies, Part I\\nDate: 9 Sep 91 00:15:04 GMT\\n\\n\\nIntroduction\\n\\nThe Usenet software allows the support and transport of hierarchies of\\nnewsgroups not part of the \"traditional\" Usenet through use of the\\ndistribution mechanism. These hierarchies of groups are available to\\nsites wishing to support them and finding a feed.  In general, these\\ngroups are not carried by the entire network because of their volume,\\nrestricted spheres of interest, or a different set of administrative\\nrules and concerns.\\n\\nIn general, it is a bad idea to forward these newsgroups to your\\nneighbors without asking them first; they should only be received at a\\nsite by choice.  Not only is this generally-accepted net etiquette, it\\nhelps to preserve the freedom to do and say as the posters please in\\nthese newsgroups, as the only people who get them are those who asked\\nto get them.  This freedom is more restricted in the Usenet as a\\nwhole, because every mainstream posting and every mainstream newsgroup\\nname must be acceptable to a much wider audience than is present in\\nthese hierarchies.  Because of the sheer size of the mainstream\\nUsenet, extra-long or controversial postings are more likely to cause\\nproblems when posted to the Usenet; however, these alternative\\nhierarchies exist precisely to support those kinds of postings (if\\ngermane to the hierarchy).\\n\\nUsually, there is is no restriction on getting these groups as long\\nas you have the capacity to receive, store, and forward the groups;\\n2.10.3 or 2.11 news is required to make the distribution mechanism\\nwork properly for these groups.  How to join each distribution is\\ndescribed below.\\n\\nNote that the \"uunet\" service carries all of these hierarchies.\\nContact uunet!uunet-request for subscription details.\\n\\nAlso note -- the lists in this article are totally unofficial and\\npossibly incomplete or inaccurate.  I try to keep the lists up-to-date,\\nbut make no guarantee that any of the information contained corresponds\\nwith the named groups in any significant way.  Corrections and comments\\nshould be mailed directly to me.\\n\\nAlt\\n\\n\"alt\" is a collection of newsgroups which are being distributed\\nby a collection of sites that choose to carry them.  Many Usenet sites\\nare not interested in these groups.  Here is a recent list\\nof the \\'alt\" newsgroups:\\n[ ... Liste gestrichen ... ]\\n\\n',\n",
       " '\\n\\n\\n\\nFuzzy Expert Systems\\n\\n\\n\\n\\nFuzzy Expert Systems\\n\\nThis is the second part in a three-part series of introductory\\narticles on the fuzzy field. The preceding article was titled\\n\"What is Fuzzy Logic?\", and\\nthe next article will be titled \"What\\nis Fuzzy Control?\". \\nOne point I didn\\'t make in my previous article, \"What is Fuzzy Logic\", is that in\\npractice, the terms fuzzy subset and membership function get used\\nnearly interchangeably. I\\'ll probably slip up and swap back and\\nforth some - my apologies in advance. \\n\\nWhat is a Fuzzy Expert System?\\nPut as simply as possible, a fuzzy expert system is an expert\\nsystem that uses fuzzy logic instead of Boolean logic. In other\\nwords, a fuzzy expert system is a collection of membership\\nfunctions and rules that are used to reason about data. Unlike\\nconventional expert systems, which are mainly symbolic reasoning\\nengines, fuzzy expert systems are oriented toward numerical\\nprocessing. \\nThe rules in a fuzzy expert system are usually of a form\\nsimilar to the following: \\n\\n    if x is low and y is high then z = medium\\n\\nwhere x and y are input variables (names for know data\\nvalues), z is an output variable (a name for a data value to be\\ncomputed), low is a membership function (fuzzy subset) defined on\\nx, high is a membership function defined on y, and medium is a\\nmembership function defined on z. The part of the rule between\\nthe \"if\" and \"then\" is the rule\\'s _premise_\\nor _antecedent_. This is a fuzzy logic expression that describes\\nto what degree the rule is applicable. The part of the rule\\nfollowing the \"then\" is the rule\\'s _conclusion_ or\\n_consequent_. This part of the rule assigns a membership function\\nto each of one or more output variables. Most tools for working\\nwith fuzzy expert systems allow more than one conclusion per\\nrule. \\nA typical fuzzy expert system has more than one rule. The\\nentire group of rules is collectively known as a _rulebase_ or\\n_knowledge base_. \\n\\nThe Inference Process\\nWith the definition of the rules and membership functions in\\nhand, we now need to know how to apply this knowledge to specific\\nvalues of the input variables to compute the values of the output\\nvariables. This process is referred to as _inferencing_. In a\\nfuzzy expert system, the inference process is a combination of\\nfour subprocesses: _fuzzification_, _inference_, _composition_,\\nand _defuzzification_. The defuzzification subprocess is\\noptional. \\nFor the sake of example in the following discussion, assume\\nthat the variables x, y, and z all take on values in the interval\\n[ 0, 10 ], and that we have the following membership functions\\nand rules defined. \\n\\n  low(t)  = 1 - t / 10\\n  high(t) = t / 10\\n\\n  rule 1: if x is low and y is low then z is high\\n  rule 2: if x is low and y is high then z is low\\n  rule 3: if x is high and y is low then z is low\\n  rule 4: if x is high and y is high then z is high\\n\\nNotice that instead of assigning a single value to the output\\nvariable z, each rule assigns an entire fuzzy subset (low or\\nhigh). \\nNotes: \\n\\nIn this example, low(t)+high(t)=1.0 for all t. This is\\n        not required, but it is fairly common. \\nThe value of t at which low(t) is maximum is the same as\\n        the value of t at which high(t) is minimum, and\\n        vice-versa. This is also not required, but fairly common. \\nThe same membership functions are used for all variables.\\n        This isn\\'t required, and is also *not* common. \\n\\n\\nFuzzification\\nIn the fuzzification subprocess, the membership functions\\ndefined on the input variables are applied to their actual\\nvalues, to determine the degree of truth for each rule premise.\\nThe degree of truth for a rule\\'s premise is sometimes referred to\\nas its _alpha_. If a rule\\'s premise has a nonzero degree of truth\\n(if the rule applies at all...) then the rule is said to _fire_. \\nFor example: \\n\\nx       y       low(x)  high(x) low(y)  high(y) alpha1  alpha2  alpha3  alpha4\\n------------------------------------------------------------------------------\\n0.0     0.0     1.0     0.0     1.0     0.0     1.0     0.0     0.0     0.0\\n0.0     3.2     1.0     0.0     0.68    0.32    0.68    0.32    0.0     0.0\\n0.0     6.1     1.0     0.0     0.39    0.61    0.39    0.61    0.0     0.0\\n0.0     10.0    1.0     0.0     0.0     1.0     0.0     1.0     0.0     0.0\\n3.2     0.0     0.68    0.32    1.0     0.0     0.68    0.0     0.32    0.0\\n6.1     0.0     0.39    0.61    1.0     0.0     0.39    0.0     0.61    0.0\\n10.0    0.0     0.0     1.0     1.0     0.0     0.0     0.0     1.0     0.0\\n3.2     3.1     0.68    0.32    0.69    0.31    0.68    0.31    0.32    0.32\\n3.2     3.3     0.68    0.32    0.67    0.33    0.67    0.33    0.32    0.32\\n10.0    10.0    0.0     1.0     0.0     1.0     0.0     0.0     0.0     1.0\\n\\n\\nInference\\nIn the inference subprocess, the truth value for the premise\\nof each rule is computed, and applied to the conclusion part of\\neach rule. This results in one fuzzy subset to be assigned to\\neach output variable for each rule. \\nI\\'ve only seen two _inference methods_ or _inference rules_:\\n_MIN_ and _PRODUCT_. In MIN inferencing, the output membership\\nfunction is clipped off at a height corresponding to the rule\\npremise\\'s computed degree of truth. This corresponds to the\\ntraditional interpretation of the fuzzy logic AND operation. In\\nPRODUCT inferencing, the output membership function is scaled by\\nthe rule premise\\'s computed degree of truth. \\nDue to the limitations of posting this as raw ASCII, I can\\'t\\ndraw you a decent diagram of the results of these methods.\\nTherefore I\\'ll give the example results in the same functional\\nnotation I used for the membership functions above. \\nFor example, let\\'s look at rule 1 for x = 0.0 and y = 3.2. As\\nshown in the table above, the premise degree of truth works out\\nto 0.68. For this rule, MIN inferencing will assign z the fuzzy\\nsubset defined by the membership function: \\n\\n    rule1(z) = { z / 10, if z <= 6.8\\n                 0.68,   if z >= 6.8 }\\n\\nFor the same conditions, PRODUCT inferencing will assign z the\\nfuzzy subset defined by the membership function: \\n\\n    rule1(z) = 0.68 * high(z)\\n             = 0.068 * z\\n\\nNote: I\\'m using slightly nonstandard terminology here.\\nIn most texts, the term \"inference method\" is used to\\nmean the combination of the things I\\'m referring to separately\\nhere as \"inference\" and \"composition.\"\\nTherefore, you\\'ll see terms such as \"MAX-MIN inference\"\\nand \"SUM-PRODUCT inference\" in the literature. They\\nmean the combination of MAX composition and MIN inference, or SUM\\ncomposition and PRODUCT inference respectively, to use my\\nterminology. You\\'ll also see the reverse terms\\n\"MIN-MAX\" and \"PRODUCT-SUM\" - these mean the\\nsame things as the reverse order. I think it\\'s clearer to\\ndescribe the two processes separately. \\n\\nComposition\\nIn the composition subprocess, all of the fuzzy subsets\\nassigned to each output variable are combined together to form a\\nsingle fuzzy subset for each output variable. \\nI\\'m familiar with two _composition rules_: _MAX composition_\\nand _SUM composition_. In MAX composition, the combined output\\nfuzzy subset is constructed by taking the pointwise maximum over\\nall of the fuzzy subsets assigned to the output variable by the\\ninference rule. In SUM composition the combined output fuzzy\\nsubset is constructed by taking the pointwise sum over all of the\\nfuzzy subsets assigned to the output variable by the inference\\nrule. Note that this can result in truth values greater than one!\\nFor this reason, SUM composition is only used when it will be\\nfollowed by a defuzzification method, such as the CENTROID\\nmethod, that doesn\\'t have a problem with this odd case. \\nFor example, assume x = 0.0 and y = 3.2. MIN inferencing would\\nassign the following four fuzzy subsets to z: \\n\\n      rule1(z) = { z / 10,     if z <= 6.8\\n                   0.68,       if z >= 6.8 }\\n\\n      rule2(z) = { 0.32,       if z <= 6.8\\n                   1 - z / 10, if z >= 6.8 }\\n\\n      rule3(z) = 0.0\\n\\n      rule4(z) = 0.0\\n\\nMAX composition would result in the fuzzy subset: \\n\\n      fuzzy(z) = { 0.32,       if z <= 3.2\\n                   z / 10,     if 3.2 <= z <= 6.8\\n                   0.68,       if z >= 6.8 }\\n\\nPRODUCT inferencing would assign the following four fuzzy\\nsubsets to z: \\n\\n      rule1(z) = 0.068 * z\\n      rule2(z) = 0.32 - 0.032 * z\\n      rule3(z) = 0.0\\n      rule4(z) = 0.0\\n\\nSUM composition would result in the fuzzy subset: \\n\\n      fuzzy(z) = 0.32 + 0.036 * z\\n\\n\\nDefuzzification\\nSometimes it is useful to just examine the fuzzy subsets that\\nare the result of the composition process, but more often, this\\n_fuzzy value_ needs to be converted to a single number - a _crisp\\nvalue_. This is what the defuzzification subprocess does. \\nThere are more defuzzification methods than you can shake a\\nstick at. A couple of years ago, Mizumoto did a short paper that\\ncompared roughly thirty defuzzification methods. Two of the more\\ncommon techniques are the CENTROID and MAXIMUM methods. In the\\nCENTROID method, the crisp value of the output variable is\\ncomputed by finding the variable value of the center of gravity\\nof the membership function for the fuzzy value. In the MAXIMUM\\nmethod, one of the variable values at which the fuzzy subset has\\nits maximum truth value is chosen as the crisp value for the\\noutput variable. There are several variations of the MAXIMUM\\nmethod that differ only in what they do when there is more than\\none variable value at which this maximum truth value occurs. One\\nof these, the AVERAGE-OF-MAXIMA method, returns the average of\\nthe variable values at which the maximum truth value occurs. \\nFor example, go back to our previous examples. Using MAX-MIN\\ninferencing and AVERAGE-OF-MAXIMA defuzzification results in a\\ncrisp value of 8.4 for z. Using PRODUCT-SUM inferencing and\\nCENTROID defuzzification results in a crisp value of 6.7 for z. \\nNote: sometimes the composition and defuzzification\\nprocesses are combined, taking advantage of mathematical\\nrelationships that simplify the process of computing the final\\noutput variable values. \\nAfter all this ... \\n\\nWhere are Fuzzy Expert Systems Used?\\nTo date, fuzzy expert systems are the most common use of fuzzy\\nlogic. They are used in several wide-ranging fields, including: \\n\\nLinear and nonlinear control. \\nPattern recognition. \\nFinancial systems. \\n\\nand many others I can\\'t think of. It\\'s late. I\\'m going home!\\n:-) \\n\\n---\\nErik Horstkotte, Togai InfraLogic, Inc.\\nThe World\\'s Source for Fuzzy Logic Solutions (The company, not me!)\\nerik@til.com, gordius!til!erik - (714) 975-8522\\ninfo@til.com for info, fuzzy-server@til.com for fuzzy mail-server\\n\\n\\n\\nWebMina@austinlinks.com\\n© 2000 SiteTerrific \\n  Web Solutions.  \\n  All rights Reserved \\n\\xa0\\n\\n\\n\\xa0\\n\\n\\n\\n',\n",
       " '\\n\\nLevenshtein Distance\\n\\nLevenshtein Distance, in Three Flavors\\nby Michael Gilleland, \\nMerriam Park Software\\n\\nThe purpose of this short essay is to describe the Levenshtein distance algorithm\\nand show how it can be implemented in three different programming languages.\\n\\n\\nWhat is Levenshtein Distance?\\nDemonstration\\nThe Algorithm\\nSource Code, in Three Flavors\\nReferences\\nOther Flavors\\n\\n\\nWhat is Levenshtein Distance?\\n\\nLevenshtein distance (LD) is a measure of the similarity between two strings,\\nwhich we will refer to as the source string (s) and the target string (t).\\nThe distance is the number of deletions, insertions, or substitutions\\nrequired to transform s into t. For example,\\n\\nIf s is \"test\" and t is \"test\", then LD(s,t) = 0, because\\nno transformations are needed. The strings are already\\nidentical. \\nIf s is \"test\" and t is \"tent\", then LD(s,t) = 1, because one\\nsubstitution (change \"s\" to \"n\") is sufficient to transform s into t.\\n\\nThe greater the Levenshtein distance, the more different the strings are.\\n\\n\\nLevenshtein distance is named after the Russian scientist Vladimir\\nLevenshtein, who devised the algorithm in 1965. If you can\\'t spell or pronounce\\nLevenshtein, the metric is also sometimes called edit distance.\\n\\n\\nThe Levenshtein distance algorithm has been used in:\\n\\nSpell checking\\nSpeech recognition\\nDNA analysis\\nPlagiarism detection\\n\\n\\n\\nDemonstration\\n\\nThe following simple Java applet allows you to experiment with\\ndifferent strings and compute their Levenshtein distance:\\n\\n\\n\\n\\n\\nThe Algorithm\\nSteps\\n\\nStep\\nDescription\\n\\n1\\nSet n to be the length of s.\\nSet m to be the length of t.\\nIf n = 0, return m and exit.\\nIf m = 0, return n and exit.\\nConstruct a matrix containing 0..m rows and 0..n columns.\\n\\n\\n\\n2\\n\\nInitialize the first row to 0..n.\\nInitialize the first column to 0..m.\\n\\n\\n\\n3\\nExamine each character of s (i from 1 to n).\\n\\n\\n4\\nExamine each character of t (j from 1 to m).\\n\\n\\n5\\nIf s[i] equals t[j], the cost is 0.\\nIf s[i] doesn\\'t equal t[j], the cost is 1.\\n\\n\\n6\\nSet cell d[i,j] of the matrix equal to the minimum of:\\na. The cell immediately above plus 1: d[i-1,j] + 1.\\nb. The cell immediately to the left plus 1: d[i,j-1] + 1.\\nc. The cell diagonally above and to the left plus the cost: d[i-1,j-1] + cost.\\n\\n\\n\\n7\\nAfter the iteration steps (3, 4, 5, 6) are complete, the\\ndistance is found in cell d[n,m].\\n\\n\\n\\nExample\\nThis section shows how the Levenshtein distance is computed when the source\\nstring is \"GUMBO\" and the target string is \"GAMBOL\".\\nSteps 1 and 2\\n\\n\\n\\xa0\\n\\xa0\\nG\\nU\\nM\\nB\\nO\\n\\n\\n\\xa0\\n0\\n1\\n2\\n3\\n4\\n5\\n\\n\\nG\\n1\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\n\\nA\\n2\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\n\\nM\\n3\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\n\\nB\\n4\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\n\\nO\\n5\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\n\\nL\\n6\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\n\\nSteps 3 to 6 When i = 1\\n\\n\\n\\xa0\\n\\xa0\\nG\\nU\\nM\\nB\\nO\\n\\n\\n\\xa0\\n0\\n1\\n2\\n3\\n4\\n5\\n\\n\\nG\\n1\\n0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\n\\nA\\n2\\n1\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\n\\nM\\n3\\n2\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\n\\nB\\n4\\n3\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\n\\nO\\n5\\n4\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\n\\nL\\n6\\n5\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\n\\nSteps 3 to 6 When i = 2\\n\\n\\n\\xa0\\n\\xa0\\nG\\nU\\nM\\nB\\nO\\n\\n\\n\\xa0\\n0\\n1\\n2\\n3\\n4\\n5\\n\\n\\nG\\n1\\n0\\n1\\n\\xa0\\n\\xa0\\n\\xa0\\n\\n\\nA\\n2\\n1\\n1\\n\\xa0\\n\\xa0\\n\\xa0\\n\\n\\nM\\n3\\n2\\n2\\n\\xa0\\n\\xa0\\n\\xa0\\n\\n\\nB\\n4\\n3\\n3\\n\\xa0\\n\\xa0\\n\\xa0\\n\\n\\nO\\n5\\n4\\n4\\n\\xa0\\n\\xa0\\n\\xa0\\n\\n\\nL\\n6\\n5\\n5\\n\\xa0\\n\\xa0\\n\\xa0\\n\\n\\nSteps 3 to 6 When i = 3\\n\\n\\n\\xa0\\n\\xa0\\nG\\nU\\nM\\nB\\nO\\n\\n\\n\\xa0\\n0\\n1\\n2\\n3\\n4\\n5\\n\\n\\nG\\n1\\n0\\n1\\n2\\n\\xa0\\n\\xa0\\n\\n\\nA\\n2\\n1\\n1\\n2\\n\\xa0\\n\\xa0\\n\\n\\nM\\n3\\n2\\n2\\n1\\n\\xa0\\n\\xa0\\n\\n\\nB\\n4\\n3\\n3\\n2\\n\\xa0\\n\\xa0\\n\\n\\nO\\n5\\n4\\n4\\n3\\n\\xa0\\n\\xa0\\n\\n\\nL\\n6\\n5\\n5\\n4\\n\\xa0\\n\\xa0\\n\\n\\nSteps 3 to 6 When i = 4\\n\\n\\n\\xa0\\n\\xa0\\nG\\nU\\nM\\nB\\nO\\n\\n\\n\\xa0\\n0\\n1\\n2\\n3\\n4\\n5\\n\\n\\nG\\n1\\n0\\n1\\n2\\n3\\n\\xa0\\n\\n\\nA\\n2\\n1\\n1\\n2\\n3\\n\\xa0\\n\\n\\nM\\n3\\n2\\n2\\n1\\n2\\n\\xa0\\n\\n\\nB\\n4\\n3\\n3\\n2\\n1\\n\\xa0\\n\\n\\nO\\n5\\n4\\n4\\n3\\n2\\n\\xa0\\n\\n\\nL\\n6\\n5\\n5\\n4\\n3\\n\\xa0\\n\\n\\nSteps 3 to 6 When i = 5\\n\\n\\n\\xa0\\n\\xa0\\nG\\nU\\nM\\nB\\nO\\n\\n\\n\\xa0\\n0\\n1\\n2\\n3\\n4\\n5\\n\\n\\nG\\n1\\n0\\n1\\n2\\n3\\n4\\n\\n\\nA\\n2\\n1\\n1\\n2\\n3\\n4\\n\\n\\nM\\n3\\n2\\n2\\n1\\n2\\n3\\n\\n\\nB\\n4\\n3\\n3\\n2\\n1\\n2\\n\\n\\nO\\n5\\n4\\n4\\n3\\n2\\n1\\n\\n\\nL\\n6\\n5\\n5\\n4\\n3\\n2\\n\\n\\nStep 7\\n\\nThe distance is in the lower right hand corner of the matrix, i.e. 2.\\nThis corresponds to our intuitive realization that \"GUMBO\" can be\\ntransformed into \"GAMBOL\" by substituting \"A\" for \"U\" and adding \"L\"\\n(one substitution and 1 insertion = 2 changes).\\n\\n\\nSource Code, in Three Flavors\\n\\nReligious wars often flare up whenever engineers discuss differences between\\nprogramming languages. A typical assertion is Allen Holub\\'s claim in a JavaWorld\\n\\narticle (July 1999):\\n\"Visual Basic, for example, isn\\'t in the least bit object-oriented.\\nNeither is Microsoft Foundation Classes (MFC) or most of the other\\nMicrosoft technology that claims to be object-oriented.\" \\n\\n\\nA salvo from a different direction is Simson Garfinkels\\'s \\n\\narticle in Salon (Jan. 8, 2001) entitled \"Java: Slow, ugly and irrelevant\",\\nwhich opens with the unequivocal words \"I hate Java\".\\n\\n\\nWe prefer to take a neutral stance in these religious wars.\\nAs a practical matter, if a problem can be solved in one \\nprogramming language, you can usually solve it in another as well.\\nA good programmer is able to move from one language to another with\\nrelative ease, and learning a completely new language should not\\npresent any major difficulties, either. A programming language is\\na means to an end, not an end in itself.\\n\\n\\nAs a modest illustration of this principle of neutrality, we\\npresent source code which implements the Levenshtein \\ndistance algorithm in the following programming languages:\\n\\nJava\\nC++\\nVisual Basic\\n\\n\\n\\nJava\\n\\npublic class Distance {\\n\\n  //****************************\\n  // Get minimum of three values\\n  //****************************\\n\\n  private int Minimum (int a, int b, int c) {\\n  int mi;\\n\\n    mi = a;\\n    if (b < mi) {\\n      mi = b;\\n    }\\n    if (c < mi) {\\n      mi = c;\\n    }\\n    return mi;\\n\\n  }\\n\\n  //*****************************\\n  // Compute Levenshtein distance\\n  //*****************************\\n\\n  public int LD (String s, String t) {\\n  int d[][]; // matrix\\n  int n; // length of s\\n  int m; // length of t\\n  int i; // iterates through s\\n  int j; // iterates through t\\n  char s_i; // ith character of s\\n  char t_j; // jth character of t\\n  int cost; // cost\\n\\n    // Step 1\\n\\n    n = s.length ();\\n    m = t.length ();\\n    if (n == 0) {\\n      return m;\\n    }\\n    if (m == 0) {\\n      return n;\\n    }\\n    d = new int[n+1][m+1];\\n\\n    // Step 2\\n\\n    for (i = 0; i <= n; i++) {\\n      d[i][0] = i;\\n    }\\n\\n    for (j = 0; j <= m; j++) {\\n      d[0][j] = j;\\n    }\\n\\n    // Step 3\\n\\n    for (i = 1; i <= n; i++) {\\n\\n      s_i = s.charAt (i - 1);\\n\\n      // Step 4\\n\\n      for (j = 1; j <= m; j++) {\\n\\n        t_j = t.charAt (j - 1);\\n\\n        // Step 5\\n\\n        if (s_i == t_j) {\\n          cost = 0;\\n        }\\n        else {\\n          cost = 1;\\n        }\\n\\n        // Step 6\\n\\n        d[i][j] = Minimum (d[i-1][j]+1, d[i][j-1]+1, d[i-1][j-1] + cost);\\n\\n      }\\n\\n    }\\n\\n    // Step 7\\n\\n    return d[n][m];\\n\\n  }\\n\\n}\\n\\n\\nC++\\n\\nIn C++, the size of an array must be a constant, and this code fragment\\ncauses an error at compile time:\\n\\n\\nint sz = 5;\\nint arr[sz];\\n\\n\\nThis limitation makes the following C++ code slightly more complicated than it would be\\nif the matrix could simply be declared as a two-dimensional array, with a size\\ndetermined at run-time.\\n\\n\\nHere is the definition of the class (distance.h):\\n\\n\\nclass Distance\\n{\\n  public:\\n    int LD (char const *s, char const *t);\\n  private:\\n    int Minimum (int a, int b, int c);\\n    int *GetCellPointer (int *pOrigin, int col, int row, int nCols);\\n    int GetAt (int *pOrigin, int col, int row, int nCols);\\n    void PutAt (int *pOrigin, int col, int row, int nCols, int x);\\n}; \\n\\n\\nHere is the implementation of the class (distance.cpp):\\n\\n\\n#include \"distance.h\"\\n#include <string.h>\\n#include <malloc.h>\\n\\n//****************************\\n// Get minimum of three values\\n//****************************\\n\\nint Distance::Minimum (int a, int b, int c)\\n{\\nint mi;\\n\\n  mi = a;\\n  if (b < mi) {\\n    mi = b;\\n  }\\n  if (c < mi) {\\n    mi = c;\\n  }\\n  return mi;\\n\\n}\\n\\n//**************************************************\\n// Get a pointer to the specified cell of the matrix\\n//************************************************** \\n\\nint *Distance::GetCellPointer (int *pOrigin, int col, int row, int nCols)\\n{\\n  return pOrigin + col + (row * (nCols + 1));\\n}\\n\\n//*****************************************************\\n// Get the contents of the specified cell in the matrix \\n//*****************************************************\\n\\nint Distance::GetAt (int *pOrigin, int col, int row, int nCols)\\n{\\nint *pCell;\\n\\n  pCell = GetCellPointer (pOrigin, col, row, nCols);\\n  return *pCell;\\n\\n}\\n\\n//*******************************************************\\n// Fill the specified cell in the matrix with the value x\\n//*******************************************************\\n\\nvoid Distance::PutAt (int *pOrigin, int col, int row, int nCols, int x)\\n{\\nint *pCell;\\n\\n  pCell = GetCellPointer (pOrigin, col, row, nCols);\\n  *pCell = x;\\n\\n}\\n\\n//*****************************\\n// Compute Levenshtein distance\\n//*****************************\\n\\nint Distance::LD (char const *s, char const *t)\\n{\\nint *d; // pointer to matrix\\nint n; // length of s\\nint m; // length of t\\nint i; // iterates through s\\nint j; // iterates through t\\nchar s_i; // ith character of s\\nchar t_j; // jth character of t\\nint cost; // cost\\nint result; // result\\nint cell; // contents of target cell\\nint above; // contents of cell immediately above\\nint left; // contents of cell immediately to left\\nint diag; // contents of cell immediately above and to left\\nint sz; // number of cells in matrix\\n\\n  // Step 1\\t\\n\\n  n = strlen (s);\\n  m = strlen (t);\\n  if (n == 0) {\\n    return m;\\n  }\\n  if (m == 0) {\\n    return n;\\n  }\\n  sz = (n+1) * (m+1) * sizeof (int);\\n  d = (int *) malloc (sz);\\n\\n  // Step 2\\n\\n  for (i = 0; i <= n; i++) {\\n    PutAt (d, i, 0, n, i);\\n  }\\n\\n  for (j = 0; j <= m; j++) {\\n    PutAt (d, 0, j, n, j);\\n  }\\n\\n  // Step 3\\n\\n  for (i = 1; i <= n; i++) {\\n\\n    s_i = s[i-1];\\n\\n    // Step 4\\n\\n    for (j = 1; j <= m; j++) {\\n\\n      t_j = t[j-1];\\n\\n      // Step 5\\n\\n      if (s_i == t_j) {\\n        cost = 0;\\n      }\\n      else {\\n        cost = 1;\\n      }\\n\\n      // Step 6 \\n\\n      above = GetAt (d,i-1,j, n);\\n      left = GetAt (d,i, j-1, n);\\n      diag = GetAt (d, i-1,j-1, n);\\n      cell = Minimum (above + 1, left + 1, diag + cost);\\n      PutAt (d, i, j, n, cell);\\n    }\\n  }\\n\\n  // Step 7\\n\\n  result = GetAt (d, n, m, n);\\n  free (d);\\n  return result;\\n\\t\\n}\\n\\n\\nVisual Basic\\n\\n\\'*******************************\\n\\'*** Get minimum of three values\\n\\'*******************************\\n\\nPrivate Function Minimum(ByVal a As Integer, _\\n                         ByVal b As Integer, _\\n                         ByVal c As Integer) As Integer\\nDim mi As Integer\\n                          \\n  mi = a\\n  If b < mi Then\\n    mi = b\\n  End If\\n  If c < mi Then\\n    mi = c\\n  End If\\n  \\n  Minimum = mi\\n                          \\nEnd Function\\n\\n\\'********************************\\n\\'*** Compute Levenshtein Distance\\n\\'********************************\\n\\nPublic Function LD(ByVal s As String, ByVal t As String) As Integer\\nDim d() As Integer \\' matrix\\nDim m As Integer \\' length of t\\nDim n As Integer \\' length of s\\nDim i As Integer \\' iterates through s\\nDim j As Integer \\' iterates through t\\nDim s_i As String \\' ith character of s\\nDim t_j As String \\' jth character of t\\nDim cost As Integer \\' cost\\n  \\n  \\' Step 1\\n  \\n  n = Len(s)\\n  m = Len(t)\\n  If n = 0 Then\\n    LD = m\\n    Exit Function\\n  End If \\n  If m = 0 Then\\n    LD = n\\n    Exit Function\\n  End If \\n  ReDim d(0 To n, 0 To m) As Integer\\n  \\n  \\' Step 2\\n  \\n  For i = 0 To n\\n    d(i, 0) = i\\n  Next i\\n  \\n  For j = 0 To m\\n    d(0, j) = j\\n  Next j\\n\\n  \\' Step 3\\n\\n  For i = 1 To n\\n    \\n    s_i = Mid$(s, i, 1)\\n    \\n    \\' Step 4\\n    \\n    For j = 1 To m\\n      \\n      t_j = Mid$(t, j, 1)\\n      \\n      \\' Step 5\\n      \\n      If s_i = t_j Then\\n        cost = 0\\n      Else\\n        cost = 1\\n      End If\\n      \\n      \\' Step 6\\n      \\n      d(i, j) = Minimum(d(i - 1, j) + 1, d(i, j - 1) + 1, d(i - 1, j - 1) + cost)\\n    \\n    Next j\\n    \\n  Next i\\n  \\n  \\' Step 7\\n  \\n  LD = d(n, m)\\n  Erase d\\n\\nEnd Function\\n\\n\\nReferences\\nOther discussions of Levenshtein distance may be found at:\\n\\n\\nhttp://www.csse.monash.edu.au/~lloyd/tildeAlgDS/Dynamic/Edit.html (Lloyd Allison)\\n\\nhttp://www.cut-the-knot.com/do_you_know/Strings.html (Alex Bogomolny)\\n\\nhttp://www-igm.univ-mlv.fr/~lecroq/seqcomp/node2.html (Thierry Lecroq)\\n\\n\\nOther Flavors\\n\\nThe following people have kindly consented to make their implementations\\nof the Levenshtein Distance Algorithm in various languages available here:\\n\\nEli Bendersky has written an implementation in\\nPerl.\\nBarbara Boehmer has written an\\nimplementation in Oracle PL/SQL.\\nRick Bourner has written an\\nimplementation in Objective-C.\\nChas Emerick has written an implementation in\\nJava, which avoids\\nan OutOfMemoryError which can occur when my Java implementation is used\\nwith very large strings.\\nJoseph Gama has written an implementation in \\nTSQL,\\nas part of a\\n\\npackage of TSQL functions at \\n\\nPlanet Source Code.\\nAnders Sewerin Johansen has written an implementation\\nin C++, which is more elegant, better optimized,\\nand more in the spirit of C++ than mine.\\nLasse Johansen has written an implementation in C#.\\nAlvaro Jeria Madariaga has written an implementation in\\nDelphi.\\nLorenzo Seidenari has written an implementation in \\nC, and\\nLars Rustemeier has provided a \\n \\nScheme wrapper for this\\nC implementation \\nas part of \\n\\nEggs Unlimited,\\na library of extensions to the \\n\\nChicken Scheme system. \\nSteve Southwell has written an implementation in \\nProgress 4gl.\\nJoerg F. Wittenberger has written an implementation in\\nRscheme.\\n\\n\\n\\nOther implementations outside these pages include:\\n\\nAn \\nEmacs Lisp implementation by Art Taylor.\\nA \\nPython implementation by Magnus Lie Hetland. \\nA Tcl\\nimplementation by Richard Suchenwirth (thanks to Stefan Seidler for pointing this out).\\n\\n\\n\\n\\n',\n",
       " '\\n\\n\\n\\n\\nLisp in Java\\n\\n\\n\\nThis project implements a subset of the Lisp language using Java.\\nThere is more information in the \\n\\n\\n\\nI have always been fascinated with LISP and with implementations of LISP.\\nBack in 1983 I wrote an implement in Pascal.  In the early 1990s I tackled\\nit once again, this time in C, because I wanted to be able to embed LISP\\ncode inside a C program easily.  The C implementation was pretty complete\\nand workable.  Then I decided to see if I could translate that \"easily\"\\ninto Java, since \"C and Java are so similar.\"  Ha!  What an eye-opener this\\nwas!  C and Java share only minimal similaries.  There are so many differences\\nthat one cannot just take C or C++ code and quickly work it into a Java\\nprogram.\\n\\n\\nAnyway, I did get the code rewritten and here is what I came up with.\\n\\n\\nThere are two ways to use the main LispInterpreter class.  One way is shown\\nin test0.java.  In this method you embed the Lisp s-expressions that you\\nwish to be evaluated in Strings and pass them to the interpreter.  This\\nnot only prints out the result but it also remembers \"state\" such as variables\\nyou might declare and their values, as well as user-defined LISP functions.\\n\\n\\nThe second way is to run a LISP shell interactively, and this is shown in\\ntest1.java.  This must be run from the MS-DOS prompt since it uses stdin.\\n\\n\\nYou can also store a bunch of s-expressions in a file and have the interpreter\\nread the file by using \\n\\n\\n     (readfile \\'filename)\\n\\n\\nA number of files have been created and tested.  They all start with the\\nprefix \"prog\" and have no suffix.  Some of them interactively evaluate\\ns-expressions while some declare user-defined functions.\\n\\n\\nWhile there are many things I didn\\'t implement, I did implement a lot of\\ninteresting things, even lambda expressions and mapcar.\\n\\n\\nBelow is more information about the project.\\n\\n\\n\\n                            Built-in functions\\n-----------------------------------------------------------------------------\\n\\n     =              atom               explode           null\\n     +              block              if                print\\n     -              car                implode           quote\\n     *              cdr                lambda            readfile\\n     /              comment            length            return\\n     %              cons               listp             return-from\\n     <              equal              mapcar            setq\\n     >              eval               not               while\\n\\n\\n                             Project History\\n-----------------------------------------------------------------------------\\n\\n6-11-97   Got one version to work finally.  This was largely based on the \\n          original C code with Mstrings added.\\n\\n6-12-97   Finished rewrite in which I gutted most of LispInterpreter and \\n          wrote a new class called LispParser.  It worked almost at once.\\n          There are a few minor bugs yet, such as the inequality of () and nil.\\n\\n5-8-98    Finally got the Windows 95 version to work.  I structured\\n          it into packages logically.  But it runs only under the MS-DOS\\n          prompt, not from the BASH-shell because it reads from standard\\n          input. \\n\\n6-4-98    Began rewriting major sections and using the new ParsedString instead\\n          of the old Mstring.\\n\\n6-6-98    Finished converting over to ParsedString.  Found numerous little\\n          bugs.  Streamlined LispSymbolTable.  Wrote up a huge number of tests\\n          and made sure it passed them all.  None of the new stuff like\\n          control structures or functions have been added.  Total LOC=1785.\\n\\n6-8-98    Started to add functions but decided to streamline by removing\\n          ParsedString and going solely with LispObject.\\n\\n6-9-98    Got the new version with only LispObject done up to pre-functions\\n          stage.  Lots of subtle debugging issues.\\n          Later got the essential functions to work.  Added if and block,\\n          which work okay, and also < and >.  Later added while and got that\\n          to work.\\n\\n6-10-98   Found a very subtle bug in the cdr() method of LispObject.  It \\n          wasn\\'t changing the \"orig\" String, hence (cdr \\'(b c)) would give\\n          the same thing over and over.  Once fixed, recursive functions\\n          worked fine.\\n\\n\\nHere are what the various demo programs do:\\n\\n\\n     test0.java  --  simple demonstration of reading an S-expression from\\n                     user, evaluating it, and printing the result\\n\\n     test1.java  --  general purpose LISP shell, calls read_eval_print()\\n\\n     test2.java  --  illustrates definition of user functions\\n\\n     test3.java  --  illustrates if statements\\n\\n     test4.java  --  illustrates block control structure\\n\\n     test5.java  --  tests all the condition functions (<, >, etc.)\\n\\n     test6.java  --  illustrates while loop\\n\\n     test7.java  --  illustrates while loop with early exit\\n\\n     test8.java  --  illustrates another while loop\\n\\n     test9.java  --  reads s-expressions from a file named \"prog\"\\n\\n     test10.java  --  reads s-expressions from a file named \"prog\", echos\\n                      the results of evaluation back to the screen\\n\\n     test11.java  --  reads s-expressions from a file named \"prog30\"\\n\\n     test12.java  --  shows how to create LispObjects in a Java program\\n\\n     test13.java  --  illustrates explode and implode functions\\n\\n     test14.java  --  illustrates mapcar\\n\\n     test15.java  --  general program that takes the name of a LISP source\\n                      file and runs it\\n\\n\\nTo compile...\\n\\nJust compile any one of the programs in this directory.  It will automatically\\ntrigger the Java compiler to descend into the \"lispinjava\" directory and\\ncompile everything there.\\n\\n\\n     % javac test0.java\\n\\n\\n\\n',\n",
       " ' Search Engine Sizes By Danny Sullivan , Editor September 2, 2003 Is bigger better, when it comes to the size of a search engine\\'s index? Not necessarily. However, a large index can help those who seek unusual or hard-to-find information. Consequently, when you seek the obscure, consider using a search engine with a large index. However, for general searches or for when looking for information about popular topics, a large index does not necessarily equal better results. Current Size Comparison - Search Engine Sizes Over Time Search Engine Size War I - SW-II - SW-III - SW-IV Related Search Engine Watch Articles Other Search Engine Size Articles - Size Resources Current Size Comparison The size figures below are unaudited and self-reported by each search engine (for audited figures, see the Search Engine Showdown web site listed below , which makes the best attempt at this). Figures show how many textual documents have been indexed, which includes HTML files, text documents, PDF files, Microsoft Office documents and other similar files. Image and multimedia files are not included. Nor are Google Groups discussion posts. Billions Of Textual Documents Indexed As of Sept 2, 2003 KEY : GG=Google, ATW=AllTheWeb, INK=Inktomi, TMA=Teoma, AV=AltaVista. See the Major Search Engines page for links to these services. Search Engine Sizes Over Time The chart below shows how self-reported search engine sizes have changed over the years. Only search engine still crawling the web are shown on the chart. Thus, players such as Northern Light, Excite, Infoseek and others that no longer crawl for their results are not displayed. Billions Of Textual Documents Indexed December 1995-September 2003 KEY: See above . Search Engine Size War I: Sept 1997-June 1999 When AltaVista appeared in December 1995, it used an index much larger than any of the other search engines at that time. Thus, competition forced others to increase their sizes in early 1996. But after these moves, sizes stayed about the same through September 1997. It was then that AltaVista and Inktomi really began the first of the Search Engine Size Wars, competing to claim the bragging rights of being biggest. Inktomi gave up after a couple of months, but Northern Light jumped in to compete with AltaVista to hit the 150 million page mark. MILLIONS Of Textual Documents Indexed December 1995-June 1999 KEY: See above plus NL=Northern Light, EX=Excite, LY=Lycos, GO=GO/Infoseek Search Engine Size War I: December 1997-June 1999 When AltaVista appeared in December 1995, it used an index much larger than any of the other search engines at that time. Thus, competition forced others to increase their sizes in early 1996. After these initial moves, sizes stayed about the same through September 1997. But by the end of that year, AltaVista and Inktomi began the first of the serious Search Engine Size Wars, competing to claim the bragging rights of being biggest. Inktomi failed to keep up, but Northern Light jumped in to compete with AltaVista in pursuit of the 150 million page mark. MILLIONS Of Textual Documents Indexed December 1995-June 1999 KEY: See above plus NL=Northern Light, EX=Excite, LY=Lycos, GO=GO/Infoseek Search Engine Size War II: September 1999-June 2000 Just when AltaVista and Northern Light were celebrating hitting the 150 million document mark, newcomer AllTheWeb appeared with a record-setting index size of 200 million documents. Suddenly, a new round of size escalation began. The title of biggest flip-flopped between AllTheWeb and AltaVista at first. However, Google put a decisive stop to the war in June 2000, when it set a new benchmark of 500 million pages indexed and started growing well past its challengers. MILLIONS Of Textual Documents Indexed September 1999-March 2002 KEY: See above plus NL=Northern Light Search Engine Size War III: June 2002-December 2002 After a long period of being the size king, AllTheWeb grabbed the title back from Google by declaring it had broken the 2 billion document mark. Soon after, Google grew the number of documents it reported indexing up to the 3 billion page mark. Inktomi also released a new version of its search engine that claimed this index level. Billions Of Textual Documents Indexed June 2002-September 2003 KEY: See above Search Engine Size War IV: August 2003-??? In August 2003, AllTheWeb claimed an index of 3.3 billion documents, putting it just past Google\\'s self-reported figure. Google responded within days by increasing its self-reported figure to 3.2 billion. It\\'s likely that over the coming months, new competition to hit the 4 billion document mark may happen. Related Search Engine Watch Articles Bigger is not necessarily better, though it can be for some searches. To better understand the importance of size, be sure to read some past articles from Search Engine Watch that deal with size issues, listed below: Search Engine Size Wars & Google\\'s Supplemental Results SearchDay, Sept. 3, 2003 http://searchenginewatch.com/searchday/article.php/3071371 Who has the biggest index? The search engine size wars have erupted again to dispute this -- and the new Google supplemental index is complicating matters. Google to Overture: Mine\\'s Bigger SearchDay, Aug. 27, 2003 http://www.searchenginewatch.com/searchday/article.php/3069221 Overture and Google have fired new salvos in the search engine size wars, expanding their databases of searchable web pages by millions of pages. FAST Sprints to 2.1 Billion Docs; Google Upgrades Appliance SearchDay, June 17, 2002 http://searchenginewatch.com/searchday/article.php/2160141 Covers the increase by FAST/AllTheWeb past the 2.1 billion document mark. Mapping the \\'Dark Net\\' SearchDay, Jan. 24, 2002 http://searchenginewatch.com/searchday/article.php/2159121 Researchers have discovered that up to 5 percent of the Internet is completely unreachable, impossible to access by web browser or search engine alike. Google Fires New Salvo in Search Engine Size Wars SearchDay, Dec. 11, 2001 http://searchenginewatch.com/searchday/article.php/2158371 Google\\'s web index has grown to more than 3 billion documents, including an unprecedented archive of Usenet newsgroup postings dating back to 1981. Google & FAST Move Up In Size The Search Engine Report, Nov. 3, 2000 Update on size increases by Google and FAST. Invisible Web Gets Deeper The Search Engine Report, Aug. 2, 2000 Covers a survey done to measure how much information exists outside of the search engines\\' reach. The company behind the survey is also offering up a solution for those who want tap into this \"hidden\" material. Google Announces Largest Index The Search Engine Report, July 5, 2000 Google breaks the 500 million page mark, but not all partners may tap into the large index. Search Engine Size Test Search Engine Watch, July 2000 http://searchenginewatch.com/sereport/article.php/2162821 Evaluated claims in index size, to see if the search engines measure up. Inktomi Reenters Battle For Biggest The Search Engine Report, June 2, 2000 Inktomi makes moves to again become one of the biggest search engines on the web. AltaVista Launches New Search Site The Search Engine Report, May 3, 2000 AltaVista adopts a crawling system similar to that used by Inktomi, as described in the article below. Numbers, Numbers -- But What Do They Mean? The Search Engine Report, March 3, 2000 A long look at the recent trend of quoting \"dual numbers\" in relation to index size and how to compare services that do this. FAST Gets Bigger, Partners With Lycos The Search Engine Report, Feb. 3, 2000 Details on FAST breaking the 300 million web page mark. Who\\'s The Biggest Of Them All? The Search Engine Report, Nov. 1, 1999 Discusses the difficulty of verifying index claims. Search Engine Coverage Study Published The Search Engine Report, Aug. 2, 1999 Detailed review of the Nature article about search engine coverage. FAST Announces Largest Search Engine The Search Engine Report, Aug. 2, 1999 Details about FAST claiming to have broken the 200 million web page barrier, with a link to more background about the company. Many Changes At AltaVista The Search Engine Report, July 6, 1999 Mentions that AltaVista plans to continue increasing its size. Google Goes Forward The Search Engine Report, July 6, 1999 Explains how Google can exceed the reach of its index. Northern Light Claims Largest Index The Search Engine Report, Feb. 2, 1999 Northern Light says that if self-reported sizes were audited, it would be number one. More on the issue, along with an update on size growth in general. Search Engine Sizes Scrutinized The Search Engine Report, April 30, 1998 Extensive details and analysis of the April 1998 Science study, which grabbed headlines across the world. The AltaVista Size Controversy The Search Engine Report, July 2, 1997 Recounts a public discussion of how AltaVista was only sampling some web pages completely ignoring other sites. How Big Are The Search Engines Search Engine Watch, June 13, 1997 http://searchenginewatch.com/sereport/article.php/2165301 Article within Search Engine Watch that explains the issues of index size in more depth. Does size really matter? Search Engine Size Articles Google Dominates New Size Showdowns Search Engine Showdown, Jan. 16, 2003 http://www.searchengineshowdown.com/newsarchive/000625.shtml Google leads the pack in terms of size, based on the latest estimates from Greg Notess When size does matter The Guardian, July 18, 2002 http://media.guardian.co.uk/newmedia/story/0,7496,757326,00.html Search Engine Watch associate editor Chris Sherman takes another look at when -- and when not -- index size matters. Openfind touts its efficiency e-Taiwannews.com, July 1, 2002 http://www.etaiwannews.com/Taiwan/2002/07/01/1025492420.htm Openfind, on taking on Google. The company claims an index of 3.5 billion pages. Google processes in the range of 150 million searches per day, by the way, not the 1.5 billion noted in the article. On the size of the World Wide Web Pandia, Oct. 14, 2001 http://www.pandia.com/sw-2001/57-websize.html There are now over 8 million web sites according to researchers at the Online Computer Library Center, but the web\\'s growth has slowed markedly when compared to previous years. The vast majority of web sites are written in English -- 73 percent, with German coming in at second place with 7 percent. Web links that stick BBC, June 14, 2000 http://news.bbc.co.uk/1/hi/sci/tech/790685.stm The average page contains 52 links, and the over 10 percent of the links on the web are broken, according to a company that\\'s developing a link monitoring service. The web is a bow tie Nature, May 11, 2000 http://www.nature.com/cgi-taf/DynaPage.taf?file=/nature/journal/v405/n6783/full/405113a0_fs.html Covers research by AltaVista and two other groups that found a \"bow tie\" pattern to how pages link across the web. Researchers work to eradicate broken hyperlinks News.com, March 7, 2000 http://news.com.com/2100-1023-237651.html Review of \"robust\" hyperlink system described in technical paper, below. Robust Hyperlinks Cost Just Five Words Each UC Berkeley, January 2000 http://www.cs.berkeley.edu/~phelps/Robust/papers/robust-hyperlinks.html Technical paper on how web pages could be assigned a lexical code to make it easier to locate them. Accessibility and Distribution of Information on the Web Nature, July 1999 http://wwwmetrics.com/ Study by the authors of the 1998 Science magazine study on search engine coverage (see below). September 1998 Search Engine Coverage Update NEC Research Institute, September 1998 http://www.neci.nj.nec.com/homepages/lawrence/websize98.html An update to the findings reported in Science magazine in April 1998, by its authors. It found that coverage was getting worse since the original study. April 1998 Search Engine Coverage Summary NEC Research Institute, April 1998 http://www.neci.nj.nec.com/homepages/lawrence/websize.html A summary of a landmark Science magazine study of search engine coverage. There\\'s also information on requesting reprints of the study and links to numerous news articles that covered the story. Searching the World Wide Web Science, April 3, 1998 http://www.sciencemag.org/cgi/content/abstract/280/5360/98 Summary of the NEC research article on search engine sizes, similar to that above. Full-text available only to Science web site subscribers. March \\'98 Measurement of Search Engines http://www.research.compaq.com/SRC/whatsnew/sem.html A study by Digital about the size of the web and search engine sizes, similar to the Science magazine study. Lost in cyberspace New Scientist, June 28, 1997 --no longer online-- An excellent look at why some search engines are moving away from an \"index everything\" attitude and instead adopting an \"index the best\" or \"sample the web\" method. Does it make a difference to searchers if some pages aren\\'t included? Search engine execs explain why they believe a sample is good enough. Search Engine Size Resources Search Engine Showdown http://www.searchengineshowdown.com/ This site from search expert Greg Notess provides a survey of search engine sizes, along with dead links estimates and other data. OCLC Web Characterization Project http://wcp.oclc.org/ Research project by the Online Computer Library Center tries to estimate the number of web sites and other statistics. How Much Information: Internet http://www.sims.berkeley.edu/research/projects/how-much-info/internet.html From UC Berkeley, this page summarizes findings from various sources to estimate the size of the web. Search Engine Watch www.searchenginewatch.com Danny Sullivan, Editor; Chris Sherman, Associate Editor Jupitermedia is publisher of the internet.com and EarthWeb.com networks. Copyright 2004 Jupitermedia Corporation All Rights Reserved. Legal Notices , \\xa0 Licensing , Reprints , & Permissions , \\xa0 Privacy Policy . \\n\\t\\n\\n-->\\n\\n\\n\\n\\nSearch Engine Sizes\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMembers Area With Exclusive Content\\n\\n\\n\\n\\nAlready a member?\\n\\n\\n\\nLearn about the benefits:\\n                     \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n>> Departments\\n\\n\\n\\n\\nSearch Engine Submission Tips\\n\\n\\n\\n\\n\\nWeb Searching Tips\\n\\n\\n\\n\\n\\nSearch Engine Listings\\n\\n\\n\\n\\n\\nReviews, Ratings & Tests\\n\\n\\n\\n\\n\\nSearch Engine Resources\\n\\n\\n\\n\\n\\nSearchDay\\n\\n\\n\\n\\n\\nSearch Engine Report\\n\\n\\n\\n\\n\\nMembers Area\\n\\n\\n\\n\\n\\n>> Site Info\\n\\n\\n\\n\\nAdvertising Info\\n\\n\\n\\n\\n\\nAbout The Site\\n\\n\\n\\n\\n\\nSite Map\\n\\n\\n\\n\\n\\n\\n>> Search\\n\\n\\n\\n\\n\\n\\n\\n\\nMembers Area\\n\\nFree Area\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch Engine Watchis part of\\nThe Internet & IT Network\\n\\n\\n\\n\\ninternet.commerce\\n\\n\\n\\n\\nShop Digital Cameras\\n\\nCheck Software Prices\\n\\n\\nInternet Jobs\\n\\n\\nDedicated Hosting\\n\\n\\nFree Virus Scan\\n\\nShopping Cart\\n\\n\\nWeb Design Templates\\n\\n\\nSave on Flat Panel TVs\\n\\nBusiness Search\\n\\nFree Bandwidth Quote\\n\\n\\n\\n\\n\\n\\n\\n\\ninternet.com\\n\\n\\n\\n\\nDeveloper\\nDevX\\nDownloads\\nEarthWeb\\nGraphics\\nInteractive Marketing\\nInternational\\nInternet Lists\\nInternet News\\nInternet Resources\\nIT\\nLinux/Open Source\\nSmall Business \\nWindows Technology\\nWireless Internet\\nxSP Resources \\n\\nSearch internet.com\\nAdvertise\\nCorporate Info\\nNewsletters\\nTech Jobs\\nE-mail Offers\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                        More resources from internet.com:\\nAd Resource\\nArtToday.com\\nChannelSeven\\nClickZ\\nCyberAtlas\\nAdvertising Report\\nJupiterdirect\\nInternetPRGuide\\nNewMedia\\nRefer-it\\nSEMList\\nSearchEngineWatch\\nTurboAds\\nWirelessAdWatch\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch Engine Watch\\'s conference on search engine marketing comes to:\\n\\n\\x95 March 1 - 4, 2004 New York, NY\\n\\x95 April 20 - 21, 2004 Tokyo, Japan\\n\\x95 May 11 - 12, 2004 Toronto, Canada\\n\\x95 June 2 - 3, 2004 London, England\\n\\x95 August 2 - 5, 2004 San Jose, CA\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n>> Search Engine Watch Marketplace\\n\\n\\n\\n\\nFind quality traffic and sales here!\\n\\n\\n\\nSearch Engine Optimization Tools and Services\\n\\n\\n\\n\\n\\n\\nDrive targeted traffic to your web site\\n\\n\\n\\n\\nReach nearly 70% of all search traffic!\\n \\n\\n\\n\\n\\nSearch Engine Marketing Services Free Report\\n\\n\\n\\n\\nWant More Customers? Click Here!\\n \\n\\n\\n\\n\\nThe Best SEO Software: Free Download!\\n\\n\\n\\nSign-up for Mamma\\'s non-bidded PPC Program\\n\\n\\n\\n\\n\\n\\nUnleash the Revenue Potential of Your Website\\n\\n\\n\\n\\nGUARANTEED inclusion- all major engines!\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n>> Search Engine Newsletters\\n\\n\\n\\nFREE!\\nOver 150,000 readers depend on our free newsletters to keep up\\nwith search engines. To join them, enter your email below:\\n\\n\\n\\n\\nDaily\\n\\t\\t\\t\\t\\t\\t\\t\\tMonthly\\nLearn more about the newsletters\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n>> \\n\\n\\n\\n\\nOnline Search Industry Research\\nGet deep analysis and actionable advice from expert analysts\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n>> Search Engine Research Reports\\n\\n\\n\\nSearch Engine Watch publisher Jupitermedia offers research reports and briefing papers on various topics about search engines.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch Engine Sizes\\nBy  Danny Sullivan, EditorSeptember 2, 2003\\n\\n\\n\\n\\n\\n\\n Is bigger better, when it comes to the size of a search engine\\'s index? Not necessarily. However, a large index can help those who seek unusual or hard-to-find\\n      information. Consequently, when you seek the obscure, consider using a search engine with a large index. However, for general searches or for when looking for \\n      information about popular topics, a large index does not necessarily equal better results.\\nCurrent Size Comparison - Search Engine Sizes Over Time\\nSearch Engine Size War I - SW-II - SW-III - SW-IV\\nRelated Search Engine Watch Articles\\nOther Search Engine Size Articles - Size Resources\\nCurrent Size\\n          Comparison\\nThe size figures below are unaudited and self-reported by each search engine (for audited figures, see the Search Engine Showdown web site listed \\n      below, which makes the best attempt at this). Figures show how many textual documents have been indexed, which includes HTML files, text documents, PDF files, \\n      Microsoft Office documents and other similar files. Image and multimedia files are not included. Nor are Google Groups discussion posts.\\nBillions Of Textual Documents Indexed\\nAs of Sept 2, 2003\\n\\nKEY: GG=Google, ATW=AllTheWeb, INK=Inktomi, TMA=Teoma, AV=AltaVista.\\n See the Major\\n      Search Engines page for links to these services.\\nSearch Engine Sizes\\n          Over Time\\nThe chart below shows how self-reported search engine sizes have changed over the years. Only search engine still crawling the web are shown on the chart. Thus, players \\n    such as Northern Light, Excite, Infoseek and others that no longer crawl for their results are not displayed.\\nBillions Of Textual Documents Indexed\\nDecember 1995-September 2003\\n\\n    KEY: See above.\\nSearch Engine Size War I: Sept 1997-June 1999\\nWhen AltaVista appeared in December 1995, it used an index much larger than any of the\\n    other search engines at that time. Thus, competition forced others to increase their\\n    sizes in early 1996. But after these moves, sizes stayed about the same through September 1997. It was then that AltaVista and Inktomi really began the first of the Search \\n    Engine Size Wars, competing to claim the bragging rights of being biggest. Inktomi gave up after a couple of months, but Northern Light jumped in to compete with AltaVista \\n    to hit the 150 million page mark.\\nMILLIONS Of Textual Documents Indexed\\nDecember 1995-June 1999\\n\\n    KEY: See above plus NL=Northern Light, EX=Excite, LY=Lycos, GO=GO/Infoseek\\nSearch Engine Size War I:\\n    December 1997-June 1999\\nWhen AltaVista appeared in December 1995, it used an index much larger than any of the\\n    other search engines at that time. Thus, competition forced others to increase their\\n    sizes in early 1996. After these initial moves, sizes stayed about the same through September 1997. But by the end of that year, AltaVista and Inktomi began the first of \\n    the serious Search Engine Size Wars, competing to claim the bragging rights of being biggest. Inktomi failed to keep up, but Northern Light jumped in to compete with \\n    AltaVista in pursuit of the 150 million page mark.\\nMILLIONS Of Textual Documents Indexed\\nDecember 1995-June 1999\\n\\n    KEY: See above plus NL=Northern Light, EX=Excite, LY=Lycos, GO=GO/Infoseek\\nSearch Engine Size War II:\\n    September 1999-June 2000\\nJust when AltaVista and Northern Light were celebrating hitting the 150 million document mark, newcomer AllTheWeb appeared with a record-setting index size of 200 \\n    million documents. Suddenly, a new round of size escalation began. The title of biggest flip-flopped between AllTheWeb and AltaVista at first. However, Google put a \\n    decisive stop to the war in June 2000, when it set a new benchmark of 500 million pages indexed and started growing well past its challengers.\\nMILLIONS Of Textual Documents Indexed\\nSeptember 1999-March 2002\\n\\n    KEY: See above plus NL=Northern Light\\nSearch Engine Size War III:\\n    June 2002-December 2002\\nAfter a long period of being the size king, AllTheWeb grabbed the title back from Google by declaring it had broken the 2 billion document mark. Soon after, Google grew \\n    the number of documents it reported indexing up to the 3 billion page mark. Inktomi also released a new version of its search engine that claimed this index level.\\nBillions Of Textual Documents Indexed\\nJune 2002-September 2003\\n\\n      KEY: See above\\nSearch Engine Size War IV:\\n    August 2003-???\\nIn August 2003, AllTheWeb claimed an index of 3.3 billion documents, putting it just past Google\\'s self-reported figure. Google responded within days by increasing its \\n    self-reported figure to 3.2 billion. It\\'s likely that over the coming months, new competition to hit the 4 billion document mark may happen.\\nRelated Search Engine Watch Articles\\nBigger is not necessarily better, though it can be for some searches. To better\\n    understand the importance of size, be sure to read some past articles from Search Engine Watch that deal with size issues, listed below:\\nSearch Engine Size Wars & Google\\'s Supplemental Results\\nSearchDay, Sept. 3, 2003\\nhttp://searchenginewatch.com/searchday/article.php/3071371\\nWho has the biggest index? The search engine size wars have erupted again to dispute this -- and the new Google supplemental index is complicating matters.\\nGoogle to Overture: Mine\\'s Bigger\\n    SearchDay, Aug. 27, 2003\\nhttp://www.searchenginewatch.com/searchday/article.php/3069221 \\nOverture and Google have fired new salvos in the search engine size wars, expanding their databases of searchable web pages by millions of pages.\\nFAST Sprints to 2.1 Billion Docs; Google Upgrades Appliance\\nSearchDay, June 17, 2002\\nhttp://searchenginewatch.com/searchday/article.php/2160141\\nCovers the increase by FAST/AllTheWeb past the 2.1 billion document mark.\\nMapping the \\'Dark Net\\'\\nSearchDay, Jan. 24, 2002\\nhttp://searchenginewatch.com/searchday/article.php/2159121\\nResearchers have discovered that up to 5 percent of the Internet is \\ncompletely unreachable, impossible to access by web browser or search engine \\nalike.\\nGoogle Fires New Salvo in Search Engine Size Wars\\nSearchDay, Dec. 11, 2001\\nhttp://searchenginewatch.com/searchday/article.php/2158371\\nGoogle\\'s web index has grown to more than 3 billion documents,\\nincluding an unprecedented archive of Usenet newsgroup postings dating back to\\n1981.\\n  Google & FAST Move Up In Size\\nThe Search Engine Report, Nov. 3, 2000\\nUpdate on size increases by Google and FAST.\\nInvisible\\n      Web Gets Deeper\\nThe Search Engine Report, Aug. 2, 2000\\nCovers a survey done to measure how much information\\nexists outside of the search engines\\' reach. The company behind the survey is\\nalso offering up a solution for those who want tap into this \"hidden\"\\nmaterial.\\nGoogle Announces Largest\\n      Index\\n      The Search Engine Report, July 5, 2000\\nGoogle breaks the 500 million page mark, but not all\\n      partners may tap into the large index.\\nSearch Engine Size Test\\nSearch Engine Watch, July 2000\\nhttp://searchenginewatch.com/sereport/article.php/2162821\\nEvaluated claims in index size, to see if\\n    the search engines measure up.\\nInktomi Reenters Battle For\\n    Biggest\\n    The Search Engine Report, June 2, 2000\\nInktomi makes moves to again become one of the biggest\\n      search engines on the web.\\nAltaVista Launches New\\n    Search Site\\nThe Search Engine Report, May 3, 2000\\nAltaVista adopts a crawling system similar to that used\\n      by Inktomi, as described in the article below.\\nNumbers, Numbers -- But What\\n          Do They Mean?\\nThe Search Engine Report, March 3, 2000\\nA long look at the recent trend of quoting \"dual\\n      numbers\" in relation to index size and how to compare services that\\n      do this.\\nFAST Gets Bigger, Partners With Lycos\\nThe Search Engine Report, Feb. 3, 2000\\nDetails on FAST breaking the 300 million web page mark.\\nWho\\'s The Biggest Of Them\\n    All?\\nThe Search Engine Report, Nov. 1, 1999\\nDiscusses the difficulty of verifying index claims.\\nSearch Engine Coverage Study Published\\nThe Search Engine Report, Aug. 2, 1999\\nDetailed review of the Nature article about search engine\\n    coverage.\\nFAST Announces Largest Search Engine\\nThe Search Engine Report, Aug. 2, 1999\\nDetails about FAST claiming to have broken the 200 million\\n    web page barrier, with a link to more background about the company.\\nMany Changes At AltaVista\\n    The Search Engine Report, July 6, 1999\\nMentions that AltaVista plans to continue increasing its\\n    size.\\nGoogle Goes Forward\\n    The Search Engine Report, July 6, 1999\\nExplains how Google can exceed the reach of its index.\\nNorthern Light Claims Largest Index\\n    The Search Engine Report, Feb. 2, 1999\\nNorthern Light says that if self-reported sizes were\\n      audited, it would be number one. More on the issue, along with an update\\n      on size growth in general.\\nSearch Engine Sizes Scrutinized\\nThe Search Engine Report, April 30, 1998\\nExtensive details and analysis of the April 1998 Science study,\\n      which grabbed headlines across the world.\\nThe AltaVista Size Controversy\\nThe Search Engine Report, July 2, 1997\\nRecounts a public discussion of how AltaVista was only sampling some web\\n    pages completely ignoring other sites.\\nHow Big Are The Search Engines\\nSearch Engine Watch, June 13, 1997\\nhttp://searchenginewatch.com/sereport/article.php/2165301\\nArticle within Search Engine Watch that explains the issues of index size in\\n    more depth. Does size really matter?\\nSearch Engine Size Articles\\nGoogle Dominates New Size Showdowns\\nSearch Engine Showdown, Jan. 16, 2003\\nhttp://www.searchengineshowdown.com/newsarchive/000625.shtml\\nGoogle leads the pack in terms of size, based on the latest\\nestimates from Greg Notess\\nWhen size does matter\\n      The Guardian, July 18, 2002\\nhttp://media.guardian.co.uk/newmedia/story/0,7496,757326,00.html\\nSearch Engine Watch associate editor Chris Sherman takes another look at when -- and when not -- index size matters.\\nOpenfind touts its efficiency\\ne-Taiwannews.com, July 1, 2002\\nhttp://www.etaiwannews.com/Taiwan/2002/07/01/1025492420.htm\\nOpenfind, on taking on Google. The company claims an index of 3.5 billion pages. Google processes in the range of 150 million searches per day, by the way, not the \\n      1.5 billion noted in the article.\\nOn the size of the World Wide Web\\nPandia, Oct. 14, 2001\\nhttp://www.pandia.com/sw-2001/57-websize.html\\nThere are now over 8 million web sites according to\\nresearchers at the Online Computer Library Center, but the web\\'s growth has\\nslowed markedly when compared to previous years. The vast majority of web sites\\nare written in English -- 73 percent, with German coming in at second place with\\n7 percent.\\nWeb links that stick\\nBBC, June 14, 2000\\nhttp://news.bbc.co.uk/1/hi/sci/tech/790685.stm\\nThe average page contains 52 links, and the over 10 percent of\\nthe links on the web are broken, according to a company that\\'s developing a link\\nmonitoring service.\\nThe\\n      web is a bow tie\\n      Nature, May 11, 2000\\n\\n      http://www.nature.com/cgi-taf/DynaPage.taf?file=/nature/journal/v405/n6783/full/405113a0_fs.html\\nCovers research by AltaVista and two other groups that\\n      found a \"bow tie\" pattern to how pages link across the web.\\nResearchers work to eradicate broken hyperlinks\\n      News.com, March 7, 2000\\nhttp://news.com.com/2100-1023-237651.html\\nReview of \"robust\" hyperlink system described\\n      in technical paper, below.\\nRobust Hyperlinks Cost Just Five Words Each\\n      UC Berkeley, January 2000\\nhttp://www.cs.berkeley.edu/~phelps/Robust/papers/robust-hyperlinks.html\\nTechnical paper on how web pages could be assigned a\\n      lexical code to make it easier to locate them.\\nAccessibility and Distribution of Information on the Web\\n      Nature, July 1999\\nhttp://wwwmetrics.com/\\nStudy by the authors of the 1998 Science magazine\\n      study on search engine coverage (see below).\\nSeptember 1998 Search Engine Coverage Update\\nNEC Research Institute, September 1998\\nhttp://www.neci.nj.nec.com/homepages/lawrence/websize98.html\\nAn update to the findings reported in Science magazine in\\n    April 1998, by its authors. It found that coverage was getting worse since\\n    the original study.\\nApril 1998 Search Engine Coverage Summary\\n    NEC Research Institute, April 1998\\nhttp://www.neci.nj.nec.com/homepages/lawrence/websize.html\\nA summary of a landmark Science magazine study of search\\n      engine coverage.\\n    There\\'s also information on requesting reprints of the study and links to\\n      numerous news articles that covered the story.\\nSearching the World Wide Web\\nScience, April 3, 1998\\nhttp://www.sciencemag.org/cgi/content/abstract/280/5360/98\\nSummary of the NEC research article on search engine sizes, similar to\\n    that above. Full-text\\n    available only to Science web site subscribers.\\nMarch \\'98 Measurement of Search Engines\\nhttp://www.research.compaq.com/SRC/whatsnew/sem.html\\nA study by Digital about the size of the web and search engine sizes,\\n    similar to the Science magazine study.\\nLost in cyberspace\\nNew Scientist, June 28, 1997\\n      --no longer online--\\nAn excellent look at why some search engines are moving away from an\\n    \"index everything\" attitude and instead adopting an \"index the best\"\\n    or \"sample the web\" method. Does it make a difference to searchers if some pages\\n    aren\\'t included? Search engine execs explain why they believe a sample is good enough.\\nSearch Engine Size Resources\\nSearch Engine Showdown\\nhttp://www.searchengineshowdown.com/\\nThis site from search expert Greg Notess provides a survey\\nof search engine sizes, along with dead links estimates and other data.\\nOCLC Web Characterization Project\\nhttp://wcp.oclc.org/\\nResearch project by the Online Computer Library Center\\n      tries to estimate the number of web sites and other statistics.\\nHow Much Information: Internet\\nhttp://www.sims.berkeley.edu/research/projects/how-much-info/internet.html\\nFrom UC Berkeley, this page summarizes findings from\\n      various sources to estimate the size of the web.\\n\\n\\n\\n\\n\\nSearch Engine Watch \\nwww.searchenginewatch.com\\nDanny Sullivan, Editor; Chris Sherman, Associate Editor\\n\\n\\n\\nJupitermedia is publisher of the internet.com and EarthWeb.com networks.\\n\\nCopyright 2004 Jupitermedia Corporation All Rights Reserved.\\nLegal Notices, \\xa0Licensing, Reprints, & Permissions, \\xa0Privacy Policy.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " ' | Recommendations | public drafts | test suites | tutorials | slides | guidelines | validation | articles | translations | charter | working group | roadmap | XForms | forums | HTML Tidy | related work | HTML 4.0/3.2/2.0 | historical NEWS 20 January 2004 : The XHTML-Print specification has been published as a Candidate Recommendation. XHTML-Print is designed to be appropriate for printing from mobile devices to low-cost printers that might not have a full-page buffer and that generally print from top-to-bottom and left-to-right with the paper in a portrait orientation. XHTML-Print is also targeted at printing in environments where it is not feasible or desirable to install a printer-specific driver and where some variability in the formatting of the output is acceptable. Please send implementation feedback to www-html-editor@w3.org ( archive ). 14 October 2003 : The World Wide Web Consortium today released XML Events specification as a Recommendation. The XML Events module defined in this specification provides XML languages with the ability to uniformly integrate event listeners and associated event handlers with DOM2 event interfaces. The specification has been reviewed by the W3C Membership, who favor its adoption by industry. 3 October 2003 : The second Last Call Working Draft of Modularization of XHTML in XML Schema has been published. It is being re-submitted for Last Call because of substantial changes in the way the Schemas are implemented to ease their use in non-XHTML context. The Last Call review period ends 14 November 2003. Please send Last Call comments to www-html-editor@w3.org ( archive ). 23 September 2003 : W3C Launched the HTML Patent Advisory Group (PAG) to study issues for HTML-related specifications raised by the US court case of Eolas v. Microsoft and US Patent 5,838,906. Public discussion takes place on the public-web-plugins mailing list. The FAQ on US Patent 5,838,906 and the W3C is available. ( Past News ) What is HTML? HTML is the lingua franca for publishing hypertext on the World Wide Web. It is a non-proprietary format based upon SGML , and can be created and processed by a wide range of tools, from simple plain text editors - you type it in from scratch- to sophisticated WYSIWYG authoring tools. HTML uses tags such as <h1> and </h1> to structure text into headings, paragraphs, lists, hypertext links etc. Here is a 10-minute guide for newcomers to HTML. W3C\\'s statement of direction for HTML is given on the HTML Activity Statement . See also the page on our work on the next generation of Web forms , and the section on Web history . What is XHTML? The Extensible HyperText Markup Language (XHTML™) is a family of current and future document types and modules that reproduce, subset, and extend HTML, reformulated in XML . XHTML Family document types are all XML-based, and ultimately are designed to work in conjunction with XML-based user agents. XHTML is the successor of HTML, and a series of specifications has been developed for XHTML. Mission of the HTML Working Group To develop the next generation of HTML as a suite of XML tag sets with a clean migration path from HTML 4. Some of the expected benefits include: reduced authoring costs, an improved match to database & workflow applications, a modular solution to the increasingly disparate capabilities of browsers, and the ability to cleanly integrate HTML with other XML applications. For further information, see the Charter for the HTML Working Group ( members only ). Note. The HTML Working Group Charter has been renewed in August 2002. Recommendations W3C produces what are known as \" Recommendations \". These are specifications, developed by W3C working groups, and then reviewed by Members of the Consortium. A W3C Recommendation indicates that consensus has been reached among the Consortium Members that a specification is appropriate for widespread use. XHTML 1.0 | HTML 4.01 | XHTML Basic | Modularization of XHTML | XHTML 1.1 | XML Events XHTML 1.0 XHTML 1.0 is the W3C\\'s first Recommendation for XHTML, following on from earlier work on HTML 4.01, HTML 4.0, HTML 3.2 and HTML 2.0. With a wealth of features, XHTML 1.0 is a reformulation of HTML 4.01 in XML, and combines the strength of HTML 4 with the power of XML. XHTML 1.0 is the first major change to HTML since HTML 4.0 was released in 1997. It brings the rigor of XML to Web pages and is the keystone in W3C\\'s work to create standards that provide richer Web pages on an ever increasing range of browser platforms including cell phones, televisions, cars, wallet sized wireless communicators, kiosks, and desktops. XHTML 1.0 is the first step and the HTML Working Group is busy on the next. XHTML 1.0 reformulates HTML as an XML application. This makes it easier to process and easier to maintain. XHTML 1.0 borrows elements and attributes from W3C\\'s earlier work on HTML 4, and can be interpreted by existing browsers, by following a few simple guidelines . This allows you to start using XHTML now! You can roll over your old HTML documents into XHTML using an Open Source HTML Tidy utility. This tool also cleans up markup errors, removes clutter and prettifies the markup making it easier to maintain. Three \"flavors\" of XHTML 1.0: XHTML 1.0 is specified in three \"flavors\". You specify which of these variants you are using by inserting a line at the beginning of the document. For example, the HTML for this document starts with a line which says that it is using XHTML 1.0 Strict. Thus, if you want to validate the document, the tool used knows which variant you are using. Each variant has its own DTD - Document Type Definition - which sets out the rules and regulations for using HTML in a succinct and definitive manner. XHTML 1.0 Strict - Use this when you want really clean structural mark-up, free of any markup associated with layout. Use this together with W3C\\'s Cascading Style Sheet language ( CSS ) to get the font, color, and layout effects you want. XHTML 1.0 Transitional - Many people writing Web pages for the general public to access might want to use this flavor of XHTML 1.0. The idea is to take advantage of XHTML features including style sheets but nonetheless to make small adjustments to your markup for the benefit of those viewing your pages with older browsers which can\\'t understand style sheets. These include using the body element with bgcolor , text and link attributes. XHTML 1.0 Frameset - Use this when you want to use Frames to partition the browser window into two or more frames. The complete XHTML 1.0 specification is available in English in several formats, including HTML, PostScript and PDF . See also the list of translations produced by volunteers. XHTML 1.0 and HTML 4.01 --> HTML 4.01 HTML 4.01 is a revision of the HTML 4.0 Recommendation first released on 18th December 1997. The revision fixes minor errors that have been found since then. The XHTML 1.0 spec relies on HTML 4.01 for the meanings of XHTML elements and attributes. This allowed us to reduce the size of the XHTML 1.0 spec very considerably. XHTML Basic XHTML Basic is the second Recommendation in a series of XHTML specifications. The XHTML Basic document type includes the minimal set of modules required to be an XHTML Host Language document type, and in addition it includes images, forms, basic tables, and object support. It is designed for Web clients that do not support the full set of XHTML features; for example, Web clients such as mobile phones, PDA s, pagers, and settop boxes. The document type is rich enough for content authoring. XHTML Basic is designed as a common base that may be extended. For example, an event module that is more generic than the traditional HTML 4 event system could be added or it could be extended by additional modules from XHTML Modularization such as the Scripting Module. The goal of XHTML Basic is to serve as a common language supported by various kinds of user agents. The document type definition is implemented using XHTML modules as defined in \" Modularization of XHTML \". The complete XHTML Basic specification is available in English in several formats, including HTML, plain text, PostScript and PDF. See also the list of translations produced by volunteers. Modularization of XHTML Modularization of XHTML is the third Recommendation in a series of XHTML specifications. This Recommendation specifies an abstract modularization of XHTML and an implementation of the abstraction using XML Document Type Definitions (DTDs). This modularization provides a means for subsetting and extending XHTML, a feature needed for extending XHTML\\'s reach onto emerging platforms. Modularization of XHTML will make it easier to combine with markup tags for things like vector graphics, multimedia, math, electronic commerce and more. Content providers will find it easier to produce content for a wide range of platforms, with better assurances as to how the content is rendered. The modular design reflects the realization that a one-size-fits-all approach will no longer work in a world where browsers vary enormously in their capabilities. A browser in a cellphone can\\'t offer the same experience as a top of the range multimedia desktop machine. The cellphone doesn\\'t even have the memory to load the page designed for the desktop browser. See also an overview of XHTML Modularization . XHTML 1.1 - Module-based XHTML This Recommendation defines a new XHTML document type that is based upon the module framework and modules defined in Modularization of XHTML. The purpose of this document type is to serve as the basis for future extended XHTML \\'family\\' document types, and to provide a consistent, forward-looking document type cleanly separated from the deprecated, legacy functionality of HTML 4 that was brought forward into the XHTML 1.0 document types. This document type is essentially a reformulation of XHTML 1.0 Strict using XHTML Modules. This means that many facilities available in other XHTML Family document types (e.g., XHTML Frames) are not available in this document type. These other facilities are available through modules defined in Modularization of XHTML, and document authors are free to define document types based upon XHTML 1.1 that use these facilities (see Modularization of XHTML for information on creating new document types). What is the difference between XHTML 1.0, XHTML Basic and XHTML 1.1? The first step was to reformulate HTML 4 in XML, resulting in XHTML 1.0 . By following the HTML Compatibility Guidelines set forth in Appendix C of the XHTML 1.0 specification, XHTML 1.0 documents could be compatible with existing HTML user agents. The next step is to modularize the elements and attributes into convenient collections for use in documents that combine XHTML with other tag sets. The modules are defined in Modularization of XHTML . XHTML Basic is an example of fairly minimal build of these modules and is targeted at mobile applications. XHTML 1.1 is an example of a larger build of the modules, avoiding many of the presentation features. While XHTML 1.1 looks very similar to XHTML 1.0 Strict, it is designed to serve as the basis for future extended XHTML Family document types, and its modular design makes it easier to add other modules as needed or integrate itself into other markup languages. XHTML 1.1 plus MathML 2.0 document type is an example of such XHTML Family document type. XML Events Note: This specification was renamed from \"XHTML Events\". The XML Events module defined in this specification provides XML languages with the ability to uniformly integrate event listeners and associated event handlers with Document Object Model (DOM) Level 2 event interfaces. The result is to provide an interoperable way of associating behaviors with document-level markup. Previous Versions of HTML HTML 4.01 The HTML 4.01 Recommendation released on 24th December 1999 fixes a number of bugs in the HTML 4.0 specification. The list of changes are detailed in appendix A . --> HTML 4.0 First released as a W3C Recommendation on 18 December 1997. A second release was issued on 24 April 1998 with changes limited to editorial corrections. This specification has now been superseded by HTML 4.01 . HTML 3.2 W3C\\'s first Recommendation for HTML which represented the consensus on HTML features for 1996. HTML 3.2 added widely-deployed features such as tables, applets, text-flow around images, superscripts and subscripts, while providing backwards compatibility with the existing HTML 2.0 Standard . HTML 2.0 HTML 2.0 ( RFC 1866 ) was developed by the IETF \\'s HTML Working Group, which closed in 1996. It set the standard for core HTML features based upon current practice in 1994. Note that with the release of RFC 2854 , RFC 1866 has been obsoleted and its current status is HISTORIC . ISO HTML ISO/ IEC 15445:2000 is a subset of HTML 4, standardized by ISO/IEC. It takes a more rigorous stance for instance, an h3 element can\\'t occur after an h1 element unless there is an intervening h2 element. Roger Price and David Abrahamson have written a user\\'s guide to ISO HTML . Other Public Drafts We would like to hear from you via email. Please send your comments to: www-html@w3.org ( archive ). Don\\'t forget to include XHTML in the subject line. HTML Working Group Roadmap This describes the timeline for deliverables of the HTML working group. It used to be a W3C NOTE but has now been moved to the MarkUp area for easier maintenance. Modularization of XHTML in XML Schema The purpose of this document is to describe a modularization framework for languages within the XHTML Namespace using XML Schema. This document provides a complete set of XML Schema modules for XHTML. In addition to the schema modules themselves, the framework presented here describes a means of further extending and modifying XHTML. An XHTML + MathML + SVG Profile An XHTML+MathML+SVG profile is a profile that combines XHTML 1.1, MathML 2.0 and SVG 1.1 together. This profile enables mixing XHTML, MathML and SVG in the same document using XML namespaces mechanism, while allowing validation of such a mixed-namespace document. This specification is a joint work with the SVG Working Group, with the help from the Math WG. XHTML 2.0 XHTML 2.0 is a markup language intended for rich, portable web-based applications. While the ancestry of XHTML 2.0 comes from HTML 4, XHTML 1.0, and XHTML 1.1, it is not intended to be backward compatible with its earlier versions. Application developers familiar with its earlier ancestors will be comfortable working with XHTML 2.0. XHTML 2 is a member of the XHTML Family of markup languages. It is an XHTML Host Language as defined in Modularization of XHTML . As such, it is made up of a set of XHTML Modules that together describe the elements and attributes of the language, and their content model. XHTML 2.0 updates many of the modules defined in Modularization of XHTML, and includes the updated versions of all those modules and their semantics. XHTML 2.0 also uses modules from Ruby , XML Events , and XForms . XFrames XFrames is an XML application for composing documents together, replacing HTML Frames. XFrames is not a part of XHTML per se, that allows similar functionality to HTML Frames, with fewer usability problems, principally by making the content of the frameset visible in its URI. XHTML 1.0 in XML Schema This document describes informative XML Schemas for XHTML 1.0. These Schemas are still work in progress, and this document does not change the normative definition of XHTML 1.0. HLink The HLink module defined in this specification provides XHTML Family Members with the ability to specify which attributes of elements represent Hyperlinks, and how those hyperlinks should be traversed, and extends XLink use to a wider class of languages than those restricted to the syntactic style allowed by XLink. XHTML-Print XHTML-Print is member of the family of XHTML Languages defined by the Modularization of XHTML . It is designed to be appropriate for printing from mobile devices to low-cost printers that might not have a full-page buffer and that generally print from top-to-bottom and left-to-right with the paper in a portrait orientation. XHTML-Print is also targeted at printing in environments where it is not feasible or desirable to install a printer-specific driver and where some variability in the formatting of the output is acceptable. Building XHTML Modules Note: This document has been incorporated into \" Modularization of XHTML \". XHTML Document Profile Requirements The increasing disparities between the capabilities of different kinds of Web browsers present challenges to Web content developers wishing to reach a wide audience. A promising approach is to formally describe profiles for documents intended for broad groups of browsers, for instance, separate document profiles for browsers running on desktops, television, handhelds, cellphones and voice browsers. Document profiles provide a basis for interoperability guarantees. If an author develops content for a given profile and a browser supports the profile then the author may be confident that the document will be rendered as expected. The requirements for document profiles are analyzed. --> Useful information for HTML/XHTML authors Tutorials Getting started with HTML by Dave Raggett is a short introduction to writing HTML, including tutorials on advanced features . Adding a touch of style by Dave Raggett is a short guide to styling your Web pages. XHTML Modules and Markup Languages - How to create XHTML Family modules and markup languages for fun and profit by Shane McCarron explains how to create XHTML Family modules and markup languages, based on Modularization of XHTML . Slides on XHTML You may also be interested in the following slides on XHTML: XHTML: The Extensible Hypertext Markup Language by Dave Raggett, at W3C LA event in Stockholm, 24 March 1999. W3C HTML Activity by Dave Raggett, as part of WWW8 W3C Track, 12 May 1999 W3C Work on XHTML by Dave Raggett, at XML \\'99 , 6 December 1999. The presentation describes the work being done by W3C on XHTML. The XHTML Family (in Japanese) by Masayasu Ishikawa, at SFC Open Research Forum 2001 , 21 September 2001. XForms, XHTML and Device Independence by Steven Pemberton, at W3C.DE-Arbeitstreffen: Cross Media Publishing , 11 April 2002. XHTML Family by Masayasu Ishikawa, as part of WWW2002 W3C Track , 9 May 2002. Slides are available in XHTML or HTML (XHTML version needs XHTML+MathML+SVG+Ruby support). XHTML 2.0 and XForms by Steven Pemberton, as part of WWW2003 W3C Track , 21 May 2003. W3C\\'s Horizontal Activities Usage: XHTML Family Case Study by Steven Pemberton, WWW2003 W3C Track, 23 May 2003. XHTML and XForms by Steven Pemberton, at Zomersessie van NGI Limburg: XHTML2 en XForms, state of the art en stage-ervaringen bij het W3C , 3 July 2003. Guidelines for authoring Here are some rough guidelines for HTML authors. If you use these, you are more likely to end up with pages that are easy to maintain, look acceptable to users regardless of the browser they are using, and can be accessed by the many Web users with disabilities. Meanwhile W3C have produced some more formal guidelines for authors. Have a look at the detailed Web Content Accessibility Guidelines 1.0 . A question of style sheets. For most people the look of a document - the color, the font, the margins - are as important as the textual content of the document itself. But make no mistake! HTML is not designed to be used to control these aspects of document layout. What you should do is to use HTML to mark up headings, paragraphs, lists, hypertext links, and other structural parts of your document, and then add a style sheet to specify layout separately, just as you might do in a conventional Desk Top Publishing Package. That way, not only is there a better chance of all browsers displaying your document properly, but also, if you want to change such things as the font or color, it\\'s really simple to do so. See the Touch of style . FONT tag considered harmful! Many filters from word-processing packages, and also some HTML authoring tools, generate HTML code which is completely contrary to the design goals of the language. What they do is to look at a document almost purely from the point of view of layout, and then mimic that layout in HTML by doing tricks with FONT, BR and &nbsp; (non-breaking spaces). HTML documents are supposed to be structured around items such as paragraphs, headings and lists. Yet some of these documents barely have a paragraph tag in sight! The problem comes when the content of pages needs to be updated, or given a new layout, or re-cast in XML (which is now to be the new mark-up language). With proper use of HTML, such operations are not difficult, but with a muddle of non-structural tags it\\'s quite a different matter; maintenance tasks become impractical. To correct pages suffering from injudicious use of FONT, try the HTML Tidy program , which will do its best to put things right and generate better and more manageable HTML. Make your pages readable by those with disabilities. The Web is a tremendously useful tool for the visually impaired or blind user, but bear in mind that these users rely on speech synthesizers or Braille readers to render the text. Sloppy mark-up, or mark-up which doesn\\'t have the layout defined in a separate style sheet, is hard for such software to deal with. Wherever possible, use a style sheet for the presentational aspects of your pages, using HTML purely for structural mark-up. Also, remember to include descriptions with each image, and try to avoid server-side image maps. For tables, you should include a summary of the table\\'s structure, and remember to associate table data with relevant headers. This will give non-visual browsers a chance to help orient people as they move from one cell to the next. For forms, remember to include labels for form fields. Do look at the accessibility guidelines for a more detailed account of how to make your Web pages really accessible. W3C Markup Validation Service To further promote the reliability and fidelity of communications on the Web, W3C has introduced the W3C Markup Validation Service at http://validator.w3.org/ . Content providers can use this service to validate their Web pages against the HTML and XHTML Recommendations, thereby ensuring the maximum possible audience for their Web pages. It also supports XHTML Family document types such as XHTML+MathML and XHTML+MathML+SVG , and also other markup vocabularies such as SVG . Software developers who write HTML and XHTML editing tools can ensure interoperability with other Web software by verifying that the output of their tool complies with the W3C Recommendations for HTML and XHTML. HTML Tidy HTML Tidy is a stand-alone tool for checking and pretty-printing HTML that is in many cases able to fix up mark-up errors, and also offers a means to convert existing HTML content into well-formed XML, for delivery as XHTML. HTML Tidy was originally written by Dave Raggett , and it is now maintained as an open source project at SourceForge by a group of volunteers. There is an archived public mailing list html-tidy@w3.org. Please send bug reports / suggestions on HTML Tidy to this mailing list. Discussion Forums Changes to HTML necessitate obtaining a consensus from a broad range of organizations. If you have a great idea, it will take time to convince others! Here are some of the places where discussion on HTML takes place: comp.infosystems.www.authoring.html A USENET newsgroup where HTML authoring issues are discussed. \"How To\" questions should be addressed here. Note that many issues related to forms and CGI, image maps, transparent gifs, etc. are covered in the WWW FAQ . www-html@w3.org A technical discussion list. If you have a proposal for a change to HTML/XHTML, you might start a discussion here to see what other developers think of it. how to subscribe archives from 1994 to present (We\\'re working on moving the old archives to W3C. Stay tuned!) www-html-editor@w3.org This is a list to report errors / send review comments on HTML/XHTML specifications. This is NOT a discussion list. Anyone may send comments without subscription, although you\\'ll be requested to give explicit approval to include your message in our publicly-readable mailing list archive at your first post. To subscribe, send subscription request to www-html-editor-request@w3.org. For more information, see how to subscribe . W3C HTML Working Group ( members only ) The Group\\'s mission is to develop the next generation of HTML as a suite of XML tag sets with a clean migration path from HTML 4. Some of the expected benefits include: reduced authoring costs, an improved match to database & workflow applications, a modular solution to the increasingly disparate capabilities of browsers, and the ability to cleanly integrate HTML with other XML applications. The Group is chaired by Steven Pemberton . w3c-translators@w3.org This is a mailing list for people working on translations of W3C specifications such as the HTML/XHTML Recommendations . To subscribe, send an email to w3c-translators-request@w3.org with the word \"subscribe\" in the subject line; (include the word \"unsubscribe\" if you want to unsubscribe.) The archive for the list is accessible online. IETF MHTML WG (closed) Developed RFC 2557 - \"MIME Encapsulation of Aggregate Documents, such as HTML (MHTML). J. Palme et al. March 1989. IETF HTML Working Group (closed) The HTML working group of the IETF , closed in 1996. Web Conferences The next international conference dedicated to the Web is WWW2004, to be held in New York city, USA, in 2004. The last was WWW2003 , which was held in Budapest, Hungary, 20-24 May 2003. Related W3C Work XML XML is the universal format for structured documents and data on the Web. It allows you to define your own mark-up formats when HTML is not a good fit. XML is being used increasingly for data; for instance, W3C\\'s metadata format RDF . Style Sheets W3C\\'s Cascading Style Sheets language ( CSS ) provides a simple means to style HTML pages, allowing you to control visual and aural characteristics; for instance, fonts, margins, line-spacing, borders, colors, layers and more. W3C is also working on a new style sheet language written in XML called XSL , which provides a means to transform XML documents into HTML. Document Object Model Provides ways for scripts to manipulate HTML using a set of methods and data types defined independently of particular programming languages or computer platforms. It forms the basis for dynamic effects in Web pages, but can also be exploited in HTML editors and other tools by extensions for manipulating HTML content. Internationalization HTML 4 provides a number of features for use with a wide variety of languages and writing systems. For instance, mixed language text, and right-to-left and mixed direction text. HTML 4 is formally based upon Unicode, but allows you to store and transmit documents in a variety of character encodings. Further work is envisaged for handling vertical text and phonetic annotations for Kanji ( Ruby ). Access for People with Disabilities HTML 4 includes many features for improved access by people with disabilities. W3C\\'s Web Accessibility Initiative is working on providing effective guidelines for making your pages accessible to all, not just those using graphical browsers. XForms Forms are a very widely used feature in web pages. W3C is working on the design of the next generation of web forms with a view to separating the presentation, data and logic, as a means to allowing the same forms to be used with widely differing presentations. Mathematics Work on representing mathematics on the Web has focused on ways to handle the presentation of mathematical expressions and also the intended meaning. The MathML language is an application of XML, which, while not suited to hand-editing, is easy to process by machine. Contacts Masayasu ISHIKAWA is the HTML Activity Lead and the Team Contact for the HTML Working Group news mission --> | Recommendations | public drafts | test suites | tutorials | slides | guidelines | validation | articles | translations | charter | working group | roadmap | XForms | forums | HTML Tidy | related work | HTML 4.0/3.2/2.0 | historical Copyright ©1995-2004 W3C ® ( MIT , ERCIM , Keio ), All Rights Reserved. W3C liability , trademark , document use and software licensing rules apply. Your interactions with this site are in accordance with our public and Member privacy statements. This page was last modified on: $ Date: 2004/01/20 19:07:55 $ \\n\\t\\n\\n-->\\n\\n\\n\\n\\n\\nW3C HTML Home Page\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHyperText Markup Language (HTML)\\nHome Page\\n\\nThis is W3C\\'s\\nhome page for the HTML Activity. \\nHere you will find pointers to our specifications for HTML/XHTML, guidelines\\non how to use HTML/XHTML to the best effect, and pointers to related work\\nat W3C. When W3C decides to become involved in an area of Web technology or\\npolicy, it initiates an activity in that area. HTML is one of many\\nActivities currently being pursued.\\nYou can learn more about the HTML Activity from\\nthe HTML Activity Statement.\\n\\nnews\\n\\n| Recommendations\\n| public drafts\\n| test suites\\n| tutorials\\n| slides\\n| guidelines\\n| validation\\n| articles\\n| translations\\n| charter\\n| working group\\n| roadmap\\n| XForms\\n| forums\\n| HTML Tidy\\n| related work\\n| HTML 4.0/3.2/2.0\\n| historical\\n\\nNEWS\\n20 January 2004:\\nThe XHTML-Print\\nspecification has been published as a Candidate Recommendation.\\nXHTML-Print is designed to be appropriate for printing from mobile devices\\nto low-cost printers that might not have a full-page buffer and that\\ngenerally print from top-to-bottom and left-to-right with the paper\\nin a portrait orientation. XHTML-Print is also targeted at printing\\nin environments where it is not feasible or desirable to install\\na printer-specific driver and where some variability in the formatting\\nof the output is acceptable.\\nPlease send implementation feedback to\\nwww-html-editor@w3.org (archive).\\n14 October 2003:\\nThe World Wide Web Consortium today released\\nXML Events\\nspecification as a Recommendation.\\nThe XML Events module defined in this specification provides XML\\nlanguages with the ability to uniformly integrate event listeners\\nand associated event handlers with DOM2 event interfaces.\\nThe specification has been reviewed by the W3C Membership, who favor\\nits adoption by industry.\\n3 October 2003:\\nThe second Last Call Working Draft of Modularization of XHTML\\nin XML Schema has been published.\\nIt is being re-submitted for Last Call because of substantial changes\\nin the way the Schemas are implemented to ease their use in non-XHTML\\ncontext. The Last Call review period ends 14 November 2003.\\nPlease send Last Call comments to\\nwww-html-editor@w3.org (archive).\\n23 September 2003:\\nW3C Launched the HTML Patent Advisory Group (PAG)\\nto study issues for HTML-related specifications raised by the US court case\\nof Eolas v. Microsoft and US Patent 5,838,906. Public discussion takes place\\non the public-web-plugins mailing list.\\nThe FAQ on US Patent 5,838,906 and the W3C\\nis available.\\n(Past News)\\n\\n\\nWhat is HTML?\\nHTML is the lingua franca for publishing hypertext on\\nthe World Wide Web. It is a non-proprietary format based upon\\nSGML, and\\ncan be created and processed by a wide range of tools, from simple plain\\ntext editors - you type it in from scratch- to sophisticated WYSIWYG authoring tools.\\nHTML uses tags such as <h1> and </h1>\\nto structure text into headings, paragraphs, lists, hypertext links etc.\\nHere is a 10-minute guide for newcomers to HTML. W3C\\'s\\nstatement of direction for HTML is given on the HTML Activity Statement. See also the page on\\nour work on the next generation of Web\\nforms, and the section on Web history.\\nWhat is XHTML?\\nThe Extensible HyperText Markup Language (XHTML™) is a family\\nof current and future document types and modules that reproduce, subset, and\\nextend HTML, reformulated in XML.\\nXHTML Family document types are all XML-based, and ultimately\\nare designed to work in conjunction with XML-based user agents.\\nXHTML is the successor of HTML, and a series\\nof specifications has been developed for XHTML.\\nMission of the HTML Working Group\\nTo develop the next generation of HTML as a suite of XML tag\\nsets with a clean migration path from HTML 4. Some of the\\nexpected benefits include: reduced authoring costs, an improved\\nmatch to database & workflow applications, a modular solution\\nto the increasingly disparate capabilities of browsers, and the\\nability to cleanly integrate HTML with other XML applications.\\nFor further information, see the Charter for\\nthe HTML Working Group\\n(members only).\\nNote.\\nThe HTML Working Group Charter has been renewed in August 2002.\\nRecommendations\\nW3C produces what are known as\\n\"Recommendations\".\\nThese are specifications, developed by W3C working groups, and\\nthen reviewed by Members of the Consortium. A W3C Recommendation\\nindicates that consensus has been reached among the Consortium\\nMembers that a specification is appropriate for widespread use.\\nXHTML 1.0 |\\nHTML 4.01 |\\nXHTML Basic |\\nModularization of XHTML |\\nXHTML 1.1 |\\nXML Events\\nXHTML 1.0\\nXHTML 1.0 is the W3C\\'s first Recommendation for XHTML,\\nfollowing on from earlier work on HTML 4.01,\\nHTML 4.0, HTML 3.2 and HTML 2.0. With a wealth of features, XHTML 1.0\\nis a reformulation of HTML 4.01 in XML, and combines the strength of\\nHTML 4 with the power of XML.\\nXHTML 1.0 is the first major change\\nto HTML since HTML 4.0 was released in 1997. It brings the rigor\\nof XML to Web pages and is the keystone in W3C\\'s work to create\\nstandards that provide richer Web pages on an ever increasing\\nrange of browser platforms including cell phones, televisions,\\ncars, wallet sized wireless communicators, kiosks, and\\ndesktops.\\nXHTML 1.0 is the first step and the HTML Working Group is busy\\non the next. XHTML 1.0 reformulates HTML as an XML application.\\nThis makes it easier to process and easier to maintain. XHTML 1.0\\nborrows elements and attributes from W3C\\'s earlier work on HTML 4,\\nand can be interpreted by existing browsers, by following a few simple\\nguidelines.\\nThis allows you to start using XHTML now!\\nYou can roll over your old HTML documents into XHTML using\\nan Open Source HTML Tidy\\nutility. This tool also cleans up markup errors, removes clutter\\nand prettifies the markup making it easier to maintain.\\nThree \"flavors\" of XHTML 1.0:\\nXHTML 1.0 is specified in three \"flavors\". You specify which\\nof these variants you are using by inserting a line at the\\nbeginning of the document. For example, the HTML for this\\ndocument starts with a line which says that it is using XHTML\\n1.0 Strict. Thus, if you want to validate the document, the\\ntool used knows which variant you are using. Each variant has its\\nown DTD - Document Type Definition - which sets out the rules and\\nregulations for using HTML in a succinct and definitive manner.\\n\\n\\nXHTML 1.0 Strict - Use this when you want really clean\\nstructural mark-up, free of any markup associated with layout. Use\\nthis together with W3C\\'s Cascading Style Sheet language (CSS) to get the font, color, and layout\\neffects you want.\\n\\n\\nXHTML 1.0 Transitional - Many people writing Web pages for\\nthe general public to access might want to use this flavor of XHTML\\n1.0. The idea is to take advantage of XHTML features including\\nstyle sheets but nonetheless to make small adjustments to your\\nmarkup for the benefit of those viewing your pages with older\\nbrowsers which can\\'t understand style sheets. These include using\\nthe body element with bgcolor, text and\\nlink attributes.\\n\\n\\nXHTML 1.0 Frameset - Use this when you want to use\\nFrames to partition the browser window into two or more frames.\\n\\n\\nThe complete XHTML 1.0\\nspecification is available in English in several formats,\\nincluding HTML, PostScript and PDF. See also the list of translations produced by\\nvolunteers.\\n\\nHTML 4.01\\nHTML 4.01 is a revision of the HTML\\n4.0 Recommendation first released on 18th December 1997. The\\nrevision fixes minor errors that have been found since then. The\\nXHTML 1.0 spec relies on HTML 4.01 for the meanings of XHTML elements\\nand attributes.\\nThis allowed us to reduce the size of the XHTML 1.0 spec very\\nconsiderably.\\nXHTML Basic\\nXHTML Basic is the second Recommendation in a series of XHTML\\nspecifications.\\nThe XHTML Basic document type includes the minimal set of modules\\nrequired to be an XHTML Host Language document type, and in addition\\nit includes images, forms, basic tables, and object support.\\nIt is designed for Web clients that do not support the full set of\\nXHTML features; for example, Web clients such as mobile phones,\\nPDAs,\\npagers, and settop boxes. The document type is rich enough for\\ncontent authoring.\\nXHTML Basic is designed as a common base that may be extended.\\nFor example, an event module that is more generic than the traditional\\nHTML 4 event system could be added or it could be extended by\\nadditional modules from XHTML Modularization such as the Scripting\\nModule. The goal of XHTML Basic is to serve as a common language\\nsupported by various kinds of user agents.\\nThe document type definition is implemented using XHTML\\nmodules as defined in \"Modularization of XHTML\".\\nThe complete XHTML Basic\\nspecification is available in English in several formats,\\nincluding HTML, plain text, PostScript and PDF. See also the list of translations produced by\\nvolunteers.\\nModularization of XHTML\\nModularization of XHTML is the third Recommendation in a series of\\nXHTML specifications.\\nThis Recommendation specifies an abstract modularization of XHTML and\\nan implementation of the abstraction using XML Document Type Definitions\\n(DTDs). This modularization provides a means for subsetting and extending\\nXHTML, a feature needed for extending XHTML\\'s reach onto emerging platforms.\\n\\nModularization of XHTML will make it easier to combine with markup tags\\nfor things like vector graphics, multimedia, math, electronic\\ncommerce and more. Content providers will find it easier to\\nproduce content for a wide range of platforms, with better\\nassurances as to how the content is rendered.\\nThe modular design reflects the realization that a\\none-size-fits-all approach will no longer work in a world where\\nbrowsers vary enormously in their capabilities. A browser in a\\ncellphone can\\'t offer the same experience as a top of the range\\nmultimedia desktop machine. The cellphone doesn\\'t even have the\\nmemory to load the page designed for the desktop browser.\\nSee also an overview of\\nXHTML Modularization.\\nXHTML 1.1 - Module-based XHTML\\nThis Recommendation defines a new XHTML document type that is based\\nupon the module framework and modules defined in Modularization of XHTML.\\nThe purpose of this document type is to serve as the basis for future\\nextended XHTML \\'family\\' document types, and to provide a consistent,\\nforward-looking document type cleanly separated from the deprecated,\\nlegacy functionality of HTML 4 that was brought forward into\\nthe XHTML 1.0 document types.\\nThis document type is essentially a reformulation of XHTML 1.0\\nStrict using XHTML Modules. This means that many facilities available\\nin other XHTML Family document types (e.g., XHTML Frames) are not\\navailable in this document type. These other facilities are available\\nthrough modules defined in Modularization of XHTML, and document\\nauthors are free to define document types based upon XHTML 1.1 that\\nuse these facilities (see Modularization of XHTML for information on\\ncreating new document types).\\nWhat is the difference\\nbetween XHTML 1.0, XHTML Basic and XHTML 1.1?\\nThe first step was to reformulate HTML 4\\nin XML, resulting in XHTML 1.0.\\nBy following the HTML\\nCompatibility Guidelines set forth in Appendix C of the XHTML 1.0\\nspecification, XHTML 1.0 documents could be compatible with existing\\nHTML user agents.\\nThe next step is to modularize the elements and attributes into\\nconvenient collections for use in documents that combine XHTML with\\nother tag sets. The modules are defined in\\nModularization of XHTML.\\nXHTML Basic is an example of fairly\\nminimal build of these modules and is targeted at mobile applications.\\nXHTML 1.1 is an example of a larger build\\nof the modules, avoiding many of the presentation features.\\nWhile XHTML 1.1 looks very similar to XHTML 1.0 Strict, it is designed\\nto serve as the basis for future extended XHTML Family document types,\\nand its modular design makes it easier to add other modules as needed or\\nintegrate itself into other markup languages.\\nXHTML\\n1.1 plus MathML 2.0 document type is an example of such XHTML Family\\ndocument type.\\nXML Events\\nNote: This specification was renamed from \"XHTML Events\".\\n\\nThe XML Events module defined in this specification provides XML\\nlanguages with the ability to uniformly integrate event listeners and\\nassociated event handlers with Document Object Model (DOM) Level 2\\nevent interfaces. The result is to provide an interoperable way of\\nassociating behaviors with document-level markup.\\n\\nPrevious Versions of HTML\\n\\n\\nHTML 4.0\\nFirst released as a W3C Recommendation on 18 December 1997. A\\nsecond release was issued on 24 April 1998 with changes limited\\nto editorial corrections. This specification has now been\\nsuperseded by HTML 4.01.\\nHTML 3.2\\nW3C\\'s first Recommendation for HTML which represented the consensus\\non HTML features for 1996. HTML 3.2 added widely-deployed features\\nsuch as tables, applets, text-flow around images,\\nsuperscripts and subscripts, while providing backwards\\ncompatibility with the existing HTML 2.0\\nStandard.\\nHTML 2.0\\nHTML 2.0\\n(RFC 1866) was developed by\\nthe IETF\\'s HTML Working\\nGroup, which closed in 1996. It set the standard for core HTML\\nfeatures based upon current practice in 1994. Note that with the release\\nof RFC 2854,\\nRFC 1866 has been obsoleted and its current status\\nis HISTORIC.\\n\\nISO HTML\\nISO/IEC 15445:2000\\nis a subset of HTML 4, standardized by ISO/IEC.\\nIt takes a more rigorous stance for instance, an h3 element\\ncan\\'t occur after an h1 element unless there is an intervening\\nh2 element. Roger Price and David Abrahamson have written\\na user\\'s\\nguide to ISO HTML.\\nOther Public Drafts\\nWe would like to hear from you via email. Please send your\\ncomments to: www-html@w3.org\\n(archive).\\nDon\\'t forget to include XHTML in the subject line.\\nHTML Working Group Roadmap\\n\\nThis describes the timeline for\\ndeliverables of the HTML working group. It used to be a W3C\\nNOTE but has now been moved to the MarkUp area for easier\\nmaintenance.\\n\\nModularization of\\nXHTML in XML Schema\\n\\nThe purpose of this document is to describe a modularization\\nframework for languages within the XHTML Namespace using XML Schema.\\nThis document provides a complete set of XML Schema modules for\\nXHTML. In addition to the schema modules themselves, the framework\\npresented here describes a means of further extending and modifying\\nXHTML.\\n\\nAn XHTML + MathML + SVG Profile\\nAn XHTML+MathML+SVG profile is a profile that combines XHTML 1.1,\\nMathML 2.0 and SVG 1.1 together.  This profile enables mixing XHTML,\\nMathML and SVG in the same document using XML namespaces mechanism,\\nwhile allowing validation of such a mixed-namespace document.\\nThis specification is a joint work with the SVG Working Group, with\\nthe help from the Math WG.\\nXHTML 2.0\\nXHTML 2.0 is a markup language intended for rich, portable web-based\\napplications. While the ancestry of XHTML 2.0 comes from HTML 4, XHTML\\n1.0, and XHTML 1.1, it is not intended to be backward\\ncompatible with its earlier versions. Application developers familiar\\nwith its earlier ancestors will be comfortable working with XHTML 2.0.\\nXHTML 2 is a member of the XHTML Family of markup languages. It is an\\nXHTML Host Language as defined in Modularization of XHTML. As such, it is made up of a set of XHTML Modules\\nthat together describe the elements and attributes of the language, and\\ntheir content model. XHTML 2.0 updates many of the modules defined in\\nModularization of XHTML, and includes the updated versions of all those\\nmodules and their semantics. XHTML 2.0 also uses modules from\\nRuby, XML Events, and\\nXForms.\\nXFrames\\nXFrames is an XML application for composing documents together,\\nreplacing HTML Frames.\\nXFrames is not a part of XHTML per se, that allows similar\\nfunctionality to HTML Frames, with fewer usability problems, principally\\nby making the content of the frameset visible in its URI.\\nXHTML 1.0 in XML Schema\\nThis document describes informative XML Schemas for XHTML 1.0.\\nThese Schemas are still work in progress, and this document does not\\nchange the normative definition of XHTML 1.0.\\nHLink\\n\\nThe HLink module defined in this specification provides XHTML\\nFamily Members with the ability to specify which attributes of\\nelements represent Hyperlinks, and how those hyperlinks should be\\ntraversed, and extends XLink use to a wider class of languages than\\nthose restricted to the syntactic style allowed by XLink.\\n\\nXHTML-Print\\n\\nXHTML-Print is member of the family of XHTML Languages defined by\\nthe Modularization of\\nXHTML.\\nIt is designed to be appropriate for printing from mobile devices to\\nlow-cost printers that might not have a full-page buffer and that\\ngenerally print from top-to-bottom and left-to-right with the paper\\nin a portrait orientation. XHTML-Print is also targeted at printing\\nin environments where it is not feasible or desirable to install\\na printer-specific driver and where some variability in the formatting\\nof the output is acceptable.\\n\\n\\nUseful information for\\nHTML/XHTML authors\\nTutorials\\n\\nGetting started with HTML\\nby Dave Raggett\\nis a short introduction to writing HTML, including tutorials on\\nadvanced features.\\nAdding a touch of style\\nby Dave Raggett is a short guide to styling your Web pages.\\nXHTML Modules and Markup\\nLanguages - How to create XHTML Family modules and markup languages for\\nfun and profit by Shane McCarron explains how to create\\nXHTML Family modules and markup languages, based on\\nModularization of XHTML.\\n\\nSlides on XHTML\\nYou may also be interested in the following slides on XHTML:\\n\\nXHTML:\\nThe Extensible Hypertext Markup Language by Dave Raggett, at\\nW3C LA event in Stockholm, 24 March 1999.\\nW3C\\nHTML Activity by Dave Raggett, as part of\\nWWW8 W3C Track, 12 May 1999\\nW3C\\nWork on XHTML by Dave Raggett, at XML \\'99,\\n6 December 1999.\\nThe presentation describes the work being done by W3C on XHTML.\\nThe XHTML Family (in Japanese) by Masayasu Ishikawa,\\nat SFC Open Research Forum 2001,\\n21 September 2001.\\nXForms, XHTML\\nand Device Independence by Steven Pemberton, at\\nW3C.DE-Arbeitstreffen: Cross Media Publishing, 11 April 2002.\\nXHTML Family\\nby Masayasu Ishikawa, as part of WWW2002\\nW3C Track, 9 May 2002.\\nSlides are available in XHTML or\\nHTML\\n(XHTML version needs XHTML+MathML+SVG+Ruby support).\\nXHTML\\n2.0 and XForms by Steven Pemberton, as part of\\nWWW2003\\nW3C Track,\\n21 May 2003.\\nW3C\\'s\\nHorizontal Activities Usage: XHTML Family Case Study by\\nSteven Pemberton, WWW2003 W3C Track, 23 May 2003.\\nXHTML\\nand XForms by Steven Pemberton, at Zomersessie van NGI Limburg: XHTML2 en XForms, state of the art en\\nstage-ervaringen bij het W3C, 3 July 2003.\\n\\nGuidelines for authoring\\nHere are some rough guidelines for HTML authors. If you use\\nthese, you are more likely to end up with pages that are easy to\\nmaintain, look acceptable to users regardless of the browser they\\nare using, and can be accessed by the many Web users with\\ndisabilities. Meanwhile W3C have produced some more formal\\nguidelines for authors. Have a look at the detailed Web Content Accessibility\\nGuidelines 1.0.\\n\\n\\nA question of style sheets. For most people the look of\\na document - the color, the font, the margins - are as important\\nas the textual content of the document itself. But make no\\nmistake! HTML is not designed to be used to control these aspects\\nof document layout. What you should do is to use HTML to mark up\\nheadings, paragraphs, lists, hypertext links, and other\\nstructural parts of your document, and then add a style sheet to\\nspecify layout separately, just as you might do in a conventional\\nDesk Top Publishing Package. That way, not only is there a better\\nchance of all browsers displaying your document properly, but\\nalso, if you want to change such things as the font or color,\\nit\\'s really simple to do so. See the\\nTouch of style.\\n\\n\\nFONT tag considered harmful! Many filters from\\nword-processing packages, and also some HTML authoring tools,\\ngenerate HTML code which is completely contrary to the design\\ngoals of the language. What they do is to look at a document\\nalmost purely from the point of view of layout, and then mimic\\nthat layout in HTML by doing tricks with FONT, BR and &nbsp;\\n(non-breaking spaces). HTML documents are supposed to be\\nstructured around items such as paragraphs, headings and lists.\\nYet some of these documents barely have a paragraph tag in sight!\\nThe problem comes when the content of pages needs to be\\nupdated, or given a new layout, or re-cast in XML (which is now\\nto be the new mark-up language). With proper use of HTML, such\\noperations are not difficult, but with a muddle of non-structural\\ntags it\\'s quite a different matter; maintenance tasks become\\nimpractical. To correct pages suffering from injudicious use of\\nFONT, try the HTML Tidy\\nprogram, which will do its best to put things right and\\ngenerate better and more manageable HTML.\\n\\n\\nMake your pages readable by those with disabilities.\\nThe Web is a tremendously useful tool for the visually impaired\\nor blind user, but bear in mind that these users rely on speech\\nsynthesizers or Braille readers to render the text. Sloppy\\nmark-up, or mark-up which doesn\\'t have the layout defined in a\\nseparate style sheet, is hard for such software to deal with.\\nWherever possible, use a style sheet for the presentational\\naspects of your pages, using HTML purely for structural\\nmark-up.\\nAlso, remember to include descriptions with each image, and\\ntry to avoid server-side image maps. For tables, you should\\ninclude a summary of the table\\'s structure, and remember to\\nassociate table data with relevant headers. This will give\\nnon-visual browsers a chance to help orient people as they\\nmove from one cell to the next. For forms, remember to include\\nlabels for form fields.\\n\\n\\nDo look at the accessibility\\nguidelines for a more detailed account of how to make your\\nWeb pages really accessible.\\nW3C Markup Validation Service\\nTo further promote the reliability and fidelity of communications\\non the Web, W3C has introduced the W3C\\nMarkup Validation Service at\\nhttp://validator.w3.org/.\\nContent providers can use this service to validate their Web\\npages against the HTML and XHTML Recommendations, thereby\\nensuring the maximum possible audience for their Web pages.\\nIt also supports XHTML Family document types such as XHTML+MathML and\\nXHTML+MathML+SVG, and also other\\nmarkup vocabularies such as SVG.\\nSoftware developers who write HTML and XHTML editing tools can ensure\\ninteroperability with other Web software by verifying that the\\noutput of their tool complies with the W3C Recommendations for\\nHTML and XHTML.\\nHTML Tidy\\nHTML Tidy is a stand-alone tool for checking and pretty-printing\\nHTML that is in many cases able to fix up mark-up errors, and also\\noffers a means to convert existing HTML content into well-formed XML,\\nfor delivery as XHTML.\\nHTML Tidy was originally written by Dave\\nRaggett, and it is now maintained as an\\nopen source project at SourceForge\\nby a group of volunteers.\\nThere is an archived\\npublic mailing list html-tidy@w3.org. Please send bug reports /\\nsuggestions on HTML Tidy to this mailing list.\\nDiscussion Forums\\nChanges to HTML necessitate obtaining a consensus from a broad\\nrange of organizations. If you have a great idea, it will take\\ntime to convince others! Here are some of the places where\\ndiscussion on HTML takes place:\\n\\ncomp.infosystems.www.authoring.html\\nA USENET newsgroup where HTML authoring issues are discussed.\\n\"How To\" questions should be addressed here. Note that many\\nissues related to forms and CGI, image maps, transparent gifs,\\netc. are covered in the WWW\\nFAQ.\\nwww-html@w3.org\\nA technical discussion list. If you have a proposal for a change\\nto HTML/XHTML, you might start a discussion here to see what other\\ndevelopers think of it. \\n\\nhow to subscribe\\narchives from\\n1994 to present\\n(We\\'re working on moving the old archives to W3C. Stay tuned!)\\n\\n\\nwww-html-editor@w3.org\\nThis is a list to report errors / send review comments on HTML/XHTML\\nspecifications. This is NOT a discussion list.  Anyone may\\nsend comments without subscription, although you\\'ll be\\nrequested to give explicit\\napproval to include your message in our publicly-readable\\nmailing\\nlist archive at your first post.  To subscribe, send subscription\\nrequest to www-html-editor-request@w3.org.  For more information, see\\nhow to subscribe.\\nW3C HTML Working Group (members only)\\nThe Group\\'s mission is to develop the next generation of HTML\\nas a suite of XML tag sets with a clean migration path from HTML 4.\\nSome of the expected benefits include: reduced authoring costs,\\nan improved match to database & workflow applications,\\na modular solution to the increasingly disparate capabilities of\\nbrowsers, and the ability to cleanly integrate HTML with other\\nXML applications. The Group is chaired by Steven Pemberton.\\nw3c-translators@w3.org\\nThis is a mailing list for people working on translations of\\nW3C specifications such as the HTML/XHTML\\nRecommendations. To subscribe, send an email to\\nw3c-translators-request@w3.org with the word \"subscribe\" in\\nthe subject line; (include the word \"unsubscribe\" if you want to\\nunsubscribe.) The archive\\nfor the list is accessible online.\\nIETF MHTML WG (closed)\\nDeveloped RFC\\n2557 - \"MIME Encapsulation of Aggregate Documents, such as\\nHTML (MHTML). J. Palme et al. March 1989.\\nIETF HTML Working\\nGroup (closed)\\nThe HTML working group of the IETF, closed in 1996.\\nWeb Conferences\\nThe next international conference dedicated to the Web is WWW2004,\\nto be held in New York city, USA, in 2004.\\nThe last was WWW2003, which was held\\nin Budapest, Hungary, 20-24 May 2003.\\n\\nRelated W3C Work\\n\\nXML\\nXML is the universal format for structured documents and data on\\nthe Web. It allows you to define your own mark-up formats when\\nHTML is not a good fit. XML is being used increasingly for data;\\nfor instance, W3C\\'s metadata format RDF.\\nStyle Sheets\\nW3C\\'s Cascading Style Sheets\\nlanguage (CSS)\\nprovides a simple means to style HTML pages, allowing you to control\\nvisual and aural characteristics; for instance, fonts, margins,\\nline-spacing, borders, colors, layers and more. W3C is also working on\\na new style sheet language written in XML called XSL, which\\nprovides a means to transform XML documents into HTML.\\nDocument Object Model\\nProvides ways for scripts to manipulate HTML using a set of\\nmethods and data types defined independently of particular\\nprogramming languages or computer platforms. It forms the basis\\nfor dynamic effects in Web pages, but can also be exploited in\\nHTML editors and other tools by extensions for manipulating HTML\\ncontent.\\nInternationalization\\nHTML 4 provides a number of features for use with a wide\\nvariety of languages and writing systems. For instance, mixed\\nlanguage text, and right-to-left and mixed direction text. HTML\\n4 is formally based upon Unicode, but allows you to store and\\ntransmit documents in a variety of character encodings. Further\\nwork is envisaged for handling vertical text and phonetic\\nannotations for Kanji (Ruby).\\nAccess for People with\\nDisabilities\\nHTML 4 includes many features for improved access by people\\nwith disabilities. W3C\\'s Web Accessibility Initiative is working\\non providing effective guidelines for making your pages\\naccessible to all, not just those using graphical browsers.\\nXForms\\nForms are a very widely used feature in web pages. W3C is\\nworking on the design of the next generation of web forms with a\\nview to separating the presentation, data and logic, as a means\\nto allowing the same forms to be used with widely differing\\npresentations.\\nMathematics\\nWork on representing mathematics on the Web has focused on\\nways to handle the presentation of mathematical expressions and\\nalso the intended meaning. The MathML language is an\\napplication of XML, which, while not suited to hand-editing, is\\neasy to process by machine.\\n\\nContacts\\n\\n\\nMasayasu ISHIKAWA is\\nthe HTML Activity Lead and the Team Contact for the HTML Working Group\\n\\n\\n\\nnews\\n\\n| Recommendations\\n| public drafts\\n| test suites\\n| tutorials\\n| slides\\n| guidelines\\n| validation\\n| articles\\n| translations\\n| charter\\n| working group\\n| roadmap\\n| XForms\\n| forums\\n| HTML Tidy\\n| related work\\n| HTML 4.0/3.2/2.0\\n| historical\\n\\n\\n\\nCopyright\\n©1995-2004 W3C®\\n(MIT, ERCIM,\\nKeio), All Rights Reserved. W3C liability, trademark, document\\nuse and software licensing\\nrules apply. Your interactions with this site are in accordance with\\nour public and\\nMember\\nprivacy statements.\\nThis page was last modified on: $Date: 2004/01/20 19:07:55 $\\n\\n\\n\\n',\n",
       " ' logographic. 3. For more nonsense spellings, see Bennet Cerf\\'s Out on a Limerick . to top of page 4. A phonemic transcription is not as precise as a phonetic transcription. The latter includes a variety of markers that can clearly indicate dialects and regional patterns. For an optimized writing tool, all one needs is a consistent orthography that provides a useful guide to pronunciation and spelling. 5. According to Beech (1992) Jour. of Gen. Psych . 119(2), p. 169f. subjects were able to regain normal reading speeds after they read about 6,000 words of regularized text. Regularizing orthography involves changing 30% of the words. Research indicates that adults regain normal reading speed after they have read 6,000 words or regularized text. 30% were spelling accurately in the new orthography by the end of the session. Writing speed improved but was still slower than with unregularized TO. Orthographic change has more impact on writing speed than on reading speed. Reading is unaffected. Writing requires several weeks to adjust. 6. Figure 5 . lists the number of different ways that 18 sounds can be spelled and the different ways that the same letter can be pronounced. Complete charts are available in the books by Dewey and Pitman. This one is limited to the 18 sounds for which there is no single Roman letter. The sounds are typically signified by a digraph or letter combination. Unfortunately, in TO they are signified by multiple digraphs. 7. IPA ( The International Phonetic Alphabet , 1890) uses the same convention, /sh/=S, /ch/=TS 8. Soffietti (1955) full reference below 9. Hanna (1971) 10. Phonemes refer to the significant sound categories used by native speakers. A category refers to a collection or group of dissimilar things that are treated as the same. Typically there is a range of sounds that will be mutuallyintelligible and interpretable as a particular phoneme. Pronunciations within this region are called allophones. 11. 25% is a number that many educators use which is based on an estimated average improvement in reading test scores and the difference in time to mastery between countries with phonemic scripts compared to countries trying to teach TO. The difference between learning 40 sound signs and over 400 spelling patterns is over 100%. By this estimate, teaching an alphabetic orthography would be 100 times as efficient as teaching TO. References Extended Bibliography Beech, John R. \"Adaption of writing to orthographic change.\" J. of General Psychology . 199(2), 169-179, 1992 The effects of spelling change on the adult reader. Spelling Progress Bulletin , 23, 7-13.1983 Bett, Steve T. The Alphabet: Its Origins and Early Development , ( unpublished ) 1996 A Pictographic Monoline Phonemic Script , (unpublished ms) 1996 Conventional Spelling & Right Writing, Louisiana Middle School Journal , 5 (1) 25-36, 1996 Can Pictographic Cues Make an Augmented Script Easier to Learn & se? . (unpublished manuscript) 1996 Pictionary Hieroglyphics , Louisiana Middle School Journal , 3 (1) 13-19, 1994 Alphabets for English. Bridge Spelling Proposals published as a personal view by the Simplified Spelling Socieity, 1996 Coulmas, Florian. The Blackwell Encyclopedia of Writing Systems .O xford: Blackwell,1996 Crystal, David (editor) Encyclopedia of the English Language . Cambridge University Press, 1995 Dewey , Godfrey. English Spelling: Roadblock to reading. Teachers College Press, N.Y., 1971 Downing, John A. The ITA Reading Experiment . Scott Foresman & Co. Chicago. 1964. Ellis, Henry (ca. 1900) referenced by Dewey and Pitman. Hanna, Paul, et al. Phoneme-Grapheme Correspondence as Cues to Spelling Improvement . Doc. OE-32008, Washington, D.C., U.S. Government Printing Office, 1971 Laubach, Frank C. Let\\'s Reform Spelling --Why and How. New Readers Press. New York, 1966 Pitman , James. and John St. John. Alphabets and Reading : The ITA.. Pitman. London. 1969 Soffietti, James P. \"Why Children Fail to Read: A Linguistic Analysis.\" The Havard Educational Review , 25, (2) 63-84, Cambridge, MA, Spring, 1955 The SSA (Simpler Spelling Association) Fonetic Alfabet . SSA, Lake Placid, N.Y., 1959 Twain, Mark. A Simplified Alphabet . What\\'s Man . Essay No. XI, 1899 Illustrated version How I would Spell It Upward, Chrisopher. Cut Spelling Handbook . Simplified Spelling Society , 1996 Yule, Valerie . Spelling as a Social Invention. 1994. full text on the Web. Zachrisson, R.E.. Anglic : An International Language. McGrath, College Pk. Md., 1970 Pictographic Monofon Vowels 7 primary vowels, 7 derived blends of the primaries Complete monofon character grid 14 vowels and 26 consonants Monofon consonants Dr. Bett [ pic ] [ homepage ] encourages and welcomes critiques of this paper, the underlying speculations and theories, as well as comments on the general topic of phonology and references to related works. Previous page <<< SiteMap web hosting • domain names • web design online games • online dating • long distance digital cameras • advertising online \\n\\t\\n\\n-->\\n\\n\\n\\n\\n\\n\\n\\nTraditional English Orthography - History\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nweb hosting\\ndomain names\\nemail addresses\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\npage\\n2\\n\\nThe\\nNature of and Alternatives to Standard English Orthography\\n\\xa0\\n\\xa0\\n\\xa0\\nAbstract:\\nMost people are not\\nfully aware of all the problems with English spelling. They confuse the\\nlanguage with its traditional orthography and have a misplaced reverence\\nfor the antiquated spellings preserved in the dictionary.\\nThis article reveals\\nsome of the absurdities of the spelling system used in English speaking\\ncountries since 1800, explains the difference between a language, a script,\\nand a spelling system, and chronicles some of the attempts to \"break the\\nspell\"*\\nadvanced by advocates of simplified and regularized spelling.\\xa0\\nHow English came to be written the way it is\\n\\xa0\\n\\n\\nThe heart of the trouble is\\nwith our foolish alphabet,\\nit doesn\\'t know how to spell and can\\'t be taught.\\n- Mark\\nTwain, 1899\\n\\n\\n\\nTo represent the 40 or so sounds (or phonemes) of English requires\\n40 to 50 sound signs (or graphemes): at least 18 more than what is available\\nin the 26 character Roman alphabet which has 3 redundant characters. (See\\nFig. 3)\\nFrom 1000 AD to 1600 AD, English scribes came up with a wide variety\\nof individualistic solutions to cope with this graphemic deficiency. The\\nmost common solution was to augment or expand the Roman alphabet with digraphs,\\nletter combinations used to represent the missing phonemes.\\nThe problem was that they created too many digraphs and letter combinations.\\nInstead of one digraph for each unrepresented sound, they created a surplus\\nof 5 to 20. Even the sounds that were represented, such as \"s\", were spelled\\nsix different ways. (s, ps, sw, c, sc, sch). Although the problem began\\nas one of graphemic deficiency, the problem with today\\'s traditional orthography\\n(TO) is one of graphemic overabundance and diversity.\\nIn the 14th Century, Chaucer lamented the \"grete dyversite in English\\nand in writyng of our tongue.\" Over the centuries, one type of diversity\\nhas been effectively abolished but the diversity of ways to spell a particular\\nsound has remained. We can no longer use a [y] for the /long I/ in \"diversity\"\\nbut we can still use this letter to represent the /long I/ in other words\\nsuch as \"fly\".\\nThe property of a written sign to have more than one value is called\\npolyvalence (Coulmas, 1996, p. 413). The typical goal\\nof a spelling reform is to reduce the degree or level of polyvalence. There\\nhave been reforms in the way that English is written but most of them increased\\npolyvalence.\\nWhile writing tends to remain the same, pronunciation changes over time.\\nThe 15th century was a period of rapid change referred to as \"the great\\nEnglish vowel shift.\" This was followed by a rise in the affordability\\nand popularity of the printed word. In the 16th century, the demand for\\nreading material was largely met by foreign (e.g., Dutch) printers. The\\nprinters not only dropped 4 of the old English characters, such as the\\nthorn (þ) , but also failed to keep pace with the sound changes that\\nwere affecting the language.\\nWith the publication of the King James Bible in 1611, a standard or\\nauthority for some uniformity and consistency in spelling was established.\\nAfter the publication of Johnson\\'s Dictionary in 1755, only one of the\\n50 or more ways that a short word could be spelled was deemed correct.\\nUnfortunately, this standardization came about before regularization. Strict\\nadherence to the alphabetic principle would not have permitted one to spell\\nfish \"G-H-O-T-I\".\\n\\xa0\\xa0\\xa0\\xa0 Solutions\\nIn the abstract, the solution to irregular spelling is simple.\\nJust make it regular: Assign one and only one sound sign to each of the\\n40 or so significant sounds or phonemes of the spoken English and allow\\nthe younger generations to use it in place of TO.\\nWhile such spelling reforms have been successful in other countries\\nand while Ben Franklin, Noah Webster, and others believed the benefits\\nwould clearly outweigh the temporary inconvenience, support for a major\\nreform has never enjoyed widespread political support. It is as if tampering\\nwith the spelling system was tantamount to tampering with the Constitution.\\n\\nPeople are more likely to\\nchange their religion\\nthan change their writing system.\\n--Charles Hockett, 1952\\n\\nWebster, due to the popularity of his dictionaries, was able to drop the\\nsilent [u] in colour and implement a few other piece-meal reforms. However,\\nTeddy Roosevelt\\'s support of *thru for *through wasn\\'t enough to sway public\\nopinion.\\xa0 [Why thru\\nis correct spelling]\\nFrom a practical standpoint, resistance to change, even change for the\\nbetter, is almost insurmountable. Most scholars agree that having a spelling\\nsystem similar to the one enjoyed by the Finns would increase literacy\\nand speed the attainment of a 5th grade reading level by more than 25%.\\nDue to the lack of awareness and a variety of mostly invalid reasons, few\\npeople beyond those who have studied the topic are ready to advance or\\nendorse any proposal for a spelling reform.\\nThe convergence among the proposals that have been advanced in the last\\n50 years can be seen in the following table. Note the close similarity\\nof the three digraphic scripts, NS, WES, and ITA (The\\nInitial Teaching Alphabet).6\\nSeveral of these systems,e.g., SpellRight,\\nFanetic, Truespel,\\xa0\\nsaxon-spanglic,\\nand Inglish, are well described on the Web.\\nLarger\\n40 character matrix\\n\\nFor the last 50 years there has been convergence of opinion\\nregarding the pragmatic solution to the problem of graphemic diversity.\\nThere are now three solutions depending on how much of the traditional\\nsystem one wants to preserve and the level of phonemicity one wants to\\nachieve: (1) streamlined traditional, (2) digraphic, and\\n(3) unigraphic augmented Roman.\\n1.\\nCut Spelng - Streamlined Spelling\\nStreamlining by removing redundant letters disrupts the pattern of traditional\\nspelling the least. It is also the least phonemic but still considerably\\nmore phonemic than traditional spelling. The Simplified Spelling Society\\ncalls this proposal Cut\\nSpelling, Cut Spelng, or simply CS. Cut spelling is illustrated in\\nFigure\\n3. When the redundant characters are cut, *knot becomes *not, *debt\\nbecomes *det, and *spelling becomes *spelng. Cut spelling also includes\\nthree substitutions which results in a minor change in the pattern TO:\\xa0\\xa0\\ngarage= garaj,\\xa0\\xa0 gin= jin,\\xa0\\xa0 graph= graf,\\xa0\\xa0\\nsigh= sy ( y = /ie/ ).\\n\\nCut spelng achevs a hyr levl\\nof fonemicity\\nwithout being basd on a fonetic\\nor alfabetic principl.\\n2. Digraphic Solutions\\nA few of the digraphic Spelling solutions (ITA, New Spelling, WES, Anglic)\\nwere shown above in columns 3-5. A related writing system is called SpellRight.\\nMost of the digraphic Roman scripts that were developed during the last\\n50 years are quite similar. Digraphic solutions select one spelling pattern\\n(for each phoneme) found in TO and make it universal. In addition to the\\npattern identified by New Spelling and its variants there is the pattern\\nreferred to as IPA YuroSpell\\nwhich are based on continental sound values rather than the most frequent\\nsound values found in TO.\\nNew Spelling, ITA, and WES are systematic but not phonemically perfect.\\nThey don\\'t have to be. A phonetic transcription of language captures dialects.\\nA phonemic transcription is a communication tool: It can be shared by those\\nwho pronounce words in slightly different ways.\\nI.T.A. (Initial Teaching\\nAlphabet) 5th column above\\nITA is actually a unigraphic solution but it looks very traditional\\nbecause the ligatures resemble traditional digraphs. The /long e/ sound\\nis ee just as with SpellRight, but the e\\'s are connected or ligatured\\nin order to be perceived as a distinct grapheme rather than a combination\\nof letters.\\nExcept for 5 graphemes (oo, uu, aa, ng, zh) and the use of a special\\nfont, ITA is the same as WES (World English Spelling) which is almost the\\nsame as SpellRight..\\nThe greatest difference is in the way that digraphs (two letter sound signs)\\nare connected. ITA uses a special font that converts the digraphs into\\nligatures (connected letters) that can be accessed\\nwith one keystroke.\\nIf the Arpabet keyboard was used, shift A would produce the æ\\nligature for the /long a/ rather than a capital letter A.\\nThe point of using the ligatures such as æ th,\\nsh, ch, etc. rather than the digraphs [ae,\\nth, sh, ch...] is to encourage students to think of combinations as a single\\nunique sound sign rather than something that can be broken down into component\\nparts. One could probably make this point without having the graphic reminder.\\nThe utility of the ITA as a way to introduce reading and writing is\\nwell documented (Downing, 1964). Those who start with ITA achieve ITA literacy\\nvery quickly. Many similar phonetic scripts have been used in the past\\nand almost all have been reported to be four times as effective as TO.\\nThe reason for this is obvious, there is less to learn before students\\nexperience success. It is far easier to learn 40 consistent sound signs\\nthan to learn three variations of 26 inconsistent sound signs (print, caps,\\nscript) with over 500 different interpretations.\\nAlphabetic systems are easier to learn than logographic (or word sign)\\nsystems. Once learned, however, logographic systems are often quicker and\\neasier to read. Once 4000 signs are mastered, Chinese, which is about 80%\\nlogographic, can be read quicker than English, which is about 25% logographic.\\nSpeed readers are not reading individual phonemes, they are not lip reading,\\nthey are reading whole word patterns.\\nPitman is accused of believing that TO can be tolerated if we are exposed\\nin stages. An increasing number of educators realize that this is only\\na half measure and that the problem of our bad spelling still remains and\\nis only postponed, not solved by learning ITA first.\\n\"It haz bin proovd aulso by the experiments ov thouzandz ov children\\nwith the perfect alphabet cauld ITA. Children studying ITA lern it not\\nonly twies as rapidly az children lerning convenshunul iregular speling,\\nbut severul timz as rapidly.\" --Laubach\\nAccording to Laubach (1966), \"Children studying ITA not only learn twice\\nas rapidly as children learning conventional irregular spelling but several\\ntimes as rapidly.\" Laubach says that it is not enough to remove half the\\nload from students, so they will progress 3/4\\'s as rapid as the children\\nin countries with more phonemic scripts. Laubach wants a reform now. Not\\nto change the past but to provide for a better future. For this to happen,\\nLaubach says, adults must tolerate new spelling and not demand that children\\nlearn the old spelling habits. They still have to learn to read TO, but\\nthey do not have to learn to write it.\\n\\n\\n3. Unigraphic Solutions\\n\\nUnigraf uses all 52 of the\\navailable upper and lower case letters plus a symbol as unique phonograms.\\nA digraphic solution is not an optimal solution to the problem of making\\nthe English script more phonemic. An elegant script should be optimized\\nacross several dimensions. Economy of writing and printing is one of those\\ndimensions. A unigraphic solution would be 10 to 20% more space efficient\\nthan TO or any digraphic system.\\nUnigraf (1999) shown above, Pitman\\'s phonotypy (1845), Goudy\\'s\\nSSA fonetic alphabet (1962), and Pictographic\\nMonofon (1995) are examples of readable unigraphic solutions that use\\nthe shifted characters as another 26 sound signs or employ an augmented\\nalphabet with new character shapes. All unigraphic solutions require more\\nthan 26 letters, but they do not require an upper case. The following solution\\nintroduces no unfamiliar characters but it does make A and a refer to different\\nsounds.\\nArpabets & ASCIIbets\\nMost proposals for regularizing English try to stay as close to TO as\\npossible. This includes using a dual redundant character set, the upper\\nand lower case letters. Arpabets (or ASCIIbets) do\\nnot use capital letters in the traditional way, they use them as unique\\nsound signs. With 52 characaters instead of 26 to represent the 40 some\\nphonemes of English, there is no need to use digraphs. Each phoneme has\\nits own grapheme.\\nOne such script, the askEbet, establishes unigraphic keyboard positions\\nfor 18 new characters. It can also be used to quickly identify the 18 special\\ncharacters. When TO is transcribed into Arpabet characters, the missing\\nsounds that require digraphs in WES show up as capital letters. Since there\\nis a logic to each of the Arpabet keyboard positions, they can be memorized\\nvery quickly. Transcribing the last line:\\n·sins TAr iz u lojik tU EC uv Du ARPAbet kEbOrd pOsiSuns,\\nTA can bE memOrIzd verE kwiklE\\nThis example illustrates both the utility of the Arpabet as an analytical\\ntool and the problems it initially introduces by deviating so far from\\ntraditional orthography. In NF and WES, the same passage would read:\\nSins ther iz a\\' lojik tu\\xa0 i,ch uv th ARPAbet\\nki,bo\\'rd p\\'zish\\'nz, thei kan be mem\\'ri,zd veri kwikli,.\\nSins thaer is a lojik to eech of the ARPAbet keeboerd poesishuns,\\nthey kan bee memoeriezd veri kwiklee.\\n\\xa0\\n\\n\\nThe askEbet is not the only writing\\nsystem that limits itself to the 52 characters found on a standard typewriter:\\nA character set is often referred to as QWERTY or ASCII.\\n\\nIPA\\nASCII also uses upper and lower case letters as indicating distinct\\nsound signs.\\xa0\\nOne of the most ambitions ASCII based proposals is for a world language\\nbased on 39 unigraphic sound signs. This script, known as ANJeL,\\nis quite similar to the askEbet except it uses the upper case letters as\\nthe default or standard and represents the sounds that other scripts represent\\nwith digraphs as lower case letters. A sentence in ANJeL is illustrated\\non the right::\\nA digraphic solution is not an optimal solution to the problem of making\\nthe English script more phonemic. An elegant script should be optimized\\nacross several dimensions. Economy of writing and printing is one of those\\ndimensions. A unigraphic solution would be 10 to 20% more space efficient\\nthan TO or any digraphic system.\\xa0\\n\\n\\n\\nSound\\nSigns in Cardinal Vowel Position\\xa0\\nstarting top-left with the high\\nfront vowel /I/\\n\\n\\n\\nANJeL Notation\\n\\n\\n\\n\\nin\\ni\\n\\n\\neel\\nE\\n\\nasia\\nia\\n\\nup\\nu\\n\\n\\nuse\\nU\\n\\n\\nooze\\nX\\n\\n\\nhook\\nx\\n\\n\\n\\n\\negg\\ne\\n\\n\\nace\\nA\\n\\nAir\\nAR\\n\\nacute\\n\\' u\\n\\n\\noil\\nQ\\n\\n\\ncow\\nC\\ndown\\n\\n\\nowe\\nO\\n\\n\\n\\n\\nat\\na\\n\\n\\nIce\\nI\\n\\n.\\n\\nalm\\no aa\\n\\n\\nurn\\nxR\\n\\n\\nawe\\no\\n\\n\\nox\\no\\n\\n\\n\\n\\nANJeL In this writing system, the new sound\\nsigns\\xa0\\nand the short vowels appear as lower case letters.\\xa0\\neVRE tiG eLS iZ uP\\'R KAS.\\n15 vowels (shown above)\\n+ 24 consonants\\nevrE\\nTiG els iz upR kAs (Unigr@f)\\nsee\\nmap-IPA.html\\n\\n\\n\\n\\n\\n\\nAlong with ANJeL, Pitman\\'s phonotypy (1845), Goudy\\'s\\nSSA fonetic alphabet (1962), and Pictographic\\nMonofon (1995) are examples of readable unigraphic solutions.\\nMonofon, a minimalist design, was illustrated in Figure\\n5. Like Kingsley Read\\'s Shavian script, Monofon\\nis space efficient and optimized for rapid writing. All characters can\\nbe formed with one quick continuous stroke. Monofon is also pictographic\\nwhich means that most of the characters represent objects which begin with\\nthe same sound. Relating the shape to the sound through the letter name\\nis a mnemonic technique used with the very first alphabets. It made ancient\\nalphabets (e.g., Phoenician) easy to teach to illiterates in a few weeks.\\nThe dBLspel Solution\\nDoubleSpell (dBLspel)\\nis a systematic notational scheme that achieves a closer approximation\\nof TO by utilizing more than one spelling pattern or grapheme per sound.\\nIt is a positional solution where the letter used to represent a sound\\ndepends on its position in a word (initial, medial, or terminal). In the\\nfirst two positions, for instance, the spelling of the sound /ou/ or *owe\\nis [ou] but in the terminal position /ou/ is spelled *o. *Although can\\ntherefore be spelled *oltho. The word *highly can be spelled *hyly because\\nthe sound associated with y depends on its position in the word: | y-consonant\\n| /ai/ | /i:/ unstressed |.\\nHere is a sample of this spelling:\\nWot we hav crEAtd in th nu speling iz a toutaly\\ncongrw\\'nt simb\\'l set wich inclwds spesificaly 18 vau\\'l simb\\'lz and 23 conson\\'nt\\nsimb\\'lz; wun les than 42, bico,z we ulau sum letrs tu serv a dw\\'l roul.\\nTher iz litl ridund\\'nsy, and o\\'nly th lo,ng vau\\'lz ar reprisented byy tw\\nletrz. This set ov 41 distinct simb\\'lz that unambigiu\\'sly reprisents eech\\nov th 42 saundz ov \\'English\\' speech o\\'penz the dorz tu funcsh\\'nl lit\\'rasy\\nfor ev\\'rywun, thouz ov us Ab\\'l tu coup with \\'English\\'s\\' inconsistenseez\\nand thouz ov us hu cu\\'d not, az wel az for fiuch\\'r jen\\'reish\\'ns.\\nCut New Follick uses a very mild form of positional\\nspelling. With this system, redundant markers are removed. Thus tu, /tu:/\\nis clipped to tu. Tu is interpreted as *to or *two because there are no\\nknown examples of an alternative interpretation of this sound sign in the\\nterminal position. *To has three different pronunciations: t\\', tu\\' and\\ntu,:\\xa0 so there is little point in being specific. In NF, *Santa Claus\\nwould be spelled *Sa,nta\\' Klo,z. There is another terminal U sound here\\nbut the letter *u is not used to represent it.\\nConclusion\\nSchool teachers charged with teaching reading, writing, and spelling\\ncan make a significant contribution to students\\' understanding of writing\\nsystems. One of the roles of teachers is to pass on tradition but this\\ndoes not have to be done uncritically. Teachers are in an ideal position\\nto correct misconceptions and clear out some of the intellectual debris\\nthat can inhibit progressive social reforms.\\nThe primary obstacle to the adoption of any simplified spelling proposal\\nis ignorance or lack of awareness. While teaching the traditional orthography,\\nteachers can point out its deficiencies and illustrate the benefits enjoyed\\nby other countries with regular spelling systems.\\nTeachers will continue to teach the irrational code, but they should\\ndo it with the full knowledge of its faults. Teachers can help break\\nthe spell by not covering up the problems, by not confusing language\\nwith orthography, and by not promoting a misplaced reverence for the antiquated\\nspellings preserved in the dictionary.\\nStandard English orthography (TO) is much more difficult and complicated\\nthan it needs to be. Using a regularized script such as ITA or WES will\\nsimplify the teaching / learning process and lighten the load on beginning\\nreaders. It is far easier to master 40 consistent sound signs or phonograms\\nthan it is to learn three varieties of the 26 Roman letters, 18 irregular\\ndigraphs, and over 550 different ways to map sounds to shapes. When one\\ncounts case distinctions and different fonts, the learning burden for today\\'s\\nchild approaches that experienced by those trying to learn 1,000 Egyptian\\nhieroglyphics or 4,000 Chinese logograms.\\nWhen the task is simplified, beginning readers will experience the motivating\\neffect of success much sooner. They achieve mastery quicker simply because\\nthere is less to learn and memorize. Numerous studies in the mid 1960\\'s\\nshowed that ITA was learned much faster than TO. Since ITA cuts the amount\\nof material a child needs to master by a factor of ten, it seems reasonable\\nthat there should be a corresponding reduction in training time. As might\\nbe expected, much of that gain was lost when the student\\'s had to transition\\nto the traditional orthography. After learning one spelling pattern, students\\nnow had to learn at least four others.\\nFirst grade reading books are far behind the child\\'s speaking ability\\nprimarily because the books have eliminated most of the irregular spellings\\nand postponed the introduction of \"crazy words\" (i.e., words that do not\\nfit the most common spelling patterns). Since half of the most frequently\\nused words are irregular, around 40 have to be introduced very early. Some\\nof the proponents of practical phonemic writing systems advocate retaining\\nthe frequently used irregular words. Memorizing 40 \"crazy words\" is certainly\\nbetter than memorizing 4,000.\\nThe benefits of a more phonemic written code are many but most are primarily\\nbenefits to the new learner. By limiting the number of orthographic options,\\na regularized script becomes much easier to spell and pronounce. In countries\\nwith more phonemic writing systems, all 5th graders can spell any word\\nthey can pronounce and pronounce any word they can spell. By adding new\\ncharacters for digraphs in addition to the regularizing the orthography,\\na new alphabet for English could also be easier to type and quicker to\\nwrite.\\nA spelling reform is not a panacea. Phonemic spelling will not help\\nspeed readers who are attending to word patterns rather than individual\\nsound signs. In general, logograms or whole word signs, can be read faster\\nthan phonograms but the development of this skill requires more time.\\nA dictionary would still have to be used to determine the correct pronunciation\\nof words with regional variations\\n(e.g., *sky is pronounced /skie/ rather than /skah/). New dictionaries\\nmight have to be created to determine the correct spelling for blended\\nvowels and words that could still be written more than one way (e.g., yooz\\n- uez,\\xa0\\xa0 yuur - yoour,\\xa0\\xa0 air - aer).\\nThe consequence of using a written code with a higher level phonemicity\\nor consistency with the alphabetic principle is limited to saving a few\\n100 million man hours and a few billion dollars a year. A regularized English\\nspelling system would provide a quicker avenue to full literacy and would\\nincrease our literacy rates by 25%. In addition, adopting\\na new spelling system for English would make the teaching learning process\\nabout 25% more efficient and make the teaching of \"phonics\" practical.\\nThe way we spell is a cultural convention and an accident of history.\\nTO has been in place since 1700 due to the weight of tradition, not because\\nit makes sense. The first step in breaking the hold that conventional spelling\\nhas on public opinion is to extend the student\\'s horizons of awareness.\\nWe teach traditional English spelling because it is regarded as a mark\\nof an educated person and because it unlocks some of the treasures of the\\npast. We do not teach it because it is right or in any way superior to\\nother spelling schemes. There are plenty of good models for superior orthographies.\\nUnfortunately, none that are in wide use, such as the Finnish or Portuguese\\nscripts, are sufficiently consistent with the most common spelling patterns\\nfound in English.\\nThree of the scripts introduced in this paper, cut spelng and two digraphic\\nscripts, are generally consistent with the most common spelling patterns\\nand have a much more phonemic orthography than TO. They are all good candidates\\nfor an alternative or auxiliary scripts.\\nSo what is the trouble with TO? Its irregular and largely inconsistent\\nwith the alphabetic principle. This makes it difficult to sound out words\\nfrom their spelling and to spell words based on how they are pronounced.\\nThe traditional spelling system makes English difficult to learn and inhibits\\nits diffusion as a world language.\\nNoah Webster summed it up as follows in the introduction to his 1806\\ndictionary: A living language must necessarily suffer gradual changes...The\\nunavoidable consequence of fixing the orthography of a living language\\nis to destroy the use of the alphabet. This effect has already taken place\\nin our own language: letters, the most useful invention that ever blessed\\nmankind, have lost and continue to lose a part of their value, by no longer\\nbeing the representatives of the sounds originally annext to them. The\\ndoctrine of no change is destroying the benefits of the alphabet...\\n\\n\\n\\nNotes\\n1. Zachrisson (1970)\\n2. Laubach, 1966. (Crystal, 1993, charts various spelling\\nsystems in more detail than shown below)\\nPhonemic\\xa0\\xa0 <--..Finnish, Spanish, Italian,\\nPortuguese, .....Icelandic.....French.....English .........Chinese -->\\xa0\\nlogographic.\\n\\n\\n\\n3. For more nonsense spellings, see Bennet Cerf\\'s Out\\non a Limerick\\xa0\\xa0 .\\nto top of page\\n4. A phonemic transcription is not as precise as a phonetic\\ntranscription. The latter includes a variety of markers that can clearly\\nindicate dialects and regional patterns. For an optimized writing tool,\\nall one needs is a consistent orthography that provides a useful guide\\nto pronunciation and spelling.\\n5. According to Beech (1992) Jour. of Gen. Psych.\\n119(2), p. 169f. subjects were able to regain normal reading speeds after\\nthey read about 6,000 words of regularized text. Regularizing orthography\\ninvolves changing 30% of the words. Research indicates that adults regain\\nnormal reading speed after they have read 6,000 words or regularized text.\\n30% were spelling accurately in the new orthography by the end of the session.\\nWriting speed improved but was still slower than with unregularized TO.\\nOrthographic change has more impact on writing speed than on reading speed.\\nReading is unaffected. Writing requires several weeks to adjust.\\n6. Figure 5. lists the number of different\\nways that 18 sounds can be spelled and the different ways that the same\\nletter can be pronounced. Complete charts are available in the books by\\nDewey and Pitman. This one is limited to the 18 sounds for which there\\nis no single Roman letter. The sounds are typically signified by a digraph\\nor letter combination. Unfortunately, in TO they are signified by multiple\\ndigraphs.\\n7. IPA (The International Phonetic Alphabet,\\n1890) uses the same convention, /sh/=S, /ch/=TS\\n8. Soffietti (1955)\\xa0\\xa0 full reference below\\n9. Hanna (1971)\\n10. Phonemes refer to the significant sound categories\\nused by native speakers. A category refers to a collection or group of\\ndissimilar things that are treated as the same. Typically there is a range\\nof sounds that will be mutuallyintelligible and interpretable as a particular\\nphoneme. Pronunciations within this region are called allophones.\\n11. 25% is a number that many educators use which is based\\non an estimated average improvement in reading test scores and the difference\\nin time to mastery between countries with phonemic scripts compared to\\ncountries trying to teach TO. The difference between learning 40 sound\\nsigns and over 400 spelling patterns is over 100%. By this estimate, teaching\\nan alphabetic orthography would be 100 times as efficient as teaching TO.\\n\\n\\nReferences\\xa0\\xa0 Extended\\nBibliography\\nBeech, John R. \"Adaption of writing to orthographic change.\"\\nJ.\\nof General Psychology. 199(2),\\xa0\\n169-179,\\n1992\\nThe\\neffects of spelling change on the adult reader. Spelling\\nProgress Bulletin, 23,\\n7-13.1983\\nBett,\\nSteve T. The\\nAlphabet: Its Origins and Early Development, (unpublished)\\n1996\\nA\\nPictographic Monoline Phonemic Script, (unpublished ms)\\n1996\\nConventional\\nSpelling & Right Writing, Louisiana Middle School\\nJournal,\\n5 (1)\\xa0\\n25-36,\\n1996\\nCan\\nPictographic Cues Make an Augmented Script Easier to Learn & se?.\\xa0(unpublished\\nmanuscript) 1996\\nPictionary\\nHieroglyphics, Louisiana Middle School Journal,\\n3\\n(1) 13-19, 1994\\n\\n\\xa0Alphabets for English. Bridge Spelling Proposals\\n\\xa0 published as a personal view by the Simplified\\nSpelling Socieity, 1996\\n\\nCoulmas, Florian.\\xa0 The Blackwell Encyclopedia of Writing Systems.Oxford:\\nBlackwell,1996\\nCrystal, David (editor) Encyclopedia of the English Language.\\nCambridge\\xa0\\nUniversity\\nPress, 1995\\nDewey,\\nGodfrey. English Spelling: Roadblock to reading. Teachers\\nCollege Press, N.Y., 1971\\nDowning, John A. The ITA Reading Experiment. Scott\\nForesman & Co. Chicago. 1964.\\nEllis, Henry (ca. 1900) referenced by Dewey and Pitman.\\xa0\\nHanna, Paul, et al. Phoneme-Grapheme Correspondence\\nas Cues to Spelling\\xa0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nImprovement.\\nDoc. OE-32008, Washington, D.C., U.S. Government\\nPrinting Office, 1971\\nLaubach, Frank C. Let\\'s Reform Spelling--Why and How. New\\nReaders Press.\\nNew\\nYork, 1966\\nPitman,\\nJames. and John St. John. Alphabets and Reading: The ITA.. Pitman.\\nLondon.\\n1969\\nSoffietti, James P. \"Why Children Fail to Read: A Linguistic Analysis.\"\\xa0\\nThe\\nHavard Educational Review,\\n25, (2) 63-84, Cambridge,\\nMA, Spring, 1955\\xa0\\nThe SSA (Simpler Spelling Association) Fonetic Alfabet. SSA,\\nLake Placid, N.Y., 1959\\nTwain, Mark. A\\nSimplified Alphabet. What\\'s Man. Essay No. XI,\\n1899\\xa0\\xa0\\nIllustrated version\\n\\nHow I would Spell It\\nUpward, Chrisopher. Cut Spelling Handbook. Simplified\\nSpelling Society, 1996\\nYule,\\nValerie. Spelling as a Social Invention. 1994.\\xa0 full text on the\\nWeb.\\nZachrisson, R.E.. Anglic: An International Language. McGrath,\\nCollege Pk. Md., 1970\\n\\n\\n\\nPictographic\\nMonofon Vowels 7 primary vowels, 7 derived blends of the primaries\\n\\nComplete\\nmonofon character grid 14 vowels and 26 consonants\\n\\nMonofon\\nconsonants\\n\\n\\nDr. Bett [pic]\\n[homepage]\\nencourages and welcomes critiques of this paper, the underlying speculations\\nand theories, as well as comments on the general topic of phonology and\\nreferences to related works.\\xa0\\n\\n\\n\\nPrevious page\\n<<<\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n\\nSiteMap\\n\\n\\n\\n\\n\\n\\nweb hosting • domain names • web designonline games • online dating  • long distancedigital cameras  • advertising online\\n\\n\\n\\n\\n\\n',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRestored English Spelling - Links to Alternate Orthographies\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nweb hosting\\ndomain names\\nemail addresses\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHome\\nPage: http://pages.whowhere.com/community/sbett\\n\\xa0\\n\\n\\n\\n\\nIndex\\nto\\nApplied Grapho-Phonology\\nInitial\\nTeaching Alphabets\\nSystematic\\nSpelling Schemes\\nOrthographic\\nReform Proposals\\nRestored\\nEnglish Spelling\\n\\n\\n\\nSteve Bett,\\nPh.D.\\n\\xa0\\n\\n\\nEnglish\\nis only 40% phonemic (i.e., only 40%\\nalphabetic and consistent).\\xa0 This is an affront to logic and makes\\nlearning unnecessarily difficult and frustrating.\\xa0\\nThe consistency of English\\nspelling can be easily improved.\\xa0 There\\nare three popular approaches to the problem of the restoring the alphabet.\\xa0\\nWe say restore because Old English was 90% alphabetic.\\xa0\\n\\n\\nEliminate redundant letters\\n-\\nalmost every letter is redundant and silent in some\\nword\\n\\nEliminate code overlaps\\n- no letter or digraph should have more than one pronunciation.\\n\\nEliminate all inconsistencies\\n- 1-to-1 correspondence between graphemes & phonemes.\\n\\nThe initial teaching alphabet\\n(ITA) eliminated redundant letters and most code overlaps.\\xa0\\nITA was a medium not a method and although it worked with any approach\\nto the teaching of reading and writing, it would have worked better (and\\nbeen easier to study) if it had been associated with a method.\\xa0 ITA\\nwas 200 times easier than the traditional orthography but when used with\\na mix of teaching methods it proved to be only 25% better over a four year\\nperiod.\\xa0 The transition to TO was never specifically taught until\\nthe end of the ITA era (early 1970\\'s).\\xa0 Some who were taught using\\nITA blame the medium on their continuing problems in matching dictionary\\nspellings.\\xa0 Such claims have never been proved.\\xa0 What was proved\\nwas that young children could learn to express themselves in ITA four times\\nas fast as a control group could in TO.\\nEducational fads, particularly\\nthose that add cost and administrative inconvenience, are usually short\\nlived.\\xa0 Such was the case with ITA.\\xa0 No major publisher currently\\nsupports this approach to the teaching of reading, writing, and spelling.\\n\\nPOETAETOE\\n(ITA spelling) (NS - new spelling)\\nITA was based on New Spelling\\n(Ellis, 1932), a notational scheme that always represented\\n\"long\" vowels as ae, ee, ie, oe, ue.\\xa0\\n\"Ie sae nue boi goe tel mee whaat yue see.\"\\xa0 This is consistent\\nand readable but doesn\\'t look much like TO.\\xa0\\n\\nPOATAYTO\\n(RES spelling)\\nThe newest scheme called restored\\nspelling (RES)\\nis more complicated than ITA but closer to TO.\\xa0 In fact it is more\\ntraditional (pre 1100) than traditional orthography which dates from 1755.\\xa0\\nThe difference is that RES spells current pronunciation the way\\nit would have been spelled in Old English.\\xa0 TO contains many silent\\nletters because the spelling reflects an earlier pronunciation.\\xa0 TO\\noften spells an ancient pronunciation: some words are spelled the same\\nas they were in 1100 but we no longer pronounce them that way. [e.g., knight/nyt,\\nenough/enuf]\\n\"TIME\" used to be pronounced\\n\\ntim-uh.\\xa0 \"GIVE\" used to be pronounced giv-uh (Cf: given).\\nIn RES, these words would be spelled TYM and GIV. In ITA, TIEM and GIV.\\nThe \"long\" or free vowels\\nin RES would be represented as ai, ee, y, oa,\\nue in situations where they need to be distinguished from checked\\nvowels, i.e., when followed by a consonant.\\xa0 Otherwise they would\\nbe represented as ay, e, I/y, o, u\\nas in say, me, I/my, silo, guru.\\xa0 Hence:\\xa0\\n\"I say nu boy go tel me wot u se.\"\\xa0 RES looks mor lyk English becaus\\nit employs positiona\\'l speling, just lyk TO.\\nRES is not phonemic since\\nthere is more than one way to spell a sound.\\xa0 It is systematic and\\npredictable:\\xa0 There is only one way to spell a sound in a particular\\nposition in a word.\\xa0 The absence of confusing code overlaps make it\\na viable candidate for a new ITM (initial teaching medium) in the schools.\\xa0\\nIt is a great place to start learning to read and write.\\xa0 No new rules\\nare needed to transition to TO since RES identifies all of the consistent\\nrules in traditional English spelling.\\xa0 The transition is one of adding\\nexceptions to the rules. (see heterographs).\\nTeachers interested in using\\nRES as a new initial teaching medium in the classroom or for a research\\nstudy may contact Dr. Steve Bett for details.\\n\\xa0\\nA dictionary for RES has\\nyet to be completed but there is one for several variants of new spelling\\n(NS, ALC\\nFonetik).\\xa0 An automated\\nconverter is available to convert any passage to ALC Fonetik.\\n\\n\\n\\xa0Alphabetical Listing\\nof Spelling Related Documents\\nTo\\nreturn to the INDEX,\\nclick the browser\\'s BACK button\\n\\xa0\\n\\n\\nac-dictionary\\xa0\\xa0\\xa0\\xa0\\n( Aurally coded dictionary idea )\\nalphabet.html\\xa0\\xa0\\xa0\\n(Alphabetic characters shapes - ancient origins)\\nautbtrspel.html\\xa0\\n(Automated spelling converter for ALC fonetic -BTRSPL)\\nCKS-nut.html\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n(English Phoneme Inventory - Chekt spelng)\\nCKS\\nchktspl.html\\xa0 (Chekt Spelling Table - large file)\\xa0\\ncut spelling\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n(cutting out redundant letters while retaining pattern)\\ndewey\\ndew-add\\xa0\\xa0\\xa0\\xa0\\n(G. Dewey\\'s frequency tables: 451 ways to spell 41 sounds)\\xa0\\nebonics.html\\xa0\\n(Ebonics - Black English, Pidjins,\\nand Creoles)\\nevalalfa\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n(Evaluation & ranking of alternative notations and writing systems)\\ngetizbrg.html\\xa0\\n(Gettysburg Address in Reformed Spelling)\\xa0\\nglobish.html\\xa0\\xa0\\n(Globish-Global English, by M. Gogate)\\nglossdb.html\\xa0\\n(Glossary of Spelling Terms used in SSS forum)\\ngreek.html\\xa0\\xa0\\xa0\\xa0\\n(Grapheme Phoneme Correspondences for Greek) -defective\\nhenu.html\\xa0\\xa0\\xa0\\xa0\\xa0\\n(The Strucutre of Egyptian Hieroglyphics) missing\\ngraphics\\nimptabl.html\\xa0\\n(A Typology of Notational Systems for English)\\ninterspl.html\\xa0\\n(International Spelling for English by Valerie Yule) large file\\xa0\\nlatin-1.html\\xa0\\xa0\\xa0\\n(An augmented romic character set for web documents)\\xa0\\nletrmatrix.html\\xa0\\n(Letter Matrix illustrating alternative notations for English)\\nmap-IPA\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n(Mapping orthographies onto IPA)\\npidjin.html\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n(Pidjins\\nand Creoles by Valerie Yule)\\npublish-it.html\\xa0\\n(How to publish on the Web - intro. to HTML)\\xa0\\nPV-7\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n(An Alphabet for English - Nu Folik & Chekt Spelng) 2,\\n3, 4\\nRES\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n(Restored English Spelling - vowel table)\\nsaundz-eng\\xa0\\xa0\\xa0\\xa0\\n(Graphical representation of English sounds)\\nschwa.html\\xa0\\xa0\\xa0\\xa0\\n(Discussion of the schwa and number of vowels)\\nshaw-pref\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n(Preface to book by George Bernard Shaw)\\nsimpspel.html\\xa0\\n(Simplified Spelling Page, Link to Bibliography)\\nspel-fun\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n(Spelling Humor - TwainSpel - Reform in Stages)\\nspel-links.html\\n(Home page - links to other spelling related pages)\\nspel-link.html\\xa0\\xa0\\n(short version - loads quickly)\\xa0\\nspelling-day.html\\n(Sept. 9, 1999 is Spelling\\nDay & Hangul Day)\\xa0\\nspel-inv\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n(Spelling as a Social Invention by Valerie Yule)\\nspelng.html\\xa0\\xa0\\xa0\\n(The Trouble with Spelling) - History of English Writing\\nsyllabics\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n(Vowels for Bosnia -\\xa0 Humerous misunderstanding\\nof syllabics)\\ntwian-spl.html\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n(Mark Twain on simplified spelling)\\xa0\\ntrain-cadmus.html\\xa0\\xa0\\xa0\\n(Mark Twain on reform of Egyptian - Cadmus)\\xa0\\nvowels.html\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n(English Vowels - click for vowel sound) no\\nsound files\\nwebster.html\\xa0\\xa0\\xa0\\xa0\\n(Noah Webster - short biography)\\nwrld-ortho.html\\xa0\\n(world orthography - broad romic notation)\\xa0\\n\\xa0\\n\\n\\nhttp://pages.whowhere.com/cgi-bin/bedit\\xa0\\xa0\\xa0\\xa0\\xa0\\nemail-sss\\nhttp://www.delphi.com/spelreform\\ngo to bottom of page, select forum\\n\\xa0\\n\\n\\n\\n\\nTo return to this page,\\nclick on the browser\\'s back button\\nMost of pages listed above\\nare under construction. The main problem is getting the time to upload\\xa0\\nall of the graphics\\xa0 into the images file.\\xa0 On this kind of server,\\nall the links must be absolute - i.e., the full URL.\\xa0 Relataive links\\ndo not work.\\nA new 20 Mb website is being\\ndeveloped at http://victorian.fortunecity.com/vangoh/555\\nThe advantage here is that\\nfiles can be uploaded via ftp - making it much faster.\\xa0 Code name\\nfor new site: rapidrytr.\\nA list of sites sites dealing\\nwith spelling reform can be found at\\xa0 http://www.delphi.com/spelreform\\n.\\xa0\\nTo join a discussion group on\\nsimplified spelling, contact anyone on the list at http://www.nik.dircon.co.uk/spelling/email_d.html\\n\\nYour comments and critiques\\nare welcomed.\\n Contact Steve\\nBett [sbett@mailcity.com]\\n\\n\\n http://www.delphi.com/spelreform\\nThe\\nSpelling Reform Forum and Archive\\n(go to bottom of\\npage for link)\\nspel-link.html\\xa0\\nWhere you should go for a long list of links to web documents on spelling\\n\\xa0\\n\\n\\n http://pages.whowhere.com/community/sbett/index.html\\n- you are here (top)\\n\\xa0\\n John\\nReilly\\'s interesting page on positional spelling and imaginary (what\\nif) history\\n\\n\\n\\nweb hosting • domain names • web designonline games • online dating  • long distancedigital cameras  • advertising online\\n\\n\\n\\n\\n\\n',\n",
       " ' Context and Page Analysis for Improved Web Search Steve Lawrence and C. Lee Giles NEC Research Institute Download paper: PS.Z PS.gz PS PDF BibTeX Publications page NEC Research Institute has developed a metasearch engine that improves the efficiency of Web searches by downloading and analyzing each document and then displaying results that show the query terms in context. Several popular and useful search engines such as AltaVista, Excite, HotBot, Infoseek, Lycos, and Northern Light attempt to maintain full-text indexes of the World Wide Web. However, relying on a single standard search engine has limitations. The standard search engines have limited coverage, 1,2 outdated databases, and are sometimes unavailable due to problems with the network or the engine itself. The precision of standard engine results can also vary because they generally focus on handling queries quickly and use relatively simple ranking schemes. 3 Rankings can be further muddled by keyword spamming to increase a page\\'s rank order. Often, the relevance of a particular page is obvious only after loading it and finding the query terms. Metasearch engines, such as MetaCrawler and SavvySearch, attempt to contend with the problem of limited coverage by submitting queries to several standard search engines at once. 4,5 The primary advantages of metasearch engines are that they combine the results of several search engines and present a consistent user interface. 5 However, most metasearch engines rely on the documents and summaries returned by standard search engines and so inherit their limited precision and vulnerability to keyword spamming. We developed the NEC Research Institute (NECI) metasearch engine [now called Inquirus] to improve the efficiency and precision of Web search by downloading and analyzing each document and then displaying results that show the query terms in context. This helps users more readily determine if the document is relevant without having to download each page. This technique is simple, yet it can be very effective, particularly when dealing with the Web\\'s large, diverse, and poorly organized database. Results from the NECI engine are returned progressively after each page is downloaded and analyzed, rather than after all pages are downloaded. Pages are downloaded in parallel and the first result is typically displayed in less time than a standard search engine takes to display its response. The NECI metasearch engine is currently in use by employees of the NEC Research Institute. This article describes its features, implementation, and performance. A recent study by Anastasios Tombros verified the advantages of summaries incorporating query term context. 6 His study found that users working with query-sensitive summaries found relevant documents faster and performed relevance judgments more accurately and rapidly than users working with an abstract or query-insensitive document summary. Query-sensitive summaries also greatly reduced the need for users to access the full text of documents. THE NECI METASEARCH ENGINE Figure 1 shows a simplified control flow diagram of the NECI metasearch engine, which consists of two main parts: the metasearch code and a parallel page retrieval daemon. The page retrieval engine is relatively simple, but does incorporate features such as queuing requests, load balancing from multiple search processes, and delaying requests to the same site to prevent overloading a site. Figure 1: Simplified control flow of the metasearch engine. Figure 2 shows the main search form for the NECI metasearch engine. Users can choose which search engines to run, how many hits to retrieve, the amount of context to display (measured in number of characters), and so on. The engine supports all common search formats, including Boolean syntax. As with many other metasearch engines, the NECI metasearch engine dynamically modifies queries to match each search engine\\'s query syntax. Figure 2: Search form for the NECI metasearch engine. Users can control the amount of text the NECI engine displays by specifying the number of characters it will show on either side of the query terms. To improve readability, the engine omits most non-alphanumeric characters and partial words at the beginning and end of the specified character count. At one point, we sought to improve context display by extracting logical sentences rather than a fixed number of characters. However, in general, users did not find this sentence-based method superior because including full sentences increased the screen space needed by each summary without significantly improving users ability to determine relevance. Because the NECI engine returns results progressively as it downloads and analyzes each page, the results are not necessarily displayed in the order listed by the individual search engines, but the order is approximately the same. Perhaps because Web search engines are not good at relevance ranking to begin with, this difference in document ranking was not a problem for users. Figure 3 shows a sample response of the NECI metasearch engine for the query \"digital watermark\". The bar at the top lets users switch between views of the search results; below it are links to the individual engine results. The tip that follows might be query sensitive, such as providing specific query format suggestions when the query looks like a proper name. Figure 3: Sample response of the NECI metasearch engine for the query \"digital watermark.\" The shaded bars to the left of the document titles indicate how close query terms are to each other in the document. With a single query term, the bar shading indicates how close the term is to the top of the document. The information to the right of the document title shows which engine found the document and the document\\'s age, for example, in the first listing, A refers to AltaVista, n/a indicates that the document\\'s age is not available. Specific Expressive Forms Information on the Web is often duplicated and expressed in a variety of forms. If all information was (correctly) expressed in all possible ways, precise information retrieval would be simple: A search for any one particular way of expressing the information would succeed. The NECI engine recognizes and transforms certain queries submitted in the form of a question into queries phrased in the form of an answer - specific expressive forms (SEFs). For example, the query \"What does NASDAQ stand for?\" is transformed into the query \"NASDAQ stands for\" \"NASDAQ is an abbreviation\" \"NASDAQ means\". Clearly the information may be expressed in forms other than these, but if the information exists in just one of these forms, it is more likely to satisfy the query. The technique thus trades recall for precision. Our informal experiments indicate that using SEFs is effective for certain retrieval tasks on the Web. Figure 4 shows the NECI engine\\'s results for the query \"What does NASDAQ stand for?\" The answer to the query is contained in the local context displayed for four out of the first five pages. In contrast, the standard search engines we queried did not have the answer in any of the documents listed on the first page, even for engines that list support for natural language queries. Figure 4: NECI metasearch engine response for the query \"What does NASDAQ stand for?\" As the amount of easily accessible information increases, so too will the viability of the SEF technique. An extension to it that we have not yet implemented is to define an order over the various SEFs. For example, \"x stands for\" might be more likely to find the answer than \"x means\". If none of the SEFs are found, the engine could fall back to a standard query. Currently, the NECI metasearch engine uses the SEF technique for a number of queries. For example, the engine recognizes \"What [is|are] x?\", \"What [causes|creates|produces] x?\", \"What does x [stand for|mean]?\", and \"[Why|how] [is|are] (a|the) x y?\" As examples of the transformations, \"What does x [stand for|mean]?\" is converted to \"x stands for\", \"x is an abbreviation\", and \"x means\" ; and \"What [causes|creates|produces] x?\" is converted to \"x is caused\", \"x is created\", \"causes x\", \"produces x\", \"makes x\", and \"creates x\". We created the SEF transformations manually, an interesting area of research would be to learn SEFs from implicit or explicit feedback. The SEF technique often relies on the engine\\'s ability to search for a phrase containing what are typically stop words. These words are almost universally filtered out by traditional information retrieval systems. Web search engines vary in their use of stop words, and we have found it necessary to filter out certain phrases on an engine-by-engine basis to prevent the engines from returning many pages that do not contain the phrases. Results Ranking Steve Kirsch has proposed a ranking scheme whereby the underlying search engines are modified to return additional information, such as how many times a term occurs in each document and the entire database. 7 With the NECI engine, this step is unnecessary as it downloads and analyzes the actual pages. It can therefore apply a uniform ranking measure to documents returned by different engines. Currently, the engine displays documents in descending order of query-term occurrence. If none of the first few pages contain all terms, the engine displays documents with the maximum number of query terms found so far. Once all pages are downloaded, the engine relists documents according to a simple relevance measure. This measure considers the number of query terms in the document, the proximity between query terms, and term frequency (inverse document frequency can also be useful 8 ). We use the following equation for pages containing more than one of the query terms; when only one query term is found we currently use the term\\'s distance from the start of the page. where N p is the number of query terms that appear in the document (each term is counted only once); N t is the total number of query terms in the document; d (i, j) is the minimum distance between the i th and j th query terms (currently measured in number of characters); c 1 is a constant that controls the overall magnitude of R , which is the document\\'s relevance score; c 2 is a constant that specifies the maximum useful distance between query terms; and c 3 is a constant that specifies term-frequency importance (currently c 1 = 100, c 2 = 5000, and c 3 = 10 c 1 ). This ranking criterion is particularly useful for Web searches. Because the Web database is so large and diverse, searching for multiple terms can return documents that use the terms in unrelated sections, such as terms that exist in different bookmarks on a bookmarks page. After all pages have been retrieved, the engine displays the top 30 pages ranked by term proximity. As Figure 5 shows, the engine then displays additional information: duplicate context strings, results clustered by site, documents with fewer or no search terms, and pages that could not be downloaded. It also displays a summary table with results for each engine queried and suggestions for subsequent queries, as the sidebar Improving User Queries describes. Figure 5: Additional information, including duplicate context strings, results clustered by site, and pages that could not be downloaded, are displayed after the query is complete. The NECI engine downloads and analyzes the actual pages, so it can apply a uniform ranking measure to documents returned by different engines. --> These added features are important. Where other metasearch engines categorize pages as duplicate if the normalized URLs are identical, the NECI metasearch engine considers pages duplicate if the relevant context strings are identical. Thus, even duplicate pages with different headers and footers will be detected, such as when a single mailing list message is archived in several places. Knowing which pages do not match the query or are not available is also important. Different engines use different relevance techniques; if one engine returns poor relevance results, it can lead to poor overall results from standard metasearch engines. Other metasearch services also provide dead link detection, but this feature is typically turned off by default or does not return results until all pages are checked. Document Display Figure 6 shows a sample document from the digital watermark search. The links at the top jump to the first occurrence of the query terms in the document, and indicate the number of occurrences. Each query term within the text also links to the next use of the term. Such linking and highlighting helps users quickly identify page relevance. The NECI engine can also track query results and page contents, automatically informing users when new matching documents are found or when a given page has been modified (Track page). Figure 6: Sample page view for the NECI metasearch engine. Query terms are highlighted; links take users to the first occurrence of the query term. Currently, the NECI engine uses two forms of caching. The engine caches all downloaded pages for a limited time period, and query terms and links are added on demand. The engine also caches the top 20 relevance-ranked results from each query. If a user repeats the query, these pages are the first displayed if they still exist and contain the query terms. IMPLEMENTATION The NECI metasearch engine is currently implemented for server operation at NEC Research Institute, where it serves about 100 users. A client implementation could also be created, which would improve scalability. The disadvantages are increased processing and memory requirements, and the need to update all clients when modifications are made to the metasearch engine. A client implementation would also decrease the caching benefits. Resource Requirements The NECI search engine uses roughly an order of magnitude more bandwidth than other search engines. These bandwidth requirements could limit the number of users that can simultaneously use a server-based implementation. However, these requirements are not as great as those required by other Web developments, such as the increasing use of audio and video, and bandwidth and access times on the Internet continue to improve. 9 Also, the engine will not necessarily need to analyze more pages per query as the Web grows (though precise queries will become more important). The prototype engine runs on a Pentium Pro 200 PC, is written in Perl, and is not optimized for efficiency. When only a few queries are executed at a time using our prototype engine, the analysis does not typically slow the response (network response time is the limiting factor). IMPROVING USER QUERIES Our analysis of 9,000 queries during the second half of 1997 showed that most queries contained only a few terms (Figure A shows the total distribution). Because simple queries often generate thousands of matching documents and poor precision in the results, we built the NECI engine to suggest query improvements to the user. For example: - for queries that do not specify phrases, the engine looks for combinations of the query terms appearing as phrases and suggests the use of a phrase if a threshold is exceeded; - for multi-term queries where no terms are required, the engine suggests the use of + or and to require terms; and - the engine stems the query terms and searches the pages for morphological variants. If any are found it suggests them as terms that can be added to the query. The first two suggestions are aimed at improving precision; the third, at improving recall. Suggesting that users introduce phrases and term requirements may seem counterintuitive from a traditional information retrieval viewpoint, as these suggestions can exclude many relevant documents. However, the Web poses different information retrieval problems from those posed by traditional databases, because it is larger and more diverse, with a lower signal-to-noise ratio. As a result, in Web searches it is often useful to trade recall (the number of documents returned) for improved precision. Figure A: The distribution of the number of terms contained in queries. Performance We analyzed the response time of the following six search engines: AltaVista, Excite, HotBot, Infoseek, Lycos, and Northern Light. The median response time from 3,000 queries to these engines during November-December 1997 was 1.9 seconds. However, if queries are made to all of the engines simultaneously, then the median time for the first engine to respond was 0.7 seconds. A similar advantage is gained by downloading the Web pages corresponding to the hits in parallel, resulting in a median time for the NECI engine to receive the first page being 1.3 seconds. On average, the parallel architecture of the NECI engine allows it to find, download, and analyze the first page faster than the standard search engines can respond, even though the standard engines do not download and analyze the current contents of the pages. In May 1998 we analyzed the time for the engine to display the first five and first 10 relevant results from 200 queries. The median time for the first five relevant results was 2.7 seconds, and the median time for the first 10 relevant results was 3.2 seconds (these figures do not include queries that did not return the target number of results). CONCLUSION The NECI metasearch engine demonstrates that real-time downloading and analysis of the pages that match a query is possible. In fact, by calling the Web search engines and downloading Web pages in parallel, the NECI metasearch engine can, on average, display the first result quicker than a standard search engine. Like other metasearch engines and various Web tools, the NECI metasearch engine relies on the underlying search engines for important and valuable services. Wide use of this or any metasearch engine requires an amiable arrangement with the underlying search engines; such arrangements might include passing through ads or micro-payment systems. There are numerous areas for future research. Because the NECI engine collects the full text of matching documents, it is a good test bed for information retrieval research. Areas we are working on include clustering, query expansion, and relevance feedback. Because the query-sensitive summaries let users better assess relevance without having to view pages, implicit feedback should be more successful and might be useful for improved relevance measures, automatic relevance feedback, and learning specific expressive forms. Other areas we are looking at include page classification and extending the specific-expressive-forms search technique. ACKNOWLEDGMENTS We thank Eric Baum, Kurt Bollacker, Adam Grove, Bill Horne, Bob Krovetz, Roy Lipski, John Oliensis, Steve Omohundro, Maximilian Ott, James Philbin, Majd Sakr, Lance Williams, the employees of NECI, and the anonymous reviewers for useful comments and suggestions. REFERENCES 1. E. Selberg and O. Etzioni, Multi-Service Search and Comparison Using the MetaCrawler, Proc. 1995 WWW Conf., 1995; available online at http://draz.cs.washington.edu/papers/www4/html/Overview.html . 2. S. Lawrence and C.L. Giles, Searching the World Wide Web, Science, Vol. 280, No. 5360, 1998, p. 98. 3. D. van Eylen, AltaVista Ranking of Query Results, available at http://www.ping.be/dirk_van_eylen/avrank.html , 1998. 4. D. Dreilinger and A. Howe, An Information Gathering Agent for Querying Web Search Engines, Tech. Report CS-96-111, Computer Science Dept., Colorado State Univ., Fort Collins, Colo., 1996. 5. E. Selberg and O. Etzioni, The MetaCrawler Architecture for Resource Aggregation on the Web, IEEE Expert, Jan.-Feb. 1997, pp. 11-14; also available at http://www.cs.washington.edu/homes/speed/papers/ieee/ieee-metacrawler.ps . 6. A. Tombros, Reflecting User Information Needs Through Query Biased Summaries, doctoral thesis, Dept. Computer Science, Univ. of Glasgow, 1997. 7. S.T. Kirsch, Document Retrieval over Networks Wherein Ranking and Relevance Scores are Computed at the Client for Multiple Database Documents, US Patent #5,659,732, US Patent Office, 1997. 8. G. Salton, Automatic Text Processing: The Transformation, Analysis and Retrieval of Information by Computer, Addison Wesley Longman, Reading, Mass., 1989. 9. 1997 Internet Performance Wrap-Up How Well Did It Perform? Keynote Systems Shows All, Business Wire, Jan. 9, 1998; available at http://www.keynote.com/measures/business/business40.html . Steve Lawrence is a scientist at the NEC Research Institute in Princeton, N.J. His research interests include information retrieval, machine learning, neural networks, face recognition, speech recognition, time series prediction, and natural language. His awards include an NEC Research Institute excellence award, ATERB and APRA priority scholarships, a QUT university medal and award for excellence, QEC and Telecom Australia Engineering prizes, and three successive prizes in the annual Australian Mathematics Competition. Lawrence received a BSc in computing and a BEng in electronic systems from the Queensland University of Technology, Australia, and a PhD in electrical and computer engineering from the University of Queensland, Australia. C. Lee Giles is a senior research scientist in computer science at NEC Research Institute, Princeton, N.J., and an adjunct professor at the Institute for Advanced Computer Studies at the University of Maryland, College Park. His research interests are in novel applications of neural computing, machine learning, agents, and AI in all areas of computing. He is on the editorial boards of IEEE Intelligent Systems, IEEE Transactions on Knowledge and Data Engineering, IEEE Transactions on Neural Networks, J. of Computational Intelligence in Finance, J. of Parallel and Distributed Computing, Neural Networks, Neural Computation, and Applied Optics. Giles is a Fellow of the IEEE and a member of AAAI, ACM, the International Neural Network Society, the Optical Society of America, and the Center for Discrete Mathematics and Theoretical Computer Science, Rutgers University. Contact Lawrence and Giles at NEC Research Institute, 4 Independence Way, Princeton, NJ 08540; {lawrence,giles}@research.nj.nec.com. RELATED WORK The idea of querying and collating results from multiple databases is not new. Companies such as PLS ( http://www.pls.com/ ), Lexis-Nexis ( http://www.lexis-nexis.com/ ), Dialog ( http://www.dialog.com/ ), and Verity ( http://www.verity.com ) long ago created systems that integrated search results from multiple heterogeneous databases. 1 There are many existing Web metasearch services, including MetaCrawler, SavvySearch, Inference Find, Fusion, ProFusion, Highway 61, Mamma, Quarterdeck WebCompass, Metabot, Symantec Internet FastFind, and WebSeeker (for a quick review of metasearch engines, see Notess 2 ). Work in the area of collection fusion is reported in the Text Retrieval Conference (TREC) and the Special Interest Group for Information Retrieval (SIGIR) conference proceedings. Several other researchers have also used relevance measures including term proximity. 3,4 Research search engines that promise improved results ranking include Laser 5 ( http://laser.cs.cmu.edu/ ) and Google 6 ( http://www.google.com/ ). These engines use the structure of HTML pages and hyperlink information to help determine page relevancy. For example, Google uses the text in links to a particular page as descriptors of that page (links often contain better descriptions of the page than the pages themselves). Google also uses a ranking algorithm called PageRank, which bases rankings on analysis of the number of pages pointing to each page. Although most of the benefits of metasearch apply to these improved search engines, displaying query term context may become less important for determining page relevancy as results rankings improve. REFERENCES 1. E. Selberg and O. Etzioni, Multi-Service Search and Comparison Using the MetaCrawler, Proc. 1995 WWW Conf., 1995; available online at http://draz.cs.washington.edu/papers/www4/html/Overview.html . 2. G.R. Notess, Internet Onesearch With the Mega Search Engines, Online, Vol. 20, No. 6, 1996, pp. 36-39. 3. E. Keen, Term Position Ranking: Some New Test Results, Proc. 15th International ACM SIGIR Conf. Research and Development in Information Retrieval, 1992, pp. 66- 76; available online at http://www.acm.org/pubs/citations/proceedings/ir/133160/p66-keen/ . 4. D. Hawking and P. Thistlewaite, Proximity Operators So Near and Yet So Far, Proc. Fourth Text Retrieval Conf., D.K. Harman, ed., 1995; available online at http://web.soi.city.ac.uk/~andym/PADRE/trec4.ps.Z . 5. J. Boyan, D. Freitag, and T. Joachims, A Machine-Learning Architecture for Optimizing Web Search Engines, Proc. AAAI Workshop Internet-Based Information Systems, 1996; available online at http://www.lb.cs.cmu.edu/afs/cs/project/reinforcement/papers/boyan.laser.ps . 6. S. Brin and L. Page, The Anatomy of a Large-Scale Hypertextual Web Search Engine, Proc. 1998 WWW Conf., 1998; available online at http://google.stanford.edu/~backrub/google.html . \\n\\t\\n\\n-->\\nContext and Page Analysis for Improved Web Search [Steve Lawrence and C. Lee Giles; NEC Research Institute]\\n\\n\\n\\n\\nIEEE Internet Computing, Volume 2, Number 4, pp. 38-46, 1998. Copyright © IEEE\\n\\nContext and Page Analysis for Improved Web Search\\nSteve Lawrence and C. Lee Giles\\nNEC Research Institute\\n\\nDownload paper: PS.Z\\xa0\\nPS.gz\\xa0\\nPS\\xa0\\nPDF\\xa0\\nBibTeX\\xa0\\nPublications page\\n\\n\\nNEC Research Institute has developed a metasearch engine that\\nimproves the efficiency of Web searches by downloading and analyzing\\neach document and then displaying results that show the query terms in\\ncontext.\\nSeveral popular and useful search engines such as\\nAltaVista, Excite, HotBot, Infoseek, Lycos, and Northern Light attempt\\nto maintain full-text indexes of the World Wide Web. However, relying\\non a single standard search engine has limitations. The standard\\nsearch engines have limited coverage,1,2 outdated\\ndatabases, and are sometimes unavailable due to problems with the\\nnetwork or the engine itself. The precision of standard engine results\\ncan also vary because they generally focus on handling queries quickly\\nand use relatively simple ranking schemes.3 Rankings can be further\\nmuddled by keyword spamming to increase a page\\'s rank order. Often,\\nthe relevance of a particular page is obvious only after loading it\\nand finding the query terms.\\nMetasearch engines, such as MetaCrawler and\\nSavvySearch, attempt to contend with the problem of limited coverage\\nby submitting queries to several standard search engines at\\nonce.4,5 The primary advantages of metasearch engines are\\nthat they combine the results of several search engines and present a\\nconsistent user interface.5 However, most metasearch\\nengines rely on the documents and summaries returned by standard\\nsearch engines and so inherit their limited precision and\\nvulnerability to keyword spamming.\\nWe developed the NEC Research Institute (NECI)\\nmetasearch engine [now called Inquirus] to improve the\\nefficiency and precision of Web search by downloading and analyzing\\neach document and then displaying results that show the query terms in\\ncontext. This helps users more readily determine if the document is\\nrelevant without having to download each page. This technique is\\nsimple, yet it can be very effective, particularly when dealing with\\nthe Web\\'s large, diverse, and poorly organized database. Results from\\nthe NECI engine are returned progressively after each page is\\ndownloaded and analyzed, rather than after all pages are\\ndownloaded. Pages are downloaded in parallel and the first result is\\ntypically displayed in less time than a standard search engine takes\\nto display its response.\\nThe NECI metasearch engine is currently in use by\\nemployees of the NEC Research Institute. This article describes its\\nfeatures, implementation, and performance.\\nA recent study by Anastasios Tombros verified the\\nadvantages of summaries incorporating query term context.6\\nHis study found that users working with query-sensitive summaries\\nfound relevant documents faster and performed relevance judgments more\\naccurately and rapidly than users working with an abstract or\\nquery-insensitive document summary. Query-sensitive summaries also\\ngreatly reduced the need for users to access the full text of\\ndocuments.\\nTHE NECI METASEARCH ENGINE\\nFigure 1 shows a simplified control flow diagram of the NECI\\nmetasearch engine, which consists of two main parts: the metasearch\\ncode and a parallel page retrieval daemon. The page retrieval engine\\nis relatively simple, but does incorporate features such as queuing\\nrequests, load balancing from multiple search processes, and delaying\\nrequests to the same site to prevent overloading a site.\\n\\nFigure 1: Simplified control flow of the metasearch engine.\\nFigure 2 shows the main search form for the NECI metasearch\\nengine. Users can choose which search engines to run, how many hits to\\nretrieve, the amount of context to display (measured in number of\\ncharacters), and so on. The engine supports all common search\\nformats, including Boolean syntax. As with many other metasearch\\nengines, the NECI metasearch engine dynamically modifies queries to\\nmatch each search engine\\'s query syntax.\\n\\nFigure 2: Search form for the NECI metasearch engine.\\nUsers can control the amount of text the NECI engine displays by\\nspecifying the number of characters it will show on either side of\\nthe query terms. To improve readability, the engine omits most\\nnon-alphanumeric characters and partial words at the beginning and end\\nof the specified character count. At one point, we sought to improve\\ncontext display by extracting logical sentences rather than a fixed\\nnumber of characters. However, in general, users did not find this\\nsentence-based method superior because including full sentences\\nincreased the screen space needed by each summary without\\nsignificantly improving users ability to determine relevance.\\nBecause the NECI engine returns results progressively as it downloads\\nand analyzes each page, the results are not necessarily displayed in\\nthe order listed by the individual search engines, but the order is\\napproximately the same. Perhaps because Web search engines are not\\ngood at relevance ranking to begin with, this difference in document\\nranking was not a problem for users.\\nFigure 3 shows a sample response of the NECI\\nmetasearch engine for the query \"digital watermark\". The bar at the\\ntop lets users switch between views of the search results; below it\\nare links to the individual engine results. The tip that follows might\\nbe query sensitive, such as providing specific query format\\nsuggestions when the query looks like a proper name.\\n\\nFigure 3: Sample response of the NECI metasearch engine for the query \"digital watermark.\"\\nThe shaded bars to the left of the document titles\\nindicate how close query terms are to each other in the document. With\\na single query term, the bar shading indicates how close the term is\\nto the top of the document. The information to the right of the\\ndocument title shows which engine found the document and the\\ndocument\\'s age, for example, in the first listing, A refers to\\nAltaVista, n/a indicates that the document\\'s age is not available.\\nSpecific Expressive Forms\\nInformation on the Web is often duplicated and\\nexpressed in a variety of forms. If all information was (correctly)\\nexpressed in all possible ways, precise information retrieval would be\\nsimple: A search for any one particular way of expressing the\\ninformation would succeed.\\nThe NECI engine recognizes and transforms certain\\nqueries submitted in the form of a question into queries phrased in\\nthe form of an answer - specific expressive forms (SEFs). For\\nexample, the query \"What does NASDAQ stand for?\" is\\ntransformed into the query \"NASDAQ stands for\" \"NASDAQ\\nis an abbreviation\" \"NASDAQ means\".  Clearly the\\ninformation may be expressed in forms other than these, but if the\\ninformation exists in just one of these forms, it is more likely to\\nsatisfy the query. The technique thus trades recall for precision.\\nOur informal experiments indicate that using SEFs\\nis effective for certain retrieval tasks on the Web. Figure 4 shows\\nthe NECI engine\\'s results for the query \"What does NASDAQ stand\\nfor?\" The answer to the query is contained in the local context\\ndisplayed for four out of the first five pages. In contrast, the\\nstandard search engines we queried did not have the answer in any of\\nthe documents listed on the first page, even for engines that list\\nsupport for natural language queries.\\n\\nFigure 4: NECI metasearch engine response for the query \"What does NASDAQ stand for?\"\\nAs the amount of easily accessible information\\nincreases, so too will the viability of the SEF technique. An\\nextension to it that we have not yet implemented is to define an order\\nover the various SEFs. For example, \"x stands for\" might be\\nmore likely to find the answer than \"x means\".  If none of\\nthe SEFs are found, the engine could fall back to a standard\\nquery.\\nCurrently, the NECI metasearch engine uses the SEF\\ntechnique for a number of queries. For example, the engine recognizes\\n\"What [is|are] x?\", \"What [causes|creates|produces]\\nx?\", \"What does x [stand for|mean]?\", and\\n\"[Why|how] [is|are] (a|the) x y?\" As examples of the\\ntransformations, \"What does x [stand for|mean]?\" is\\nconverted to \"x stands for\", \"x is an\\nabbreviation\", and \"x means\" ; and \"What\\n[causes|creates|produces] x?\" is converted to \"x is\\ncaused\", \"x is created\", \"causes x\",\\n\"produces x\", \"makes x\", and \"creates\\nx\".  We created the SEF transformations manually, an interesting\\narea of research would be to learn SEFs from implicit or explicit\\nfeedback.\\nThe SEF technique often relies on the engine\\'s\\nability to search for a phrase containing what are typically stop\\nwords. These words are almost universally filtered out by traditional\\ninformation retrieval systems. Web search engines vary in their use of\\nstop words, and we have found it necessary to filter out certain\\nphrases on an engine-by-engine basis to prevent the engines from\\nreturning many pages that do not contain the phrases.\\nResults Ranking\\nSteve Kirsch has proposed a ranking scheme whereby the underlying\\nsearch engines are modified to return additional information, such as\\nhow many times a term occurs in each document and the entire\\ndatabase.7 With the NECI engine, this step is unnecessary as it\\ndownloads and analyzes the actual pages. It can therefore apply a\\nuniform ranking measure to documents returned by different\\nengines. Currently, the engine displays documents in descending order\\nof query-term occurrence. If none of the first few pages contain all\\nterms, the engine displays documents with the maximum number of query\\nterms found so far.\\n\\nOnce all pages are downloaded, the engine relists\\ndocuments according to a simple relevance measure. This measure\\nconsiders the number of query terms in the document, the proximity\\nbetween query terms, and term frequency (inverse document frequency\\ncan also be useful8). We use the following equation for\\npages containing more than one of the query terms; when only one query\\nterm is found we currently use the term\\'s distance from the start of\\nthe page.\\n\\nwhere Np is the number of query terms that appear in\\nthe document (each term is counted only once); Nt is\\nthe total number of query terms in the document; d (i, j) is\\nthe minimum distance between the ith and jth query terms\\n(currently measured in number of characters); c1 is\\na constant that controls the overall magnitude of R, which is\\nthe document\\'s relevance score; c2 is a constant\\nthat specifies the maximum useful distance between query terms; and\\nc3 is a constant that specifies term-frequency\\nimportance (currently c1 = 100, c2\\n= 5000, and c3 = 10 c1).\\nThis ranking criterion is particularly useful for Web\\nsearches. Because the Web database is so large and diverse, searching\\nfor multiple terms can return documents that use the terms in\\nunrelated sections, such as terms that exist in different bookmarks on\\na bookmarks page.\\nAfter all pages have been retrieved, the engine displays the top 30\\npages ranked by term proximity. As Figure 5 shows, the engine then\\ndisplays additional information: duplicate context strings, results\\nclustered by site, documents with fewer or no search terms, and pages\\nthat could not be downloaded. It also displays a summary table with\\nresults for each engine queried and suggestions for subsequent\\nqueries, as the sidebar Improving User Queries describes.\\n\\nFigure 5: Additional information, including duplicate context strings, results clustered by site, and pages that\\ncould not be downloaded, are displayed after the query is complete.\\n\\nThese added features are important. Where other metasearch engines\\ncategorize pages as duplicate if the normalized URLs are identical,\\nthe NECI metasearch engine considers pages duplicate if the relevant\\ncontext strings are identical. Thus, even duplicate pages with\\ndifferent headers and footers will be detected, such as when a single\\nmailing list message is archived in several places. Knowing which\\npages do not match the query or are not available is also\\nimportant. Different engines use different relevance techniques; if\\none engine returns poor relevance results, it can lead to poor\\noverall results from standard metasearch engines. Other metasearch\\nservices also provide dead link detection, but this feature is\\ntypically turned off by default or does not return results until all\\npages are checked.\\nDocument Display\\nFigure 6 shows a sample document from the digital\\nwatermark search. The links at the top jump to the first occurrence of\\nthe query terms in the document, and indicate the number of\\noccurrences. Each query term within the text also links to the next\\nuse of the term. Such linking and highlighting helps users quickly\\nidentify page relevance. The NECI engine can also track query results\\nand page contents, automatically informing users when new matching\\ndocuments are found or when a given page has been modified (Track\\npage).\\n\\nFigure 6: Sample page view for the NECI metasearch engine. Query terms are highlighted; links take users to the first\\noccurrence of the query term.\\nCurrently, the NECI engine uses two forms of caching. The engine\\ncaches all downloaded pages for a limited time period, and query terms\\nand links are added on demand. The engine also caches the top 20\\nrelevance-ranked results from each query. If a user repeats the query,\\nthese pages are the first displayed if they still exist and contain\\nthe query terms.\\nIMPLEMENTATION\\nThe NECI metasearch engine is currently implemented for server\\noperation at NEC Research Institute, where it serves about 100\\nusers. A client implementation could also be created, which would\\nimprove scalability. The disadvantages are increased processing and\\nmemory requirements, and the need to update all clients when\\nmodifications are made to the metasearch engine. A client\\nimplementation would also decrease the caching benefits.\\nResource Requirements\\nThe NECI search engine uses roughly an order of magnitude more\\nbandwidth than other search engines. These bandwidth requirements\\ncould limit the number of users that can simultaneously use a\\nserver-based implementation.\\nHowever, these requirements are not as great as those required by\\nother Web developments, such as the increasing use of audio and video,\\nand bandwidth and access times on the Internet continue to improve.9\\nAlso, the engine will not necessarily need to analyze more pages per\\nquery as the Web grows (though precise queries will become more\\nimportant).\\nThe prototype engine runs on a Pentium Pro 200 PC, is written in Perl,\\nand is not optimized for efficiency. When only a few queries are\\nexecuted at a time using our prototype engine, the analysis does not\\ntypically slow the response (network response time is the limiting\\nfactor).\\nIMPROVING USER QUERIES\\nOur analysis of 9,000 queries during the second half of 1997 showed\\nthat most queries contained only a few terms (Figure A shows the total\\ndistribution). Because simple queries often generate thousands of\\nmatching documents and poor precision in the results, we built the\\nNECI engine to suggest query improvements to the user. For example:\\n\\n- for queries that do not specify phrases, the engine looks for\\ncombinations of the query terms appearing as phrases and suggests the\\nuse of a phrase if a threshold is exceeded;\\n- for multi-term queries where no terms are required, the engine suggests the use of + or and\\nto require terms; and\\n- the engine stems the query terms and searches\\nthe pages for morphological variants. If any are found it suggests\\nthem as terms that can be added to the query.\\n\\nThe first two suggestions are aimed at improving precision; the third,\\nat improving recall.\\nSuggesting that users introduce phrases and term requirements\\nmay seem counterintuitive from a traditional information retrieval\\nviewpoint, as these suggestions can exclude many relevant\\ndocuments. However, the Web poses different information retrieval\\nproblems from those posed by traditional databases, because it is\\nlarger and more diverse, with a lower signal-to-noise ratio. As a\\nresult, in Web searches it is often useful to trade recall (the\\nnumber of documents returned) for improved precision.\\n\\nFigure A: The distribution of the number of terms contained in queries.\\n\\n\\nPerformance\\nWe analyzed the response time of the following six search engines:\\nAltaVista, Excite, HotBot, Infoseek, Lycos, and Northern Light. The\\nmedian response time from 3,000 queries to these engines during\\nNovember-December 1997 was 1.9 seconds. However, if queries are made\\nto all of the engines simultaneously, then the median time for the\\nfirst engine to respond was 0.7 seconds. A similar advantage is gained\\nby downloading the Web pages corresponding to the hits in parallel,\\nresulting in a median time for the NECI engine to receive the first\\npage being 1.3 seconds. On average, the parallel architecture of\\nthe NECI engine allows it to find, download, and analyze the first\\npage faster than the standard search engines can respond, even though\\nthe standard engines do not download and analyze the current contents\\nof the pages.\\nIn May 1998 we analyzed the time for the engine to\\ndisplay the first five and first 10 relevant results from 200\\nqueries. The median time for the first five relevant results was 2.7\\nseconds, and the median time for the first 10 relevant results was 3.2\\nseconds (these figures do not include queries that did not return the\\ntarget number of results).\\nCONCLUSION\\nThe NECI metasearch engine demonstrates that real-time downloading and\\nanalysis of the pages that match a query is possible. In fact, by\\ncalling the Web search engines and downloading Web pages in parallel,\\nthe NECI metasearch engine can, on average, display the first result\\nquicker than a standard search engine.\\nLike other metasearch engines\\nand various Web tools, the NECI metasearch engine relies on the\\nunderlying search engines for important and valuable services. Wide\\nuse of this or any metasearch engine requires an amiable arrangement\\nwith the underlying search engines; such arrangements might include\\npassing through ads or micro-payment systems.\\nThere are numerous\\nareas for future research. Because the NECI engine collects the full\\ntext of matching documents, it is a good test bed for information\\nretrieval research. Areas we are working on include clustering, query\\nexpansion, and relevance feedback. Because the query-sensitive\\nsummaries let users better assess relevance without having to view\\npages, implicit feedback should be more successful and might be useful\\nfor improved relevance measures, automatic relevance feedback, and\\nlearning specific expressive forms. Other areas we are looking at\\ninclude page classification and extending the\\nspecific-expressive-forms search technique.\\nACKNOWLEDGMENTS\\nWe thank\\nEric Baum, Kurt Bollacker, Adam Grove, Bill Horne, Bob Krovetz, Roy\\nLipski, John Oliensis, Steve Omohundro, Maximilian Ott, James Philbin,\\nMajd Sakr, Lance Williams, the employees of NECI, and the anonymous\\nreviewers for useful comments and suggestions.\\nREFERENCES\\n1. E. Selberg and O. Etzioni, Multi-Service Search and Comparison\\nUsing the MetaCrawler, Proc. 1995 WWW Conf., 1995; available online at\\nhttp://draz.cs.washington.edu/papers/www4/html/Overview.html.\\n2. S. Lawrence\\nand C.L. Giles, Searching the World Wide Web, Science, Vol. 280,\\nNo. 5360, 1998, p. 98.\\n3. D. van Eylen, AltaVista Ranking of Query\\nResults, available at http://www.ping.be/dirk_van_eylen/avrank.html,\\n1998.\\n4. D. Dreilinger and A. Howe, An Information Gathering Agent for\\nQuerying Web Search Engines, Tech. Report CS-96-111, Computer Science\\nDept., Colorado State Univ., Fort Collins, Colo., 1996.\\n5. E. Selberg\\nand O. Etzioni, The MetaCrawler Architecture for Resource Aggregation\\non the Web, IEEE Expert, Jan.-Feb. 1997, pp. 11-14; also available at\\nhttp://www.cs.washington.edu/homes/speed/papers/ieee/ieee-metacrawler.ps.\\n6. A. Tombros, Reflecting User Information Needs Through Query Biased\\nSummaries, doctoral thesis, Dept. Computer Science, Univ. of Glasgow,\\n1997.\\n7. S.T. Kirsch, Document Retrieval over Networks Wherein Ranking\\nand Relevance Scores are Computed at the Client for Multiple Database\\nDocuments, US Patent #5,659,732, US Patent Office, 1997.\\n8. G. Salton,\\nAutomatic Text Processing: The Transformation, Analysis and Retrieval\\nof Information by Computer, Addison Wesley Longman, Reading, Mass.,\\n1989.\\n9.  1997 Internet Performance Wrap-Up How Well Did It Perform?\\nKeynote Systems Shows All, Business Wire, Jan. 9, 1998; available at\\nhttp://www.keynote.com/measures/business/business40.html.\\nSteve Lawrence is a scientist at the NEC Research Institute in\\nPrinceton, N.J. His research interests include information retrieval,\\nmachine learning, neural networks, face recognition, speech\\nrecognition, time series prediction, and natural language. His awards\\ninclude an NEC Research Institute excellence award, ATERB and APRA\\npriority scholarships, a QUT university medal and award for\\nexcellence, QEC and Telecom Australia Engineering prizes, and three\\nsuccessive prizes in the annual Australian Mathematics\\nCompetition. Lawrence received a BSc in computing and a BEng in\\nelectronic systems from the Queensland University of Technology,\\nAustralia, and a PhD in electrical and computer engineering from the\\nUniversity of Queensland, Australia.\\nC. Lee Giles is a senior research scientist in computer science at NEC\\nResearch Institute, Princeton, N.J., and an adjunct professor at the\\nInstitute for Advanced Computer Studies at the University of Maryland,\\nCollege Park. His research interests are in novel applications of\\nneural computing, machine learning, agents, and AI in all areas of\\ncomputing. He is on the editorial boards of IEEE Intelligent Systems,\\nIEEE Transactions on Knowledge and Data Engineering, IEEE\\nTransactions on Neural Networks, J. of Computational Intelligence in\\nFinance, J. of Parallel and Distributed Computing, Neural Networks,\\nNeural Computation, and Applied Optics. Giles is a Fellow of the IEEE\\nand a member of AAAI, ACM, the International Neural Network Society,\\nthe Optical Society of America, and the Center for Discrete\\nMathematics and Theoretical Computer Science, Rutgers\\nUniversity.\\nContact Lawrence and Giles at NEC Research Institute, 4\\nIndependence Way, Princeton, NJ 08540; {lawrence,giles}@research.nj.nec.com.\\n\\nRELATED WORK\\nThe idea of querying and collating results from multiple databases is\\nnot new. Companies such as PLS (http://www.pls.com/), Lexis-Nexis\\n(http://www.lexis-nexis.com/), Dialog (http://www.dialog.com/), and\\nVerity (http://www.verity.com) long ago created systems that\\nintegrated search results from multiple heterogeneous databases.1\\nThere are many existing Web metasearch services, including\\nMetaCrawler, SavvySearch, Inference Find, Fusion, ProFusion, Highway\\n61, Mamma, Quarterdeck WebCompass, Metabot, Symantec Internet\\nFastFind, and WebSeeker (for a quick review of metasearch engines, see\\nNotess2).\\nWork in the area of collection fusion is reported in the\\nText Retrieval Conference (TREC) and the Special Interest Group for\\nInformation Retrieval (SIGIR) conference proceedings. Several other\\nresearchers have also used relevance measures including term\\nproximity.3,4\\nResearch search engines that promise improved results\\nranking include Laser5 (http://laser.cs.cmu.edu/) and Google6\\n(http://www.google.com/). These engines use the structure of HTML\\npages and hyperlink information to help determine page relevancy. For\\nexample, Google uses the text in links to a particular page as\\ndescriptors of that page (links often contain better descriptions of\\nthe page than the pages themselves). Google also uses a ranking\\nalgorithm called PageRank, which bases rankings on analysis of the\\nnumber of pages pointing to each page. Although most of the benefits\\nof metasearch apply to these improved search engines, displaying query\\nterm context may become less important for determining page relevancy\\nas results rankings improve.\\nREFERENCES\\n1. E. Selberg and O. Etzioni,\\nMulti-Service Search and Comparison Using the MetaCrawler, Proc. 1995\\nWWW Conf., 1995; available online at\\nhttp://draz.cs.washington.edu/papers/www4/html/Overview.html.\\n2. G.R. Notess,\\nInternet Onesearch With the Mega Search Engines, Online, Vol. 20,\\nNo. 6, 1996, pp. 36-39.\\n3. E. Keen, Term Position Ranking: Some New\\nTest Results, Proc. 15th International ACM SIGIR Conf. Research and\\nDevelopment in Information Retrieval, 1992, pp. 66- 76; available\\nonline at http://www.acm.org/pubs/citations/proceedings/ir/133160/p66-keen/.\\n4. D. Hawking and P. Thistlewaite, Proximity\\nOperators So Near and Yet So Far, Proc. Fourth Text Retrieval Conf.,\\nD.K. Harman, ed., 1995; available online at\\nhttp://web.soi.city.ac.uk/~andym/PADRE/trec4.ps.Z.\\n5. J. Boyan,\\nD. Freitag, and T. Joachims, A Machine-Learning Architecture for\\nOptimizing Web Search Engines, Proc. AAAI Workshop Internet-Based\\nInformation Systems, 1996; available online at\\nhttp://www.lb.cs.cmu.edu/afs/cs/project/reinforcement/papers/boyan.laser.ps.\\n6. S. Brin and L. Page,\\nThe Anatomy of a Large-Scale Hypertextual Web Search Engine,\\nProc. 1998 WWW Conf., 1998; available online at\\nhttp://google.stanford.edu/~backrub/google.html.\\n\\n\\n\\n',\n",
       " '\\n\\n\\n\\nFuzzy Environmental Control\\n\\n\\n\\n\\nFuzzy Environmental Control\\n\\nFor those looking for practical applications of fuzzy logic in\\na real-world product, please read on. This isn\\'t a sexy\\napplication with millions of dollars thrown at it - it\\'s just a\\ncompany trying to build a better product using fuzzy logic. They\\ndid just that ... \\n\\n\\n     Number 31                                            October 92\\n  \\n                      The Huntington Technical Brief  \\n                          By David Brubaker Ph.D.\\n  \\n                       FUZZY ENVIRONMENTAL CONTROL\\n                       ---------------------------\\n\\nINTRODUCTION\\nIn January 1992, Liebert Corporation of Columbus, Ohio,\\nspecialists in environment control units for computer\\ninstallations, introduced the LogiCool, a precision temperature\\nand humidity controller with a fuzzy control unit at its heart.\\nThis Technical Brief briefly discusses both the design of\\nLogiCool\\'s fuzzy controller, and the process through which\\nLiebert elected to use a fuzzy approach. \\n\\nDESIGN GOALS\\nControl of the environment for large computing systems is\\noften a far greater challenge than for rooms inhabited by people.\\nNot only do the systems themselves generate heat, but they are\\noften specified by their manufacturers to be maintained in as\\nlittle as a plus-or-minus 1 degree (Fahrenheit) range. Humidity\\nis also a challenge, causing, for example, corrosion and jamming\\nof associated mechanical systems at high humidity levels and the\\nenhanced possibility of static discharge with low levels.\\nHumidity control is often specified to be 50% relative humidity,\\nwith a maximum swing of plus-or-minus 3% per hour. \\nIn addition, the design of a precision environmental control\\nsystem also faces nonlinearities, caused by such system behavior\\nas air flow delay and dead times, uneven airflow distribution\\npatterns, and duct work layouts. Uncertainties in system\\nparameters are often present, for example, room size and shape,\\nlocation of heat-producing equipment, thermal mass of equipment\\nand walls, and amount and timing of external air introduction. \\nRecognizing these challenges, Liebert undertook the design of\\na control system requiring (in general terms): \\n\\nPrecision temperature and humidity control; \\nMinimization of cycling times (i.e., the opening and\\n        closing of the damper and turning on and off of the\\n        compressor), thereby increasing reliability and component\\n        life, and also resulting in increased energy efficiency; \\nStraightforward and therefore inexpensive control\\n        electronics. \\n\\nIn short, Liebert wanted to precisely control with simple\\nhardware a nonlinear system with significant uncertainties.\\nSeveral traditional linear approaches were considered but proved\\ninadequate. A fuzzy logic approach was investigated and\\nultimately implemented. Design specifics - The LogiCool control\\nsystem has six fuzzy inputs, three fuzzy outputs, and 144\\nprinciples (rules). It runs on a Motorola 6803 microprocessor,\\nand is programmed in C. \\nLogiCool\\'s fuzzy input variables are: e_temperature, the\\ntemperature relative to a setpoint; delta_T/delta_t, the rate of\\ntemperature change; e_humidity, the humidity relative to a\\nsetpoint; delta_H/delta_t, the rate of humidity change; and two\\nproprietary variables associated with the action of the\\ncontrollers. \\nFuzzy outputs control: 1) amount of cooling, 2) amount of\\ndehumidification, and 3) heat. Outputs can also be treated as\\nfedback input variables, and time delays are treated as fuzzy\\noutputs as well. Each fuzzy variable is assigned seven membership\\nfunctions as values, with the traditional Large_Negative,\\nMedium_Negative, Small_Negative, Near_Zero, Small_ Positive,\\nMedium_Positive, and Large_Positive as labels. Ranges for the\\nvalues of each variable are proprietary. \\nAn example of a temperature control principle, using the as\\n...then ... (rather than the if ... then ...) syntax, is: \\n\\nas temperature relative to set point is small_positive and\\n    temperature rate of change is medium_positive then amount of\\n    cooling is small_positive; \\n\\nThe Liebert design also incorporates time delays into their\\nprinciples. The following demonstrates both this as well as the\\nuse of a fuzzy output as a feedback variable. \\n\\nas temperature relative to setpoint is small_negative and\\n    amount of cooling is small_positive then wait delay to\\n    cooling change is medium_positive; \\n\\nA fuzzy OR operator (maximizer) is used as the defuzzification\\ntechnique, avoiding the complicated calculations associated with\\na centroid approach. Liebert has found that with the large number\\nof principles, a more elaborate approach is unnecessary. Inputs\\nare sampled, the principle-base accessed, and outputs are updated\\nonce a second. The \"long\" inter-sample delay allows the\\n6803, a simple eight-bit microprocessor, to implement this rather\\nlarge fuzzy system. \\n\\nECONOMIZER\\nA key feature of the system is LogiCool\\'s Economizer, which\\nalso runs under fuzzy control. When appropriate (for example in\\nthe cool of the morning), the outdoor ambient temperature is used\\nto assist in internal temperature control. While on/off control\\nof the Economizer could have been used, fuzzy logic greatly\\nreduced the number of system cycles, thereby significantly\\nreducing wear on the damper. \\n\\nDESIGN PROCESS\\nThe sequence that resulted in the decision to use fuzzy logic\\nstarted roughly eighteen months ago. From the engineering side,\\nthe decision was driven by being able to meet difficult\\nrequirements. Two Liebert engineers, Terry Bush and Dennis Weber,\\nhad already read, become interested in, and familiarized\\nthemselves with fuzzy technology, and were therefore able to\\nrecognize that a fuzzy approach could satisfy the stringent\\nrequirements. Simultaneously, Liebert marketing was both aware of\\nfuzzy logic controlled air conditioning systems available outside\\nthe United States, and was also getting feedback from customers\\nindicating that an improved method of handling complex internal\\nenvironmental control was needed. Putting the two together,\\nengineering was asked to investigate a fuzzy controller. \\nLiebert engineers designed and implemented the system\\nin-house, with the entire fuzzy logic controller portion\\ncompleted in two to three months. A commercially available fuzzy\\ndevelopment tool was not used, primarily because in the beginning\\nthere was insufficient confidence in the approach to justify the\\nexpense of such a tool. Liebert is still satisfied with the\\ndecision to \"roll its own\" system, as it resulted in a\\ndesign that incorporates a number of features not easily provided\\nin the tools available at the time, for example the delaying of\\noutput actions by fuzzily defined wait delays. \\nLiebert engineers did ultimately write a PC-based simulator to\\ntest the fuzzy design under simulation prior to committing it to\\nhardware. Conclusion - Although quantitative metrics are not\\navailable, Liebert reports that LogiCool has fully met its design\\ngoals. Damper and compressor cycling times have been greatly\\nreduced, especially during Economizer operation. This reduction\\nin cycling times results in increased reliability and increased\\nexpected component life. LogiCool also meets Liebert\\'s\\noperational requirements associated with precisely controlling\\ntemperature and humidity in rooms with uncertain and nonlinear\\ncharacteristics. Moreover, installation includes no tuning\\nprocedure - the same set of principles satisfies all\\ninstallations. \\nLiebert is completely satisfied with the response to LogiCool.\\nSales since its introduction last January are better than\\nexpected, and production run sizes are being increased to respond\\nto the demand. In addition, in recognition of the innovation in\\noverall design, the LogiCool has been recognized by HVAC News as\\na 1992 Design Winner. For more information on the LogiCool, call\\nLiebert Corporation, at 1-800-877-9222. \\n\\nThe Huntington Technical Brief is published, monthly and free\\nof charge, as part of the marketing effort of Dr. David Brubaker\\nof The Huntington Group. A full collection of past issues\\n(starting with number 5 -- issues 1 through 4 are unrelated to\\nfuzzy logic and are unavailable) may be obtained directly from\\nDr. Brubaker by calling 415-325-7554. There is minimal charge for\\ngetting back issues.\\n\\n\\nCopyright 1992 by The Huntington Group \\n883 Santa Cruz Avenue,Suite 27 Menlo Park, CA 94025-4608 \\n\\n\\nThis information is provided by \\nAptronix FuzzyNet \\n408-428-1883 Data USR V.32bis \\n\\n\\n\\nWebMina@austinlinks.com\\n© 2000 SiteTerrific \\n  Web Solutions.  \\n  All rights Reserved \\n\\xa0\\n\\n\\n\\xa0\\n\\n\\n\\n',\n",
       " '\\n\\n\\n\\n\\n2 Pairwise Alignment via Dynamic Programming\\n\\n\\n\\n\\n\\n\\n\\n     \\n Next: 3 Weight Matrices for\\nUp: Pairwise Sequence Alignments\\n Previous: 1 Distance and Similarity\\n  \\n2 Pairwise Alignment via Dynamic Programming\\n\\n2.1 Calculating Edit Distances and Optimal Alignments\\n\\n\\xa0\\n(Edit Dist. Calculation)\\nThe number of possible alignments between two sequences is gigantic, and \\nunless the weight function is very simple, it may seem difficult to pick \\nout an optimal alignment. But fortunately, there is an easy and systematic \\nway to find it. The algorithm described now is very famous in biocomputing,\\nit is usually called ``the dynamic programming algorithm\\'\\'.\\n\\nConsider two prefixes  and , \\nwith . Let us assume we already know optimal alignments between all \\nshorter prefixes of  and , in\\nparticular of\\n  and , of \\n  and , and of\\n  and .\\n\\nAn optimal alignment of  and  \\nmust be an extension of one of the above by\\n a Replacement, or a Match,\\ndepending on whether \\n a Deletion, or\\n an Insertion.\\n\\n(Edit Dist. Recursion)\\nWe simply have to choose the minimum:\\n\\n\\n\\nThere is no choice when one of the prefixes is empty, i.e. , or\\n, or both:\\n\\n\\n\\nAccording to this scheme, and for a given , the edit distances of all \\nprefixes of  and  define an distance matrix \\n with .\\n\\nThe three-way choice in the minimization formula for  leads to the \\nfollowing pattern of dependencies between matrix elements:\\n\\n\\n\\nThe bottom right corner of the distance matrix contains the desired result:\\n.\\n\\n(Edit Dist. Matrix)\\nThis is the distance matrix for our previous example with\\n\\n:\\n\\n\\n\\n\\n\\nIn the second diagram, we have drawn a path through the distance matrix indicating \\nwhich case was chosen when taking the minimum. A diagonal line means\\nReplacement or Match, \\na vertical line means Deletion, and a horizontal line means Insertion. Thus, \\nthis path indicates the edit operation protocol of the optimal alignment with .\\nNote that in some cases, the minimal choice is not unique,\\nand different paths could have been drawn which indicate alternative \\noptimal alignments.\\n\\nAnother example is \\nhere.\\n\\nIn which order should we calculate the matrix entries? The only constraint \\nis the above pattern of dependencies. The most common order of calculation \\nis line by line (each line from left to right), or column by column \\n(each column from top-to-bottom).\\n\\nSome Exercises involving Dynamic Programming\\n\\n Find out the cost model used by the BioMOO aligner. Calculate a dynamic \\nprogramming matrix and alignment for the sequences ATT and TTC. Check your results using the \\nBioMOO alignment, \\ni.e. type  \"opt_align ATT TTC matrix with #90\" \\non the MOO.\\n(You can also use the WWW-Interface, see\\n  this tutorial.)\\nHow many optimal alignments are there ?\\n The number of possible alignments is described as\\n\"gigantic\". How many are there for the sequences ATT and TTC ?\\n(Extra Credit.) If you wish do devise a formula for the \\nnumber of alignments, which method can be used to enumerate\\nthem systematically ? Devise such a formula.\\n\\n\\n2.2 A Word on the Dynamic Programming Paradigm\\n\\n``Dynamic Programming\\'\\' is a very general programming technique. It is\\napplicable when a large search space can be structured into a succession of\\nstages, such that\\n the initial stage contains trivial solutions to sub-problems,\\n each partial solution in a later stage can be calculated by recurring on\\n  only a fixed number of partial solutions in an earlier stage,\\n the final stage contains the overall solution.\\n\\n\\nThis applies to our distance matrix: The columns are the stages, the first\\ncolumn is trivial, the final one contains the overall result. A matrix entry\\n is the partial solution  and can be\\ndetermined from two solutions in the previous column\\n and  plus one in the same column,\\nnamely\\n.\\nSince calculating edit distances is the predominant approach to sequence\\ncomparison, some people simply call this THE dynamic programming\\nalgorithm. Just note that the dynamic programming paradigm has many other\\napplications as well, even within bioinformatics.\\n\\n2.3 A Word on Scoring Functions and Related Notions\\n\\nMany authors use different words for essentially the same idea: scores,\\nweights, costs, distance and similarity functions all attribute a numeric\\nvalue to a pair of sequences.\\n ``distance\\'\\' should only be used when the metric axioms are satisfied.\\n  In particular, distance values are never negative. The optimal alignment\\n  minimizes distance.\\n The term ``costs\\'\\' usually implies positive values, with the overall\\n  cost to be minimized. However, metric axioms are not assumed.\\n ``weights\\'\\' and ``scores\\'\\' can be positive or negative. The most popular\\n  use is that a high score is good, i.e. it indicates a lot of similarity.\\n  Hence, the optimal alignments maximize scores.\\n The term ``similarity\\'\\' immediately implies that large values are good,\\n  i.e. an optimal alignment maximizes similarity. Intuitively, one would expect\\n  that similarity values should not be negative (what is less than zero\\n  similarity?). But don\\'t be surprised to see negative similarity scores\\n  shortly.\\n\\n\\nMathematically, distances are a little more tractable than the others. In terms of\\nprogramming, general scoring functions are a little more flexible. For example,\\nthe algorithm for local similarity presented in section 5.1 depends on\\nthe use of both positive and negative scores. The accumulated score of two\\nsubsequences may rise above the threshold value, and may fall below it after\\nencountering some negative scores.\\n\\nLet us close with another caveat concerning the influence of sequence length\\non similarity. Let us just count exact matches and let us assume that two\\nsequences of length  and ,\\nrespectively, have 99 exact matches.  \\nLet  be\\nthe similarity score calculated for  and  under this cost model. \\nSo,\\n. What this means depends on : If\\n, the sequences are\\nvery similar - almost identical. If , we have only 10% identity!  (Two typos were corrected in this paragraph on Wed May 15 17:06:38 MDT 1996.)\\nSo if we relate sequences of varying length, it makes sense to use\\nlength-relative scores - rather than  we use  for\\nsequence comparison.\\n\\nBack to VSNS BioComputing Division Home Page.\\nVSNS-BCD Copyright 1995/1996.\\nRobert Giegerich\\n\\n     \\nNext: 3 Weight Matrices for\\nUp: Pairwise Sequence Alignments\\n Previous: 1 Distance and Similarity\\n  \\n \\n\\n\\nMon Apr 29 18:31:03 MET DST 1996\\n\\n\\n\\n\\n',\n",
       " '\\n\\n\\n\\nGAMS : Project Summary\\n\\n\\n\\n\\n\\n[Home] . . . \\nSearch by \\n[Problem] \\n[Package] \\n[Module] \\n[Keyword] \\n . . . [Math at NIST]\\n\\n\\n\\n Project Summary\\n\\nThe  Guide to Available Mathematical Software project of the  National Institute of Standards and Technology\\n(NIST) studies techniques to provide scientists and engineers with improved\\naccess to reusable computer software components which are available to them for use in\\nmathematical modeling and statistical analysis.  \\nOne of the products of this work is an on-line cross-index of available mathematical software.  \\nThis system also operates as a virtual software repository.  That is, it provides\\ncentralized access to such items as abstracts, documentation, and source code\\nof software modules that it catalogs; however, rather than operate a physical\\nrepository of its own, this system provides transparent access to multiple\\nrepositories operated by others.\\n\\n\\nCurrently two software repositories are indexed: one maintained for use by\\nNIST staff (parts of which are accessible to public), and netlib, a publically\\naccessible software collection maintained by Oak Ridge National Laboratory and\\nthe University of Tennessee at Knoxville \\n(netlib in Tennessee) and Bell Labs  (netlib\\nat Bell Labs).  This represents some 9,000 problem-solving modules from\\nmore than 100 software packages.  The majority of this software consists of\\nFortran subprograms for mathematical problems which commonly occur in\\ncomputational science and engineering, such as solution of systems of linear\\nalgebraic equations, computing matrix eigenvalues, solving nonlinear systems of\\ndifferential equations, finding minima of nonlinear functions of several\\nvariables, evaluating the special functions of applied mathematics, and\\nperforming nonlinear regression.  These components are especially useful to\\nresearch scientists developing applications software.\\nAmong the packages cataloged are :  \\nthe IMSL, NAG, PORT, and SLATEC libraries; the BLAS, EISPACK, FISHPAK,\\nFNLIB, FFTPACK, LAPACK, LINPACK, and STARPAC packages; the DATAPLOT and SAS\\nstatistical analysis systems; as well as other collections such as the\\nCollected Algorithms of the ACM.  \\nNote that some of the libraries and packages cataloged are proprietary, and are\\navailable only from vendors.  (The developers are identified for each package.)\\nSource code of proprietary software products are not available\\nthrough GAMS, although related items such as documentation and example programs often are.\\nThese are cataloged here since they are available for use internally at NIST.\\nSoftware from freely available packages can be downloaded directly via GAMS.\\n\\n\\nAll cataloged problem-solving software modules are assigned one or more problem\\nclassifications from a tree-structured taxonomy of\\nmathematical and statistical problems.  Users can browse through modules in\\nany given problem class.  To find an appropriate class, one can utilize the\\ntaxonomy as a decision tree, or enter keywords which are then mapped to problem\\nclasses.  Search filters can be declared which allow users to specify\\npreferences such computing precision or programming language.  In addition,\\nusers can browse through all modules in a given package, all modules with a\\ngiven name, or all modules with user-supplied keywords in their abstracts.\\n\\n\\nSeveral user interfaces to the system have been developed : \\n\\n\\n an HTTP gateway (you are using it now)\\n a Java-powered World Wide Web client\\n\\n\\nPlease report any problems to \\n\\ngams@nist.gov.\\n\\n\\nFor further information, contact \\n\\n\\nDr. Ronald F. Boisvert\\nMail Stop 8910\\nMathematical and Computational Sciences Division\\nNational Institute of Standards and Technology\\nGaithersburg, MD 20899-8910 USA\\nPhone : 301-975-3812\\nEmail : boisvert@nist.gov\\n\\n\\nDisclaimer:This system catalogs mathematical and statistical software made\\navailable for use at NIST by its Information Technology Laboratory.  \\nIdentification of commercial products does not imply\\nrecommendation or endorsement by NIST.\\n\\n\\nNote: This service is not related to the Generalized Algebraic\\nModeling System (GAMS), software marketed by \\nGAMS Development Corporation, \\n1217 Potomac St NW, Washington, DC 20007.\\n\\n\\nGAMS is a service of the\\nMathematical and Computational Sciences Division / \\nInformation Technology Laboratory / \\nNational Institute of Standards and Technology\\n\\n\\nLast change in this page : 12 February 2002.\\n[ \\ngams@nist.gov ].\\n\\n\\n\\n',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\tProximity Technology Inc.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\t\\tThe Proximity Wizard (261K)\\n\\n\\n\\n\\n\\nProximity Technology, Inc. provides portable and intelligent linguistic software\\nsub-systems: spelling correction and verification, hyphenation, thesaurus,\\ndictionary, fuzzy database search and more in numerous languages.\\n\\nEstablished in 1979, Proximity was the first company to develop and provide\\nintelligent linguistic systems for original equipment manufacturers (OEMs). \\n\\n\\tAcquired in 1988 by \\n\\n\\n\\n\\tFranklin Electronic Publishers, Inc.\\n\\n\\t, the world leader in electronic book\\n\\tpublishing, Proximity\\'s technology is incorporated into \\n\\tFranklin\\'s hand-held linguistic products.\\n\\n     \\tToday, Proximity Technology provides high quality linguistic \\n\\tSoftware Development Kits (SDKs)\\n     \\tfor customers internationally, including\\n\\tthe best spelling correction technology on the planet.\\n\\n\\n\\n\\n\\t\\tTable of Contents\\n\\n\\n \\t\\tOnline Demos \\t\\t\\t\\t\\n \\t\\tEnd User Products\\t\\t\\t\\n\\n\\tDeveloper\\'s Overview \\t\\t\\t\\n \\tQuality and Service \\t\\t\\t\\n \\tClients \\t\\t\\t\\t\\n\\n\\t\\tLanguages \\t\\t\\t\\t\\n\\n\\t\\tSpell Checker\\t\\t\\t\\t\\n\\t\\tHyphenation\\t\\t\\t\\t\\n\\t\\tThesaurus\\t\\t\\t\\t\\n\\t\\tDictionary\\t\\t\\t\\t\\n\\t\\tFranklin Dictionary\\t\\t\\t\\n\\n\\t\\tOCR Spell Checker\\t\\t\\t\\n\\t\\tProximity-Scan Fuzzy Database Search\\t\\n\\n \\tPrivate Developer Site \\t\\t\\t\\n\\tContact Information\\t\\t\\t\\n\\n\\n\\n\\n\\n\\t\\t\\tOnline Demos\\n\\n\\n\\nCurrent Products\\n\\n\\n\\n\\n\\tSpell Checker\\n\\t\\n\\n\\n\\tStand Alone Hyphenation\\n\\t\\n\\n\\n\\tThesaurus\\n\\t\\n\\n\\n\\tAmerican English Dictionary\\n\\t\\n\\n\\n\\tAmerican English Kids\\' Dictionary (Grades 1-3)\\n\\t\\n\\n\\n\\tBritish English Kids\\' Dictionary (Grades 1-3)\\n\\t\\n\\n\\n\\tOCR Speller\\n\\t\\n\\n\\n\\tFuzzy Database Search (Proximity-SCAN Demo)\\n\\t\\n\\n\\nFuture Products\\n\\n\\nFranklin Bilingual Dictionaries \\n  \\n\\n\\n\\n\\tEnglish/Spanish\\n\\t\\nEnglish/French\\n    English/German\\n    English/Portuguese\\n    German/French\\n    German/Italian\\n  \\n\\nFranklin Monolingual Dictionaries \\n  \\n\\nFrench\\n    Italian\\n  \\n\\n\\n\\n\\n\\n\\t\\tEnd User Products\\n\\n\\n\\n\\n\\tSorry, at this time all the information at this site is for \\n\\tmanufacturers only.\\n\\n\\n\\tIn the past, Proximity sold the following software products\\n\\tto the general public in retail stores:\\n\\n\\n \\tLanguage Master (TM) Dictionary and Thesaurus\\n \\tFriendly Finder (TM) The intelligent database search companion\\n\\n\\t\\n\\tSimilar products are currently under development, and will be\\n\\tavailable here online at some future date.\\n\\n\\n\\n\\n\\n\\t\\t\\tDeveloper\\'s Overview \\t\\t\\t\\n\\n\\n\\n\\n\\n \\n\\t\\t\\tOur Software Sub-Systems \\n\\n\\n\\n\\tProximity develops and licenses software sub-systems with \\n\\tflexible API\\'s for integration into larger applications, \\n\\tand commonly refers to this collection as the \\n\\n\\tProximity Linguistic System (PLS).\\n\\n\\tThe current release is Revision 7.1 \\n\\n\\n\\n\\n\\n\\tOne Source-Code Base\\n\\n\\n\\n\\tOur software sub-systems consist of portable \"C\" source code with \\n\\tlocalized operating system dependencies limited to file I/O and \\n\\tmemory allocation.  This makes the products portable across\\n\\tall platforms, including:\\n\\n\\n\\tWindows\\n\\tNT\\n\\tDOS\\n\\tOS/2\\n\\n\\tMacintosh\\n\\tUNIX\\n\\tVMS\\n\\n\\n\\tAll software sub-systems can be mixed and matched with\\n\\tno conflicts in file names or symbols.\\n\\n\\n\\n\\n\\tPortable Compressed Databases\\n\\n\\n\\n\\n\\tOur binary pre-compressed databases are architecture independent,\\n\\tallowing the same file to be accessed by all computers in a\\n\\theterogeneous network.\\n\\n\\tProximity\\'s proprietary data compression technology squeezes 80,000 \\n\\twords with definitions and other dictionary information \\n\\t- 6 megabytes of ASCII information - \\n\\tinto less than 1 megabyte of disk space, or more than one million\\n\\tItalian words with \\n\\tfull phonetic spelling correction information into less than 110K,\\n\\twhich yields a compression ratio of 1.15 words per BIT! \\n\\t(this is not a misprint)\\n\\n\\tIn addition, all our compressed databases, except the definitions \\n\\tdictionary and thesaurus, can be used in ROM based environments.\\n\\n\\n\\n\\n \\n\\t\\t\\tYour Product \\n\\n\\n\\n\\tHere is a sample list of the kinds of products which can benefit\\n\\tfrom using our software:\\n\\n\\n \\tContact management \\n \\tDesktop publishing \\n \\tElectronic mail \\n \\tForms processing \\n \\tGraphics presentation \\n \\tIntegrated office systems \\n \\tIntelligent character recognition (ICR) \\n \\tInternet search engines \\n \\tOptical character recognition (OCR) \\n \\tText Retrieval\\n \\tTypewriters \\n \\tWord processing\\n\\n\\n\\n\\n\\n\\n\\t\\tQuality and Service\\n\\n\\n\\n\\n\\tProximity provides high-quality linguistic products for more \\n\\tthan 200 hardware and software customers, representing \\n\\tmillions of users around the globe.\\n\\n\\tState-of-the-art programming and affiliation with world-renowned \\n\\tlinguists provide a depth of expertise to deliver quality, \\n\\tcutting-edge products.  Flexible license terms increase the \\n\\tattractiveness of partnering with Proximity.\\n\\n\\tWhether your customers use electronic typewriters, PCs, \\n\\tpen-based devices, minicomputers or mainframes, \\n\\tthe Proximity Linguistic System helps them produce more \\n\\teffective written communications by giving them instant access \\n\\tto text proofing tools such as:\\n\\n \\tSpell Checking\\n \\tHyphenation\\n \\tThesaurus\\n \\tDictionary\\n\\n\\n\\tProximity\\'s development support team works closely with\\n\\tour OEMs to help them deliver the best solutions to\\n\\tmeet ever-changing business needs.  Each member of the\\n\\tsupport team is committed to total customer\\n\\tresponsiveness, whether your company is large or\\n\\tsmall.  Our customers depend on outstanding service and\\n\\tpersonal attention at all levels.  \\n\\n\\tWe believe our\\n\\tcommitment to quality products, responsiveness to our\\n\\tcustomers\\' needs and unmatched technical support is the\\n\\tkey to customer satisfaction.\\n\\n\\n\\tWe\\'ve invested the time, money, and resources to deliver \\n\\taffordable, high-quality linguistic solutions to \\n\\tyour programming needs.\\n\\n\\n\\n\\n\\n\\tClients include: \\t\\t\\n\\n\\n\\n\\n\\n\\tAdobe Systems Inc. (Pagemaker, Framemaker, Persuasion)\\n\\n\\n\\n\\tApplix (Applixware, Anyware)\\n\\n\\n\\n\\tArbortext (Publisher)\\n\\n\\n\\n\\tBroderbund Software (Carmen Sandiego Word Detective)\\n\\n\\n\\n\\tCaere (OmniPage, WordScan, High-End OCR)\\n\\n\\n\\n\\tFranklin Electronic Publishers, Inc.\\n\\n\\n\\n\\tLotus (Ami Pro)\\n\\n\\n\\n\\tMacromedia (Freehand)\\n\\n\\n\\n\\tOlivetti (Typewriters)\\n\\n\\n\\n\\tSharp Corporation (typewriters)\\n\\n\\n\\n\\tStenograph Corporation (court transcription products)\\n\\n\\n\\n\\tUnisys Corporation\\n\\n\\n\\n\\tXyvision\\n\\n\\n\\n\\n\\n\\n\\t\\t\\tLanguages \\n\\n\\n\\n\\tMultilingual spell checking, hyphenation, and thesauruses are \\n\\tavailable in the following languages:\\n\\n\\n \\tEnglish (American)\\n \\tEnglish (British)\\n \\tEnglish (Canadian) *\\n \\tCatalan *\\n \\tDanish\\n\\tDutch (De Nieuwe Spelling)\\n \\tFinnish *\\n \\tFrench\\n \\tFrench (Canadian)\\n \\tGerman (Rechtschreibreform)\\n \\tGerman (Swiss)\\n \\tItalian\\n \\tNorwegian (Bokmål)\\n \\tNorwegian (Nynorsk) *\\n \\tPolish *\\n \\tPortuguese *\\n \\tPortuguese (Brazilian) *\\n \\tRussian *\\n \\tSpanish\\n \\tSwedish\\n\\n\\n\\t* Thesaurus in development\\n\\n\\n\\n\\n\\n\\t\\t\\tSpell Checker \\n\\n\\n\\n\\tOur word lists are created specifically for electronic use; our \\n\\tlinguistic experts and editorial partners constantly monitor the \\n\\t\"living\" or everyday language in each country to ensure the databases\\n\\tare current.\\n\\n     The Spell-Checker provides the following functions:\\n\\n        Text Parser\\n        Spelling Verification\\n        Spelling Correction\\n        User Dictionary\\n\\n\\n\\n\\n\\n     Text Parser\\n\\n\\n\\n     \\n     Our text parser simplifies proofreading for the user by allowing \\n     certain \"non-word\" constructs to be skipped or flagged prior to\\n     word verification.\\n\\n\\n     The parser isolates a word suitable for spell checking from its \\n     surrounding text, and classifies it using a modular, \\n     extensible set of options.  These options can be used to implement features\\n     such as:\\n\\n\\n Find Uncapitalized Start of Sentence\\n Find Repeated Words\\n\\n Ignore Numbers: e.g., 1-800-266-5626\\n Ignore Ordinals: e.g., 1st, 2nd, 3rd\\n Ignore Roman Numerals, e.g., IV, VII\\n Ignore Parentheses in Words: e.g., sales(wo)man, shoe(s)\\n\\n\\nThis is a specialized token parser, not a full text parser, and in\\nits  most common usage assumes that the text passed to it has already \\nbeen separated from any surrounding whitespace.\\n\\n\\n\\n\\n     Spelling Verification\\n\\n\\n\\n     \\n     The Spelling Verification routine offers uncompromising accuracy, fast \\n     and error-free performance accelerators and complete and high-quality \\n     data.  It checks spelling, including the capitalization and \\n     punctuation pertinent to the word by itself.  \\n     \\n     Two types of word lists are provided for this routine:  \\n\\n \\tDisk-based (lexicon, user dictionary) \\n\\tRAM-based (corelex, cache, \\ntsp \\n).\\n\\n\\n\\tThe lexicon is a closed \\n     reference, the user dictionary can be changed.  \\n     The corelex is a list \\n     of most frequently used words and is available in various sizes to \\n     accommodate your software packaging needs.  The cache is a list that is \\n     built as the spell-checking process is performed.\\n     The tsp (tiny-speller) is a highly compressed database (under 128K) \\n     which can be used in place of the main lexicon and corelex when available.\\n\\n\\tThe verifier recognizes the following word formations:\\n\\n Capitalization of proper nouns, and acronyms\\n Closed Compounds\\n Enclitics and Contractions \\n Hyphenated Compounds\\n Alternate Spellings\\n\\n \\nForgot to hit the space bar when \\ntyping?\\n\\nThe uncatwords() function will verify that \"thecat\" is really \"the cat\".\\n\\n\\n\\n\\n     Spelling Correction\\n\\n\\n\\n     This routine performs phonetic and typographic analysis using a \\n     proprietary technique which corrects several types of mistakes.  A \\n     misspelled word is corrected by comparing it against lists of valid \\n     words, including the User Dictionary. \\n\\n     Spelling-alternatives found by \\n     the analysis are displayed in a list - ordered from most likely to \\n     least likely - from which the user can choose the right word.\\n     \\n\\n\\n\\n     User Dictionary\\n\\n\\n\\n     The User Dictionary allows users to customize their systems by adding \\n     words not found in the word list; there is no restriction on the number \\n     of entries.  The User Dictionary has a large storage capacity with \\n     space for more than a million words, minimizing disk usage.  \\n     \\n     More than \\n     one User Dictionary can be created, allowing the user to categorize \\n     different types of supplemental words.  Hyphenation of a word can be \\n     included, and words within the list can be edited.\\n\\n\\n\\n\\n\\n\\n\\t\\t\\tHyphenation \\n\\n\\n\\n     Proximity provides hyphenation for electronic publishing systems and \\n     word processing programs requiring perfect hyphenation.  Our standard \\n     ensures 100% accuracy.\\n\\n     \\n\\n\\n\\n\\n     Editorial Excellence\\n\\n\\n\\n\\n     The word lists used for our linguistic products are built and \\n     maintained by major international dictionary publishers.  \\n     Words in \\n     these lists are hyphenated and hand-checked by lexicographers to \\n     ensure all hyphenation points are correct.\\n     \\n\\n     Proximity runs each list through a computerized pattern generation \\n     process that determines what rules govern the hyphenation of the \\n     words.  These rules always ensure 100% accuracy against the words in \\n     our lists, including irregular forms.  In addition, these rules also \\n     provide high probability of accuracy for words not found in our lists \\n     - our hyphenation system knows which rule most likely governs an \\n     \"unknown\" word\\'s hyphenation.\\n     \\n\\n\\n\\n     Implementation Options\\n\\n\\n\\n\\n\\n     \\n     Stand Alone Hyphenation (SAH) - a fast, compact package, provides \\n     100% accuracy for all words found in our lists - without the need to \\n     check the word list.\\n     Our proprietary technology combines an \\n     algorithm with pattern files, replicating the results of \\n     dictionary-based hyphenation.\\n     \\n\\n\\n     Verified Hyphenation - provides the benefits of SAH with an extra \\n     measure of confidence.  Words are verified against the Main and User \\n     word list.  If a word is found in either word list, verified hyphenation \\n     will hyphenate correctly with 100% confidence.  If a word is not \\n     found, this function will alert the user.\\n\\n\\n\\n\\n\\n\\n\\t\\t\\tThesaurus \\n\\n\\n\\n     Proximity\\'s International Thesauruses offer the tools for users to \\n     prepare effective cross-cultural communications and gain a significant \\n     competitive advantage.\\n\\n     \\n     Proximity\\'s thesauruses differ in certain aspects; several languages \\n     offer synonyms only, while others provide combinations of supplemental \\n     lists (i.e., antonyms, related words, compared or contrasted words).\\n\\n\\n\\n\\n\\n     Inflection\\n\\n\\n\\n     Inflection is the process of altering a base word, e.g., when a noun \\n     becomes plural or  a verb changes tense.  Unlike other thesauruses \\n     which only refer you to the base word\\'s synonyms, the Proximity \\n     thesaurus inflects the synonyms so you can replace the word in the \\n     correct form.  \\n\\n     For example, the past tense/past participle of the word \\n     \"entwine\" is \"entwined\".  The Proximity thesaurus gives you the \\n     inflected (the past/past participle) forms of the synonyms:  i.e., \\n     \"wound,\" \"coiled,\" etc.  By inflecting the words, the thesaurus saves \\n     you time and embarrassing grammatical errors.\\n\\n     The American and British English thesauruses support code-based \\n     inflection.  An inflected query is converted to its root form and \\n     synonyms returned are cast into the appropriate inflected form.\\n     \\n\\n\\n\\n     Meanings\\n\\n\\n\\n     The American English thesaurus also offers meaning cores, \\n     characterized by a part of speech (i.e., noun, verb, etc.).  A meaning \\n     core is a brief statement which summarizes a particular usage of an \\n     entry word and illustrates its relationship to associated words.  \\n     Synonyms and other words are returned specifically for a selected \\n     meaning core.  \\n\\n     The other Proximity thesauruses provide responses \\n     grouped by meaning.  In addition, all data files support closure of \\n     the synonym data; i.e., any response given as a response to a query is \\n     also a valid entry point.\\n     \\n\\n\\n\\n\\n\\n\\t\\t\\tDictionary \\n\\n\\n\\n\\tProximity offers one of the most comprehensive electronic dictionaries \\n\\ton the market.  \\n\\tOur Concise Electronic Dictionary is an intelligent reference product \\n\\tproduced by the combination of our proprietary recognition algorithms, \\n\\ttop-level programming and a partnership with the nation\\'s foremost \\n\\tdictionary publisher, Merriam-Webster.  \\n\\tAvailable in American English only, the dictionary \\n\\tprovides concise definitions for almost 80,000 words.\\n\\n\\n\\n\\n\\n\\t\\t\\tFranklin Dictionary \\n\\n\\n\\n        Proximity\\'s Franklin Dictionary is a software product which\\n        provides access to the same dictionary\\n        databases used in Franklin Electronic Publishers\\' Handheld\\n        Reference Products. These highly compressed databases utilize\\n        Franklin\\'s patented Fielded Dictionary Compression Techniques.\\n        It provides the following features:\\n\\n\\n    Random access lookup with inflection recognition\\n      Phonetic and typographic correction\\n\\n\\n\\n\\n\\n\\t\\t\\tOCR Spell Checker \\n\\n\\n\\n\\tProximity\\'s Optical Character Recognition (OCR) Spell Checker \\n\\tis a powerful software library designed to improve the \\n \\n\\taccuracy \\n\\n\\tand \\n \\n\\tspeed \\n\\n\\tof a developer\\'s graphics-to-text \\n\\tconversion product.  \\n\\n\\n\\tAccuracy increases by using linguistic data to validate the output \\n\\tof the developer\\'s character recognition engine.\\n\\n\\n\\tSpeed increases depend on the type of product.  \\n\\tFor example:\\n\\n\\n\\tFull document processing can be speeded up by minimizing the\\n\\ttime required for interactive post-conversion proofing.\\n \\n\\n\\tInteractive proofing can be eliminated or \\n\\tpostponed for automatic tagging of graphic images, by creating a \\n\\tdictionary of words found in the image; these can be used for future \\n\\tqueries.\\n\\n\\n\\tThe OCR Spell Checker addresses a different problem area than the \\n\\ttypical word processor spell checker.\\n\\n\\n\\tOCR Requires:\\n\\n \\tDisambiguation of words in which glyphs have been joined or \\n\\tsplit, e.g., (rn ==> m) or (m ==> rn).\\n \\tSelection of correct words given uncertain spelling.\\n\\n\\n\\tWord-processing Requires:\\n\\n \\tVerification of words given exact spelling.\\n \\tSelection of possible matches given an unverified word.\\n\\n\\n\\tThe software consists of linguistic independent C source code and \\n\\tpre-made compressed word lists, which contain the same words \\n\\tused in our standard spelling product.\\n\\n\\n\\n\\n\\n\\t\\t\\tProximity-Scan \\n\\n\\n\\n\\tProximity-Scan is an innovative process for searching databases.\\n\\tThis proprietary and patented technology judges overall \\n\\tsimilarity between a query and records in a database.  \\n\\n\\n\\tIt is a fuzzy matching technique \\n\\twhich enables users to find records in a database despite \\n\\tmisspellings, words out of order, missing words \\n\\tand abbreviations.  \\n\\n\\n\\tIt ranks database records in order of similarity to a \\n\\tgiven query; there is no need to sort, index, translate or modify your \\n\\tdatabase.  \\n\\n\\n\\tRecord retrieval no longer requires painstaking accuracy.  \\n\\tThere are no magic characters or hard to remember operations; your \\n\\tbest guess is all that is needed.  \\n\\n\\n\\tProximity-Scan is friendly, simple, efficient, and fast.\\n\\n\\n\\n\\n\\n\\t\\tPrivate Developer Site\\n\\n\\n\\n\\tWe maintain a separate restricted site for our clients and \\n\\tprospective clients which includes API documents,\\n\\ttechnical reports, etc.,\\n\\n\\n\\tPlease contact us for the necessary access information.\\n\\n\\n\\n\\n\\n\\t\\tContact Information\\n\\n\\n\\n\\tThanks for visiting Proximity\\'s home page. We hope to hear\\n\\tfrom you again soon.\\n\\tFor further information on Proximity\\'s products, please contact:\\n\\n\\n\\t\\tMaria Armstrong\\n \\n\\t\\t(609) 386-2500 ext. 4520\\n\\n\\tProximity Technology Inc. c/o \\n\\n\\tFranklin Electronic Publishers \\n \\n\\tOne Franklin Plaza \\t\\n\\n\\tBurlington, NJ 08016\\n\\n\\tUSA \\n\\n\\nEmail Address: \\n\\n\\tproximity@franklin.com\\n\\n\\n\\n© Copyright 2001 Proximity Technology Inc.\\n\\nAll Rights reserved.\\n\\n\\n\\n',\n",
       " \"\\n\\n\\n\\n1 Motivation\\n\\n\\n\\n\\n\\n\\n\\n  Next: 2 Related Work\\nUp: Interactive Visualization of Large \\n Previous: List of Tables\\n  \\n1 Motivation\\n\\n\\xa0\\n\\nNode-link graphs are simple, powerful, and elegant abstractions\\nthat have broad applicability in computer science and many other\\nfields. Any domain that can be modelled as a collection of linked\\nnodes can be represented as a graph. For example, in the domain of the\\nWorld-Wide Web, nodes represent web pages and links represent\\nhyperlinks. For a dictionary, nodes represent words and links\\nrepresent word relationships such as  is-a,  part-of, \\nmodifier, and so on. Biological taxonomies are trees, which are a\\nsubset of general graphs: nodes represent species, and links represent\\nevolutionary descent. In a graph of the Internet, nodes could\\nrepresent routers and links would imply direct network connectivity.\\n\\nThe field of graph theory offers a powerful set of domain-independent\\nalgorithms for computationally manipulating graphs efficiently, even\\nif they are very large. Graphs have a natural visual\\nrepresentation as nodes and connecting links arranged in space. Visual\\nrepresentations of small graphs are pervasive: people\\nroutinely sketch such a picture when thinking about a domain, or\\ninclude pictures of graphs in explanatory documents.\\n\\nAn informal statement that explains the popularity of graph pictures\\nis that people must find an explicit visual representation of the graph structure\\nhelpful for some tasks. A more formal analysis of\\ntheir utility is that visual depictions of graphs and networks are\\n external representations that exploit human visual processing to\\nreduce the cognitive load of a task. Endeavors that require\\nunderstanding global or local graph structure can be handled more\\neasily when that structure is interpreted by the visual processing\\ncenters of the brain, often without conscious attention, than when\\nthat structure has to be cognitively inferred and kept in working\\nmemory. External representations change the nature of a task: an\\nexternal memory aid anchors and structures cognitive behavior by\\nproviding information that can be directly perceived and used without\\nbeing interpreted and formulated explicitly [Zha91].\\nA graph is a  topo-visual formalism: that is, the important\\naspect of  drawn graphs is non-metric topological connectedness as\\nopposed to pure geometric distance [Har88,Noi94].\\n\\n1.1 Three Design Studies\\n\\nIn this thesis we extend the reach of graph drawing using ideas from\\ninformation visualization, particularly by incorporating interactivity\\nand domain-specific information. We present and analyze three\\nspecialized systems for interactively exploring large graphs. The\\ncommon choice in all three design studies was to relax the constraint for\\ntotal generality so as to achieve greater scalability and\\neffectiveness. The amount of domain-specific specialization, as shown\\nin Figure 1.1, ranges from a highly targeted system\\ndesigned to fit the needs very small user community of computational\\nlinguists, to the middle ground of a geographic network layout, to a\\nmore general layout originally intended for the hyperlink structure of\\nweb sites that is suited for a entire class of graphs that we call\\n quasi-hierarchical.\\n\\nThe goal of this thesis is two-fold: not only the creation of new\\nalgorithms for graph layout and drawing, but also an analysis that\\nlinks a specific intended task with our choices of spatial layout and\\nencoding. Our analysis provides an evaluation of these graph drawing\\nsystems from an information visualization perspective that distills\\nthe lessons that we learned from building these specific systems into\\na more general framework for designing and evaluating visualization\\nsystems.\\n\\n\\xa0\\n\\n\\nFigure 1.1:  Range of specificity. All three systems in this thesis\\ntrade off generality for scalability, ranging from the relatively\\ngeneral H3 system suited for the class of quasi-hierarchical graphs,\\nto the more focused geographic network visualization of the Planet\\nMulticast system, to the highly targeted Constellation system.\\n\\nThe three software systems that we present are Planet Multicast, H3,\\nand Constellation. The Planet Multicast system was developed in 1996\\nfor displaying the tunnel topology of the Internet's multicast\\nbackbone, and provides a literal 3D geographic layout of arcs on a\\nglobe to help MBone maintainers find potentially misconfigured\\nlong-distance tunnels. The H3 system was developed between 1996 and\\n1998 for visualizing the hyperlink structures of web sites. It scales\\nto datasets of over 100,000 nodes by using a carefully chosen spanning\\ntree as the layout backbone, presents a Focus+Context view using 3D\\nhyperbolic geometry, and provides a fluid interactive experience\\nthrough guaranteed frame rate drawing. The Constellation system was\\ndeveloped between 1998 and 1999, and features a highly specialized 2D\\nlayout intended to spatially encode domain-specific information for\\ncomputational linguists checking the plausibility of a large semantic\\nnetwork created from dictionaries.\\n\\n1.2 Information Visualization\\n\\nThe field of computer-based information visualization draws on ideas\\nfrom several intellectual traditions: computer science, psychology,\\nsemiotics, graphic design, cartography, and art. The two main threads\\nof computer science relevant for visualization are computer graphics\\nand human-computer interaction. The areas of cognitive and perceptual\\npsychology offer important scientific guidance on how humans perceive\\nvisual information. A related conceptual framework from the humanities\\nis semiotics, the study of symbols and how they convey meaning.\\nDesign, as the name suggests, is about the process of creating\\nartifacts well-suited for their intended purpose. Cartographers have a\\nlong history of creating visual representations that are\\ncarefully chosen abstractions of the real world. Finally, artists have\\nrefined methods for conveying visual meaning in subdisciplines ranging\\nfrom painting to cinematography.\\n\\nInformation visualization has gradually emerged over the past fifteen\\nyears as a distinct field with its own research agenda. The\\ndistillation of results from areas with other goals into specific\\nprescriptive advice that can help us design and evaluate visualization\\nsystems is nontrivial. Although these traditions have much to\\noffer, effective synthesis of such knowledge into a useful methodology\\nof our own requires arduous gleaning.\\n\\nThe standard argument for visualization is that exploiting visual\\nprocessing can help people\\nexplore or explain data. We have an active field of study because the\\ndesign challenges are significant and not fully understood. \\nQuestions about visual encoding are even more central to information\\nvisualization \\nthan to scientific visualization. The subfield names grew out of an\\naccident of history, and have some slightly unfortunate connotations\\nwhen juxtaposed: information visualization is not unscientific, and\\nscientific visualization is not uninformative. The distinction between\\nthe two still not agreed on by all, but the definition used here is\\nthat  information visualization hinges on finding a spatial mapping of\\ndata that is not inherently spatial, whereas  scientific visualization\\nuses a spatial layout that is implicit in the data.\\n\\nMany scientific datasets have naturally spatialized data as a\\nsubstrate: for instance, airflow over an airplane wing is given as\\nvalues of a three-dimensional vector field sampled at regular\\nintervals that provides an implicit 3D spatial structure. Scientific\\nvisualization would use the same 3D spatialization in a visual\\nrepresentation of the dataset, perhaps by drawing small arrows at the\\nspots where samples were taken, pointing in the direction of the fluid\\nflow at that spot, with color coded according to velocity. Scientific\\nvisualization is often used as an augmentation of the human sensory\\nsystem, by showing things that are on timescales too fast or slow for the\\neye to perceive, or structures much smaller or larger than human\\nscale, or phenomena such as X-rays or infrared radiation that we\\ncannot directly sense.\\n\\nIn contrast, a typical information visualization dataset would be a\\ndatabase of film information that is more abstract than the previous\\nexample, since it does not have an underlying spatial variable. One\\npossible spatialization would be to show a 2D scatterplot with the\\nyear of production on one axis and the film length on the other, with\\nthe scatterplot dots colored according to genre [AS94]. That\\nchoice of spatialization was an explicit choice of visual metaphor by\\nthe visualization designer, and is appropriate for some tasks but not\\nfor others.\\n\\n1.2.1 Visual Encoding\\n\\n\\xa0\\n\\nIn all visualizations, graphical elements are used as a visual syntax\\nto represent semantic meaning [RRGK96]. For instance, in the\\nscientific fluid flow dataset mentioned earlier, the position of the\\narrow denotes the position of that sample data point, the direction of\\nthe fluid flow was mapped to the orientation of the graphical element\\nof an arrow, and the color of that element represented fluid velocity.\\nWe call these mappings of information to display elements  visual\\nencodings, and the combination of several encodings in a single\\ndisplay results in a complete  visual metaphor.\\n\\nSeveral people have proposed visual encoding taxonomies, including\\nBertin [Ber83,Ber81], Cleveland [CM84]\\n[Cle94, Chapter 4,], Mackinlay [CM97a,Mac86a,Mac86b] [CMS99, Chapter 1,], and Wilkinson\\n[Wil99a]. The fundamental substrate of visualizations is\\nspatial position. Marks such as points, lines, or area-covering\\nelements can be placed on this substrate. These marks can carry\\nadditional information independent of their spatial position, such as\\nsize, greyscale luminance (brightness) value, surface texture density,\\ncolor hue, color saturation, curvature, angle, and shape. The\\nliterature contains many different names for these kinds of visual\\nencodings: retinal variables, retinal attributes, elementary graphical\\nperception tasks, perceptual tasks, perceptual dimensions, perceptual\\nchannels, display channels, display dimensions, and so on.\\n\\nThe critical insight of Cleveland was that not all perceptual channels\\nare created equal: some have provably more representational power than\\nothers because of the constraints of the human perceptual system\\n[CM84]. Mackinlay extended Cleveland's analysis with\\nanother key insight that the efficacy of a perceptual channel depends\\non the characteristics of the data [Mac86a]. The levels of\\nmeasurement originally proposed by Stevens [Ste46] classify\\ndata into types.  Nominal data has items that are distinguishable\\nbut not ranked: for instance, the set of fruit contains apples and\\noranges.  Ordinal data has an explicit ordering that allows\\nranking between items, for example mineral hardness. \\nQuantitative data is numeric, such that not only ranking but also distances\\nbetween items is computable.\\n\\nThe efficacy of a retinal variable depends on the data\\ntype: for instance, hue coding is highly salient for nominal data but\\nmuch less effective for quantitative data. Size or length coding is\\nhighly effective for quantitative data, but less useful for ordinal or\\nnominal data. Shape coding is ill-suited for quantitative or ordinal\\ndata, but somewhat more appropriate for nominal data.\\n\\nSpatial position is the most effective way to encode any kind of data:\\nquantitative, ordinal, or nominal. The power and flexibility of\\nspatial position makes it the most fundamental factor in the choice of\\na visual metaphor for information visualization. Its primacy is the\\nreason that we devote entire sections to spatial layout in later\\nchapters, separate from the section discussing all other visual\\nencoding choices.\\n\\n1.2.2 Integral vs. Separable Dimensions\\n\\nPerceptual dimensions fall on a continuum ranging from almost\\ncompletely separable to highly integrated. Separable dimensions are the\\nmost desirable for visualization, since we can treat them as\\northogonal and combine them without any visual or perceptual\\n``cross-talk''. For example, position is highly separable from color.\\nIn contrast, red and green hue perceptions tend to interfere with each\\nother because they are integrated into a holistic perception of yellow\\nlight.\\n\\nThere is a fundamental tension in visualizing a network of nodes\\nconnected by links because of the interference of two perceptual\\nconflicts: proximity and size. The Gestalt proximity principle means\\nthat nodes drawn close together are perceived as related, whereas those\\ndrawn far apart are unrelated. However, the size channel has the\\nopposite effect: a long edge is more visually salient than a short\\none. Almost all graph drawing systems solve this conflict by choosing\\nproximity instead of size, which makes sense given the primacy of\\nspatial positioning described in section 1.2.1.\\n\\n1.2.3 Preattentive Processing\\n\\n\\xa0\\n\\nAnother fundamental cognitive principle is whether processing of\\ninformation is done deliberately or pre-consciously. Some low-level\\nvisual information is processed automatically by the human perceptual\\nsystem without the conscious focus of attention. This type of\\nprocessing is called  automatic,  preattentive, or \\nselective. An example of preattentive processing is the  visual\\npopout effect that occurs when a single yellow object is instantly\\ndistinguishable from a sea of grey objects, or a single large object\\ncatches one's eye. Exploiting pre-cognitive processing is desirable in\\na visualization system so that cognitive resources can be freed up for\\nother tasks. Many features can be preattentively processed, including\\nlength, orientation, contrast, curvature, shape, and hue\\n[TG88].\\nHowever, preattentive processing will work for only a\\nsingle feature in all but a few exceptional cases, so most searches\\ninvolving a conjunction of more than one feature are not\\npre-cognitive. For instance, a red square among red and green squares\\nand circles will not pop out, and can be discovered only by a much\\nslower conscious search process.\\n\\n1.3 Approach\\n\\nIn addition to the design principles of the previous section, our\\ninformation visualization approach differs from traditional graph\\ndrawing by our emphasis in three key areas: interactivity, domain\\nspecificity, and scalability.\\n\\n1.3.1 Interactivity\\n\\n\\xa0\\n\\nThe word  interaction is often used in  different contexts,\\nsometimes interchangeably with  animation. The following four\\nmeaning are often conflated, but in this thesis we only intend the\\nfirst three:\\n\\nNavigation\\n Interactive navigation consists of changing either the\\nviewpoint or the position of an object in a scene.\\n\\nMaking Choices\\n Interactivity is also common in non-navigational\\nsettings, for example through radio buttons on a control panel or menu\\nchoices that affect the display.\\n\\nAnimated Transitions\\n Viewers have a much easier time retaining\\ntheir mental model of an object if changes to its structure or its\\nposition are shown as smooth transitions instead of discrete jumps\\n[RCM93].\\n\\nVCR-style Animation\\n Many studies of multimedia\\napplications have compared user performance between still imagery and\\nprescripted animations where the user has start, pause, and stop\\ncontrols [MTB00].\\n\\n\\n\\nInteractivity is the great challenge and opportunity of computer-based\\nvisualization. Visual exposition has a long and successful historical\\ntradition, but until recently it was confined to static\\ntwo-dimensional\\nmedia such as paper. The invention of film and video led to\\nnew kinds of visual explanations that could take advantage of dynamic\\nanimation. The advent of computers sets the stage for designing\\ninteractive visualization systems of unprecedented power and\\nflexibility.\\n\\nThe most straightforward kind of interactive visualization system\\nmimics the real world. A two-dimensional interface can implement the\\nsemantics of paper by allowing panning and zooming. In three\\ndimensions, virtual objects can be manipulated like real objects with\\ncontrols for rotation, translation, and scaling. Literal interaction\\nis relatively well-understood: much of the cognitive psychology\\nliterature on graphical perception focuses on the kinds of displays\\nthat can be drawn equally well on a piece of paper as on a computer\\nscreen[CM84]. Most of the effective knowledge transfer\\nfrom cognitive psychology to visualization has been the theory of\\nperceptual dimensions and retinal variables for static 2D displays.\\n\\nComputers offer the possibility of moving beyond simple imitations of\\nreality, since we can tie user input to the visual display to get\\nsemantics impossible in the real world. For instance, distortion\\nmethods allow the user to see a large amount of context around a\\nchangeable area of focus, and multiscale methods result in the visual\\nappearance of an object changing radically based on distance from the\\nuser's virtual viewpoint. However, only a few of the cognitive principles\\ninvolving exotic semantics are understood: for instance, studies on\\nenvironmental [Gol87, Section 5.4.6: Cognitive Distance,] and\\nspatial [Tve92] cognition provide some evidence that\\nappropriate distortion is cognitively defensible. Moreover, perceptual\\nchannel taxonomies have been extended to nonstatic attributes such as\\nvelocity, direction, frequency, phase, and disparity [Gre98].\\nAlthough these studies provide some guidance, there is a huge\\nparameter space of possible interaction techniques that has not yet\\nbeen thoroughly analyzed. Two of the three design studies in this\\nthesis are forays into the parameter space of nonliteral interaction\\ntechniques.\\n\\n1.3.2 Domain and Task Focus\\n\\nA hallmark of many information visualization systems is a focus on the\\ntasks of a group of intended users in a particular domain. Methods\\nfrom user-centered design [ND86] and ethnography can help the\\nvisualization practitioner understand the workflow of a user group to\\nunderstand their high-level goals. For instance, the goals of\\nwebmasters would be to create and maintain a web site.\\n\\nHowever, these goals are too high level to directly address with\\nsoftware: they must be broken down into tasks at a lower level\\n[MT93]. In the webmaster example, a low-level task might be\\noptimizing end-user navigation by minimizing the number of hops\\nbetween the entry page and other pages deemed important by the site\\ndesigner, or finding and fixing broken links. Such tasks are specific\\nenough that a visualization designer can make decisions about\\nappropriate visual encodings, so that perceptual inferences can be\\nsubstituted for the more cognitively demanding logical inferences\\n[Cas91]. Finally, the low-level task breakdown provides a\\nhandle for evaluating the effectiveness of the resulting visualization\\nsystem.\\n\\n1.3.2.1 Evaluation\\n\\n\\xa0\\n\\nEvaluating a visualization system is much more difficult than\\nevaluating most graphics systems, because it is hard to judge whether\\nsome piece of software really helped somebody get something done\\nmore easily. Graphics software such as a rendering system can be\\nquantitatively evaluated on whether it is faster or more\\nphotorealistic than previous work. Some aspects of visualization are\\nsimilarly quantitative: the implicit (or explicit) assumption that a\\nprevious technique is effective allows researchers to argue that a new\\nalgorithm is better because it is faster or scales to larger datasets.\\n\\nHowever, the effectiveness criteria for a visualization system are far\\nless understood than the low-level psychophysics of human vision. One\\nway to document the effect of a visualization system is to mention the\\nsize of the user community that has chosen to adopt the software.\\nUser testing can be more rigorous, documenting not only whether people\\nliked it, but whether performance for a particular task improved. User\\ntesting range from informal usability observations in an iterative\\ndesign cycle to full formal studies designed to gather statistically\\nsignificant results. User studies are championed by some as the path\\nto scientific legitimacy, but are tricky to construct without\\nconfounding variables. Well-designed studies are a critical part\\nof the evaluation arsenal, but it is sometimes difficult to convince\\nothers that the positive results of a study merit high-level\\nconclusions about the validity of an approach. A less contentious use\\nof user testing is for fine-tuning a visualization system by exposing\\nthe best choice from among similar alternatives.\\nAnecdotal evidence of discoveries attributed by a user to insights\\nfrom a visualization system is important in cases where user studies\\nare infeasible because the target audience is small, or the task is\\nsomething long-term and serendipitous such as scientific discovery.\\n\\nFinally, an analysis which relates design choices to a  conceptual\\nframework is a powerful evaluation method. \\nSuch a conceptual analysis can be useful\\nboth as a means to evaluate the merits of a visualization system for a\\nparticular task, and to analyze what other tasks such a system might\\nbe well-suited to help with. Taxonomies and principles can help us go\\nbeyond simply asking  whether something helps by offering tools\\nto answer questions of  why and  how it helps\\n[SR96]. Several authors have presented such frameworks, in\\naddition to the taxonomies of Bertin, Cleveland, and Mackinlay\\ndiscussed in Section 1.2.1. Shneiderman has also been\\nactive in taxonomizing the field [Shn96], with his\\n``overview first, zoom and filter, then details-on-demand'' mantra.\\nWilkinson offers an elaborate framework based on a design grammar\\nthat evolved out of his experience in designing statistical graphics\\n[Wil99a]. Ware's recent textbook on information\\nvisualization provides a great deal of prescriptive advice based on a\\ndetailed analysis of the psychophysical and cognitive underpinnings of\\nhuman perception [War00].\\n\\nIn this thesis we draw on ideas from several of these frameworks in\\nour analytical evaluation of all three visualization systems. We also\\nevaluate our software systems using a combination of the preceding\\nmethods. The H3 system evaluation includes algorithmic improvements\\nover previous related techniques, a discussion of user adoption, and a\\nformal user study. The Planet Multicast project evaluation consists\\nmainly of anecdotal evidence. In the Constellation chapter we discuss\\nthe influence of our informal usability observations on the system\\ndesign, in addition to a heavy emphasis on the conceptual framework\\nanalysis.\\n\\n1.3.3 Scalability\\n\\n\\xa0\\n\\n\\nFigure 1.2: \\n System scalability and dataset size. Previous graph\\ndrawing systems, \\nshown in blue, fall far short of many large real-world datasets,\\nshown in green. The three systems in this thesis, shown in red,\\nstart to close this gap by aiming at datasets ranging from thousands\\nto over one hundred thousand nodes.\\n\\nVery small graphs can be laid out and drawn by hand, but automatic\\nlayout and drawing by a computer program can scale to much larger\\ngraphs, and provides the possibility of fluid interaction with\\nresulting drawings. The goal of these automatic graph layout systems\\nis to help humans understand the graph structure, as opposed to some\\nother context such as VLSI layout. Researchers have begun to codify\\naesthetic criteria of helpful drawings, such as minimizing edge\\ncrossings and emphasizing symmetry [BMK95,BT98,DC98,PCJ95,Pur97].\\n\\nHowever, almost all previous automatic graph drawing systems have been\\nlimited to small datasets. The scalability discrepancy between systems\\nfor nonvisual graph manipulation and those designed to create visual\\nrepresentations of them is attributable to the difficulty of general\\ngraph layout. Most useful operations for drawing general\\ngraphs have been proved to be NP-complete [Bra88]. Most\\nprevious systems are designed to create highly aesthetic layouts of\\ngeneral graphs. A paper in 1994 declared graphs of more than\\n128 nodes to be ``huge'' [FLM94]. More than one-half of the\\nexisting graph drawing systems handle only very small input graphs of\\nless than one hundred nodes [Döm94,FW94,Rei94,GT96]. A few exceptional systems such as Gem3D [BF95] and\\n dot [GKNV93] can handle hundreds or even a few thousand nodes.\\n\\nAlthough a few thousand or even a few hundred nodes is more than one\\nwould want to lay out by hand, Figure 1.2 shows that many\\nreal-world datasets are far larger. Backbone Internet routers have\\nover 70,000 other hosts in their routing tables, and the number of\\nhosts on the entire Internet is over 70 million and growing. Dictionaries\\ncontain millions of words defined in terms of each other. The Web\\nconsists of over a billion hyperlinked documents, and even\\nmoderately-sized single Web sites such as the Stanford graphics group\\nsite have over 100,000 documents.\\n\\n1.4 Contributions\\n\\nThe contributions of this thesis fall into two areas: analysis and\\nalgorithms.\\n\\nOur analytical contribution is a detailed analysis of three\\nspecialized systems for the interactive exploration of large graphs,\\nrelating the intended tasks to the spatial layout and visual encoding\\nchoices. Each of these three systems provides a completely different view of\\nthe graph structure, and we evaluate their efficacy for the intended\\ntask. We generalize these findings in our analysis of the importance\\nof interactivity and specialization for graph visualization systems\\nthat are effective and scalable.\\n\\nOur algorithmic contribution is two novel algorithms for specialized\\nlayout and drawing. The H3 algorithm trades off generality for\\nscalability, whereas the Constellation algorithm trades off generality\\nfor effectiveness. The H3 system for visualizing quasi-hierarchical\\ngraphs has the following advantages over previous work:\\n\\n a highly scalable layout algorithm that handles very large\\ndatasets quickly (over\\n100,000 nodes in 12 seconds)\\n a novel layout results in high information density without\\nclutter, by exploiting mathematical advantages of 3D hyperbolic space\\nfor a Focus+Context view\\n a guaranteed frame rate novel drawing algorithm that uses a combination\\nof graph-theoretic and viewpoint dependent information for a fluid\\ninteractive experience\\n\\n\\nThe Constellation system for visualizing paths through semantic\\nnetworks has the following features:\\n\\n a novel specialized layout highly tuned for effectiveness in a\\ncarefully documented task by visually encoding domain-specific\\nsemantics \\n a novel drawing algorithm tuned for maximum task-based label\\nreadability at multiple viewing levels\\n a novel interaction technique of selectively highlighting sets of\\nnodes and edges\\n the effective use of multiple perceptual channels to visually\\ndistinguish interactively chosen foreground layer from unobtrusive\\nbackground\\n\\n\\nThe H3 project was a solo undertaking. The Planet Multicast project was\\njoint work with Eric Hoffman, K. Claffy, and Bill Fenner. The\\nConstellation project was joint work with François Guimbretière.\\n\\n1.5 Thesis Organization\\n\\nThis thesis begins with motivation for the interactive visualization\\nof graphs and background material on information visualization,\\nfollowed by a summary of our original research contributions. We\\ndiscuss related work in Chapter 2.\\n\\nThe next three chapters discuss the software systems at the core of\\nthis thesis. Each chapter begins with a task analysis, then covers\\nspatial layout and visual encoding choices, and concludes with a\\ndiscussion of the results.\\n\\nThe H3 system for visualizing large quasi-hierarchical graphs in 3D\\nhyperbolic space is covered in Chapter 3. Parts of this\\nchapter were described in series of publications. We have published\\nthe H3 layout algorithm [Mun97] and the H3Viewer guaranteed\\nframe rate drawing algorithm [Mun98a]. We have also presented\\na brief overview of both the layout and drawing algorithms, augmented\\nwith a discussion of possible tasks that could benefit from a graph\\ndrawing system [Mun98b]. A paper that is in press describes the\\nthe user study that demonstrated statistically significant benefits of\\na browsing system incorporating the H3Viewer [RCMC00].\\n\\nChapter 4 describes Planet Multicast, a 3D geographic\\nsystem that displays the tunnel topology of the Internet's multicast\\nbackbone as arcs on a globe to help MBone maintainers find\\npotentially misconfigured long-distance tunnels. We have presented\\nthis system as a case study [MHCF96].\\n\\nChapter 5 is about the Constellation system for\\nvisualizing semantic networks using a custom 2D spatial layout. A\\nbrief paper described the key aspects of this visualization system\\n[MGR99].\\n\\nWe finish with discussion, future work, and conclusions in chapter\\n6.\\n\\n  Next: 2 Related Work\\nUp: Interactive Visualization of Large \\n Previous: List of Tables\\n  \\n \\n\\nTamara Munzner\\n\\n\\n\",\n",
       " \"\\n\\n\\nICSI Speech FAQ - 2.2 What are the basic approaches to speech recognition?\\n\\n\\nICSI Speech FAQ:\\n2.2 What are the basic approaches to speech recognition?\\nAnswer by: dpwe - 2000-07-22\\n\\n\\nEssentially all speech recognition systems use the same basic three-stage \\narchitecture: \\n\\n\\nFeature detection in which the raw acoustic waveform \\n\\tis rerepresented in a more useful space, typically a \\n\\tlow-dimensional feature space based on coarse spectral \\n\\tmeasurements over a 10-50ms time window.\\n  Probabilistic classification of the feature vectors, \\n\\tin which the frames are scored as looking more or less likely \\n\\tas versions of a number of predefined subword linguistic units.\\n  Search for best word-sequence hypothesis in which a\\n\\tword sequence is found that is consistent with the constraints of \\n\\tlexicon and grammar, and which corresponds to subword unit \\n\\tsequence that is highly-ranked in the classifier output.\\n\\n\\nThese stages are illustrated in the following overview block diagram:\\n\\n\\n\\nOf course, particular systems may blur the line between these stages, for \\ninstance by involving the subword likelihood estimation as a part of the \\nsearch for well-matched word sequences.\\n\\n\\nSystems can vary at any of these stages.  \\n\\n\\nFor feature extraction, there are any number of different \\n\\talgorithms to derive feature vectors from speech, differing \\n\\tin their ability to emphasize linguistically-relevant information \\n\\trelative to irrelevant information (such as speaker identity), \\n\\ttheir robustness to noise and distortion, and their ability \\n\\tto produce vectors that somehow make the job of classification \\n\\teasier.  However, since the result is, in all cases, a feature \\n\\tspace in which to make classification, varying the feature \\n\\textraction has little impact on the overall recognizer architecture;\\n\\tit merely affects the accuracy.\\n  Classification can be done by any of the techniques known to \\n\\tpattern recognition.  Early speech recognizers used vector \\n\\tquantization to reduce the speech features to a discrete \\n\\tset, then learned associations between particular vectors \\n\\tand particular subword units.  Modern systems learn \\n\\tcontinuous classification of the feature vector space, most \\n\\toften by parametric modeling of the distribution associated \\n\\twith each speech class, typically with Gaussian mixture models (GMMs).  \\n\\tA significant alternative is the use of neural networks to \\n\\tdiscriminatively classify a speech vector (or short sequence), \\n\\testimating the probability that it arises from each of the \\n\\tclasses.\\n  Hypothesis search is usually performed by the hidden Markov model (HMM), \\n\\ta formulation that expresses the constraints of pronunciation \\n\\t(lexicon or dictionary) and word sequence (grammar) in a single \\n\\tfinite-state network, for which efficient search and training \\n\\talgorithms are known.  The main alternative is \\n\\tdynamic time warping, the simpler predecessor to HMMs, \\n\\tin which template models are stretched and compressed in time \\n\\tto match the observed features.\\n\\n\\nAt ICSI we have historically used neural nets as our acoustic models \\n- the so-called 'hybrid connectionist' approach pioneered by Morgan and \\nBourlard - rather than the more common Gaussian mixture models.  For \\na discussion of why, see \\nthe next FAQ answer.\\n\\n\\n\\nPrevious: 2.1 What is speech recognition? - Next: 2.3 Why do we use connectionist rather than GMM?\\n\\nBack to ICSI Speech FAQ index\\n\\n\\nGenerated by build-faq-index on Tue Mar 25 20:52:27 PST 2003\\n\\n\\n\\n\",\n",
       " \"\\n\\nInternet Mathematics Contest \\n\\n\\nInternet Mathematics Contest \\n\\nThe first two contests are over. These are the details of the third\\ncontest.\\n\\nChallenging, interesting problems motivate much of my exploration of \\nmathematics. I have collected problems which should be suitable for a \\nwide range of those interested in mathematics. I am unsatisfied with the \\nquantity and formats of the most popular mathematics competitions, so I \\nhave decided to promote these problems by organizing a contest.\\n\\nWho is eligible:\\n\\n\\nAnyone without a degree in mathematics, computer science, or a natural\\nscience at the Bachelor's (4 year) level or higher is eligible. Further,\\nthose who do  have such a degree but are under 21 years of age as of\\nJanuary 1, 1998 are eligible to compete. \\n The prizes:  \\n At the moment I will guarantee USD500 for 1st prize. I'm looking for\\na sponsor. I reserve the right to declare that there is no winner if every\\nentry is unsatisfactory (scoring fewer points than the number of vertices\\nin the answer to the last problem), in which case I will donate the money\\nto charity. Please feel free to submit solutions and questions about the\\nproblems even if you do not feel you have a chance to win the\\ncontest.\\n\\nThe problems:  \\n\\nThe problems are drawn from draft 8 of my collection of motivational\\nproblems in mathematics. These are 60 problems of widely varying\\ndifficulties. Some are doable in under a minute while some are likely to\\ntake hours of thought. Some are phenomenological while others are\\nabstract. The statements of the problems are intentionally specific; more\\nadvanced or stronger students should find exploring the generalizations of\\nthe given problems challenging.\\n\\nI chose 20 of these problems to count toward the contest. These are not\\nnecessarily the hardest or most important of the problems, but I want\\nto cut down on the amount of time required. I hope that this does\\nnot discourage people from working on the other 40. This also\\nallows me to write up my solutions to some of the other problems\\nwithout worrying about being fair to all participants. \\n\\nThe contest\\nproblems are those numbered 3, 5, 7, 10, 13, 14, 19, 21, 24, 29, 33,\\n41, 42, 44, 46, 51, 52, 54, 57, and 59.\\n\\nScoring:\\n\\nI don't like arbitrary systems, so I'll try to make this simple and as \\neasy on me as possible: \\n1 point for each problem as stated submitted by the deadline. \\n1 point for a solid generalization. \\n1 point for a generalization on the level of original research. \\nThe level of proof is that you need to convince me that you can make \\nthe argument rigorous.\\n\\nThe deadline:\\n\\nSolutions must be sent to me by June 1, 1998. I welcome early, \\npartial submissions and will return those with feedback. I will try \\nto announce the winner as soon as possible after receiving the \\nentries; I don't know how many people will participate, so I cannot \\nestimate how long scoring will take.\\n\\nCollaboration and use of other sources:\\n\\nPlease use the library, internet, and discussions with mathematicians; a lot \\ncan be learned in this way. (Readers of sci.math might note that I have\\nposted a few solutions. I have also posted problems and solutions on my\\ndoor in Avery house.) You should indicate any non-textbook sources \\nin a scholarly fashion. Scoring standards will be raised accordingly, but \\nthe point is to learn. \\n\\nIf anything here needs clarification, if you need technical assistance \\ngetting the problems, or if you have any comments, please e-mail me at \\nzare@math.columbia.edu\\nor snail to \\nDouglas Zare\\n509 Mathematics\\nColumbia University\\nNew York, NY 10027\\nTo the \\nintroduction to the problems \\nAll\\nproblems/Contest\\nproblems\\n Back to my home \\npage.\\nLast modified 11/19/99.\\n\\n\",\n",
       " \"\\n\\n\\n\\nComputational Intelligence - Overhead Transparencies\\n\\n\\n\\nComputational\\nIntelligence\\nA Logical Approach\\nOverhead Transparencies\\n\\n This page contains transparencies from Poole, Mackworth and Goebel, Computational\\nIntelligence: A Logical Approach, Oxford University Press,\\n1998. All lecture materials are copyright © Poole, Mackworth,\\nGoebel, and Oxford University Press, 1997-2002. All Rights reserved. \\n These transparencies are in Adobe\\nPDF format and can be read using the free acrobat\\nreader or with recent versions of Ghostscript. You\\ncan also access the lectures through a pdf\\ninterface. Clicking on the chapter numbers in this file or on it\\nup-arrows in the slides gives a pdf overview of individual chapters. \\nYou can also get the latest distribution\\nof all of the slides as a gzipped\\ntar file. \\n We have divided the slides roughly into lectures. The division is\\nlargely on logical separation, rather than what can be carried out in\\none say 50 or 90 minute slot. We have found that one lecture here takes\\nbetween 30 and 100 minutes to explain in class (augmented with class\\ndiscussion and more detailed examples). We haven't attempted to cover\\nevery topic in these lectures; rather, we have attempted to give a\\ndeeper view of fewer topics. Revising these slides is an ongoing\\nactivity; we would appreciate any feedback\\nyou would like to give. \\n \\n\\nChapter 1: (html)\\nComputational Intelligence and Knowledge\\n\\nLecture 1 in which we introduce\\ncomputational intelligence and the role of agents. \\nLecture 2 in which we introduce the\\napplications domains. \\n\\nChapters 2 & 3: (html) A Representation and Reasoning System\\n& Using Definite Knowledge\\nThese two chapters are presented together as they form a coherent\\nwhole. They are separated in the book to keep the formalisms and the\\nmethodology separate.\\n\\nLecture 1 in which we introduce\\nrepresentation and reasoning systems, Datalog, its assumptions, and its\\nsyntax. \\nLecture 2 in which we present the\\nsemantics of ground (variable-free) Datalog. \\nLecture 3 in which we introduce\\nvariables, queries, answers, recursion, and limitations. \\nLecture 4 in which we introduce\\nproofs, present the ground bottom-up procedure, and show soundness and\\ncompleteness. \\nLecture 5 in which we introduce\\ntop-down proof procedure (SLD Resolution). \\nLecture 6 in which we introduce\\nvariables and function symbols and how they are handled in proof\\nprocedures. \\n\\nChapter 4: (html)\\nSearching\\n\\nLecture 1 in which we introduce\\nsearching and graphs. \\nLecture 2 in which we present some\\nblind search strategies. \\nLecture 3 in which we present\\nheuristic search, including best-first search and A* search. \\nLecture 4 in which we present various\\nrefinements to search strategies, including loop checking, multiple-path\\npruning, iterative deepening, bidirectional search, and dynamic\\nprogramming. (This will probably take two classes to cover). \\nLecture 5 in which we introduce\\nconstraint satisfaction problems. \\nLecture 6 in which we consider\\nconsistency algorithms (arc consistency) and hill climbing for solving\\nCSPs. \\n\\nChapter 5: (html)\\nRepresenting Knowledge\\n\\nLecture 1 in which we introduce\\nknowledge representation issues and problem specification. \\nLecture 2 in which we consider\\nrepresentation languages and mapping from problems into representations. \\nLecture 3 in which we present semantic\\nnetworks, frames, and property inheritance. \\n\\nChapter 6: (html)\\nKnowledge Engineering\\n\\nLecture 1 in which we introduce\\nknowledge-based systems architectures and ask-the-user mechanisms \\nLecture 2 in which we introduce\\nknowledge-based explanation and debugginf\\nLecture 3 .in which we introduce the\\nnotions of metalanguages and object languages and meta-interpreters., \\nLecture 4 in which we present more\\nsophisticated meta-interpreters. \\n\\nChapter 7: (html)\\nBeyond Definite Knowledge\\n\\nLecture 1 in which we cover equality,\\ninequality and the unique names assumption. \\nLecture 2 in which we cover the\\ncomplete knowledge assumption and negation as failure. \\nLecture 3 in which we introduce\\nintegrity constraints and consistency-based diagnosis. \\n\\nChapter 8: (html)\\nActions and Planning\\n\\nLecture 1 in which we introduce\\nactions and planning and the robot planning domain. \\nLecture 2 in which we present the\\nSTRIPS representation. \\nLecture 3 in which we present the\\nsituation calculus. \\nLecture 4 in which we introduce\\nplanning \\nLecture 5 in which we present the\\nSTRIPS planner. \\nLecture 6 in which we present\\nregression planner. \\n\\nChapter 9: (html)\\nAssumption-based Reasoning\\n\\nLecture 1 in which we introduce\\nassumption-based reasoning. \\nLecture 2 in which we show how to\\nreason with defaults. \\nLecture 3 in which we introduce\\nabduction and how it can be combined with default reasoning. \\nLecture 4 in which we show how to\\ncombine evidential and causal reasoning. \\n\\nChapter 10: (html)\\nUsing Uncertain Knowledge\\n\\nLecture 1 in which we overview\\nuncertainty and the role of probability. \\nLecture 2 in which we look at\\nconditional independence and the representation of belief networks. \\nLecture 3 in which we try to\\nunderstand the consequences of the independence assumptions in belief\\nnetworks. \\nLecture 4 in which we look at\\nprobabistic inference. \\nLecture 5 in which we look at\\ncombining probability and time. \\nLecture 6 in which we look at making\\ndecisions under uncertainty. \\n\\nChapter 11: (html)\\nLearning\\n\\nLecture 1 in which we introduce\\nmachine learning and the issues facing any learning algorithm. \\nLecture 2 in which we introduce\\ndecision tree learning \\nLecture 3 in which we introduce\\nneural networks. \\nLecture 4 in which we introduce\\ncase-based reasoning. \\nLecture 5 in which we present\\nlearning under uncertainty. \\n\\nChapter 12: (html)\\nBuilding Situated Robots\\n\\nLecture 1 in which we introduce\\nagents, robotic systems and robot controllers. \\nLecture 2 in which we overview robot\\narchitectures and present hierarchical decomposition of robots. \\n\\n\\n Last updated 3 September 2002, David Poole,\\npoole@cs.ubc.ca \\n\\n\\n\",\n",
       " '\\n\\n\\nAn Introduction to text-to-speech synthesis\\n\\n\\n\\n\\n\\nA Short Introduction to Text-to-Speech Synthesis\\n\\nby Thierry Dutoit\\nTTS research team, TCTS Lab.\\nAbstract\\nI try to give here a short but comprehensive introduction to state-of-the-art Text-To-Speech (TTS) synthesis by highlighting its Digital Signal Processing (DSP) and Natural Language Processing (NLP) components. As a matter of fact, since very few people associate a good knowledge of DSP with a comprehensive insight into NLP, synthesis mostly remains unclear, even for people working in either research area.\\nAfter a brief definition of a general TTS system and of its commercial applications, in Section 1, the paper is basically divided into two parts. Section 2.1 begins with a presentation of the many practical NLP problems which have to be solved by a TTS system. I then examine, in Section 2.2, how synthetic speech can be obtained by simply concatenating elementary speech units, and what choices have to be made for this operation to yield high quality. I finaly give a word on existing TTS solutions, with special emphasis on the computational and economical constraints which have to be kept in mind when designing TTS systems.\\nFor a much more detailed introduction to the subject, the reader is invited to refer to my recently published book on TTS synthesis(Dutoit, 1996)\\nFor a printable version of this text, see \\n\"High-Quality Text-to-Speech Synthesis : an Overview\", Journal of Electrical & Electronics Engineering, Australia: Special Issue on Speech Recognition and Synthesis, vol. 17 n°1, pp. 25-37.\\nIntroduction\\nA Text-To-Speech (TTS) synthesizer is a computer-based system that should be able to read any text aloud, whether it was directly introduced in the computer by an operator or scanned and submitted to an Optical Character Recognition (OCR) system. Let us try to be clear. There is a fundamental difference between the system we are about to discuss here and any other talking machine (as a cassette-player for example) in the sense that we are interested in the automatic production of new sentences. This definition still needs some refinements. Systems that simply concatenate isolated words or parts of sentences, denoted as Voice Response Systems, are only applicable when a limited vocabulary is required (typically a few one hundreds of words), and when the sentences to be pronounced respect a very restricted structure, as is the case for the announcement of arrivals in train stations for instance. In the context of TTS synthesis, it is impossible (and luckily useless) to record and store all the words of the language. It is thus more suitable to define Text-To-Speech as the automatic production of speech, through a grapheme-to-phoneme transcription of the sentences to utter. \\nAt first sight, this task does not look too hard to perform. After all, is not the human being potentially able to correctly pronounce an unknown sentence, even from his childhood ? We all have, mainly unconsciously, a deep knowledge of the reading rules of our mother tongue. They were transmitted to us, in a simplified form, at primary school, and we improved them year after year. However, it would be a bold claim indeed to say that it is only a short step before the computer is likely to equal the human being in that respect. Despite the present state of our knowledge and techniques and the progress recently accomplished in the fields of Signal Processing and Artificial Intelligence, we would have to express some reservations. As a matter of fact, the reading process draws from the furthest depths, often unthought of, of the human intelligence.\\n1. Automatic Reading : what for ?\\nEach and every synthesizer is the result of a particular and original imitation of the human reading capability, submitted to technological and imaginative constraints that are characteristic of the time of its creation. The concept of high quality TTS synthesis appeared in the mid eighties, as a result of important developments in speech synthesis and natural language processing techniques, mostly due to the emergence of new technologies (Digital Signal and Logical Inference Processors). It is now a must for the speech products family expansion.\\nPotential applications of High Quality TTS Systems are indeed numerous. Here are some examples :\\n\\nTelecommunications services. TTS systems make it possible to access textual information over the telephone. Knowing that about 70\\xa0% of the telephone calls actually require very little interactivity, such a prospect is worth being considered. Texts might range from simple messages, such as local cultural events not to miss (cinemas, theatres,... ), to huge databases which can hardly be read and stored as digitized speech. Queries to such information retrieval systems could be put through the user\\'s voice (with the help of a speech recognizer), or through the telephone keyboard (with DTMF systems). One could even imagine that our (artificially) intelligent machines could speed up the query when needed, by providing lists of keywords, or even summaries. In this connection, AT&T has recently organized a series of consumer tests for some promising telephone services [Levinson et al. 93]. They include : Who\\'s Calling (get the spoken name of your caller before being connected and hang up to avoid the call), Integrated Messaging (have your electronic mail or facsimiles being automatically read over the telephone), Telephone Relay Service (have a telephone conversation with speech or hearing impaired persons thanks to ad hoc text-to-voice and voice-to-text conversion), and Automated Caller Name and Address (a computerized version of the \"reverse directory\"). These applications have proved acceptable, and even popular, provided the intelligibility of the synthetic utterances was high enough. Naturalness was not a major issue in most cases.\\nLanguage education. High Quality TTS synthesis can be coupled with a Computer Aided Learning system, and provide a helpful tool to learn a new language. To our knowledge, this has not been done yet, given the relatively poor quality available with commercial systems, as opposed to the critical requirements of such tasks. \\nAid to handicapped persons. Voice handicaps originate in mental or motor/sensation disorders. Machines can be an invaluable support in the latter case : with the help of an especially designed keyboard and a fast sentence assembling program, synthetic speech can be produced in a few seconds to remedy these impediments. Astro-physician Stephen Hawking gives all his lectures in this way. The aforementioned Telephone Relay Service is another example. Blind people also widely benefit from TTS systems, when coupled with Optical Recognition Systems (OCR), which give them access to written information. The market for speech synthesis for blind users of personal computers will soon be invaded by mass-market synthesisers bundled with sound cards. DECtalk (TM) is already available with the latest SoundBlaster (TM) cards now, although not yet in a form useful for blind people.\\nTalking books and toys. The toy market has already been touched by speech synthesis. Many speaking toys have appeared, under the impulse of the innovative \\'Magic Spell\\' from Texas Instruments. The poor quality available inevitably restrains the educational ambition of such products. High Quality synthesis at affordable prices might well change this.\\nVocal Monitoring. In some cases, oral information is more efficient than written messages. The appeal is stronger, while the attention may still focus on other visual sources of information. Hence the idea of incorporating speech synthesizers in measurement or control systems.\\nMultimedia, man-machine communication. In the long run, the development of high quality TTS systems is a necessary step (as is the enhancement of speech recognizers) towards more complete means of communication between men and computers. Multimedia is a first but promising move in this direction.\\nFundamental and applied research. TTS synthesizers possess a very peculiar feature which makes them wonderful laboratory tools for linguists : they are completely under control, so that repeated experiences provide identical results (as is hardly the case with human beings). Consequently, they allow to investigate the efficiency of intonative and rhythmic models. A particular type of TTS systems, which are based on a description of the vocal tract through its resonant frequencies (its formants) and denoted as formant synthesizers, has also been extensively used by phoneticians to study speech in terms of acoustical rules. In this manner, for instance, articulatory constraints have been enlightened and formally described.\\n\\n2. How does a machine read\\xa0?\\nFrom now on, it should be clear that a reading machine would hardly adopt a processing scheme as the one naturally taken up by humans, whether it was for language analysis or for speech production itself. Vocal sounds are inherently governed by the partial differential equations of fluid mechanics, applied in a dynamic case since our lung pressure, glottis tension, and vocal and nasal tracts configuration evolve with time. These are controlled by our cortex, which takes advantage of the power of its parallel structure to extract the essence of the text read : its meaning. Even though, in the current state of the engineering art, building a Text-To-Speech synthesizer on such intricate models is almost scientifically conceivable (intensive research on articulatory synthesis, neural networks, and semantic analysis give evidence of it), it would result anyway in a machine with a very high degree of (possibly avoidable) complexity, which is not always compatible with economical criteria. After all, flies do not flap their wings !\\nFigure 1 introduces the functional diagram of a very general TTS synthesizer. As for human reading, it comprises a Natural Language Processing module (NLP), capable of producing a phonetic transcription of the text read, together with the desired intonation and rhythm (often termed as prosody), and a Digital Signal Processing module (DSP), which transforms the symbolic information it receives into speech. But the formalisms and algorithms applied often manage, thanks to a judicious use of mathematical and linguistic knowledge of developers, to short-circuit certain processing steps. This is occasionally achieved at the expense of some restrictions on the text to pronounce, or results in some reduction of the \"emotional dynamics\" of the synthetic voice (at least in comparison with human performances), but it generally allows to solve the problem in real time with limited memory requirements.\\n\\nFigure 1. A simple but general functional diagram of a TTS system.\\n2.1. The NLP component\\nFigure 2 introduces the skeleton of a general NLP module for TTS purposes. One immediately notices that, in addition with the expected letter-to-sound and prosody generation blocks, it comprises a morpho-syntactic analyser, underlying the need for some syntactic processing in a high quality Text-To-Speech system. Indeed, being able to reduce a given sentence into something like the sequence of its parts-of-speech, and to further describe it in the form of a syntax tree, which unveils its internal structure, is required for at least two reasons : \\n\\nAccurate phonetic transcription can only be achieved provided the part of speech category of some words is available, as well as if the dependency relationship between successive words is known.\\nNatural prosody heavily relies on syntax. It also obviously has a lot to do with semantics and pragmatics, but since very few data is currently available on the generative aspects of this dependence, TTS systems merely concentrate on syntax. Yet few of them are actually provided with full disambiguation and structuration capabilities.\\n\\n\\nFig 2. The NLP module of a general Text-To-Speech conversion system. \\n2.1.1. Text analysis\\nThe text analysis block is itself composed of :\\n\\n \\tA pre-processing module, which organizes the input sentences into manageable lists of words. It identifies numbers, abbreviations, acronyms and idiomatics and transforms them into full text when needed. An important problem is encountered as soon as the character level : that of punctuation ambiguity (including the critical case of sentence end detection). It can be solved, to some extent, with elementary regular grammars.\\n \\tA morphological analysis module, the task of which is to propose all possible part of speech categories for each word taken individually, on the basis of their spelling. Inflected, derived, and compound words are decomposed into their elementery graphemic units (their morphs) by simple regular grammars exploiting lexicons of stems and affixes (see the CNET TTS conversion program for French [Larreur et al. 89], or the MITTALK system [Allen et al. 87]).\\n \\tThe contextual analysis module considers words in their context, which allows it to reduce the list of their possible part of speech categories to a very restricted number of highly probable hypotheses, given the corresponding possible parts of speech of neighbouring words. This can be achieved either with n-grams [see Kupiec 92, Willemse & Gulikers 92, for instance], which describe local syntactic dependences in the form of probabilistic finite state automata (i.e. as a Markov model), to a lesser extent with mutli-layer perceptrons (i.e., neural networks) trained to uncover contextual rewrite rules, as in [Benello et al. 89], or with local, non-stochastic grammars provided by expert linguists or automatically inferred from a training data set with classification and regression tree (CART) techniques [Sproat et al. 92, Yarowsky 94].\\n \\tFinally, a syntactic-prosodic parser, which examines the remaining search space and finds the text structure (i.e. its organization into clause and phrase-like constituents) which more closely relates to its expected prosodic realization (see below).\\n\\n2.1.2. Automatic phonetization\\nA poem of the Dutch high school teacher and linguist G.N. Trenite surveys this problem in an amusing way. It desperately ends with :\\n\\n\\nFinally, which rimes with \"enough\",\\nThough, through, plough, cough, hough, or tough ?\\nHiccough has the sound of \"cup\",\\nMy advice is ... give it up !\\n\\n\\nThe Letter-To-Sound (LTS) module is responsible for the automatic determination of the phonetic transcription of the incoming text. It thus seems, at first sight, that its task is as simple as performing the equivalent of a dictionary look-up ! From a deeper examination, however, one quickly realizes that most words appear in genuine speech with several phonetic transcriptions, many of which are not even mentioned in pronunciation dictionaries. Namely : \\n\\nPronunciation dictionaries refer to word roots only. They do not explicitly account for morphological variations (i.e. plural, feminine, conjugations, especially for highly inflected languages, such as French), which therefore have to be dealt with by a specific component of phonology, called morphophonology. \\nSome words actually correspond to several entries in the dictionary, or more generally to several morphological analyses, generally with different pronunciations. This is typically the case of heterophonic homographs, i.e. words that are pronounced differently even though they have the same spelling, as for \\'record\\' (/rek\\x8dùd/ or /rIk\\x8dùd/), constitute by far the most tedious class of pronunciation ambiguities. Their correct pronunciation generally depends on their part-of-speech and most frequently contrasts verbs and non-verbs , as for \\'contrast\\' (verb/noun) or \\'intimate\\' (verb/adjective), although it may also be based on syntactic features, as for \\'read\\' (present/past)\\nPronunciation dictionaries merely provide something that is closer to a phonemic transcription than from a phonetic one (i.e. they refer to phonemes rather than to phones). As denoted by Withgott and Chen [1993] : \"while it is relatively straightforward to build computational models for morphophonological phenomena, such as producing the dictionary pronunciation of \\'electricity\\' given a baseform \\'electric\\', it is another matter to model how that pronunciation actually sounds\". Consonants, for example, may reduce or delete in clusters, a phenomenon termed as consonant cluster simplification, as in \\'softness\\' [s\\x81fnIs] in which [t] fuses in a single gesture with the following [n]. \\nWords embedded into sentences are not pronounced as if they were isolated. Surprisingly enough, the difference does not only originate in variations at word boundaries (as with phonetic liaisons), but also on alternations based on the organization of the sentence into non-lexical units, that is whether into groups of words (as for phonetic lengthening) or into non-lexical parts thereof (many phonological processes, for instance, are sensitive to syllable structure). \\nFinally, not all words can be found in a phonetic dictionary : the pronunciation of new words and of many proper names has to be deduced from the one of already known words.\\n\\nClearly, points 1 and 2 heavily rely on a preliminary morphosyntactic (and possibly semantic) analysis of the sentences to read. To a lesser extent, it also happens to be the case for point 3 as well, since reduction processes are not only a matter of context-sensitive phonation, but they also rely on morphological structure and on word grouping, that is on morphosyntax. Point 4 puts a strong demand on sentence analysis, whether syntactic or metrical, and point 5 can be partially solved by addressing morphology and/or by finding graphemic analogies between words. \\nIt is then possible to organize the task of the LTS module in many ways (Fig. 3), often roughly classified into dictionary-based and rule-based strategies, although many intermediate solutions exist.\\nDictionary-based solutions consist of storing a maximum of phonological knowledge into a lexicon. In order to keep its size reasonably small, entries are generally restricted to morphemes, and the pronunciation of surface forms is accounted for by inflectional, derivational, and compounding morphophonemic rules which describe how the phonetic transcriptions of their morphemic constituents are modified when they are combined into words. Morphemes that cannot be found in the lexicon are transcribed by rule. After a first phonemic transcription of each word has been obtained, some phonetic post-processing is generally applied, so as to account for coarticulatory smoothing phenomena. This approach has been followed by the MITTALK system [Allen et al. 87] from its very first day. A dictionary of up to 12,000 morphemes covered about 95% of the input words. The AT&T Bell Laboratories TTS system follows the same guideline [Levinson et al. 93], with an augmented morpheme lexicon of 43,000 morphemes [Coker 85]. \\nA rather different strategy is adopted in rule-based transcription systems, which transfer most of the phonological competence of dictionaries into a set of letter-to-sound (or grapheme-to-phoneme) rules. This time, only those words that are pronounced in such a particular way that they constitute a rule on their own are stored in an exceptions dictionary. Notice that, since many exceptions are found in the most frequent words, a reasonably small exceptions dictionary can account for a large fraction of the words in a running text. In English, for instance, 2000 words typically suffice to cover 70% of the words in text [Hunnicut 80]. \\nIt has been argued in the early days of powerful dictionary-based methods that they were inherently capable of achieving higher accuracy than letter-to-sound rules [Coker et al 90], given the availability of very large phonetic dictionaries on computers. On the other hand, considerable efforts have recently been made towards designing sets of rules with a very wide coverage (starting from computerized dictionaries and adding rules and exceptions until all words are covered, as in the work of Daelemans & van den Bosch [1993] or that of Belrhali et al [1992]). Clearly, some trade-off is inescapable. Besides, the compromise is language-dependent, given the obvious differences in the reliability of letter-to-sound correspondences for different languages. \\n\\nFig. 3. Dictionary-based (left) versus rule-based (right) phonetization. \\n2.1.3. Prosody generation\\nThe term prosody refers to certain properties of the speech signal which are related to audible changes in pitch, loudness, syllable length. Prosodic features have specific functions in speech communication (see Fig. 4). The most apparent effect of prosody is that of focus. For instance, there are certain pitch events which make a syllable stand out within the utterance, and indirectly the word or syntactic group it belongs to will be highlighted as an important or new component in the meaning of that utterance. The presence of a focus marking may have various effects, such as contrast, depending on the place where it occurs, or the semantic context of the utterance.\\n\\n\\nFig. 4. Different kinds of information provided by intonation (lines indicate pitch movements; solid lines indicate stress).\\t\\t\\t\\na. Focus or given/new information;\\t\\t\\t\\t\\t\\t\\nb. Relationships between words (saw-yesterday; I-yesterday; I-him)\\t\\nc. Finality (top) or continuation (bottom), as it appears on the \\t last syllable;\\t\\nd. Segmentation of the sentence into groups of syllables.\\n\\nAlthough maybe less obvious, there are other, more systematic or general functions. \\nProsodic features create a segmentation of the speech chain into groups of syllables, or, put the other way round, they give rise to the grouping of syllables and words into larger chunks. Moreover, there are prosodic features which indicate relationships between such groups, indicating that two or more groups of syllables are linked in some way. This grouping effect is hierarchical, although not necessarily identical to the syntactic structuring of the utterance.\\nSo what ? Does this mean that TTS systems are doomed to a mere robot-like intonation until a brilliant computational linguist announces a working semantic-pragmatic analyzer for unrestricted text (i.e. not before long) ? There are various reasons to think not, provided one accepts an important restriction on the naturalness of the synthetic voice, i.e. that its intonation is kept \\'acceptable neutral\\' : \\n\"Acceptable intonation must be plausible, but need not be the most appropriate intonation for a particular utterance : no assumption of understanding or generation by the machine need be made. Neutral intonation does not express unusual emphasis, contrastive stress or stylistic effects : it is the default intonation which might be used for an utterance out of context. (...) This approach removes the necessity for reference to context or world knowledge while retaining ambitious linguistic goals.\" [Monaghan 89]\\nThe key idea is that the \"correct\" syntactic structure, the one that precisely requires some semantic and pragmatic insight, is not essential for producing such a prosody [see also O\\'Shaughnessy 90]. \\nWith these considerations in mind, it is not surprising that commercially developed TTS system have emphasized coverage rather than linguistic sophistication, by concentrating their efforts on text analysis strategies aimed to segment the surface structure of incoming sentences, as opposed to their syntactically, semantically, and pragmatically related deep structure. The resulting syntactic-prosodic descriptions organize sentences in terms of prosodic groups strongly related to phrases (and therefore also termed as minor or intermediate phrases), but with a very limited amount of embedding, typically a single level for these minor phrases as parts of higher-order prosodic phrases (also termed as major or intonational phrases, which can be seen as a prosodic-syntactic equivalent for clauses) and a second one for these major phrases as parts of sentences, to the extent that the related major phrase boundaries can be safely obtained from relatively simple text analysis methods. In other words, they focus on obtaining an acceptable segmentation and translate it into the continuation or finality marks of Fig. 4.c, but ignore the relationships or contrastive meaning of Fig. 4.a and b.\\nLiberman and Church [1992], for instance, have recently reported on such a very crude algorithm, termed as the chinks \\'n chunks algorithm, in which prosodic phrases (which they call f-groups) are accounted for by the simple regular rule : \\na (minor) prosodic phrase = a sequence of chinks followed by a sequence of chunks\\t\\nin which chinks and chunks belong to sets of words which basically correspond to function and content words, respectively, with the difference that objective pronouns (like \\'him\\' or \\'them\\') are seen as chunks and that tensed verb forms (such as \\'produced\\') are considered as chinks. They show that this approach produces efficient grouping in most cases, slightly better actually than the simpler decomposition into sequences of function and content words, as shown in the example below : \\n\\n\\n\\nfunction words / content words\\n\\nchinks / chunks\\n\\n\\nI asked\\n\\nI asked them \\n\\n\\nthem if they were going home\\n\\nif they were going home\\n\\n\\nto Idaho\\n\\nto Idaho\\n\\n\\nand they said yes\\n\\nand they said yes\\n\\n\\nand anticipated\\n\\nand anticipated one more stop\\n\\n\\none more stop\\n\\nbefore getting home (6.7)\\n\\n\\nbefore getting home (6.6)\\n\\n\\xa0\\n\\n\\n\\n\\n\\xa0\\nOther, more sophisticated approaches include syntax-based expert systems as in the work of [Traber 93] or [Bachenko & Fitzpatrick 90], and automatic, corpus-based methods as with the classification and regression tree (CART) techniques of Hirschberg [1991].\\nOnce the syntactic-prosodic structure of a sentence has been derived, it is used to obtain the precise duration of each phoneme (and of silences), as well as the intonation to apply on them. This last step, however, is not straightforward either. It requires to formalize a lot of phonetic or phonological knowledge, either obtained from experts or automatically acquired from data with statistical methods. More information on this can be found in [Dutoit 96].\\n2.2. The DSP component\\nIntuitively, the operations involved in the DSP module are the computer analogue of dynamically controlling the articulatory muscles and the vibratory frequency of the vocal folds so that the output signal matches the input requirements. In order to do it properly, the DSP module should obviously, in some way, take articulatory constraints into account, since it has been known for a long time that phonetic transitions are more important than stable states for the understanding of speech [Libermann 59]. This, in turn, can be basically achieved in two ways :\\n\\nExplicitly, in the form of a series of rules which formally describe the influence of phonemes on one another;\\nImplicitly, by storing examples of phonetic transitions and co-articulations into a speech segment database, and using them just as they are, as ultimate acoustic units (i.e. in place of phonemes).\\n\\nTwo main classes of TTS systems have emerged from this alternative, which quickly turned into synthesis philosophies given the divergences they present in their means and objectives : synthesis-by-rule and synthesis-by-concatenation.\\n2.2.1. Rule-based synthesizers\\nRule-based synthesizers are mostly in favour with phoneticians and phonologists, as they constitute a cognitive, generative approach of the phonation mechanism. The broad spreading of the Klatt synthesizer [Klatt 80], for instance, is principally due to its invaluable assistance in the study of the characteristics of natural speech, by analytic listening of rule-synthesized speech. What is more, the existence of relationships between articulatory parameters and the inputs of the Klatt model make it a practical tool for investigating physiological constraints [Stevens 90].\\nFor historical and practical reasons (mainly the need for a physical interpretability of the model), rule synthesizers always appear in the form of formant synthesizers. These describe speech as the dynamic evolution of up to 60 parameters [Stevens 90], mostly related to formant and anti-formant frequencies and bandwidths together with glottal waveforms. Clearly, the large number of (coupled) parameters complicates the analysis stage and tends to produce analysis errors. What is more, formant frequencies and bandwidths are inherently difficult to estimate from speech data. The need for intensive trials and errors in order to cope with analysis errors, makes them time-consuming systems to develop (several years are commonplace). Yet, the synthesis quality achieved up to now reveals typical buzzyness problems, which originate from the rules themselves : introducing a high degree of naturalness is theoretically possible, but the rules to do so are still to be discovered. \\nRule-based synthesizers remain, however, a potentially powerful approach to speech synthesis. They allow, for instance, to study speaker-dependent voice features so that switching from one synthetic voice into another can be achieved with the help of specialized rules in the rule database. Following the same idea, synthesis-by-rule seems to be a natural way of handling the articulatory aspects of changes in speaking styles (as opposed to their prosodic counterpart, which can be accounted for by concatenation-based synthesizers as well). No wonder then that it has been widely integrated into TTS systems (MITTALK [Allen et al. 87] and the JSRU synthesizer [Holmes et al. 64] for English, the multilingual INFOVOX system [Carlson et al. 82], and the I.N.R.S system [O\\'Shaughnessy 84] for French).\\n2.2.2. Concatenative synthesizers\\nAs opposed to rule-based ones, concatenative synthesizers possess a very limited knowledge of the data they handle : most of it is embedded in the segments to be chained up. This clearly appears in figure 6, where all the operations that could indifferently be used in the context of a music synthesizer (i.e. without any explicit reference to the inner nature of the sounds to be processed) have been grouped into a sound processing block, as opposed to the upper speech processing block whose design requires at least some understanding of phonetics.\\nDatabase preparation\\nA series of preliminary stages have to be fulfilled before the synthesizer can produce its first utterance. At first, segments are chosen so as to minimize future concatenation problems. A combination of diphones (i.e. units that begin in the middle of the stable state of a phone and end in the middle of the following one), half-syllables, and triphones (which differ from diphones in that they include a complete central phone) are often chosen as speech units, since they involve most of the transitions and co-articulations while requiring an affordable amount of memory. When a complete list of segments has emerged, a corresponding list of words is carefully completed, in such a way that each segment appears at least once (twice is better, for security). Unfavourable positions, like inside stressed syllables or in strongly reduced (i.e. over-co-articulated) contexts, are excluded. A corpus is then digitally recorded and stored, and the elected segments are spotted, either manually with the help of signal visualization tools, or automatically thanks to segmentation algorithms, the decisions of which are checked and corrected interactively. A segment database finally centralizes the results, in the form of the segment names, waveforms, durations, and internal sub-splittings. In the case of diphones, for example, the position of the border between phones should be stored, so as to be able to modify the duration of one half-phone without affecting the length of the other one.\\n\\nFigure 5. A general concatenation-based synthesizer. The upper left hatched block corresponds to the development of the synthesizer (i.e. it is processed once for all). Other blocks correspond to run-time operations. Language-dependent operations and data are indicated by a flag.\\nSegments are then often given a parametric form, in the form of a temporal sequence of vectors of parameters collected at the output of a speech analyzer and stored in a parametric segment database. The advantage of using a speech model originates in the fact that :\\n\\nWell chosen speech models allow data size reduction, an advantage which is hardly negligible in the context of concatenation-based synthesis given the amount of data to be stored. Consequently, the analyzer is often followed by a parametric speech coder.\\nA number of models explicitly separate the contributions of respectively the source and the vocal tract, an operation which remains helpful for the pre-synthesis operations\\xa0: prosody matching and segments concatenation.\\n\\n\\tIndeed, the actual task of the synthesizer is to produce, in real-time, an adequate sequence of concatenated segments, extracted from its parametric segment database and the prosody of which has been adjusted from their stored value, i.e. the intonation and the duration they appeared with in the original speech corpus, to the one imposed by the language processing module. Consequently, the respective parts played by the prosody matching and segments concatenation modules are considerably alleviated when input segments are presented in a form that allows easy modification of their pitch, duration, and spectral envelope, as is hardly the case with crude waveform samples. \\nSince segments to be chained up have generally been extracted from different words, that is in different phonetic contexts, they often present amplitude and timbre mismatches. Even in the case of stationary vocalic sounds, for instance, a rough sequencing of parameters typically leads to audible discontinuities. These can be coped with during the constitution of the synthesis segments database, thanks to an equalization in which related endings of segments are imposed similar amplitude spectra, the difference being distributed on their neighbourhood. In practice, however, this operation, is restricted to amplitude parameters : the equalization stage smoothly modifies the energy levels at the beginning and at the end of segments, in such a way as to eliminate amplitude mismatches (by setting the energy of all the phones of a given phoneme to their average value). In contrast, timbre conflicts are better tackled at run-time, by smoothing individual couples of segments when necessary rather than equalizing them once for all, so that some of the phonetic variability naturally introduced by co-articulation is still maintained. In practice, amplitude equalization can be performed either before or after speech analysis (i.e. on crude samples or on speech parameters).\\nOnce the parametric segment database has been completed, synthesis itself can begin.\\nSpeech synthesis\\nA sequence of segments is first deduced from the phonemic input of the synthesizer, in a block termed as segment list generation in Fig. 5, which interfaces the NLP and DSP modules. Once prosodic events have been correctly assigned to individual segments, the prosody matching module queries the synthesis segment database for the actual parameters, adequately uncoded, of the elementary sounds to be used, and adapts them one by one to the required prosody. The segment concatenation block is then in charge of dynamically matching segments to one another, by smoothing discontinuities. Here again, an adequate modelization of speech is highly profitable, provided simple interpolation schemes performed on its parameters approximately correspond to smooth acoustical transitions between sounds. The resulting stream of parameters is finally presented at the input of a synthesis block, the exact counterpart of the analysis one. Its task is to produce speech.\\nSegmental quality\\nThe efficiency of concatenative synthesizers to produce high quality speech is mainly subordinated to :\\n\\nThe type of segments chosen. \\nSegments should obviously exhibit some basic properties : \\n\\nThey should allow to account for as many co-articulatory effects as possible. \\nGiven the restricted smoothing capabilities of the concatenation block, they should be easily connectable.\\nTheir number and length should be kept as small as possible. \\nOn the other hand, longer units decrease the density of concatenation points, therefore providing better speech quality. Similarly, an obvious way of accounting for articulatory phenomena is to provide many variants for each phoneme. This is clearly in contradiction with the limited memory constraint. Some trade-off is necessary. Diphones are often chosen. They are not too numerous (about 1200 for French, including lots of phoneme sequences that are only encountered at word boundaries, for 3 minutes of speech, i.e. approximately 5 Mbytes of 16 bits samples at 16 kHz) and they do incorporate most phonetic transitions. No wonder then that they have been extensively used. They imply, however, a high density of concatenation points (one per phoneme), which reinforces the importance of an efficient concatenation algorithm. Besides, they can only partially account for the many co-articulatory effects of a spoken language, since these often affect a whole phone rather than just its right or left halves independently. Such effects are especially patent when somewhat transient phones, such as liquids and (worst of all) semi-vowels, are to be connected to each other. Hence the use of some larger units as well, such as triphones.\\n\\n\\nThe model of speech signal, to which the analysis and synthesis algorithms refer. \\n\\nThe models used in the context of concatenative synthesis can be roughly classified into two groups, depending on their relationship with the actual phonation process. Production models provide mathematical substitutes for the part respectively played by vocal folds, nasal and vocal tracts, and by the lips radiation. Their most representative members are Linear Prediction Coding (LPC) synthesizers [Markel & Gray 76], and the formant synthesizers we mentioned in section 2.2.1. On the contrary, phenomenological models intentionally discard any reference to the human production mechanism. Among these pure digital signal processing tools, spectral and time-domain approaches are increasingly encountered in TTS systems. Two leading such models exist : the hybrid Harmonic/Stochastic (H/S) model of [Abrantes et al. 91] and the Time-Domain Pitch-Synchronous-OveraLap-Add (TD-PSOLA) one [Moulines & Charpentier 90]. The latter is a time-domain algorithm : it virtually uses no speech explicit speech model. It exhibits very interesting practical features : a very high speech quality (the best currently available) combined with a very low computational cost (7 operations per sample on the average). The hybrid Harmonic/stochastic model is intrinsically more powerful than the TD-PSOLA one, but it is also about ten times more computationally intensive. PSOLA synthesizers are now widely used in the speech synthesis community. The recently developed MBROLA algorithm [Dutoit 93,96] even provides a time-domain algorithm which exhibits the very efficient smoothing capabilities of the H/S model (for the spectral envelope mismatches that cannot be avoided at concatenation points) as well as its very high data compression ratios (up to 10 with almost no additional computational cost) while keeping the computational complexity of PSOLA.\\n3. Conclusion\\nLet us bow to the facts : there is still a long way to HAL, the brilliant talking computer of \\'2001, a space odyssey\\'. A number of advances in the area of NLP or DSP, however, have recently boosted up the quality and naturalness of available voices, and this is likely to continue. Important issues need now be further addressed in that purpose. Among others :\\n\\nHow to best account for coarticulatory phenomena ? In the context of concatenation-based synthesis, this question mostly reduces to : how to derive optimized sets of segments from speech data ? \\nHow to best formalize the relationship between syntax, semantics, pragmatics and prosody, and how to derive natural sounding intonation and duration from abstract prosodic patterns\\xa0? \\nA fundamental feature of speech has seldom been taken into consideration by TTS systems : its variability. Prosodic patterns, for instance, are submitted to a particular kind of variability which cannot be confused with randomness in that variations maintain some hidden coherency with each other. \\nHow to account for speaker and speaking style effects ? \\n\\nReaders willing to have a deeper understanding of the problems mentioned in this paper could advantageously report to the forthcoming [Dutoit 96], which analyses DSP and NLP solutions with much more details. A number of internet sites can also be consulted, some of which propose demo programs and/or speech files. See for example the speech synthesis virtual museum at URL address :\\n http://www.cs.bham.ac.uk/~jpi/synth/museum.html\\nExecutable versions of the aforementioned MBROLA synthesizer can be downloaded, together lots of language databases to test it, from URL address\\xa0: \\n http://tcts.fpms.ac.be/synthesis\\nReferences\\n[Abrantes et al. 91]\\xa0\\xa0\\xa0A.J. ABRANTES, J.S. MARQUES, I.M. TRANSCOSO, \"Hybrid Sinusoïdal Modeling of Speech without Voicing Decision\", EUROSPEECH 91, pp. 231-234.\\n[Allen 85]\\xa0\\xa0\\xa0J. ALLEN, \"A Perspective on Man-Machine Communication by Speech\", Proceedings of the IEEE, vol. 73, n°11, November 1985, pp. 1541-1550.\\n[Allen et al. 87]\\xa0\\xa0\\xa0J. ALLEN, S. HUNNICUT, D. KLATT, From Text To Speech, The MITTALK System, Cambridge University Press, 1987, 213 pp.\\n[Bachenko & Fitzpatrick 90]\\xa0\\xa0\\xa0J. BACHENKO, E. Fitzpatrick, \"Acomputational grammar of discourse-neutral prosodic phrasing in English\", Computational Linguistics, n°16, September 1990, pp. 155-167.\\n[Belrhali et al. 94]\\xa0\\xa0R. BELRHALI, V. AUBERGE, L.J. BOE, \"From lexicon to rules : towards a descriptive method of French text-to-phonetics transcription\", Proc. ICSLP 92, Alberta, pp. 1183-1186.\\n[Benello et al. 88]\\xa0\\xa0\\xa0J. BENELLO, A.W. MACKIE, J.A. ANDERSON, \"Syntactic category disambiguation with neural networks\", Computer Speech and Language, 1989, n°3, pp. 203-217.\\n[Carlson et al. 82]\\xa0\\xa0\\xa0R. CARLSON, B. GRANSTRÖM, S. HUNNICUT, \"A multi-language Text-To-Speech module\", ICASSP 82, Paris, vol. 3, pp. 1604-1607.\\n[Coker 85]\\xa0\\xa0\\xa0C.H. COKER, \"A Dictionary-Intensive Letter-to-Sound Program\", J. Ac. Soc. Am., suppl. 1, n°78, 1985, S7.\\n[Coker et al. 90]\\xa0\\xa0\\xa0C.H. COKER, K.W. CHURCH, M.Y. LIBERMAN, \"Morphology and rhyming : Two powerful alternatives to letter-to-sound rules for speech synthesis\", Proc. of the ESCA Workshop on Speech Synthesis, Autrans (France), 1990, pp. 83-86.\\n[Daelemans & van den Bosch 93]\\xa0\\xa0W. DAELEMANS, A. VAN DEN BOSCH, \"TabTalk : Reusability in data-oriented grapheme-to-phoneme conversion\", Proc. Eurospeech 93, Berlin, pp. 1459-1462.\\n[Dutoit 93] T. DUTOIT, H. LEICH, \"MBR-PSOLA : Text-To-Speech Synthesis based on an MBE Re-Synthesis of the Segments Database\", Speech Communication, Elsevier Publisher, November 1993, vol. 13, n°3-4.\\n[Dutoit 96]\\xa0\\xa0\\xa0T. DUTOIT, An Introduction to Text-To-Speech Synthesis¸ Kluwer Academic Publishers, 1996, 326 pp.\\n[Flanagan 72]\\xa0\\xa0\\xa0J.L. FLANAGAN, Speech Analysis, Synthesis, and Perception, Springer Verlag, 1972, pp. 204-210.\\n[Hirschberg 91]\\xa0\\xa0\\xa0J. HIRSCHBERG, \"Using text analysis to predict intonational boundaries\", Proc. Eurospeech 91, Genova, pp. 1275-1278.\\n[Holmes et al. 64]\\xa0\\xa0\\xa0J. HOLMES, I. MATTINGLY, J. SHEARME, \\'Speech synthesis by rule\\', Language and Speech, Vol 7, 1964, pp.127-143\\n[Hunnicut 80]\\xa0\\xa0\\xa0S. HUNNICUT, \"Grapheme-to-Phoneme rules : a Review\", Speech Transmission Laboratory, Royal Institute of Technology, Stockholm, Sweden, QPSR 2-3, pp. 38-60.\\n[Klatt 80]\\xa0\\xa0\\xa0D.H. KLATT, \\'Software for a cascade /parallel formant synthesizer\\', J. Acoust. Soc. AM., Vol 67, 1980, pp. 971-995. \\n[Klatt 86]\\xa0\\xa0\\xa0D.H. KLATT, \"Text-To-Speech : present and future\", Proc. Speech Tech \\'86, pp. 221-226.\\n[Kupiec 92]\\xa0\\xa0\\xa0J. KUPIEC, \"Robust part-of-speech tagging using a Hidden Markov Model\", Computer Speech and Language, 1992, n°6, pp. 225-242.\\n[Larreur et al. 89]\\xa0\\xa0\\xa0D. LARREUR, F. EMERARD, F. MARTY, \"Linguistic and prosodic processing for a text-to-speech synthesis system\", Proc. Eurospeech 89, Paris, pp. 510-513.\\n[Levinson et al. 93]\\xa0\\xa0\\xa0S.E. LEVINSON, J.P. OLIVE, J.S. TSCHIRGI, \"Speech Synthesis in Telecommunications\", IEEE Communications Magazine, November 1993, pp. 46-53.\\n[Liberman & Church 92]\\xa0\\xa0\\xa0M.J. LIBERMAN, K.W. CHURCH, \"Text analysis and word pronunciation in text-to-speech synthesis\", in Advances in Speech Signal Processing, S. Furuy, M.M. Sondhi eds., Dekker, New York, 1992, pp.791-831.\\n[Lingaard 85]\\xa0\\xa0\\xa0R. LINGAARD, Electronic synthesis of speech, Cambridge University Press, 1985, pp 1-17.\\n[Markel & Gray 76] J.D. MARKEL, A.H. GRAY Jr, Linear Prediction of Speech, Springer Verlag, New York, pp. 10-42, 1976.\\n[Monaghan 90a]\\xa0\\xa0\\xa0A.I.C. MONAGHAN, \"A multi-phrase parsing strategy for unrestricted text\", Proc. ESCA Workshop on speech synthesis, Autrans, 1990, pp. 109-112.\\n[Moulines & Charpentier 90]\\xa0\\xa0\\xa0E.MOULINES, F. CHARPENTIER, \"Pitch Synchronous waveform Processing techniques for Text-To-Speech Synthesis using diphones\", Speech Communication, Vol. 9, n°5-6.\\n[O\\' Shaughnessy 84]\\xa0\\xa0\\xa0D. O\\' SHAUGHNESSY, \\'Design of a real-time French text-to-speech system\\', Speech Communication, Vol 3, pp. 233-243.\\n[O\\'Shaughnessy 90]\\xa0\\xa0\\xa0D. O\\'SHAUGHNESSY, \"Relationships between syntax and prosody for speech synthesis\", Proceedings of the ESCA tutorial day on speech synthesis, Autrans, 1990, pp. 39-42.\\n[Sproat et al. 92]\\xa0\\xa0\\xa0R. SPROAT, J. HIRSHBERG, D. YAROWSKY, \"A Corpus-based Synthesizer\", Proc. ICSLP 92 Alberta, pp. 563-566.\\n[Stevens 90]\\xa0\\xa0\\xa0K.N. STEVENS, \\'Control parameters for synthesis by rule\\', Proceedings of the ESCA tutorial day on speech synthesis, Autrans, 25 sept 90, pp. 27-37.\\n[Traber 93]\\xa0\\xa0\\xa0C. TRABER, \"Syntactic Processing and Prosody Control in the SVOX TTS System for German\", Proc. Eurospeech 93, Berlin, vol. 3, pp. 2099-2102.\\n[Willemse & Gulikers 92]\\xa0\\xa0\\xa0R. WILLEMSE, L. GULIKERS, \"Word class assignment in a Text-To-Speech system\", Proc. Int. Conf. on Spoken Language Processing, Alberta, 1992, pp. 105-108.\\n[Withgott & Chen 93] M. M. WITHGOTT, F.R. CHEN, Computational models of American English, CSLI Lecture Notes, n°32, 143pp.\\n[Witten 82]\\xa0\\xa0\\xa0I.H. WITTEN, Principles of Computer Speech, Academic Press, 1992, 286 pp.\\n[Yarowsky 94]\\xa0\\xa0\\xa0D. YAROWSKY, \"Homograph Disambiguation in Speech Synthesis\\'\\', Proceedings, 2nd ESCA/IEEE Workshop on Speech Synthesis, New Paltz, NY, 1994.\\n\\xa0\\n\\nLast updated December 17, 1999, send comments to dutoit@tcts.fpms.ac.be \\n\\n\\n\\n\\n',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\nREIS Project: Standardisation in Indexing, Searching and Information Retrieval\\n\\n\\nStandardisation\\nin Indexing, Searching and Information Retrieval\\nRecent developments in Indexing, Searching and Information\\nRetrieval Technologies\\n\\n\\n\\xa0\\n\\n\\nW3C Work: HTML/XML\\nIETF Work: Common Indexing Protocol\\nIETF work: other standards\\nMetadata and XML/RDF\\n\\n\\nStandardisation\\n\\nMetadata/RDF Resources and Publications\\n\\nXML Searching\\n\\n\\nOther Sections\\n\\n\\nStandardisation\\n\\nSearch Engines\\n\\nSubject Gateways\\n\\nAutomatic Classification\\n\\nOther SE related technologies\\n\\nExpert Systems\\n\\nSE Business News\\n\\nDirectories Business news\\n\\nIndex page\\n\\n\\nThis page is updated regularly, please send your suggestions to: demchenko@terena.nl\\n\\n\\nW3C Work: HTML/XML\\nW3C Web Content accessibility initiative (WAI)\\nWeb Content accessibility Guidelines\\nhttp://www.w3.org/TR/WAI-WEBCONTENT\\nWeb Architecture: Describing and Exchanging Data\\nW3C Note 7 June 1999\\nhttp://www.w3.org/1999/04/WebData\\nBuilding a space where automated agents can contribute - just beginning\\nto build the Semantic Web. The RDF Schema design and XML Schema design\\nbegan independently, proposed common model where they fit together as interlocking\\npieces of the semantic web technology.\\nComposite Capability/Preference Profiles (CC/PP): A user side framework\\nfor content negotiation\\nW3C Note 27 July 1999\\nhttp://www.w3.org/TR/NOTE-CCPP/\\nIn this note we describe a method for using RDF, the Resource Description\\nFormat of the W3C, to create a general, yet extensible framework for describing\\nuser preferences and device capabilities. This information can be provided\\nby the user to servers and content providers. The servers can use this\\ninformation describing the user\\'s preferences to customize the service\\nor content provided. The ability of RDF to reference profile information\\nvia URLs assists in minimizing the number of network transactions required\\nto adapt content to a device, while the framework fits well into the current\\nand future protocols being developed a the W3C and the WAP Forum.\\nInternational Layout\\nW3C Working Draft 26-July-1999\\nhttp://www.w3.org/TR/WD-i18n-format/\\nThe following specification extends CSS to support East Asian and Bi-directional\\ntext formatting.\\nPlatform for Privacy Preferences (P3P) Specification\\nW3C Working Draft 7 April 1999\\nhttp://www.w3.org/TR/WD-P3P/\\nThis document describes the Platform for Privacy Preferences (P3P).\\nP3P enables Web sites to express their privacy practices and enables users\\nto exercise preferences over those practices.\\nPOIX: Point Of Interest eXchange Language Specification\\nW3C Note - 24 June 1999\\nhttp://www.w3.org/TR/poix/\\nThe \"POIX\" proposed here defines a general-purpose specification language\\nfor describing location information, which is an application of XML (Extensible\\nMarkup Language). POIX is a common baseline for exchanging location data\\nvia e-mail and embedding location data in HTML and XML documents. This\\nspecification can be used by mobile device developers, location-related\\nservice providers, and server software developers.\\nAnnotation of Web Content for Transcoding\\nW3C Note 10 July 1999\\nhttp://www.w3.org/TR/annot/\\nThis proposal presents annotations that can be attached to HTML/XML\\ndocuments to guide their adaptation to the characteristics of diverse information\\nappliances. It also provides a vocabulary for transcoding, and syntax of\\nthe language for annotating Web content. Used in conjunction with device\\ncapability information, style sheets, and other mechanisms, these annotations\\nenable a high quality user experience for users who are accessing Web content\\nfrom information appliances.\\nXML Schema Part 1: Structures\\nW3C Working Draft 6-May-1999\\nhttp://www.w3.org/TR/xmlschema-1/\\nXML Schema: Structures is part one of a two part draft of the specification\\nfor the XML Schema definition language. This document proposes facilities\\nfor describing the structure and constraining the contents of XML 1.0 documents.\\nThe schema language, which is itself represented in XML 1.0, provides a\\nsuperset of the capabilities found in XML 1.0 document type definitions\\n(DTDs.).\\nXML Schema Part 2: Datatypes\\nWorld Wide Web Consortium Working Draft 06-May-1999\\nhttp://www.w3.org/TR/xmlschema-2/\\nThis document specifies a language for defining datatypes to be used\\nin XML Schemas and, possibly, elsewhere.\\nXHTML&trade; 1.0: The Extensible HyperText Markup Language\\nA Reformulation of HTML 4.0 in XML 1.0\\nW3C Working Draft 5th May 1999\\nhttp://www.w3.org/TR/xhtml1/\\nThis specification defines XHTML 1.0, a reformulation of HTML 4.0 as\\nan XML 1.0 application, and three DTDs corresponding to the ones defined\\nby HTML 4.0. The semantics of the elements and their attributes are defined\\nin the W3C Recommendation for HTML 4.0. These semantics provide the foundation\\nfor future extensibility of XHTML. Compatibility with existing HTML user\\nagents is possible by following a small set of guidelines.\\nDocument Object Model (DOM) Level 2 Specification\\nVersion 1.0\\nW3C Working Draft 19 July, 1999\\nThis specification defines the Document Object Model Level 2, a platform-\\nand language-neutral interface that allows programs and scripts to dynamically\\naccess and update the content, structure and style of documents. The Document\\nObject Model Level 2 builds on the Document Object Model Level 1 (http://www.w3.org/TR/REC-DOM-Level-1\\n).\\nThis release of the Document Object Model Level 2 has all of the interfaces\\nthat the final version is expected to have. It contains interfaces for\\ncreating a document, importing a node from one document to another, supporting\\nXML namespaces, associating stylesheets with a document, the Cascading\\nStyle Sheets object model, the Range object model, filters and iterators,\\nand the Events object model. The DOM WG wants to get feedback on these,\\nand especially on the two options presented for XML namespaces, so that\\nfinal decisions can be made for the DOM Level 2 specification.\\nIBM online XML education courses\\nhttp://www2.software.ibm.com/developer/education.nsf/xml-onlinecourse-bytitle\\nIETF Work: Common Indexing Protocol\\nRFC 2651: The Architecture of the Common Indexing Protocol (CIP)\\nJ. Allen, M. Mealling\\nftp://ftp.isi.edu/in-notes/rfc2651.txt\\nThis document describes the CIP framework, including its architecture\\nand the protocol specifics of exchanging indices.\\nRFC 2652: MIME Object Definitions for the Common Indexing Protocol (CIP)\\nJ. Allen, M. Mealling\\nftp://ftp.isi.edu/in-notes/rfc2652.txt\\nThis document describes the definitions of those objects as well as\\nthe methods and requirements needed to define a new index type.\\nRFC 2653: CIP Transport Protocols\\nJ. Allen, P. Leach, R. Hedberg\\nftp://ftp.isi.edu/in-notes/rfc2653.txt\\nThis document specifies three protocols for transporting CIP requests,\\nresponses and index objects, utilizing TCP, mail, and HTTP.\\nRFC 2654: A Tagged Index Object for use in the Common Indexing Protocol\\nR. Hedberg, B. Greenblatt, R. Moats, M. Wahl\\nftp://ftp.isi.edu/in-notes/rfc2654.txt\\nThis document defines a mechanism by which information servers can\\nexchange indices of information from their databases by making use of the\\nCommon Indexing Protocol (CIP). This document defines the structure of\\nthe index information being exchanged, as well as the appropriate meanings\\nfor the headers that are defined in the Common Indexing Protocol.\\nRFC 2655: CIP Index Object Format for SOIF Objects\\nT. Hardie, M. Bowman, D. Hardy, M. Schwartz, D. Wessels\\nftp://ftp.isi.edu/in-notes/rfc2655.txt\\nThis document describes SOIF, the Summary Object Interchange Format,\\nas an index object type in the context of the CIP framework.\\nRFC 2656: Registration Procedures for SOIF Template Types\\nT. Hardie\\nftp://ftp.isi.edu/in-notes/rfc2656.txt\\nThe registration procedure described in this document is specific to\\nSOIF template types.\\nRFC 2657: LDAPv2 Client vs. the Index Mesh\\nR. Hedberg\\nftp://ftp.isi.edu/in-notes/rfc2657.txt\\nLDAPv2 clients as implemented according to RFC 1777 have no notion\\non referral. The integration between such a client and an Index Mesh, as\\ndefined by the Common Indexing Protocol, heavily depends on referrals and\\ntherefore needs to be handled in a special way. This document defines one\\npossible way of doing this.\\n\\nIETF work: other standards\\nRFC: 2616. Hypertext Transfer Protocol - HTTP/1.1\\nR. Fielding,\\xa0 J. Gettys, J. Mogul, H. Frystyk, L. Masinter, P.\\nLeach, T. Berners-Lee\\nftp://ftp.isi.edu/in-notes/rfc2616.txt\\nHTTP has been in use by the World-Wide Web global information\\xa0\\ninitiative since 1990. This specification defines the protocol referred\\nto as \"HTTP/1.1\", and is an update to RFC 2068 HTTP/1.0.\\n\\nLDAP Service Deployment BOF (lsd2) at IETF45 in Oslo\\nhttp://www.ietf.org/ietf/99jul/lsd2-agenda-99jul.txt\\nThe purpose of this BOF is to examine the question of deploying LDAP\\n services beyond the context of a single service provider.\\nPresentations about TISDAG, NDD, DESIRE 2 Directory Service.\\xa0\\nSuggested areas of activity: description of service models for large-scale\\ndirectory, schema recommendations (including i18n/l10n issues), client\\nextensions.\\n\\nTechnical Specification The Norwegian Directory of Directories (NDD)\\nR.Hedberg, H.Alvestrand\\nhttp://www.ietf.org/internet-drafts/draft-hedberg-alvestrand-ndd-00.txt\\nThis specification describs what is proposed to be the necessary infrastructure\\nto provide a national directory server infrastructure in Norway for publicly\\naccessible directory servers.\\n\\nTechnical Infrastructure for Swedish Directory Access Gateways (TISDAG)\\nLeslie Daigle, Roland Hedberg\\nhttp://www.ietf.org/internet-drafts/draft-daigle-tisdag-00.txt\\nThe strength of the TISDAG project\\'s DAG proposal is that it defines\\nthe necessary technical infrastructure to provide a single-access-point\\nservice for information on Swedish Internet users.\\xa0 The resulting\\nservice will provide uniform access for all information -- the same level\\nof access to information (7x24 service), and the same information made\\navailable, irrespective of the service provider responsible for maintaining\\nthat information, their directory service protocols, or the end-user\\'s\\nclient access protocol.\\n\\nAn Architecture for Integrated Directory Services\\nLeslie Daigle, Thommy Eklof\\nhttp://www.ietf.org/internet-drafts/draft-daigle-arch-ids-00.txt\\nDrawing from experiences with the TISDAG ([TISDAG]) project, this document\\noutlines an approach to providing the necessary infrastructure for integrating\\nsuch widely-scattered servers into a single service, rather than attempting\\nto mandate a single protocol and schema set for all participating servers\\nto use.\\nThe proposed architecture inserts a coordinated set of modules between\\nthe client access software and participating servers.\\xa0 While the client\\nsoftware interacts with the service at a single entry point, the remaining\\nmodules are called upon (behind the scenes) to provide the necessary application\\nsupport.\\xa0 This may come in the form of modules that provide query\\nproxying, schema translation, lookups, referrals, security infrastructure,\\netc.\\n\\nUse of Language Codes in LDAP\\nM. Wahl, T. Howes\\nftp://ftp.isi.edu/in-notes/rfc2596.txt\\nThe Lightweight Directory Access Protocol (LDAPv3, RFC 2251) provides\\na means for clients to interrogate and modify information stored in a distributed\\ndirectory system.\\xa0 The information in the directory is maintained\\nas attributes (RFC 2252) of entries.\\xa0 Most of these attributes have\\nsyntaxes which are human-readable strings, and it is desirable to be able\\nto indicate the natural language associated with attribute values.\\nThis document describes how language codes (RFC 1766) are carried in\\nLDAP and are to be interpreted by LDAP servers.\\xa0 All implementations\\nMUST be prepared to accept language codes in the LDAP protocols.\\xa0\\nServers may or may not be capable of storing attributes with language codes\\nin the directory.\\xa0 This document does not specify how to determine\\nwhether particular attributes can or cannot have language codes.\\n\\nUniform Object Locator - UOL\\nJ. Boynton\\nhttp://www.ietf.org/internet-drafts/draft-boynton-uol-00.txt\\nA Uniform Object Locator (UOL) provides a hierarchical \"human-readable\"\\nformat for describing the location of any single attribute within any data\\nobject. A UOL emulates the internal structure of a data object by dividing\\na partial URL into two re-usable components; An object constructor and\\nan object name.\\nThe UOL format is particularly suited for retrieval and storage of\\nparameter values through multiple object layers. Its basic construction\\nallows it to be combined with a URL; without modification. Possible uses\\ninclude distributed object management, XML, and e-business development.\\nContext and Goals for Common Name Resolution\\nLarry Masinter, Michael Mealling, Nicolas Popp, Karen Sollins\\nhttp://www.ietf.org/internet-drafts/draft-popp-cnrp-goals-00.txt\\nThis document establishes the context and goals for a Common Name Resolution\\nProtocol.\\nInternationalized Uniform Resource Identifiers (IURI),\\nLarry Masinter, Martin Duerst\\nhttp://www.ietf.org/internet-drafts/draft-masinter-url-i18n-04.txt\\nTags for the Identification of Languages\\nH. Alvestrand\\nhttp://www.ietf.org/internet-drafts/draft-alvestrand-lang-tags-v2-00.txt\\nThis document describes a language tag for use in cases where it is\\ndesired to indicate the language used in an information object. It also\\ndefines a Content-language: header, for use in the case where one desires\\nto indicate the language of document.\\nRFC 2611: URN Namespace Definition Mechanisms\\nL. Daigle, D. van Gulik, R. Iannella, P. Faltstrom\\nftp://ftp.isi.edu/in-notes/rfc2611.txt\\ni18n and Multilingual support in Internet mail. Standards Overview.\\nYuri Demchenko\\nhttp://www.terena.nl/libr/tech/mldoc-review.html\\nOther Standardisation\\nSearch Engine Standards Project\\nhttp://www.searchenginewatch.com/standards/\\nDomain Restriction Proposal\\nhttp://www.searchenginewatch.com/standards/proposals.html\\nStandard for Robot Exclusion\\nhttp://info.webcrawler.com/mak/projects/robots/norobots.html\\nRobots META Tag\\nhttp://www.searchtools.com/info/robots/robots-meta.html\\n\\xa0\\nMetadata and XML/RDF\\nStandardisation\\nRFC-2413 Dublin Core Metadata for Resource Discovery\\nhttp://www.ietf.org/rfc/rfc2413.txt\\nEncoding Dublin Core Metadata in HTML\\nInternet Draft\\nhttp://www.ietf.org/internet-drafts/draft-kunze-dchtml-01.txt\\nGuidance on expressing the Dublin Core within the Resource Description\\nFramework (RDF)\\nhttp://www.ukoln.ac.uk/metadata/resources/dc/datamodel/WD-dc-rdf/\\nResource Description Framework - RDF\\nhttp://www.ukoln.ac.uk/metadata/resources/rdf/\\nW3C Resource Description Framework (RDF) Model and Syntax - recommendation\\nhttp://www.w3.org/TR/REC-rdf-syntax/\\nW3C Resource Description Framework (RDF) Schemas - proposed recommendation\\nhttp://www.w3.org/TR/PR-rdf-schema/\\nResource Description Framework (RDF)\\nhttp://www.w3.org/RDF/\\nMetadata and Resource Description\\nhttp://www.w3.org/Metadata/\\nDublin Core\\nhttp://purl.org/metadata/dublin_core/\\nDublin Core Metadata Element Set: Reference Description\\nhttp://purl.oclc.org/DC/about/element_set.htm\\nUser Guide Working Draft 1998-07-31\\nhttp://purl.oclc.org/DC/documents/working_drafts/wd-guide-current.htm\\n1999-07-02: Dublin Core Elements, Version 1.1 moves to Proposed Recommendation\\nThe Dublin Core Directorate is pleased to announce that a set of revised\\nelement definitions (Dublin Core Elements, Version 1.1) has been completed\\nand is for public review and comment as a Proposed Recommendation of the\\nDublin Metadata Initiative.\\nhttp://purl.org/dc/documents/proposed_recommendations/pr-dces-19990702.htm\\n\\xa0\\nCEN/ISSS Workshop on MMI (Metadata for Multimedia Information)\\nhttp://www.cenorm.be/isss/Workshop/MMI/Default.htm\\nCEN/ISSS Metadata Framework, edited by Stewart Granger\\nhttp://dialspace.dial.pipex.com/town/way/gkh12/frame/main.html\\nCEN/ISSS\\' The European XML/EDI Pilot Project\\nhttp://www.cenorm.be/isss/workshop/ec/xmledi/isss-xml.html\\nThe Role of the XML/EDI Guidelines\\nhttp://www.cenorm.be/isss/workshop/ec/xmledi/xmlbook.htm\\nGuidelines for using XML for Electronic Data Interchange, Version 0.05,\\n25th January 1998\\nhttp://www.xmledi.net/guide.htm\\nThe Global Repository Initiative\\nhttp://www.xmledi.com/repository/\\nWhite Paper on XML Repositories for XML/EDI\\nhttp://www.xmledi.com/repository/xml-repWP.htm\\nDublin Core/MARC/GILS Crosswalk\\nNetwork Development and MARC Standards Office\\nhttp://www.loc.gov/marc/dccross.html\\nCharacter Set and Language Negotiation (2) in Z39.50\\nhttp://lcweb.loc.gov/z3950/agency/defns/charsets.html\\nRegistry of Z39.50 Object Identifiers\\nhttp://lcweb.loc.gov/z3950/agency/defns/oids.html\\nMetadata.Net - Metadata Tools and Services\\nhttp://metadata.net/\\nMeta Data Coalition\\nhttp://www.mdcinfo.com/\\nAn Introduction to the Meta Data Coalition\\'s Initiatives\\nhttp://www.MDCinfo.com/papers/intro.html\\nOpen Information Model\\nMDC OIM Version 1.0 review draft, April 1999\\nhttp://www.mdcinfo.com/OIM/OIM10.html\\nOIM proposed models\\nKnowledge Description Model\\nhttp://www.mdcinfo.com/OIM/models/KDM.html\\nMeta Data Interchange Specification MDIS Version 1.1\\nhttp://www.mdcinfo.com/MDIS/MDIS11.html\\nMetadata/RDF Resources and Publications\\nMetadata Resources at UKOLN\\nhttp://www.ukoln.ac.uk/metadata/resources/\\nPrototype Metadata Registry for DESIRE project\\nhttp://homes.ukoln.ac.uk/~lisrmh/reginfo-v1.htm\\nRDF Tools - Briefing document\\nhttp://www.ukoln.ac.uk/web-focus/events/seminars/what-is-rdf-may1998/rdf-briefing.html\\nDC News, 1999-08-18\\nCIMI Announces the release of the Guide to Best Practice: Dublin Core.\\nThe document is one important result of the Dublin Core Testbed, an on-going\\neffort to explore the usability, simplicity, and technical feasibility\\nof Dublin Core for museum information. The Guide addresses Dublin Core\\n1.0 as documented in RFC 2413.\\nhttp://www.cimi.org/documents/meta_bestprac_final_ann.html\\nNew Metadata Handbook from European Schoolnet\\n1st December 1998\\nhttp://www.en.eun.org/eng/metadatabook-en.html\\nDescribes extended Metadata element set has been extended with a range\\nof additional local (sub)elements from other metadata initiatives including\\nthe IMS (http://www.imsproject.org/\\n- Instructional Management System) and the ARIADNE set (http://ariadne.unil.ch/\\n- Alliance of Remote Instructional Authoring and Distribution Network for\\nEurope).\\nThe EUN metadata harmonisation is happening in close co-operation with\\nEUC (European Universal Classroom) which has been studying DBS/GER (http://dbs.schule.de/indexe.html\\n- Deutscher Bildungs-Server / German Educational Resources), GEM (http://gem.syr.edu\\n- The Gateway to Educational Materials) and EdNA (http://www.edna.edu.au/-\\nEducation Network Australia). In the following you will find a guideline\\nto create and publish metadata, a presentation of the syntax and a thorough\\ndescription of each of the EUN elements.\\nDave Beckett\\'s Resource Description Framework (RDF) Resources\\nhttp://www.cs.ukc.ac.uk/people/staff/djb1/research/metadata/rdf.shtml\\nAutomatic RDF Metadata Generation for Resource Discovery\\nCharlotte Jenkins, Mike Jackson, Peter Burden, Jon Wallis\\nhttp://www.scit.wlv.ac.uk/~ex1253/rdf_paper/\\nClassifier/matadata generator Demo\\nhttp://www.scit.wlv.ac.uk/~ex1253/metadata.html\\nMapping Entry Vocabulary to Unfamiliar Metadata Vocabularies\\nMichael Buckland, with Aitao Chen, Hui-Min Chen, Youngin Kim, Byron\\nLam, Ray Larson, Barbara Norgard, and Jacek Purat\\nhttp://www.dlib.org/dlib/january99/buckland/01buckland.html\\nXML Searching\\nBuilding a XML-based Metasearch Engine on the Server\\nhttp://xml.com/pub/1999/07/metasearch/metasearch2.html\\nGoXML Search Engine\\nhttp://www.goxml.com/\\nGoXML.com v1.0 - BETA is an XML Context-based Search Processor. Online\\ndocumentation (http://www.goxml.com/about/supported.xsp\\n) and Demonstration (http://www.goxml.com/help_srch.xsp\\n). The Goxml Project was launched to create a new breed of Search Vehicle\\nwhich can index, store and allow accurate searching of XML data. The primary\\nfocus is to allow XML developers a tool to locate XML documents on the\\ninternet.\\n\\xa0\\n\\n\\nThis page is updated regularly, please send your suggestions to: demchenko@terena.nl\\n\\n\\n',\n",
       " ' Objective Checklist References Answers to Questions Introduction Long term memory has a profound effect on the way we live our everyday lives and is integral to many aspects of cognition. Whether we are recalling episodes of childhood abuse in the witness box, retrieving our favourite pumpkin scone recipe, or just trying to remember where we parked the car, memory plays a key role in allowing us to function, providing the texture of our experiences and defining our identities. Most of us have had the annoying experience of recognizing a face but not being able to remember to whom it belongs. We curse, make a reference to our age and reach for the nearest pop-psychology manual on how to improve our memories in ten easy steps. When you stop to think about it, however, the really astonishing thing is how often we don\\'t forget. In a study by Shepard (1967) subjects were given a list of 580 arbitrary words to remember. On a forced choice test they scored at just under 88% correct. That\\'s impressive. Furthermore, I could ask you what you were doing at 12:30pm one week ago and there is a good chance you would be correct, even if it isn\\'t something you do all the time. Yet between then and now you have had thousands of experiences any one of which I could have queried. Somehow all of those experiences have left their mark, often without you even thinking about it. Long term memory is big. The other really astonishing thing about memory is how flexibly we can access it. Suppose I ask you to list all the situation comedies you have ever watched. Most people from television dependent cultures wont have too many difficulties coming up with a reasonably large collection. Yet how could you answer this question given the obscure cue \"situation comedy\"? A database system might store a set of records such as {(Giligan\\'s Island, situation comedy), (Full House, situation comedy), (World News, current affairs), etc.} and then cycle through these one at a time retrieving those that have the situation comedy tag. But this would require that when you are watching television shows you are constantly tagging them as situation comedy, current affairs etc. If you had tagged the program as \"the funny show with the skinny sailor in it\" the search for situation comedy would come back with NO RECORDS FOUND . Human memory is much more robust. We often use what seem to be very obscure cues yet we are able to retrieve well. xenophobic ?\". These questions require you to comment on the information stored in memory without necessarily retrieving the detail of any particular memory. --> Much of the research into human memory has been an exploration of just how flexible our memories are - of the different sorts of questions that we can use our memories to answer. In this chapter, we will consider two ways in which the questions that we ask of our long term memories can differ. The first of these refers to the nature of the output that a question requires - are we asking for a specific name, word or other item from memory (retrieval) or do we require a yes/no answer about whether we remember some fact or episode (matching). The second distinction refers to the role of context in the query - are we asking about what happened in a given episode or context (episodic) or is the query about the way that things tend to be in general (semantic). After considering these tasks, we will look in some detail at the Matrix Model of long term memory (Pike, 1984; Humphreys, Bain & Pike, 1989) which provides a theory of how these questions could be answered using the formalism of matrix algebra. Matching Versus Retrieval Tasks: The Nature of the Output Memory tasks differ with respect to the output that is required. The usual task that people have in mind when they think about human memory is one in which you are given one piece of information such as someone\\'s face and you must retrieve another piece of information such as that person\\'s name. These sorts of tasks seem to rely on a discrete or discontinuous form of information. The required output is an item. Two common examples of retrieval tasks are free association and cued recall with a list associate. Free Association provides a subject with a cue (e.g. \"Type of animal\") and requires them to respond with the first word that comes to mind (e.g. cat). Sometimes a prior study list is presented to the subject and it has been shown that words that occur in the prior study list are more likely to be produced even though the subject is not instructed to use the study list to make their decisions. Free association is a retrieval task because the required response is a word. Cued Recall with a List Associate requires the subject to study a list of pairs of words. At test, subjects are given a list of words and asked which word occurred with each of the test words during the study episode (e.g. \"Which word occurred with boy in the study list?\"). Again, cued recall with a list associate is a retrieval task because it requires the subject to respond with a word. In contrast some memory tasks known as matching tasks require what seems to be a more quantitative answer based on a continuous measure. The examples with which we will be concerned are familiarity rating and recognition. Familiarity rating refers to a task in which subjects rate (on a five point scale, for instance) how familiar a word is to them in general (e.g. \"How familiar is the word house to you?\"). Familiarity rating is a matching task because it seems to be based on a continuous form of information. Recognition requires a subject to study a list of words. At test, the subject is given a second list of words - some of which appeared in the first list and some of which did not. The subjects\\' task is distinguish the targets (words that were on the list) from distractors (words that weren\\'t on the list). Either they are asked to make a yes/no decision or they provide their confidence that the word is old (which might be rated on a five point scale, for instance). Recognition is considered a matching task because it relies upon a continuous form of information. Episodic and Semantic Tasks: The Role of Context Another important dimension on which memory tasks can vary is whether they make reference to a study episode. Tasks that do specify the study episode are known as episodic tasks, whereas tasks that do not are known as semantic tasks (or generalized tasks). Tulving (1972) realized that the learning in a study episode is not continuous with the learning that occurs before study. In particular, he realized that when we ask a subject in a recognition task do they recognize a word we are not asking them whether they know the word at all (often all of the test words are known to the subject). What we are asking is if the word occurred in a given list (the study list). Similarly, in cued recall with a list associate, we are not asking what word generally goes with boy . We are asking what word went with boy in the study list. In contrast, familiarity rating and free association make no specific reference to a study episode and are semantic tasks. Table 1 categorizes the four memory tasks described above in terms of the nature of the output and the role of context. Table 1: Examples of Episodic/Semantic Tasks and Matching/Retrieval Tasks (adapted from Humphreys, Bain & Pike, 1989) Use of Context Cue Access Process Episodic Memory Semantic Memory Matching produces a rating value Recognition Familiarity Rating Retrieval produces a word Cued Recall Free Association Before looking at how the Matrix Model accounts for the differences between these tasks we need a grounding in tensors, the operations that can be performed on tensors and how tensors can be mapped to neural network architectures. If you are comfortable with these ideas you may skip the next section. Tensors Explained The Matrix Model of memory is built upon the mathematics of tensors. Tensors are convenient ways of collecting together numbers. For instance, a vector, which is also known as a rank one tensor, could be used to describe the age, gender, and salary of an employee. If Jeff is 50 years old, is male (where male = 0 and female = 1) and earns $ 56000 per annum then we could describe Jeff with the vector [50, 1, 56000] (see figure 1). Note that vectors (and tensors in general) are ordered. The vector [56000, 1, 50] would describe someone who was 56000 years old who made a total of $ 50 per annum! Figure 1: Examples of vectors. (a) a row vector describing Jeff, (b) a column vector, (c) a vector with N components. The rank one tensor described above has a dimension of three because it contains three components. There is no reason that vectors need be restricted to three dimensions, however. We could have added shoe size, for instance, to increase the dimension to four. Similarly, there is no reason that we need to restrict ourselves to a single row of numbers. A tensor with N rows and M columns is known as an NxM matrix and has a rank of two, indicating that the array of numbers extends in two directions (see figure 2). Figure 2: Examples of matrices. (a) a 2x2 matrix, (b) a 3x2 matrix, (c) an NxN matrix. The process of extending the number of directions in which the array extends can theoretically continue indefinitely, creating tensors of rank three, four, five etc. In the following sections, we will look at vectors, matrices and tensors of rank three (see figure 3) as they are critical to understanding the Matrix Model. Other models, such as the STAR model of analogical reasoning (Halford, Wiles, Humphreys and Wilson 1992), employ tensors of higher rank. Figure 3: A rank three tensor (NxNxN). Vectors - Rank One Tensors Tensors, and in particular vectors, can be represented in many different forms including: Cartesian form in which the components are enumerated explicitly. Figure 1 depicts vectors represented in Cartesian form. Geometric form in which the vector is plotted in N dimensional space. For instance, figure 4 shows the vector representing Jeff plotted in three dimensional space. Figure 4: The vector representing Jeff plotted in three dimensional space (Geometric form). Algebraic form in which a vector is represented as a bolded lower case letter (e.g. v ). Algebraic form is a particularly concise form of representation, which makes it easy to talk about the operations that can be performed on vectors such as addition (e.g. w = v + t ). Neural network form which diagrams a neural network architecture in which either a set of units or a set of weights contain the elements of the vector. For instance, a vector can be mapped to a two layer network (one input and one output layer) as depicted in Figure 5. The number of units in the input layer corresponds to the number of dimensions in the original vector, while the output layer contains only 1 unit. Each input unit is connected to each output unit. The input units represent one vector and the weights represent a second vector. Figure 5: The network corresponding to a vector memory. The output of this network is defined to be the dot product (or inner product) of the input and weight vectors. A Dot Product is calculated by multiplying together the values which are in the same position within the two vectors, and then adding the results of these multiplications together to get a scalar (see Figure 6a). In the case of the neural network, this involves multiplying each input unit activation by the corresponding weight value and then adding. The dot product of two vectors represents the level of similarity between them and can be extended to higher rank tensors (see figure 6b) Figure 6: The Dot Product. The dot product is expressed algebraically as a dot, that is, the dot product of the vectors v and w is written v . w . Learning occurs in this network by adding the input vectors. Vector addition superimposes vectors of the same dimension. It is calculated by adding together the elements in a particular position in each vector (see Figure 7a). In this way, multiple memories can be stored within the same vector. [Note: the network actually employs Hebbian learning (see Neural Networks by Example: Chapter three). However, when the output unit is fixed at one Hebbian learning is identical to vector addition.] Figure 7: (a) Vector Addition, (b) Matrix Addition. Again vector addition can be extended to tensors of arbitrary rank (see figure 7b). Vector addition is expressed algebraically as a plus sign (+). So if we wanted to talk about the dot product of v with the addition of w and x we would write v .( w + x ). Another useful property to keep in mind is that the dot product distributes over addition. That is: v .( w + x ) = v . w + v . x In the following exercises, you will build a vector network that learns to discriminate between stored items and new items (see figure 8). Figure 8: A vector memory network. Follow the instructions below to create the network and then work through the exercises. Load BrainWave Select \\'New Hebbian Network\\' Set up the vector memory network: Create two input units and one output unit. Connect both input units to the output unit. Set up VALUE objects for the units and weights. Create the data sets: Create an Input Set containing the two input units Create a Test Set containing the two input units Create an Output Set containing the output unit The items in this exercise are FROG [0.95, 0.32], TOAD [0.49, 0.87], KOALA [0.32, -0.95]. Add the FROG pattern to the input set. Add a pattern containing a 1 to the output set. Add all three items, FROG, TOAD and KOALA, to the test set. Exercise 1: Train the network for one epoch and record the weights in the TRAIN FROG row of the following table. How have the weights changed? Weight 1 Weight 2 TRAIN FROG TRAIN FROG & KOALA TRAIN FROG & TOAD Exercise 2: Test each of the items, FROG, TOAD and KOALA, and record the match values (the activation of the output unit) in the second table. Explain the match values. TEST FROG TEST TOAD TEST KOALA TRAIN FROG TRAIN FROG & KOALA TRAIN FROG & TOAD Exercise 3: Train the network for one more epoch and test again. What happens to the match values after a second training trial? Why? Exercise 4: Add KOALA to the input set and an output value of 1 to the output set. Zero the weights (using the ACTIONS menu) and retrain the network on the updated input set. Test the network as before, recording the values in the table in the TRAIN FROG & KOALA row. Exercise 5: Delete KOALA from the input set and add TOAD. Zero the weights, retrain and test as above, recording the values in the TRAIN FROG & TOAD row. You should have six weight values and nine match values for each training trial. Create a graph of the match values after the first training trial: plot three lines, one for each test item, against the three training conditions. Explain the shape of each line on the graph. Exercise 6: For each of the three training conditions (FROG alone, FROG & KOALA, FROG & TOAD): Draw the geometric (graphical) representation of the weights, Provide the algebraic representation of the weights. Matrices - Rank Two Tensors The vector memory, discussed above, was capable of storing items so that at a later time it could be determined if they had appeared. A matrix memory allows two items to be associated - so that given one we can retrieve the other. Algebraically, a matrix is usually represented as a bolded upper case letter (e.g. M ). Associations are formed using the outer product operation. A outer product between two vectors is calculated by multiplying each element in one vector by each element in the other vector (see Figure 8). If the first vector has dimension d 1 and the second vector dimension d 2 , the outer product matrix has dimension d 1 xd 2 . For instance, a three dimensional vector multiplied by a two dimensional vector has dimension 3x2. Figure 8: The outer product. The outer product operation is expressed algebraically by placing the vectors to be multiplied next to each other. So the outer product of v and w is written as v w . These association matrices are then added into the memory matrix (as in the vector memory case) - so that all associations are stored as a single composite. A matrix memory maps to a two layer network (one input and one output layer) as depicted in Figure 9. The number of input units corresponds to the number of rows in the original matrix, while the number of output units corresponds to the number of columns. Each input unit is connected to each output unit. Figure 9: The network representation of a matrix. In the following exercise you will use a matrix memory network to store and recall pairs of items. Exercise 7: Load the simulator, BrainWave. From the NETWORKS menu - select Matrix Model (1). What rank tensor does this network implement? What are its dimensions? Exercise 8: The items in this exercise are: Cues: FROG [0.5, -0.5, 0.5, -0.5] KOALA [0.5, 0.5, -0.5, -0.5] SNAIL [-0.5, 0.5, 0.5, -0.5] TOAD [0.5, 0.4, 0.6, 0.45] Targets: FLIES [0.7, 0.5, 0.5] LEAVES [0.7, -0.5, -0.5] LETTUCE [0, -0.7, 0.7] The input set contains the items FROG, KOALA and SNAIL, paired with items in the output set FLIES, LEAVES and LETTUCE, respectively. Another input item, TOAD [0.5, 0.4, 0.6, 0.45], can be used to test the network on unfamiliar input. Calculate the similarity value (i.e. dot product) of the items FROG, KOALA, SNAIL and TOAD with themselves, and each other, and record the values in the table below: FROG KOALA SNAIL TOAD FROG KOALA SNAIL TOAD Exercise 9: Train the network for one epoch. Test each of the items FROG, KOALA, SNAIL and TOAD. What output is produced in each case? (Give the output pattern and also describe the output patterns in terms of their similarity to FLIES, LEAVES and LETTUCE). FROG KOALA SNAIL TOAD Exercise 10: Give the algebraic equation that describes the matrix memory formed from the three pairs of associates: M = Exercise 11: Give the equations that describe each of the retrievals in exercise 9. Use the similarity measures from the table above to simplify each equation to a weighted sum of the target patterns. FROG KOALA SNAIL TOAD Tensors of Rank Three and Above The final sort of tensor we need to demonstrate the matrix model is the rank three tensor. The rank three tensor allows a three way association to be represented. For instance, we could store the information that John loves Mary - [Loves John Mary] - or that Apple appeared with Pencil in List 1 [List 1, Apple, Pencil]. A tensor of rank three maps to a three layer network (one input layer with two sets of units, one output layer, and one layer of hidden units) as depicted in Figure 10. The number of units in the input sets and the output set correspond to the dimensionality of the tensor. The number of hidden units corresponds to the number of units in one input set times the number of units in the other input set. Each hidden unit has a connection from one input unit from each input set, with a hidden unit existing for each possible combination. These hidden units are SigmaPi units, the value of which is set to the multiplication of the two input units to which it is connected. To implement a rank three tensor, the weights in the first layer are frozen at one. Consequently, a hidden unit\\'s activation will equal the multiplication of the activations of the input units to which it is connected. Each hidden unit is then connected to each output unit. Figure 10: The network representation of a rank three tensor. In these exercises, you will use both rank two and three tensor networks to store and recall triples of items. Exercise 12: Load the simulator, BrainWave. From the NETWORKS menu - Matrix Model (2). What rank tensor does this network implement? Exercise 13: The items in this exercise are: Cues: FROG [0.5, -0.5, 0.5, -0.5] KOALA [0.5, 0.5, -0.5, -0.5] SNAIL [-0.5, 0.5, 0.5, -0.5] TOAD [0.5, 0.4, 0.6, 0.45] Relations: EATS [0.5, -0.5, -0.5, 0.5] LIVES-IN [0.5, 0.5, -0.5, -0.5] Targets: FLIES [0.7, 0.5, 0.5] LEAVES [0.7, -0.5, -0.5] LETTUCE [0, -0.7, 0.7] POND [0.89, 0.43, -0.22] TREE [-0.22, 0.76, 0.62] SHELL [0.43, -0.5, 0.76] Notice that the vectors for the cues are the same as those used above. Also notice that EATS and LIVES-IN are orthogonal to each other - that is they have a dot product of zero. Calculate the similarity (dot product) table for the targets. FLIES LEAVES LETTUCE POND TREE SHELL FLIES LEAVES LETTUCE POND TREE SHELL Exercise 14: The cue+relation input set contains the items FROG-EATS, KOALA-EATS, SNAIL-EATS, FROG-LIVES_IN, KOALA-LIVES_IN and SNAIL-LIVES_IN, paired with items in the output set FLIES, LEAVES, LETTUCE, POND, TREE, and SHELL, respectively. Two other input items, TOAD-EATS and TOAD-LIVES_IN, can be used to test the network\\'s response to unfamiliar input. Train the network for one epoch. Test each of the items FROG-EATS, KOALA-EATS, SNAIL-EATS, FROG-LIVES_IN, KOALA-LIVES_IN, SNAIL-LIVES_IN, TOAD-EATS and TOAD-LIVES_IN. What output is produced in each case? (Give the output pattern and also describe the output patterns in terms of their similarity to FLIES, LEAVES, LETTUCE, POND, TREE and SHELL) FROG-EATS KOALA-EATS SNAIL-EATS FROG-LIVES_IN KOALA-LIVES_IN SNAIL-LIVES_IN TOAD-EATS TOAD-LIVES_IN Exercise 15: How does the performance of this network compare with the performance of the network in Exercise 8? Why is it not as good? Exercise 16: Give the algebraic equation that describes the matrix memory formed from the three pairs of associates: M = Exercise 17: Give the equations that describe each of the retrievals from exercise 14. Use the similarity measures from the table above to simplify each equation to a weighted sum of the target patterns. FROG-EATS KOALA-EATS SNAIL-EATS FROG-LIVES_IN KOALA-LIVES_IN SNAIL-LIVES_IN TOAD-EATS TOAD-LIVES_IN Exercise 18: From the NETWORKS menu - select Matrix Model (3). What rank tensor does this network implement? Exercise 19: The inputs and outputs for this network are the same as for the previous one, but the connections and hidden SigmaPi units perform different calculations on the inputs to try and achieve the correct outputs. Train the network for one epoch. Test each of the items FROG-EATS, KOALA-EATS, SNAIL-EATS, FROG-LIVES_IN, KOALA-LIVES_IN, SNAIL-LIVES_IN, TOAD-EATS and TOAD-LIVES_IN. What output is produced in each case? (Give the output pattern and also describe the output patterns in terms of their similarity to FLIES, LEAVES, LETTUCE, POND, TREE and SHELL). FROG-EATS KOALA-EATS SNAIL-EATS FROG-LIVES_IN KOALA-LIVES_IN SNAIL-LIVES_IN TOAD-EATS TOAD-LIVES_IN Exercise 20: Which of the two networks performs the memory task better? Why? Exercise 21: Give the algebraic equation that describes the matrix memory formed from the three pairs of associates: M = Exercise 22: Give the equations that describe each of the cued recall tests from question 19. Use the similarity measures from the table above to simplify each equation to a weighted sum of the target patterns. FROG-EATS KOALA-EATS SNAIL-EATS FROG-LIVES_IN KOALA-LIVES_IN SNAIL-LIVES_IN TOAD-EATS TOAD-LIVES_IN In this section, we have been looking at the way in which tensors of rank one, two and three can be used to store information. In the next section, we will examine the Matrix Model, which uses precisely this mechanism to explain the nature of human memory. The Matrix Model The Matrix Model of Memory was developed by Humphreys, Bain and Pike (1989) and Pike (1984) to provide a coherent theoretical account of a range of different memory tasks, including episodic tasks, such as recognition and recall, and semantic tasks, such as familiarity rating and indirect production tasks. It is a distributed associative model in which items are modelled and stored as vectors of feature weights or elements, just as was the case in the previous section. Elements within each vector contribute conjointly to the representation of items. Thus memory representations are not located at specific points within a memory network, or within specific memory systems. Instead they are conceptualised as unique patterns of activation over a common set of elements. Typically, these patterns are thought to be sparse representations meaning that only a few of the elements are active. Memory Representations The memory representations in the Matrix Model include items, contexts or, combinations of items and contexts (associations). Items - Items can be any sort of stimuli including words, pictures, melodies etc. For the most part, however, the experiments to which the model has been applied use words. Each item is modelled as a vector of feature weights. Feature weights are used to specify the degree to which certain features form part of an item. There are two possible levels of vector representation for items. These are: modality specific peripheral representations (e.g., graphemic or phonemic representations of words) modality independent central representations (e.g., semantic representations of words). Item vectors are distinguished by subscripts (e.g. a i ). A distractor vector is indicated by a d . Contexts - To distinguish between episodic and non-episodic tasks the Matrix Model assumes the episode or context in which items are studied is also represented by a vector of feature weights. In episodic tasks this context vector must be reinstated so that it may be used as a cue to the memory system. The context vector is represented by an x . Associations - While individual items and contexts are represented as single vectors ( a , b , x ), associations between items and contexts are represented by matrices derived from the matrix product of these vectors. The resulting matrix product represents the association (or binding) between either items, or between items and context. The memory of the matrix Model is formed by adding these associations together. The model posits a number of different kinds of associations including: Two-way associations between a single item ( a ) and a context ( x , e.g. bacon x breakfast this morning) are represented as a context-to-item association ( x a ), where x = n element column vector a = n element row vector Associations between a list of items ( a 1 , a 2 ,..., a k ) and a context ( x ) are represented by multiplying each of the item vectors by the context vector and summing the resulting matrices. This sum represents the memory of the study list ( E ). E = x a 1 + x a 2 + ... + x a k Three-way associations between a list of word pairs ( a 1 b 1 , a 2 b 2 , ... a k b k ) and context ( x , e.g., bacon x dog x breakfast this morning) are represented by the rank three tensor ( x a j b j ), where x = n element column vector a j = n element row vector b j = n element orthogonal vector The resulting associations can be summed to form the memory for the list ( E ). E = x a j b j Note: the type of vector (i.e., row, column, orthogonal) can also be inferred from the order of the vector symbols, where: 1st vector = column vector; 2nd vector = row vector; and 3rd vector = orthogonal. Pre-existing memories ( S ) are added to list memories ( E ) because test performance can be influenced by both list memories and pre-existing memories. M = x a j b j + S Accessing Memory Representations Having constructed the memory matrix, we can now see how the Matrix Model goes about accessing this representation at test for a number of different tasks. All retrieval in the Matrix Model is direct . The memory matrix is presented with cues and access occurs in parallel. There is no sequential search process. Presenting the model with a cue involves taking the inner product (or dot product) of the cue vector with the memory matrix. One of the strengths of the Matrix Model is in the number of a ways in which information from the model can be accessed. In the introductory section, two dimensions on which tasks can differ were outlined. The first was the matching/retrieval dimension. Matching tasks are those based on a continuous form of information that typically require either a yes/no answer or a rating response (e.g. recognition). Retrieval tasks, by contrast, require a specific item to be returned (e.g. cued recall). This distinction is captured in the Matrix Model by the nature of the tensor that results once all cues have been applied. If the resultant tensor is a scalar we are dealing with a matching process. This scalar can be compared against criteria to determine a yes/no or rating value. If the resultant tensor is a vector then we have a retrieval process. The vector can be compared against all item vectors with the item being the output of the process. The next sections, goes through the mathematics of recognition and cued recall with a list associate demonstrating how matching and retrieval tasks are accomplished within the model. The second task dimension discussed in the introduction focussed on the the episodic/semantic dimension. Episodic tasks refer to a specific context, whereas in semantic (generalized) tasks information is integrated over a large number of experiences. The Matrix Model captures this distinction. In episodic tasks, a reinstated context vector is used as a cue. In semantic (generalized) tasks, a vector which is equally similar to all contexts is used so as to average over all experiences with the cue items (typically, this is a vector with all components set to 1/n where n is the dimension of the vector). The section entitled \"Episodic versus Semantic Memory: Cuing with the Context Vector\" describes an experiment designed to demonstrated the importance of the distinction and leads you through the process of modelling this experiment using the BrainWave simulator. Matching Versus Retrieval Tasks: Scalar or Vector Output In this section, a matching task, namely recognition, and a retrieval task, namely cued recall with a list associate, are compared within the Matrix Model framework. Recognition Recognition involves a matching process, where the overall similarity between the test cues ( x and a i ) and memory ( M ) is calculated. Because this is an episodic task, the test cues involve both word cues and a context cue. This episodic matching process is accomplished by combining the test cues into an associative matrix ( x a i ) and determining a dot product between: the cue matrix ( x a i ), and the memory matrix ( M = x a j + S ). [Note: Because the dot product operation is associative, the results are identical regardless of whether you form a combined x a i matrix and then take a dot product or take the dot product of each of the cues with the memory matrix progressively.] Studied Test Word (a i ) xa i . M = xa i . ( xa j + S ) = x a i . x a j + x a i . S = (x . x) (a i . a j ) + x a i . S = (x . x) (a i . a i ) + (x . x) (a i . a j ) + xa i . S Inserting the expected matching value: E[ x a i . M ] = c s + (k - 1) c m + g where c = similarity between the study and test context (assumed to large) s = similarity between the same word encoded at study and test (assumed to be large) m = similarity between different words at study and test (assumed to be small) g = contribution of pre-existing memories Non studied Test Word (d) x d . M = x d . ( xa j + S) = xd . xa j + xd . S = (x . x) (d . a j ) + xd . S where E[ x d . M ] = c m k + g Note that the matching operations in the above equations can be collapsed down into several components, including : a match between the test cue and the pre-experimental memories (i.e., x a i . S or x d . S ), and a match between the test cue and the experimental memories (i.e., x a i . x a j or x d . x a j ) The match between the test cue and the experimental memories can further be collapsed down into : a match between the context on study and test occasions ( x . x = c), and a match between the study and test items ( a i . a i = s and a i . a j = m) or ( d . a j = m) Thus the final dot product derived from these equations, represents the match of the contexts on the study and test occasions (c), weighted by the match of the items on the study and test occasions (s and m). Consequently, memories that are conjointly defined by context and test cues will be weighted more heavily than items not studied in that context. This mechanism enables the model to avoid interference (large weights) from other items studied in the same context and also from previous contexts in which items have appeared. Cued Recall with a List Associate Cued recall with a list associate involves a subject studying a list of pairs. At test they are given an item and are required to produce the word with which it was paired at study. This is an important task because it can be used to demonstrate that three way association are necessary to model human memory. Simple associations two-way associations between items are insufficient (Humphreys, Bain & Pike 1989). For this reason, cued recall with a list associate is modelled using rank three tensors that associate word pairs ( a 1 b 1 , a 2 b 2 ,... a k b k ) and context ( x ). The tensor is formed by taking the outer product of the context vector x and the two item vectors, a j and b j . M = x a j b j + S Subjects are then asked to recall list targets ( b i ) at test, using list associates ( a i ) and context ( x ) as cues. The retrieval cues ( x and a j ) are combined to form an associative matrix cue ( x a i ). Retrieval then involves the pre-multiplication of the rank three tensor ( M ) by the retrieval cue ( x a i ). x a i . M = x a i . x a j b j + S = [( x a i )( x a j )] b j + x a i . S = [( x . x ) ( a i . a j )] b j + x a i . S = ( x . x ) ( a i . a i ) b i + ( x . x ) ( a i . a j ) b j + x a i . S Inserting the expected values: E[ x a i . M ] = c s b i + c m b j + x a i . S The end product (matrix product) of this process will comprise a target vector of feature weights. This featural information can be used to produce a word or item response. The target vector is weighted by: the similarity of the context on the study and test occasions ( x . x = c), and the similarity of the list cue on the study and test occasions ( a i . a i = s) and ( a i . a j = m) Note that the weights for the same associate (s) will be greater than the weights for different associates (m) making the resulting vector look more like the correct associate (on average) than any other item. Noise will also be generated by the pre-existing memories. The assumption is that, in general, the similarity of the pre-existing contexts and the current context will be small leading to low levels of interference. Of course, if a recent context also included the cue word then much more interference will be generated because the context vectors will be more similar. Recall in the Absence of Recognition The Task and Phenomenon The relationship between recall and recognition has been central to the study of episodic memory for several decades. Most people have the intuition that recalling a specific item is more difficult than simply recognizing that you have seen an item, and often this is the case (I know I\\'ve seen that person before, but I can\\'t remember their name). However, under some circumstances recall can be better than recognition (Tulving & Thompson 1973; Watkins & Tulving 1975). The discovery of recognition failure of recallable words has had an important impact on the development of memory theory. In particular, models in which recognition is a simple subprocess of recall (the generate - recognize models) underwent substantial modification as a consequence. Before looking more closely at the implications for memory modelling, however, we will describe the sort of experiment that has demonstrated recall without recognition as exemplified by Watkins and Tulving (1975, experiment one). The basic methodology of the Watkins and Tulving experiment is presented in table 1. Table 1: Basic methodology: Schematized Sequence of Procedures. Reproduced from Watkins and Tulving (1975). Step Procedure Example 1a List 1 Presented badge - BUTTON 1b Cued recall of List 1 badge - button 2a List presented preach - RANT 2b Cued recall of List 2 preach - rant 3 List 3 presented glue - CHAIR 4a Free-association stimuli presented table 4b Free-association responses made table 5a Recognition test sheets presented DESK TOP CHAIR 5b Recognized items circled DESK | TOP | CHAIR 5c Recognition confidence of circled items attempted DESK | TOP | 1 CHAIR 5d Recall of list cues of circled items attempted TOP - can\\'t recall 6 Cued recall of List 3 glue - chair Steps one and two involve cued recall tests designed to influence how subjects encode lists of word pairs. Word pairs such as glue - CHAIR are presented and subjects are asked to learn them for a later test. The first word ( glue ) is designated the cue and the second word (CHAIR) is designated the target. Typically, subjects must encode the two words interactively for cued recall performance to be good. These first two steps are practise trials aimed at making sure their encodings are strong. The third step is the critical study list. It is similar to the first two study lists. This time, however, instead of an immediate cued recall test, subjects were asked to free associate to a set of words that did not appear on the study list (step four). These words were chosen so that it was likely that subjects would respond with one of the target words. The fifth step was the recognition test. Subjects were given groups of three words and asked to chose the word which appeared as a target in the study list (step three). Then they had to rate their confidence and try to recall the cue word that appeared with that target. Finally, in step six, subjects were given a cued recall test for the study list. Table 1: The Proportions of Targets Recognized and Recalled (Watkins & Tulving 1975) Recognized Not Recognized Recalled .25 .24 Not Recalled .14 .36 Table 1 shows the proportion of items recalled and recognized. The key point is that half of the items that were recalled were not recognized. This occurs despite the fact that the recognition test comes first, so that forgetting should affect recall more than recognition. --> In the last two sections we have seen how, in a mathematical sense, the Matrix Model distinguishes between matching and retrieval tasks. In the next section, we will examine the episodic/semantic distinction by using the Matrix Model to simulate data generated by Bain & Humphreys (1989). Episodic versus Semantic Memory: Cuing with the Context Vector Bain & Humphreys (1989, pg. 229) report an experiment which clearly demonstrates the difference between episodic and semantic matching tasks by reinstating the context during some, but not all, of the test conditions. Subjects were given a set of words and asked to produce a synonym for each. One week later the same subjects were given a passage containing unhighlighted target words, and asked to read the text and then answer questions on it. Half of the target words were common to both training stages. In addition to the test items already mentioned (synonym, passage, or both), words which appeared in neither training stage were also included as test items. Each set of test items contained equal numbers of high and low frequency words. The subjects were grouped into three test conditions. Group A was asked to give a general familiarity rating for the words (a generalized matching condition). Group B was asked to recognise which words had been in the synonym generation task (an episodic matching condition). Group C was asked to recognise which words had been in the passage reading task (also an episodic matching condition). The mean recognition and familiarity ratings are displayed in Figure 11. Figure 11: Mean ratings for three tasks as a function of presentation list(s) and word frequency. (a) Familiarity Rating Task (b) Recognition of Synonym Task words and (c) Recognition of Passage Task words. Note that in generalized familiarity task ratings depended only on the frequency of the word. For the episodic tasks, however, the lists in which the subjects were exposed to the word are critical. As Figure 11 shows, subjects performing the episodic matching tasks were affected by the training context indicated in the task instructions, while subjects performing the general matching task were not influenced by the prior training conditions. Furthermore, the subjects did not have trouble reinstating the synonym context as opposed to the passage context, and vice versa. These results suggest that subjects are able to distinguish episodic and semantic (or generalized) memory tasks quite well. One explanation is that the episodic and semantic memory systems are located in two different compartments in the brain. In the generalized familiarity task, subjects access the semantic store, in the episodic recognition task subjects access the episodic store. This may well be the case, however, Humphreys, Bain and Pike (1989) showed using the Matrix model that it need not be. The episodic/semantic distinction can be captured in a single coherent memory system by assuming differences in the types of cues supplied. In the following exercises, the Matrix Model will be used to demonstrate how the difference between generalized familiarity and episodic recognition can be captured. To simplify the modelling process we assume a design similar to that employed by Bain and Humphreys (1989), but in which only one study list is presented. What we are looking for is a difference in the pattern of results for target and distractor words when asking for generalized familiarity versus episodic recognition. The key distinction, from the model\\'s point of view, is in the nature of the context cue. In episodic recognition it will be assumed that the context cue is the same as that at study. In contrast, when modelling generalized familiarity the context cue will be a a vector in which all components are 0.1. This context vector will be similar to all of the pre-experimental contexts and the study context to approximately the same degree and will therefore produce an output which is approximately the mean of all exposures - not just the study list exposures. Exercise 23: Load the simulator, BrainWave. From the NETWORKS menu - select Familiarity vs Recognition. This network contains three sets of units - the input units, which will contain the context vectors, the output units, which will contain the items to which a context is associated and the match units, which contain the item to be tested. Weights are connected between the input units and the output units. What rank tensor does this network implement? Above the units is a global value called \"Dot Product\". This global value indicates the dot product of the output units and the match units and is updated when you click on cycle. It is this value which will indicate the strength of a match in both the episodic recognition and generalized familiarity conditions. In addition, there are three collections of pattern sets. The pre-experimental sets contain the input/output pairs representing the subjects experience before entering the experiment. Each context is different indicating that subjects pre-experimental experience with words arises from many different contexts. Each context vector has just three units active and these units are active to different degrees. The same is true for the output patterns which represent the words. However, some of the word patterns are repeated representing the difference between high and low frequency words. The high frequency words are repeated three times while the low frequency words appear just once. Note that real words occur much more often. We have decreased the numbers here to facilitate modelling. It is important to consider, however, what effect increasing the numbers of presentations would have. A later exercise will be directed towards this question. In the pre-experimental output set (as well as the match and experimental output sets), the words are followed by a tag such as hft or lfd. The hf or lf stands for high frequency and low frequency respectively, and the t or d stands for target or distractor. This tag just allows you to easily remember the type of each word without having to cycle through the relevant pattern sets. Exercise 24: Click through the pre-experimental output set. How many presentations are there? How many unique words are there? The experimental set represents a subject\\'s experience during the study list. At study, words are all presented the same number of times and in the experimental output set each word appears just once. In all cases the study context is the same. Note that only target words appear in the experimental list. Exercise 25: Click through the experimental output set. How many words are there? The final collection of pattern sets are those that will be used for testing the network. The input set contains the Study Context pattern and the Generalized Context pattern. When testing episodic recognition the Study Context pattern should be selected, when testing generalized familiarity the Generalized Context pattern should be selected. The output set contains no patterns because these sets will only be used for cycling, not for learning. The match set contains a copy of each of the words - both the targets and the distractors. Exercise 26: Click through the match set. How many words are there? Now we are ready to train and test the system. Train the network for one epoch with the Pre-experimental input and output sets and then for one epoch with the Experimental input and output sets (If you are learning for a second time remember to reset the weights - Actions Menu - so that current learning doesn\\'t accumulate with the prior learning). To test whether the network is familiar with a word in the study context, or is familiar with a word generally (it can be both): select the test output set; select the word from the match set; select either the Study Context or the General Context from the test input set and cycle once. Exercise 27: Simulate the generalized familiarity task and fill in the the dot product values in Table 1 below. Table 1: Generalized Familiarity Task: Dot Product Values High Frequency Target Low Frequency Target High Frequency Distractor Low Frequency Distractor child avery horse crept phone elope space flank woman adage eight broth light dally sound envoy visit graft april aural green banjo leave debit river fidel table guise MEANS Exercise 28: Simulate the episodic recognition task and fill in the the dot product values in Table 2 below. Table 2: Episodic Recognition Task: Dot Product Values High Frequency Target Low Frequency Target High Frequency Distractor Low Frequency Distractor child avery horse crept phone elope space flank woman adage eight broth light dally sound envoy visit graft april aural green banjo leave debit river fidel table guise MEANS Exercise 29: Produce graphs similar to those in figure 11 for the mean values of the dot products. That is, plot the mean dot product values for targets and distractors for both low and high frequency words in the generalized familiarity condition on one graph, and the mean dot product values for targets and distractors for both low and high frequency words in the episodic recognition condition on another graph. Are the generalized familiarity graphs flatter than the episodic recognition graphs? Why? Exercise 30: In the generalized familiarity graph the model\\'s results tend not to be as flat as the subject\\'s data. Why might this be the case, and does it represent a refutation of the model? (Hint: consider the nature of pre-experimental experience). Objective Checklist In this chapter, we have been looking at the Matrix Model of long term memory. The following is a check list of skills and knowledge which you should obtain while working on this chapter. Go through the list and tick off those things you are confident you can do. For any item outstanding, you should refer back to the appropriate section or consult your tutor. understand the distributed representation of items and associations calculate the vector memory values when two patterns are superimposed, in terms of: network weights, Cartesian co-ordinates, vector addition. explain the difference between matching and retrieval tasks and model this difference in the Matrix Model explain the difference between episodic and semantic tasks and model this difference in the Matrix Model References Bain, J.D., & Humphreys, M.S. (1989). Instructional reinstatement of context: The forgotten prerequisite. In K. McConkey and A. Bennett (Eds.), Proceedings of the XXIV International Congress of Psychology, Vol. 3. Elsevier, North-Holland. Halford, G. S., Wiles, J., Humphreys, M. S., & Wilson, W. H. (1992). Parallel distributed processing approaches to creative reasoning: Tensor models of memory and analogy. unpublished manuscript. Humphreys, M.S., Bain, J.D., & Burt, J.S. (1989). Episodically unique and generalized memories: Applications to human and animal amnesics. In S. Lewandowsky, J.C. Dunn & K. Kirsner (Eds.) Implicit Memory: Theoretical Issues. (pp. 139-158). Erlbaum Associates: Hillsdale, N.J. Humphreys, M.S., Bain, J.D., & Pike, R. (1989). Different ways to cue a coherent memory system: A theory for episodic, semantic and procedural tasks. Psychological Review, 96, 208-233. Pike, R. (1984). A comparison of convolution and matrix distributed memory systems. Psychological Review, 91, 281-294. Wiles, J., & Humphreys, M.S. (1993). Using artificial neural networks to model implicit and explicit memory. In P.Graf & M. Masson (Eds.) Implicit Memory: New Directions in Cognition, Development, and Neuropsychology. (pp. 141-166). Erlbaum: Hillsdale, New Jersey. \\n\\t\\n\\n-->\\n\\n\\nConnectionist Models of Cognition: Long Term Memory and the Matrix Model\\n\\n\\nLong Term Memory and the Matrix Model\\n\\nSimon Dennis, Janet Wiles and Rachael Gibson\\nSpecial thanks to Jill White\\n\\n\\n\\nMEMORY! Always there, of course, but usually hidden. And then,\\nsometimes, as a result of just the right kind of push, it could emerge\\nsuddenly, sharply defined, all in color, bright and moving and alive.\\nRobots and Empire, Isaac Asimov \\n\\n\\nTable of Contents\\n\\nIntroduction\\n\\nMatching versus Retrieval Tasks: The Nature of the Output\\nEpisodic versus Semantic Tasks: The Role of Context\\n\\nTensors Explained\\n\\nVectors - Rank One Tensors\\nMatrices - Rank Two Tensors\\nTensors of Rank 3 and Above\\n\\nThe Matrix Model\\n\\nMemory Representations\\nAccessing Memory Representations\\nMatching versus Retrieval Tasks: Scalar or Vector Output\\n\\nEpisodic versus Semantic Memory: Cuing with the Context Vector\\n\\n\\nObjective Checklist\\nReferences\\nAnswers to Questions\\n\\nIntroduction\\n\\nLong term memory has a profound effect on the way we live our everyday\\nlives and is integral to many aspects of cognition. Whether we are\\nrecalling episodes of childhood abuse in the witness box, retrieving\\nour favourite pumpkin scone recipe, or just trying to remember where we\\nparked the car, memory plays a key role in allowing us to function,\\nproviding the texture of our experiences and defining our\\nidentities.\\n\\nMost of us have had the annoying experience of recognizing a face but\\nnot being able to remember to whom it belongs.  We curse, make a\\nreference to our age and reach for the nearest pop-psychology manual on\\nhow to improve our memories in ten easy steps.  When you stop to think\\nabout it, however, the really astonishing thing is how often we don\\'t\\nforget. In a study by Shepard (1967) subjects were given a list of 580\\narbitrary words to remember. On a forced choice test they scored at\\njust under 88% correct. That\\'s impressive. Furthermore, I could ask you\\nwhat you were doing at 12:30pm one week ago and there is a good chance\\nyou would be correct, even if it isn\\'t something you do all the time.\\nYet between then and now you have had thousands of experiences any one\\nof which I could have queried. Somehow all of those experiences have\\nleft their mark, often without you even thinking about it. Long term\\nmemory is big.\\n\\nThe other really astonishing thing about memory is how flexibly we can\\naccess it. Suppose I ask you to list all the situation comedies you\\nhave ever watched. Most people from television dependent cultures wont\\nhave too many difficulties coming up with a reasonably large\\ncollection. Yet how could you answer this question given the obscure\\ncue \"situation comedy\"? A database system might store a set of records\\nsuch as {(Giligan\\'s Island, situation comedy), (Full House, situation\\ncomedy), (World News, current affairs), etc.} and then cycle through\\nthese one at a time retrieving those that have the situation comedy\\ntag. But this would require that when you are watching television shows\\nyou are constantly tagging them as situation comedy, current affairs\\netc. If you had tagged the program as \"the funny show with the skinny\\nsailor in it\" the search for situation comedy would come back with\\nNO RECORDS FOUND. Human memory is much more robust.  We often\\nuse what seem to be very obscure cues yet we are able to retrieve well.\\n\\n\\nMuch of the research into human memory has been an exploration of just\\nhow flexible our memories are - of the different sorts of questions\\nthat we can use our memories to answer. In this chapter, we will\\nconsider two ways in which the questions that we ask of our long term\\nmemories can differ. The first of these refers to the nature of the\\noutput that a question requires - are we asking for a specific\\nname, word or other item from memory (retrieval) or do we require a\\nyes/no answer about whether we remember some fact or episode\\n(matching). The second distinction refers to the role of context in the\\nquery - are we asking about what happened in a given episode or context\\n(episodic) or is the query about the way that things tend to be in\\ngeneral (semantic). After considering these tasks, we will look in some\\ndetail at the Matrix Model of long term memory (Pike, 1984; Humphreys,\\nBain & Pike, 1989) which provides a theory of how these questions could\\nbe answered using the formalism of matrix algebra.\\n\\nMatching Versus Retrieval Tasks: The Nature of the Output\\n\\nMemory tasks differ with respect to the output that is required. The\\nusual task that people have in mind when they think about human memory\\nis one in which you are given one piece of information such as\\nsomeone\\'s face and you must retrieve another piece of information such\\nas that person\\'s name. These sorts of tasks seem to rely on a discrete\\nor discontinuous form of information. The required output is an item.\\nTwo common examples of retrieval tasks are free association and\\ncued recall with a list associate.\\nFree Association provides a subject with a cue (e.g. \"Type of\\nanimal\") and requires them to respond with the first word that comes to\\nmind (e.g.  cat). Sometimes a prior study list is presented to the\\nsubject and it has been shown that words that occur in the prior study\\nlist are more likely to be produced even though the subject is not\\ninstructed to use the study list to make their decisions. Free association\\nis a retrieval task because the required response is a word.\\nCued Recall with a List Associate requires the subject to study\\na list of pairs of words. At test, subjects are given a list of words\\nand asked which word occurred with each of the test words during the\\nstudy episode (e.g. \"Which word occurred with boy in the study\\nlist?\"). Again, cued recall with a list associate is a retrieval task\\nbecause it requires the subject to respond with a word.\\n\\nIn contrast some memory tasks known as matching tasks require\\nwhat seems to be a more quantitative answer based on a continuous\\nmeasure. The examples with which we will be concerned are familiarity\\nrating and recognition.\\nFamiliarity rating refers to a task in which subjects rate (on a\\nfive point scale, for instance) how familiar a word is to them in\\ngeneral (e.g. \"How familiar is the word house to you?\").  \\nFamiliarity rating is a matching task because it seems to be based on a\\ncontinuous form of information.\\nRecognition requires a subject to study a list of words. At test,\\nthe subject is given a second list of words - some of which appeared in\\nthe first list and some of which did not. The subjects\\' task is\\ndistinguish the targets (words that were on the list) from distractors\\n(words that weren\\'t on the list). Either they are asked to make a\\nyes/no decision or they provide their confidence that the word is old\\n(which might be rated on a five point scale, for instance). Recognition\\nis considered a matching task because it relies upon a continuous form\\nof information.\\n\\nEpisodic and Semantic Tasks: The Role of Context\\n\\nAnother important dimension on which memory tasks can vary is whether\\nthey make reference to a study episode. Tasks that do specify the study\\nepisode are known as episodic tasks, whereas tasks that do not are known\\nas semantic tasks (or generalized tasks). Tulving (1972) realized that the\\nlearning in a study episode is not continuous with the learning that\\noccurs before study. In particular, he realized that when we ask a\\nsubject in a recognition task do they recognize a word we are not\\nasking them whether they know the word at all (often all of the test\\nwords are known to the subject). What we are asking is if the word\\noccurred in a given list (the study list). Similarly, in cued recall\\nwith a list associate, we are not asking what word generally goes with\\nboy.  We are asking what word went with boy in the study\\nlist. In contrast, familiarity rating and free association make no\\nspecific reference to a study episode and are semantic tasks.  Table 1\\ncategorizes the four memory tasks described above in\\nterms of the nature of the output and the role of context.\\n\\nTable 1: Examples of Episodic/Semantic Tasks and Matching/Retrieval\\nTasks (adapted from Humphreys, Bain & Pike, 1989)\\n\\n\\nUse of Context Cue\\nAccess ProcessEpisodic MemorySemantic Memory\\nMatchingproduces a rating valueRecognitionFamiliarity Rating\\nRetrievalproduces a wordCued RecallFree Association\\n\\n\\n\\nBefore looking at how the Matrix Model accounts for the differences between\\nthese tasks we need a grounding in tensors, the operations that can be\\nperformed on tensors and how tensors can be mapped to neural network\\narchitectures. If you are comfortable with these ideas you may skip\\nthe next section.\\n\\n\\nTensors Explained\\n\\nThe Matrix Model of memory is built upon the mathematics of tensors.\\nTensors are convenient ways of collecting together numbers. For instance,\\na vector, which is also known as a rank one tensor, could be used to\\ndescribe the age, gender, and salary of an employee. If Jeff is 50\\nyears old, is male (where male = 0 and female = 1) and earns $56000 per\\nannum then we could describe Jeff with the vector [50, 1, 56000] (see\\nfigure 1). Note that vectors (and tensors in general) are ordered. The\\nvector [56000, 1, 50] would describe someone who was 56000 years old\\nwho made a total of $50 per annum!\\n\\n\\n\\n\\nFigure 1: Examples of vectors. (a) a row vector describing Jeff, (b) a\\ncolumn vector, (c) a vector with N components.\\n\\nThe rank one tensor described above has a dimension of three because it\\ncontains three components. There is no reason that vectors need be\\nrestricted to three dimensions, however. We could have added shoe size,\\nfor instance, to increase the dimension to four. Similarly, there is no\\nreason that we need to restrict ourselves to a single row of numbers.\\nA tensor with N rows and M columns is known as an NxM matrix and has a\\nrank of two, indicating that the array of numbers extends in two\\ndirections (see figure 2). \\n\\n\\n\\nFigure 2: Examples of matrices. (a) a 2x2 matrix, (b) a 3x2 matrix, (c) \\nan NxN matrix.\\n\\nThe process of extending the number of directions in which the array\\nextends can theoretically continue indefinitely, creating tensors of\\nrank three, four, five etc.  In the following sections, we will look at\\nvectors, matrices and tensors of rank three (see figure 3) as they are\\ncritical to understanding the Matrix Model.  Other models, such as the\\nSTAR model of analogical reasoning (Halford, Wiles, Humphreys and\\nWilson 1992), employ tensors of higher rank.\\n\\n\\n\\n\\n\\nFigure 3: A rank three tensor (NxNxN).\\nVectors - Rank One Tensors\\n\\nTensors, and in particular vectors, can be represented in \\nmany different forms including:\\n\\nCartesian form in which the components are enumerated explicitly.\\nFigure 1 depicts vectors represented in Cartesian form.\\nGeometric form in which the vector is plotted in N dimensional\\nspace. \\nFor instance, figure 4 shows the vector representing Jeff plotted in three \\ndimensional space. \\n\\n\\n\\nFigure 4: The vector representing Jeff plotted in three dimensional space (Geometric form).\\nAlgebraic form in which a vector is represented as a\\nbolded lower case letter (e.g. v). Algebraic form is\\na particularly concise form of representation, which makes it\\neasy to talk about the operations that can be performed on vectors\\nsuch as addition (e.g. w = v + t).\\nNeural network form which diagrams a neural network architecture\\nin which either a set of units or a set of weights contain the \\nelements of the vector.\\nFor instance, a vector can be mapped to a two layer network (one input\\nand one output layer) as depicted in Figure 5.  The number of units in\\nthe input layer corresponds to the number of dimensions in the original\\nvector, while the output layer contains only 1 unit.  Each input unit\\nis connected to each output unit. The input units represent one vector\\nand the weights represent a second vector. \\n\\n\\n\\nFigure 5: The network corresponding to a vector memory.\\n\\nThe output of this network is defined to be the dot product (or inner\\nproduct) of the input and weight vectors. A Dot Product is\\ncalculated by multiplying together the values which are in the same\\nposition within the two vectors, and then adding the results of these\\nmultiplications together to get a scalar (see Figure 6a). In the case\\nof the neural network, this involves multiplying each input unit\\nactivation by the corresponding weight value and then adding. The dot\\nproduct of two vectors represents the level of similarity between them\\nand can be extended to higher rank tensors (see figure 6b)\\n\\n\\n\\nFigure 6: The Dot Product.\\n\\nThe dot product is expressed algebraically as a dot, that is, the dot\\nproduct of the vectors v and w is written\\nv.w.\\n\\nLearning occurs in this network by adding the input vectors.  Vector\\naddition superimposes vectors of the same dimension.  It is\\ncalculated by adding together the elements in a particular position in\\neach vector (see Figure 7a). In this way, multiple memories can be\\nstored within the same vector.  [Note: the network actually employs\\nHebbian learning (see Neural Networks by Example: Chapter three).\\nHowever, when the output unit is fixed at one Hebbian learning is\\nidentical to vector addition.]\\n\\n\\n\\n\\nFigure 7: (a) Vector Addition, (b) Matrix Addition.\\n\\nAgain vector addition can be extended to tensors of arbitrary rank (see\\nfigure 7b). Vector addition is expressed algebraically as a plus sign\\n(+). So if we wanted to talk about the dot product of v with the\\naddition of w and x we would write v.(w +\\nx). Another useful property to keep in mind is that the dot product\\ndistributes over addition. That is:\\nv.(w + x) = v.w + v.x\\n\\n\\nIn the following exercises, you will build a vector network that learns\\nto discriminate between stored items and new items (see figure 8). \\n\\n\\n\\nFigure 8: A vector memory network.\\n\\nFollow the\\ninstructions below to create the network and then work through the\\nexercises.\\n\\nLoad BrainWave\\nSelect \\'New Hebbian Network\\'\\nSet up the vector memory network:\\n\\nCreate two input units and one output unit.\\nConnect both input units to the output unit.\\nSet up VALUE objects for the units and weights.\\n\\nCreate the data sets:\\n\\nCreate an Input Set containing the two input units\\nCreate a Test Set containing the two input units\\nCreate an Output Set containing the output unit\\nThe items in this exercise are FROG  [0.95, 0.32], TOAD [0.49, 0.87],\\nKOALA [0.32, -0.95]. Add the FROG pattern to the input set.\\nAdd a pattern containing a 1 to the output set.\\nAdd all three items, FROG, TOAD and KOALA, to the test set.\\n\\n\\nExercise 1:  Train the network for one epoch and record the weights\\nin the TRAIN FROG row of the following table. How have the weights\\nchanged?  \\n\\n\\nWeight 1Weight 2\\nTRAIN FROG\\xa0\\xa0\\nTRAIN FROG & KOALA\\xa0\\xa0\\nTRAIN FROG & TOAD\\xa0\\xa0\\n\\n\\nExercise 2:  Test each of the items, FROG, TOAD and KOALA, and\\nrecord the match values (the activation of the output unit) in the\\nsecond table. Explain the match values.\\n\\n\\nTEST FROGTEST TOADTEST KOALA\\nTRAIN FROG\\xa0\\xa0\\xa0\\nTRAIN FROG & KOALA\\xa0\\xa0\\xa0\\nTRAIN FROG & TOAD\\xa0\\xa0\\xa0\\n\\n\\nExercise 3: Train the network for one more epoch and test again.\\nWhat happens to the match values after a second training trial? Why?\\n\\nExercise 4: Add KOALA to the input set and an output value of 1 to\\nthe output set.  Zero the weights (using the ACTIONS menu) and retrain\\nthe network on the updated input set.  Test the network as before,\\nrecording the values in the table in the TRAIN FROG & KOALA\\nrow.\\nExercise 5: Delete KOALA from the input set and add TOAD. Zero the\\nweights, retrain and test as above, recording the values in the TRAIN\\nFROG & TOAD row. You should have six weight values and nine match\\nvalues for each training trial.  Create a graph of the match values\\nafter the first training trial: plot three lines, one for each test\\nitem, against the three training conditions.  Explain the shape of each\\nline on the graph.  \\nExercise 6: For each of the three training conditions (FROG alone, FROG \\n& KOALA, FROG & TOAD): \\n\\nDraw the geometric (graphical) representation of the weights,\\nProvide the algebraic representation of the weights.\\n\\n\\n\\n\\nMatrices - Rank Two Tensors\\n\\nThe vector memory, discussed above, was capable of storing items so\\nthat at a later time it could be determined if they had appeared. A\\nmatrix memory allows two items to be associated - so that given one we\\ncan retrieve the other. Algebraically, a matrix is usually represented\\nas a bolded upper case letter (e.g. M).\\n\\nAssociations are formed using the outer product operation. A outer\\nproduct between two vectors is calculated by multiplying each\\nelement in one vector by each element in the other vector (see Figure\\n8). If the first vector has dimension d1 and the second\\nvector dimension d2, the outer product matrix has dimension\\nd1xd2. For instance, a three dimensional vector\\nmultiplied by a two dimensional vector has dimension 3x2.\\n\\n\\n\\nFigure 8: The outer product.\\n\\nThe outer product operation is expressed algebraically by placing the vectors\\nto be multiplied next to each other. So the outer product of v\\nand w is written as v w.\\n\\nThese association matrices are then added into the memory matrix (as in\\nthe vector memory case) - so that all associations are stored as a\\nsingle composite.  A matrix memory maps to a two layer network (one\\ninput and one output layer) as depicted in Figure 9.  The number of\\ninput units corresponds to the number of rows in the original  matrix,\\nwhile the number of output units corresponds to the number of columns.\\nEach input unit is connected to each output unit.\\n\\n\\n\\nFigure 9: The network representation of a matrix.\\n\\nIn the following exercise you will use a matrix memory network to store\\nand recall pairs of items. \\nExercise 7: Load the simulator, BrainWave. From the NETWORKS menu -\\nselect Matrix Model (1). What rank tensor does this network implement? What\\nare its dimensions?\\n\\nExercise 8: The items in this exercise are:\\n\\nCues:\\n\\nFROG    [0.5, -0.5, 0.5, -0.5]\\n\\nKOALA   [0.5, 0.5, -0.5, -0.5]\\n\\nSNAIL   [-0.5, 0.5, 0.5, -0.5]\\n\\nTOAD    [0.5, 0.4, 0.6, 0.45]\\n\\n\\nTargets:\\n\\nFLIES   [0.7, 0.5, 0.5]\\n\\nLEAVES  [0.7, -0.5, -0.5]\\n\\nLETTUCE [0, -0.7, 0.7]\\n\\n\\n\\nThe input set contains the items FROG, KOALA and SNAIL, paired with\\nitems in the output set FLIES, LEAVES and LETTUCE, respectively.\\nAnother input item, TOAD [0.5, 0.4, 0.6, 0.45], can be used to test the\\nnetwork on unfamiliar input.\\nCalculate the similarity value (i.e. dot product) of the  items FROG, KOALA,\\nSNAIL and TOAD with themselves, and each other,\\nand record the values in the table below:\\n\\n\\nFROGKOALASNAILTOAD\\nFROG\\xa0\\xa0\\xa0\\xa0\\nKOALA\\xa0\\xa0\\xa0\\xa0\\nSNAIL\\xa0\\xa0\\xa0\\xa0\\nTOAD\\xa0\\xa0\\xa0\\xa0\\n\\n\\nExercise 9: Train the network for one epoch.  Test each of the items\\nFROG, KOALA, SNAIL and TOAD.  What output is produced in each case?\\n(Give the output pattern and also describe the output patterns in terms\\nof their similarity to FLIES, LEAVES and LETTUCE).\\n\\n\\nFROG\\xa0\\xa0\\xa0\\nKOALA\\xa0\\xa0\\xa0\\nSNAIL\\xa0\\xa0\\xa0\\nTOAD\\xa0\\xa0\\xa0\\n\\n\\n\\nExercise 10: Give the algebraic equation that describes the matrix memory\\nformed from the three pairs of associates: \\n\\n\\nM = \\n\\nExercise 11: Give the equations that describe each of the \\nretrievals in exercise 9.  Use the similarity measures from the table\\nabove to simplify each equation to a weighted sum of the target patterns.\\n\\nFROG\\n\\nKOALA\\n\\nSNAIL\\n\\nTOAD\\n\\n\\n\\n\\nTensors of Rank Three and Above\\n\\nThe final sort of tensor we need to demonstrate the matrix model is the\\nrank three tensor. The rank three tensor allows a three way association\\nto be represented. For instance, we could store the information that\\nJohn loves Mary - [Loves John Mary] - or that Apple appeared with\\nPencil in List 1 [List 1, Apple, Pencil].\\n\\nA tensor of rank three maps to a three layer network (one input layer\\nwith two sets of units, one output layer, and one layer of hidden\\nunits) as depicted in Figure 10.  The number of units in the input sets\\nand the output set correspond to the dimensionality of the tensor.  The\\nnumber of hidden units corresponds to the number of units in one input\\nset times the number of units in the other input set.  Each hidden unit\\nhas a connection from one input unit from each input set, with a hidden\\nunit existing for each possible combination.  These hidden units are SigmaPi\\nunits, the value of which is set to the multiplication of the two input\\nunits to which it is connected. To implement a rank three tensor, the\\nweights in the first layer are frozen at one. Consequently, a hidden\\nunit\\'s activation will equal the multiplication of the activations of\\nthe input units to which it is connected. Each hidden unit is then\\nconnected to each output unit.\\n\\n\\n\\nFigure 10: The network representation of a rank three tensor.\\n\\nIn these exercises, you will use both rank two and three tensor\\nnetworks to store and recall triples of items.\\n\\nExercise 12: Load the simulator, BrainWave.\\nFrom the NETWORKS menu - Matrix Model (2).\\nWhat rank tensor does this network implement?\\n\\nExercise 13: The items in this exercise are:\\nCues:\\n\\nFROG     [0.5, -0.5, 0.5, -0.5]\\n\\nKOALA    [0.5, 0.5, -0.5, -0.5]\\n\\nSNAIL    [-0.5, 0.5, 0.5, -0.5]\\n\\nTOAD     [0.5, 0.4, 0.6, 0.45]\\n\\n\\nRelations:\\n\\nEATS     [0.5, -0.5, -0.5, 0.5]\\n\\nLIVES-IN [0.5, 0.5, -0.5, -0.5]\\n\\nTargets:\\n\\nFLIES    [0.7, 0.5, 0.5]\\n\\nLEAVES   [0.7, -0.5, -0.5]\\n\\nLETTUCE  [0, -0.7, 0.7]\\n\\nPOND     [0.89, 0.43, -0.22]\\n\\nTREE     [-0.22, 0.76, 0.62]\\n\\nSHELL    [0.43, -0.5, 0.76]\\n\\n\\nNotice that the vectors for the cues are the same as those used above.\\nAlso notice that EATS and LIVES-IN are orthogonal to each other - that is they\\nhave a dot product of zero.\\n\\nCalculate the similarity (dot product) table for the targets.\\n\\n\\n\\nFLIESLEAVESLETTUCEPONDTREESHELL\\nFLIES\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nLEAVES\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nLETTUCE\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nPOND\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nTREE\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nSHELL\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n\\n\\nExercise 14: The cue+relation input set contains the items\\nFROG-EATS, KOALA-EATS, SNAIL-EATS, FROG-LIVES_IN, KOALA-LIVES_IN and\\nSNAIL-LIVES_IN, paired with items in the output set FLIES, LEAVES,\\nLETTUCE, POND, TREE, and SHELL, respectively.  Two other input items,\\nTOAD-EATS and TOAD-LIVES_IN, can be used to test the network\\'s response\\nto unfamiliar input.\\n\\nTrain the network for one epoch.  Test each of the items FROG-EATS,\\nKOALA-EATS, SNAIL-EATS, FROG-LIVES_IN, KOALA-LIVES_IN, SNAIL-LIVES_IN,\\nTOAD-EATS and TOAD-LIVES_IN.  What output is produced in each case?\\n(Give the output pattern and also describe the output patterns in terms\\nof their similarity to FLIES, LEAVES, LETTUCE, POND, TREE and SHELL)\\n\\n\\nFROG-EATS\\n\\n\\nKOALA-EATS\\n\\n\\nSNAIL-EATS\\n\\n\\nFROG-LIVES_IN\\n\\n\\nKOALA-LIVES_IN\\n\\n\\nSNAIL-LIVES_IN\\n\\n\\nTOAD-EATS\\n\\n\\nTOAD-LIVES_IN\\n\\n\\n\\n\\n\\nExercise 15: How does the performance of this network compare with\\nthe performance of the network in Exercise 8?  Why is it not as\\ngood?\\nExercise 16: Give the algebraic equation that describes the matrix memory\\nformed from the three pairs of associates: \\n\\n\\nM = \\n\\nExercise 17: Give the equations that describe each of the retrievals\\nfrom exercise 14.  Use the similarity measures from the table\\nabove to simplify each equation to a weighted sum of the target patterns.\\n\\n\\nFROG-EATS\\n\\n\\nKOALA-EATS\\n\\n\\nSNAIL-EATS\\n\\n\\nFROG-LIVES_IN\\n\\n\\nKOALA-LIVES_IN\\n\\n\\nSNAIL-LIVES_IN\\n\\n\\nTOAD-EATS\\n\\n\\nTOAD-LIVES_IN\\n\\n\\n\\n\\n\\nExercise 18: From the NETWORKS menu - select Matrix Model (3).\\nWhat rank tensor does this network implement?\\n\\n\\nExercise 19: The inputs and outputs for this network are the same as\\nfor the previous one, but the connections and hidden SigmaPi units\\nperform different calculations on the inputs to try and achieve the\\ncorrect outputs.  Train the network for one epoch.  Test each of the\\nitems FROG-EATS, KOALA-EATS, SNAIL-EATS, FROG-LIVES_IN, KOALA-LIVES_IN,\\nSNAIL-LIVES_IN, TOAD-EATS and TOAD-LIVES_IN.\\n\\nWhat output is produced in each case? (Give the output pattern and also\\ndescribe the output patterns in terms of their similarity to FLIES,\\nLEAVES, LETTUCE, POND, TREE and SHELL).\\n\\nFROG-EATS\\n\\n\\nKOALA-EATS\\n\\n\\nSNAIL-EATS\\n\\n\\nFROG-LIVES_IN\\n\\n\\nKOALA-LIVES_IN\\n\\n\\nSNAIL-LIVES_IN\\n\\n\\nTOAD-EATS\\n\\n\\nTOAD-LIVES_IN\\n\\n\\n\\n\\n \\nExercise 20: Which of the two networks performs the memory task better?  Why?\\nExercise 21: Give the algebraic equation that describes the matrix memory\\nformed from the three pairs of associates:\\n\\n\\nM = \\n\\n\\nExercise 22: Give the equations that describe each of the cued recall\\ntests from question 19.  Use the similarity measures from the table\\nabove to simplify each equation to a weighted sum of the target patterns.\\n\\n\\nFROG-EATS\\n\\n\\nKOALA-EATS\\n\\n\\nSNAIL-EATS\\n\\n\\nFROG-LIVES_IN\\n\\n\\nKOALA-LIVES_IN\\n\\n\\nSNAIL-LIVES_IN\\n\\n\\nTOAD-EATS\\n\\n\\nTOAD-LIVES_IN\\n\\n\\n\\n\\n\\n\\nIn this section, we have been looking at the way in which tensors of\\nrank one, two and three can be used to store information. In the next\\nsection, we will examine the Matrix Model, which uses precisely this\\nmechanism to explain the nature of human memory.\\n\\n\\nThe Matrix Model\\n\\n \\nThe Matrix Model of Memory was developed by Humphreys, Bain and Pike\\n(1989) and Pike (1984) to provide a coherent theoretical account of a\\nrange of different memory tasks, including episodic tasks, such as\\nrecognition and recall, and semantic tasks, such as familiarity rating\\nand indirect production tasks.  It is a distributed associative model\\nin which items are modelled and stored as vectors of feature weights or\\nelements, just as was the case in the previous section.  Elements\\nwithin each vector contribute conjointly to the representation of\\nitems.  Thus memory representations are not located at specific points\\nwithin a memory network, or within specific memory systems.  Instead\\nthey are conceptualised as unique patterns of activation over a common\\nset of elements. Typically, these patterns are thought to be sparse\\nrepresentations meaning that only a few of the elements are active.\\nMemory Representations\\n\\nThe memory representations in the Matrix Model include items, contexts\\nor, combinations of items and contexts (associations).\\n\\nItems - Items can be any sort of stimuli including words, \\npictures, melodies etc. For the most part, however, the experiments to\\nwhich the model has been applied use words. Each\\nitem is modelled as a vector of feature weights.   Feature weights are\\nused to specify the degree to which certain features form part of an\\nitem.  There are two possible levels of vector representation for items.\\nThese are:\\n\\nmodality specific peripheral representations\\n\\t(e.g., graphemic or phonemic representations of words)\\nmodality independent central representations\\n\\t(e.g., semantic representations of words).\\n\\n\\nItem vectors are distinguished by subscripts (e.g. ai). A\\ndistractor vector is indicated by a d.\\nContexts - To distinguish between episodic and non-episodic\\ntasks the Matrix Model assumes the episode or context in which items\\nare studied is also represented by a vector of feature weights. In\\nepisodic tasks this context vector must be reinstated so that it may be\\nused as a cue to the memory system. The context vector is represented\\nby an x.\\nAssociations - While individual items and contexts are\\nrepresented as single vectors  (a, b, x),\\nassociations between items and contexts are represented by matrices\\nderived from the matrix product of  these vectors.  The resulting\\nmatrix product represents the association (or binding) between either\\nitems, or between items and context. The memory of the matrix Model is\\nformed by adding these associations together. The model posits a number\\nof different kinds of associations including:\\n\\nTwo-way associations between a single item (a) and a context (x,\\ne.g. bacon x breakfast this morning) are represented as a\\ncontext-to-item association  (x a),  where \\nx  = n element column vector \\na  = n element row vector\\nAssociations between a list of items (a1,\\na2,...,ak) and a context (x)\\nare represented by multiplying each of the item vectors by the context\\nvector and summing the resulting matrices.  This sum represents the\\nmemory of the study list (E).\\nE = x a1 + x a2 + ... + x ak \\nThree-way associations between a list of word pairs\\n(a1 b1, a2 b2, ...\\nak bk) and context (x, e.g., bacon x dog x\\nbreakfast this morning) are represented by the rank three tensor\\n(x aj bj), where\\nx  = n element column vector\\naj = n element row vector\\nbj = n element orthogonal vector\\n\\nThe resulting associations can be summed to form the memory for the\\nlist (E).\\nE =  x aj bj\\n\\nNote: the type of vector (i.e., row, column, orthogonal) can also be\\ninferred from the order of the vector symbols, where:  1st vector =\\ncolumn vector;  2nd vector = row vector; and 3rd vector = orthogonal.\\nPre-existing memories (S) are added to list memories\\n(E) because test performance can be influenced by both list\\nmemories and pre-existing memories.\\nM = x aj bj  + S \\n\\n\\xa0\\n\\xa0\\n\\xa0\\n\\nAccessing Memory Representations\\n\\nHaving constructed the memory matrix, we can now see how the Matrix\\nModel goes about accessing this representation at test for a number of\\ndifferent tasks. All retrieval in the Matrix Model is direct.\\nThe memory matrix is presented with cues and access occurs in\\nparallel.  There is no sequential search process. Presenting the model\\nwith a cue involves taking the inner product (or dot product) of the\\ncue vector with the memory matrix.\\n\\nOne of the strengths of the Matrix Model is in the number of a ways in\\nwhich information from the model can be accessed. In the introductory\\nsection, two dimensions on which tasks can differ were outlined. \\nThe first was the matching/retrieval dimension. Matching tasks are\\nthose based on a continuous form of information that typically require\\neither a yes/no answer or a rating response (e.g. recognition).\\nRetrieval tasks, by contrast, require a specific item to be returned\\n(e.g. cued recall). This distinction is captured in the Matrix Model by\\nthe nature of the tensor that results once all cues have been applied.\\nIf the resultant tensor is a scalar we are dealing with a matching\\nprocess. This scalar can be compared against criteria to determine a\\nyes/no or rating value.  If the resultant tensor is a vector then we\\nhave a retrieval process. The vector can be compared against all item\\nvectors with the item being the output of the process. The next sections,\\ngoes through the mathematics of recognition and cued recall with a \\nlist associate demonstrating how matching and retrieval tasks \\nare accomplished within the model.\\n\\nThe second task dimension discussed in the introduction focussed on the\\nthe episodic/semantic dimension. Episodic tasks refer to a specific\\ncontext, whereas in semantic (generalized) tasks information is\\nintegrated over a large number of experiences. The Matrix Model\\ncaptures this distinction.  In episodic tasks, a reinstated context\\nvector is used as a cue. In semantic (generalized) tasks, a vector\\nwhich is equally similar to all contexts is used so as to average over\\nall experiences with the cue items (typically, this is a vector with\\nall components set to 1/n where n is the dimension of the vector). The\\nsection entitled \"Episodic versus Semantic Memory: Cuing with the\\nContext Vector\" describes an experiment designed to demonstrated the\\nimportance of the distinction and leads you through the process of\\nmodelling this experiment using the BrainWave simulator.\\nMatching Versus Retrieval Tasks: Scalar or Vector Output\\n\\nIn this section, a matching task, namely recognition, and a retrieval\\ntask, namely cued recall with a list associate, are compared within\\nthe Matrix Model framework.\\nRecognition\\n\\nRecognition involves a matching process, where the overall similarity\\nbetween the test cues (x and ai) and memory\\n(M) is calculated.  Because this is an episodic task, the test\\ncues involve both word cues and a context cue.  This episodic matching\\nprocess is accomplished by combining the test cues into an associative\\nmatrix (x ai) and determining a dot product between: \\n\\nthe cue matrix (x ai), and  \\nthe memory matrix (M =   x aj + S). \\n\\n\\n[Note: Because the dot product operation is associative, the results\\nare identical regardless of whether you form a combined x\\nai matrix and then take a dot product or take the dot\\nproduct of each of the cues with the memory matrix progressively.]\\nStudied Test Word (ai) \\nxai . M = xai . ( xaj + S) \\n\\t=   x ai . x aj + x ai . S \\n\\t=   (x . x) (ai . aj) + x ai . S \\n\\t= (x . x) (ai . ai ) +  (x . x) (ai . aj) + xai . S \\n \\nInserting the expected matching value:\\n\\nE[x ai . M] = c s + (k - 1) c m + g\\n\\n\\nwhere \\n\\nc = similarity between the study and test context (assumed to large)\\ns = similarity between the same word encoded at study and test (assumed to be large) \\nm = similarity between different words at study and test (assumed to be small) \\ng = contribution of pre-existing memories\\n\\nNon studied Test Word (d) \\nx d . M = x d . ( xaj+ S)\\n=   xd . xaj + xd . S \\n=   (x . x) (d . aj) + xd . S \\nwhere \\n\\nE[x d . M] = c m k + g\\n\\n \\nNote that the matching operations in the above equations can be\\ncollapsed down into several components, including :\\n\\na match between the test cue and the pre-experimental memories \\n(i.e.,  x ai . S  or  x d . S), and \\na match between the test cue and the experimental memories \\n(i.e.,  x ai . x aj or  x d . x aj ) \\n\\n \\nThe match between the test cue and the experimental memories can\\nfurther be collapsed down into :\\n\\na match between the context on study and test occasions  \\n\\t\\t(x . x = c), and \\na match between the study and test items \\n\\t\\t(ai . ai = s  and  ai . aj = m) or (d . aj = m) \\n\\n\\nThus the final dot product derived from these equations, represents\\nthe match of the contexts on the study and test occasions (c), weighted\\nby the match of the items on the study and test occasions (s and m).\\nConsequently, memories that are conjointly defined by context and test\\ncues will be weighted more heavily than items not studied in that\\ncontext.  This mechanism enables the model to avoid interference (large\\nweights) from other items studied in the same context and also from\\nprevious contexts in which items have appeared.\\nCued Recall with a List Associate\\n\\nCued recall with a list associate involves a subject studying a list of\\npairs. At test they are given an item and are required to produce the\\nword with which it was paired at study. This is an important task\\nbecause it can be used to demonstrate that three way association are\\nnecessary to model human memory. Simple associations two-way\\nassociations between items are insufficient (Humphreys, Bain & Pike\\n1989).\\n\\nFor this reason, cued recall with a list associate is modelled using\\nrank three tensors that associate word pairs (a1\\nb1, a2 b2,...\\nak bk) and context (x). The tensor\\nis formed by taking the outer product of the context vector x and the two item vectors, aj and bj.\\nM =  x aj\\nbj  + S \\n\\nSubjects are then asked to recall list targets (bi) at test,\\nusing list associates (ai) and context (x) as cues.  The\\nretrieval cues (x and aj) are combined to form an\\nassociative matrix cue (x ai).  Retrieval then involves the\\npre-multiplication of the rank three tensor (M) by the retrieval cue\\n(x ai).\\nx ai . M =  x ai .  x aj bj + S \\n= [(x ai)(x aj)] bj + x ai . S\\n= [(x . x) (ai .  aj)] bj + x ai . S\\n= (x . x) (ai . ai) bi  +   (x . x) (ai . aj) bj + x ai . S\\n\\nInserting the expected values: \\n\\nE[x ai . M] = c s bi + c m bj  + x ai . S\\n\\nThe end product (matrix product) of this process will comprise a target\\nvector of feature weights.  This featural information can be used to\\nproduce a word or item response.\\n\\n\\n\\nThe target vector is weighted by: \\n\\nthe similarity of the context on the study and test occasions  \\n\\t( x . x = c), and \\nthe similarity of the list cue on the study and test occasions \\n\\t(ai . ai = s) and (ai . aj = m) \\n\\n \\nNote that the weights for the same associate (s) will be greater than\\nthe weights for different associates (m) making the resulting vector\\nlook more like the correct associate (on average) than any other item.\\nNoise will also be generated by the pre-existing memories. The\\nassumption is that, in general, the similarity of the pre-existing\\ncontexts and the current context will be small leading to low levels of\\ninterference. Of course, if a recent context also included the cue word\\nthen much more interference will be generated because the context\\nvectors will be more similar.\\n\\n\\n\\nIn the last two sections we have seen how, in a mathematical sense, the\\nMatrix Model distinguishes between matching and retrieval tasks.  In\\nthe next section, we will examine the episodic/semantic distinction\\nby using the Matrix Model to simulate data generated by Bain &\\nHumphreys (1989).\\n\\n\\n\\nEpisodic versus Semantic Memory: Cuing with the Context Vector\\n\\nBain & Humphreys (1989, pg. 229) report an experiment which clearly\\ndemonstrates the difference between episodic and semantic matching\\ntasks by reinstating the context during some, but not all, of the test\\nconditions.  Subjects were given a set of words and asked to produce a\\nsynonym for each.  One week later the same subjects were given a\\npassage containing unhighlighted target words, and asked to read the\\ntext and then answer questions on it.  Half of the target words were\\ncommon to both training stages.  In addition to the test items already\\nmentioned (synonym, passage, or both), words which appeared in neither\\ntraining stage were also included as test items.  Each set of test\\nitems contained equal numbers of high and low frequency words.\\n\\nThe subjects were grouped into three test conditions.  Group A was\\nasked to give a general familiarity rating for the words (a generalized\\nmatching condition).  Group B was asked to recognise which words had\\nbeen in the synonym generation task (an episodic matching condition).\\nGroup C was asked to recognise which words had been in the passage\\nreading task (also an episodic matching condition).  The mean\\nrecognition and familiarity ratings are displayed in Figure 11.\\n\\n\\n\\n\\nFigure 11: Mean ratings for three tasks as a function of presentation\\nlist(s) and word frequency. (a) Familiarity Rating Task (b) Recognition\\nof Synonym Task words and (c) Recognition of Passage Task words. Note\\nthat in generalized familiarity task ratings depended only on the\\nfrequency of the word. For the episodic tasks, however, the lists in\\nwhich the subjects were exposed to the word are critical.\\n\\nAs Figure 11 shows, subjects performing the episodic matching tasks were\\naffected by the training context indicated in the task instructions,\\nwhile subjects performing the general matching task were not influenced\\nby the prior training conditions.  Furthermore, the subjects did not\\nhave trouble reinstating the synonym context as opposed to the passage\\ncontext, and vice versa.\\n\\nThese results suggest that subjects are able to distinguish episodic\\nand semantic (or generalized) memory tasks quite well. One explanation\\nis that the episodic and semantic memory systems are located in two\\ndifferent compartments in the brain. In the generalized familiarity\\ntask, subjects access the semantic store, in the episodic recognition\\ntask subjects access the episodic store. This may well be the case,\\nhowever, Humphreys, Bain and Pike (1989) showed using the Matrix model\\nthat it need not be. The episodic/semantic distinction can be captured\\nin a single coherent memory system by assuming differences in the types\\nof cues supplied.\\n\\nIn the following exercises, the Matrix Model will be used to\\ndemonstrate how the difference between generalized familiarity and\\nepisodic recognition can be captured. To simplify the modelling process\\nwe assume a design similar to that employed by Bain and Humphreys\\n(1989), but in which only one study list is presented. What we are\\nlooking for is a difference in the pattern of results for target and\\ndistractor words when asking for generalized familiarity versus\\nepisodic recognition. The key distinction, from the model\\'s point of\\nview, is in the nature of the context cue. In episodic recognition it\\nwill be assumed that the context cue is the same as that at study. In\\ncontrast, when modelling generalized familiarity the context cue will\\nbe a a vector in which all components are 0.1. This context vector will\\nbe similar to all of the pre-experimental contexts and the study\\ncontext to approximately the same degree and will therefore produce an\\noutput which is approximately the mean of all exposures - not just the\\nstudy list exposures.\\nExercise 23: Load the simulator, BrainWave. From the NETWORKS menu -\\nselect Familiarity vs Recognition.  This network contains three sets of\\nunits - the input units, which will contain the context vectors, the\\noutput units, which will contain the items to which a context is\\nassociated and the match units, which contain the item to be tested.\\nWeights are connected between the input units and the output units.\\nWhat rank tensor does this network implement?\\n\\nAbove the units is a global value called \"Dot Product\". This global\\nvalue indicates the dot product of the output units and the match units\\nand is updated when you click on cycle. It is this value which will\\nindicate the strength of a match in both the episodic recognition and\\ngeneralized familiarity conditions.\\n\\nIn addition, there are three collections of pattern sets. The\\npre-experimental sets contain the input/output pairs representing the\\nsubjects experience before entering the experiment. Each context is\\ndifferent indicating that subjects pre-experimental experience with\\nwords arises from many different contexts.  Each context vector has\\njust three units active and these units are active to different\\ndegrees. The same is true for the output patterns which represent the\\nwords. However, some of the word patterns are repeated representing the\\ndifference between high and low frequency words. The high frequency\\nwords are repeated three times while the low frequency words appear\\njust once. Note that real words occur much more often. We have\\ndecreased the numbers here to facilitate modelling. It is important to\\nconsider, however, what effect increasing the numbers of presentations\\nwould have. A later exercise will be directed towards this question.\\nIn the pre-experimental output set (as well as the match and\\nexperimental output sets), the words are followed by a tag such as hft\\nor lfd. The hf or lf stands for high frequency and low frequency\\nrespectively, and the t or d stands for target or distractor. This tag\\njust allows you to easily remember the type of each word without having\\nto cycle through the relevant pattern sets.\\nExercise 24: Click through the pre-experimental output set. How many\\npresentations are there? How many unique words are there?\\n\\nThe experimental set represents a subject\\'s experience during the study\\nlist.  At study, words are all presented the same number of times and\\nin the experimental output set each word appears just once. In all\\ncases the study context is the same. Note that only target words\\nappear in the experimental list.\\nExercise 25: Click through the experimental output set.  How many\\nwords are there?\\n\\nThe final collection of pattern sets are those that will be used for\\ntesting the network. The input set contains the Study Context pattern\\nand the Generalized Context pattern. When testing episodic recognition\\nthe Study Context pattern should be selected, when testing generalized\\nfamiliarity the Generalized Context pattern should be selected. The\\noutput set contains no patterns because these sets will only be used for\\ncycling, not for learning. The match set contains a copy of each of the\\nwords - both the targets and the distractors.\\nExercise 26: Click through the match set.  How many\\nwords are there?\\n\\n\\nNow we are ready to train and test the system. Train the network for one\\nepoch with the Pre-experimental input and output sets and then for one\\nepoch with the Experimental input and output sets (If you are learning \\nfor a second time remember to reset the weights - Actions Menu -\\nso that current learning doesn\\'t accumulate with the prior learning).\\n\\nTo test whether the network is familiar with a word in the study\\ncontext, or is familiar with a word generally (it can be both): select\\nthe test output set; select the word from the match set; select either\\nthe Study Context or the General Context from the test input set and\\ncycle once.\\nExercise 27: Simulate the generalized familiarity task and fill in the\\nthe dot product values in Table 1 below.\\n\\n\\nTable 1:  Generalized Familiarity Task: Dot Product Values\\n\\n\\nHigh Frequency TargetLow Frequency TargetHigh Frequency DistractorLow Frequency Distractor\\nchild\\xa0avery\\xa0horse\\xa0crept\\xa0\\nphone\\xa0elope\\xa0space\\xa0flank\\xa0\\nwoman\\xa0adage\\xa0eight\\xa0broth\\xa0\\nlight\\xa0dally\\xa0sound\\xa0envoy\\xa0\\nvisit\\xa0graft\\xa0april\\xa0aural\\xa0\\ngreen\\xa0banjo\\xa0leave\\xa0debit\\xa0\\nriver\\xa0fidel\\xa0table\\xa0guise\\xa0\\nMEANS\\xa0\\xa0\\xa0\\xa0\\n\\n\\nExercise 28: Simulate the episodic recognition task and fill in the\\nthe dot product values in Table 2 below.\\n\\n\\nTable 2:  Episodic Recognition Task: Dot Product Values\\n\\n\\nHigh Frequency TargetLow Frequency TargetHigh Frequency DistractorLow Frequency Distractor\\nchild\\xa0avery\\xa0horse\\xa0crept\\xa0\\nphone\\xa0elope\\xa0space\\xa0flank\\xa0\\nwoman\\xa0adage\\xa0eight\\xa0broth\\xa0\\nlight\\xa0dally\\xa0sound\\xa0envoy\\xa0\\nvisit\\xa0graft\\xa0april\\xa0aural\\xa0\\ngreen\\xa0banjo\\xa0leave\\xa0debit\\xa0\\nriver\\xa0fidel\\xa0table\\xa0guise\\xa0\\nMEANS\\xa0\\xa0\\xa0\\xa0\\n\\n\\nExercise 29: Produce graphs similar to those in figure 11 for the\\nmean values of the dot products. That is, plot the mean dot product\\nvalues for targets and distractors for both low and high frequency\\nwords in the generalized familiarity condition on one graph, and the\\nmean dot product values for targets and distractors for both low and\\nhigh frequency words in the episodic recognition condition on another\\ngraph. Are the generalized familiarity graphs flatter than the episodic\\nrecognition graphs? Why?\\nExercise 30: In the generalized familiarity graph the model\\'s\\nresults tend not to be as flat as the subject\\'s data. Why might this\\nbe the case, and does it represent a refutation of the model? (Hint:\\nconsider the nature of pre-experimental experience).\\nObjective Checklist\\n\\nIn this chapter, we have been looking at the Matrix Model of long term\\nmemory. The following is a check list of skills and knowledge which you\\nshould obtain while working on this chapter. Go through the list\\nand tick off those things you are confident you can do. For any item\\noutstanding, you should refer back to the appropriate section or consult\\nyour tutor.\\n\\nunderstand the distributed representation of items and associations\\n\\ncalculate the vector memory values when two patterns are superimposed, in terms of:\\n\\nnetwork weights,\\n\\nCartesian co-ordinates,\\n\\nvector addition.\\n\\nexplain the difference between matching and retrieval tasks and model\\nthis difference in the Matrix Model\\nexplain the difference between episodic and semantic tasks and model\\nthis difference in the Matrix Model\\n\\nReferences\\n\\nBain, J.D., & Humphreys, M.S. (1989). Instructional reinstatement of\\ncontext: The forgotten prerequisite. In K. McConkey and A. Bennett\\n(Eds.), Proceedings of the XXIV International Congress of Psychology,\\nVol. 3. Elsevier, North-Holland.\\n\\nHalford, G. S., Wiles, J., Humphreys, M. S., & Wilson, W. H. (1992).\\nParallel distributed processing approaches to creative reasoning:\\nTensor models of memory and analogy. unpublished manuscript.  \\n\\nHumphreys, M.S., Bain, J.D., & Burt, J.S. (1989). Episodically unique\\nand generalized memories: Applications to human and animal amnesics.\\nIn  S. Lewandowsky, J.C. Dunn & K. Kirsner (Eds.) Implicit Memory:\\nTheoretical Issues. (pp. 139-158). Erlbaum Associates: Hillsdale, N.J.\\n\\nHumphreys, M.S., Bain, J.D., & Pike, R. (1989). Different ways to cue a\\ncoherent memory system: A theory for episodic, semantic and procedural\\ntasks. Psychological Review, 96, 208-233.\\n\\nPike, R. (1984). A comparison of convolution and matrix distributed\\nmemory systems.  Psychological Review, 91, 281-294.\\n\\nWiles, J., & Humphreys, M.S. (1993). Using artificial neural networks\\nto model implicit and explicit memory. In P.Graf & M. Masson (Eds.)\\nImplicit Memory: New Directions in Cognition, Development, and\\nNeuropsychology. (pp. 141-166). Erlbaum: Hillsdale, New Jersey.\\n\\n\\n',\n",
       " '\\n Is \"meaning\" the answer to bad spelling?\\n\\n\\n\\nIs \"meaning\" the answer to bad spelling?\\n\\nA reply to Melvyn Ramsden\\'s Rescuing Spelling\\n\\nBack to \\nSpelling Reform Page\\n I now realise that some of my criticism of this book goes too far so please\\ndon\\'t skip my postscript. \\n\\nThere are two sides to this book.  On the surface it is a simple text book explaining how teachers should teach English spelling to children.  Givven that he bases his approach on English as it has been constructed it is likely that there is something of value in what he has to say.  On the other hand the subtext is a starry eyed celebration of English spellings \\'rich tapestry\\' and an attack on the idea of Spelling reform.  If you hav already read this book you might challenge me on the last for he doesn\\'t once mention spelling reform but be patient I will attempt to justify that later.\\n\\n\\tRamsden\\'s basic theme is that the teaching of spelling is in a mess because it has been forgotten that English spelling is based on ensuring that words of the same meaning hav the same spelling.  Hence there are whole families of words that hav the same root and the differing meanings produced by adding prefixes and suffixes in a regular way.  If the pronunciation of a word changes over time the spelling must stay fixed for otherwise its affinity to the other members of its family will be disguised.  Hence machine will be spelt with a \\'ch\\' rather than a \\'sh\\' because this keeps the link in meaning with mechanic.  The secret of spelling, when uncertain of a difficult word, is to recall a similar meaning word and spell it in the same way.\\n\\tRamsden givs the example of a class that was able to suss the spelling of celebration, in which the second \\'e\\' is pronounced as a short /i/, by using the related word celebrity in which the /e/ sound is quite distinct.  Looking at English spelling in this way  explains a lot of the double consonants at the beginning of many English words.  Why has offer two \\'f\\'s?  Simply recall that that there is whole family of words with \\'fer\\' such as differ, transfer.  Clearly \\'fer\\' is a root and \\'offer\\' is simply the prefix \\'of\\' plus \\'fer\\'.  Blowing the dust from my Latin dictionary that I hav not touched since school many years ago I find \"fero, ferre: to bear, bring, carry\".\\n\\tAh yes Latin.  In a way Ramsden\\'s argument is simply the traditional justification for teaching Latin - that it is an excellent way to teach English spelling.  Teaching Latin however is a lost cause but English is so full of Latin that it is possible to teach children the Latin they need by simply opening their eyes to the archaeological remains that litter modern English.  And, to be fair, this goes beyond the Latin heritage.  One of the examples that Ramsden uses (that of \\'attach\\' and \\'detach\\') is actually originally of Germanic origin.\\n\\tBut is meaning really such a solid rock to build a spelling system on?  The family of meanings I gave earlier of \\'offer\\', \\'transfer\\' etc includes \\'suffer\\'.  There is far from any obvious connection between the meaning of \\'offer\\' and \\'suffer\\'.  \\'Suffer\\' is made up of the prefix \\'suf\\', which means under, plus carry.  That is \\'under carry\\' and that\\'s not too far from endure.  Logical if you know but obvious?  And meanings change just as does pronunciation.  One of Ramsden\\'s examples is the family round the root \\'aster\\'.  Without even mentioning Latin it is quickly obvious that a family that includes asteroid, asterisk, astronaut must hav something to do with stars but why is disaster in there?  Disaster breaks down to bad star and apparently disaster originally meant a bad horoscope reading.  Gradually the meaning shifted to mean the resulting catastrophe rather than the bad star that predicted it.  Once one has discovered a connection it makes it far more easy to remember a difficult spelling but this still makes learning spelling a laborious process givven the large number of words in a person\\'s active vocabulary.\\n\\tFurther, Ramsden\\'s model of spelling being based invariant roots and a (fairly) simple system for adding affixes has plenty of exceptions itself.  One of the aberrations that has been blamed on Dr Johnson is the \\'p\\' that has crept in the word receipt that has a similar sound to conceit which has no \\'p\\'.  If we follow Ramsden and see it as part of a family of words such as \\'reception\\', \\'concept\\', \\'contraception\\' and \\'accept\\' based on \\'cept\\' then the \\'p\\' is quite logical but then \\'receipt\\' then should be spelled \\'recept\\'.  For Ramsden\\'s method to be reliable method of predicting spelling the rule that roots are invariant must hav no exceptions.  Givven we hav exceptions, we are back to the same confusing array of exception lists that we hav with phonetic spelling rules.  It is also no help to say that adding an ending like \\'able\\' follows consistent rules if there is no reliable way of telling if in fact we should be adding \\'ible\\'.\\n\\tFinally we come to what Ramsden describes as laws, that is to say rules with no exceptions.  Hence words never end in a \\'v\\' but must hav an \\'e\\'.  Sadly for Ramsden this law already been broken now that a word \\'spiv\\' has been around long enough to shake of its label as slang.  More important this \\'law\" has no logical basis in either meaning or phonetics.  It was simply a convention brought over by Norman scribes.  Similarly the \"law\" that no English word can hav \\'u\\' followed by \\'v\\' is because in old gothic writing \\'uv\\' was a confusing row of upstrokes.  This may be an \\'interesting\\' explanation of why we spell \\'luv\\' in the way we do rather than as \\'luv\\' but it hardly inspires confidence in the logic of the English language.\\n\\t\\'Interesting\\' is a word that Ramsden is a little over fond.  Any of the illogicalities of English spelling (such as the fact that one the two \\'laws\\' of English spelling is designed to compensate for a script that no one would ever use today) is interesting.  When we hit a feature of English spelling that is not merely illogical but insane then Ramsden has to resort to \"enjoyable\".  I hav the image of Ramsden\\'s school lessons as like evangelical prayer meetings (which I suppose is certainly an improvement on some of the tedious lessons I was subjected to as a child).  The bottom line is that leaning English spelling is long and laborious and, what\\'s more,  it doesn\\'t hav to be this way.  Other languages hav spelling systems that are simple and easy.  To describe English spelling as beautiful because it is riddled with exceptions, anachronisms and bungled attempts at etymology is propaganda.\\n\\tRamsden may well defend this as necessary propaganda.  If teachers are enthusiastic about English spelling then this may rub off on the pupils tho Ramsden\\'s enthusiasm is so overdone that I suspect some may react as if to a double glazing salesman.  And I don\\'t think that this is the real reason.  Ramsden has all the fervor of one who has a strong emotional commitment to an idea yet has inner doubts.  Ramsden it seems to me has not simply written a book on how to teach spelling.  It is also a polemic against spelling reform.\\n\\tRamsden as I said doesn\\'t mention spelling reform.  He indeed goes to great pains to avoid mentioning spelling reform.  I assume this is on the principle of \\'teaching sin\\' (I am thinking of the way that Medieval priests, when denouncing deviant sexual practices, would never specify what those practices were in case their flock went \"Sheesh, never thought of that - lets try it!\").  To an extent it is implicit in the whole book.  If what makes English spelling wonderful is the way that meaning is reflected in the spelling then to make spelling conform to the way English is spoken would destroy this.  But Ramsden does include a section that can only be an attack on spelling reform, provided you look at the arguments he actually uses rather than what he says he is arguing against,.  To tell the reader about the existence of advocates of spelling reform might inspire readers to look deeper but nonetheless Ramsden seems to feel the need to ensure that the reader is supplied with a ready list of counter arguments should they at a later date stumble on the idea of spelling reform.\\n\\tEarly on there is a section \"Beware of Phonetics!\"  He writes \"Let\\'s suppose English spelling is phonetic (it isn\\'t, but we\\'ll assume for the moment that it is).  We immediately encounter some fundamental problems.\"  After that you might expect to find statements such as \"If English spelling was phonetic you would expect letters never to be silent but in fact every letter will in at least one word hav no effect on how the word is pronounced.\"  What he actually writes is something quite different.\\n\\tThe first point he makes is that English has many more sounds than letters of the Alphabet.  Indeed English has well over 40 - mainly because their are good deal more vowels than the five vowel letters.  To make English a phonetic language, he says, would require a much larger Alphabet and this is, of course, impractical.  What he is really gunning for is not the idea that spelling is phonetic but that it should be reformed by havving a letter for every sound.  Very few spelling reform proposals would try this but the one that has is the most famous.  Bernard Shaw left a large sum for the cause of spelling reform and the money was spent on designing a new alphabet that looks something like a runic script.  As such it did the cause of spelling reform a great disservice in associating it with a clearly impractical proposal.  But Ramsden immediately supplies the solution.  That is to use a combination of letters to express the extra sounds.  Hence we use \\'sh\\' and \\'ch\\' to express sounds that English has not got the spare letters for.\\n\\tA quick digression about couple of the terms used in phonetics.  The basic sounds are called phonemes.  For example \\'meet\\' has three phonemes because in this word \\'ee\\' represents only one sound despite being written as two letters.  A grapheme is letter group of letters used to represent phoneme.  Hence \\'ee\\' is the grapheme that represents the phoneme of the vowel sound in meet.  Basically the phoneme is what you get when you break down the sound of words into the smallest building blocks that are capable of conveying meaning and a grapheme is the letters we use to describe that sound on paper.\\n\\tBack to the point.  The reason why English spelling is not phonetic (strictly speaking \"phonemic\" for those who are more linguistically minded) is not because it has more phonemes (sounds) than letters.  The problem is quite the reverse.  To express the 40 to 50 phonemes it uses a far larger number of graphemes (letter and letter combinations).  The same sound that is represented by the grapheme \\'ee\\' in meet also is represented by the grapheme \\'ea\\' in heat.  Givven that English has far too many graphemes you might hav thought that that there would be no need for a grapheme to do two jobs but we know that this is not true.  Hence the grapheme \\'ch\\' in machine has to be drafted in to cover a different phoneme in church.\\n\\tClearly it is not a problem for advocates of spelling reform to find adequate graphemes to represent the full range of English phonemes.  Indeed the task of a full reform proposal would be to brutally cull the current excess number of graphemes so that we are left with one grapheme for every phoneme.\\n\\tRamsden\\'s next objection to spelling reform is that it is impossible to cover all dialects of English.  This is a more serious objection.  Scottish has a distinct phoneme for the grapheme \\'ch\\' in loch.  I along with most English speakers will pronounce it like lock.  Do we hav a special grapheme for a sound only used by a minority?  And the high prestige RP of southern England (formerly known as BBC English) is also quite deviant in lacking [r]\\'s where most English speakers do.  This is a real problem because when the idea of making the language say-write and write-say what they really hav in mind is \"If only I could write as I say.\"  Spelling reform is not quite so appealing when the proposal is that English should be written as other people speak English.\\n\\tFor the immediate future this is a false problem.  The experience of spelling reforms of other countries is that a limited reform that removes some of the glaring inconsistencies is far more likely than one producing a fully phonemic system.  Givven that the current spelling is so divergent from all forms of spoken English then it is reasonably easy to draw up a proposal of English spelling that will leave everyone better off.\\n\\tA successful reform of English spelling would create a precedent and hopefully make a further reform easier at a later date.  Such a second reform would hav to take into account the problem of dialects.  But spelling mistakes as a result of pronunciation differing from the spelling is not a great problem.  Children from Scottish rural areas who pronounce bone and home as /ben/ and /hem/ seem to handle it OK provided they hav this drawn to their attention (P Trudgill, On Dialect p195).  This is probably due to the fact they are at least aware of the alternativ pronunciation even tho they do not speak it themselves.  The politics of such a change might well be more tricky givven that already some Scottish nationalists wish to develop Scottish as a separate language, so would be reformers would be well advised to take into account Scottish pronunciation.\\n\\tHis final problem is the way words pronunciation changes as a result of different endings.  When you add an \\'s\\' to \\'prints\\' you actually get /prins/ so it sounds like prince.  Again plurals are often pronounced with a final [z] rather than a final [s].  Givven that people often make these changes without being aware they are doing so, it would be confusing to insist on a change of spelling.  In these cases I\\'m fully in agreement with Ramsden that meaning should take precedence over pronunciation.  Further no one in the right mind would try and reflect in spelling the way words run each other in connected speech.  When we speak often words are placed together that produce awkward combinations so we quite unthinkingly change the pronunciation slightly to ease the flow but when searching for the spelling we naturally think of how a word sounds when \\'stand alone\\' and the spelling should reflect that.  \\n\\tBut in all this Ramsden is arguing not against spelling reform but against spelling reform in a particular direction.  The pattern of meaning that Ramsden sees in English spelling is not the result of some natural evolution but the result of a successful Englsh spelling reform but a reform in the reverse direction.  During the 15th and 16th century English spelling fell into the hands of a group of reformers who were determined to make English spelling conform to the Greek and Latin origin of the words.  Often they got it wrong.  Anchor is spelt the way it is because they, wrongly, believed it was related to the Greek word anchorite.  Island is also the result of mistaken etymology and was formerly spelt iland. (Reading, Writing and Dyslexia: Andrew Ellis p6)\\n\\tClearly it was a mistake.  We know this because in countries that hav logical spelling there is much less need to teach spelling.  This is because many children who might be overwhelmed by a system as complicated as English are quite able to handle a spelling system which is simple and logical.  If we keep English spelling as it is then it is plausible that Ramsden\\'s approach may well prove promising but it is still making do when the real solution to children failing at spelling is changing the system itself.  As Ramsden admits, even if children are taught in the way he considers best they require long periods being taught spelling.\\n\\n\\tThe real solution is a reformed sound based spelling system.\\nPostscript\\n\\nSince writing this Melvyn Ramsden has written to me and it seems I was wrong to assume that he is a opponent of spelling reform.  At risk of again misrepresenting him, he is at most a skeptic who considers any reform needs to be well thought out to ensure it does not do more harm than good.  At several points I hav played the dangerous game of reading between the lines and much of this I hav to retract.  I do think my criticism of the text is still valid.  Ramsden may not hav intended to write a celebration of English spelling as it is but that is, I still believe, the most obvious reading of what is written.  \\nThe reason for this misunderstanding is that Ramsden when writing his book set out simply to show a method whereby the teaching of spelling could be successful and enjoyable.  He did not set out to make clear his views on spelling reform but that is my primary interest.  If your interest is primarily in looking for methods of teaching spelling of English Spelling, as it is, then it is well worth a look.  Indeed it is because it is so well written that I felt it so important to put a counter view.\\nRewrites are difficult.  Most of what I wrote I stand by but the bit that I would now want to qualify is a thread that runs thru the whole piece.  To take it out would leave it lifeless.  My intention was to encourage those who go on to read Ramsden\\'s book to do so with a critical eye.  I must also encourage you to read what I myself hav written with a critical eye.\\n\\nNB the spelling here is US spelling plus dictionary variants such as \"thru\" plus the dropping\\nof a final e when its only function is to prevent a word ending in v.  As this means that words like give are no longer pretending to hav short vowels they need a double consonant when they have suffixes that end in a vowel.  Hence give + en becomes givven.\\n\\n',\n",
       " \"\\n\\n\\nA Focus+Context Technique Based on Hyperbolic Geometry     \\nfor Visualizing Large Hierarchies.     \\n\\n\\n\\n\\n\\n\\n\\nA Focus+Context Technique Based on Hyperbolic Geometry     \\nfor Visualizing Large Hierarchies.     \\n\\n\\n     \\nJohn Lamping, Ramana Rao, and Peter Pirolli     \\n\\n\\n\\n Xerox Palo Alto Research Center     \\n 3333 Coyote Hill Road     \\n Palo Alto, CA     \\n\\n\\n lamping@parc.xerox.com     \\n rao@parc.xerox.com     \\n pirolli@parc.xerox.com     \\n\\n\\n© ACM\\n\\n\\n\\nAbstract     \\nWe present a new focus+context (fisheye) technique for visualizing and     \\nmanipulating large hierarchies.  Our technique assigns more display space     \\nto a portion of the hierarchy while still embedding it in the context of     \\nthe entire hierarchy.  The essence of this scheme is to lay out the     \\nhierarchy in a uniform way on a hyperbolic plane and map this plane onto a     \\ncircular display region.  This supports a smooth blending between focus and     \\ncontext, as well as continuous redirection of the focus.  We have developed     \\neffective procedures for manipulating the focus using pointer clicks as     \\nwell as interactive dragging, and for smoothly animating transitions across     \\nsuch manipulation.  A laboratory experiment comparing the hyperbolic     \\nbrowser with a conventional hierarchy browser was conducted.     \\n\\nKeywords:  Hierarchy Display, Information Visualization, Fisheye     \\nDisplay, Focus+Context Technique.     \\n\\nIntroduction      \\n     \\nIn the last few years, Information Visualization research has explored the     \\napplication of interactive graphics and animation technology to visualizing and     \\nmaking sense of larger information sets than would otherwise be practical     \\n(Robertson, Card and Mackinlay, 1994).  One recurring theme has been the power     \\nof focus+context techniques, in which detailed views of particular parts of an     \\ninformation set are blended in some way with a view the of the overall     \\nstructure of the set.  In this paper, we present a new technique, called the     \\nhyperbolic browser, for visualizing and manipulating large hierarchies.     \\n     \\n     \\nThe hyperbolic browser, illustrated in Figure 1, was originally     \\ninspired by the Escher woodcut shown in Figure 2.  Two salient     \\nproperties of the figures are: first that components diminish in size as     \\nthey move outwards, and second that there is an exponential (devilish)     \\ngrowth in the number of components.  These properties---``fisheye''     \\ndistortion and the ability to uniformly embed an exponentially growing     \\nstructure---are the aspects of this construction (the Poincar\\\\'e mapping     \\nof the hyperbolic plane) that originally attracted our attention.     \\n     \\n     \\n\\n\\n Full Size Image\\n     \\nFigure 1: An organization chart.      \\n     \\n\\n\\n Full Size Image\\n     \\nFigure 2: Original inspiration for the hyperbolic tree browser.     \\nCircle Limit IV (Heaven and Hell), 1960, (c) 1994 M.C. Escher      \\nCordon Art -- Baarn -- Holland.  All rights reserved.  Printed with     \\npermission.     \\n     \\n     \\nThe hyperbolic browser initially displays a tree with its root at the center,     \\nbut the display can be smoothly transformed to bring other nodes into focus, as     \\nillustrated in Figure 3.  In all cases, the amount of space available to a node     \\nfalls off as a continuous function of its distance in the tree from the point     \\nin the center.  Thus the context always includes several generations of     \\nparents, siblings, and children, making it easier for the user to explore the     \\nhierarchy without getting lost.     \\n     \\n\\n\\n\\n\\n\\n     \\nFull Size  Sequence  Strip\\n     \\nFigure 3: Changing the focus.     \\n     \\nThe hyperbolic browser supports effective interaction with much larger     \\nhierarchies than conventional hierarchy viewers and complements the strengths     \\nof other novel tree browsers.  In a 600 pixel by 600 pixel window, a standard     \\n2-d hierarchy browser can typically display 100 nodes (w/ 3 character text     \\nstrings).  The hyperbolic browser can display 1000 nodes of which about the 50     \\nnearest the focus can show from 3 to dozens of characters of text.  Thus the     \\nhyperbolic browser can display up to 10 times as many nodes while providing     \\nmore effective navigation around the hierarchy.  The scale advantage is     \\nobtained by the dynamic distortion of the tree display according to     \\nthe varying interest levels of its parts.     \\n     \\n     \\nOur approach exploits hyperbolic geometry (Coxeter, 1965) (Moise, 1974).  The     \\nessence of the approach is to lay out the hierarchy on the hyperbolic plane and     \\nmap this plane onto a circular display region.  The hyperbolic plane is a     \\nnon-Euclidean geometry in which parallel lines diverge away from each other.     \\nThis leads to the convenient property that the circumference of a circle on the     \\nhyperbolic plane grows exponentially with its radius, which means that     \\nexponentially more space is available with increasing distance.  Thus     \\nhierarchies---which tend to expand exponentially with depth---can be laid out     \\nin hyperbolic space in a uniform way, so that the distance (as measured in the     \\nhyperbolic geometry) between parents, children, and siblings is approximately     \\nthe same everywhere in the hierarchy.     \\n     \\n     \\nWhile the hyperbolic plane is a mathematical object, it can be mapped in a     \\nnatural way onto the unit disk, which provides a means for displaying it on     \\nan ordinary (Euclidean) display.  This mapping displays portions of the plane     \\nnear the origin using more space than other portions of the plane.  Very     \\nremote parts of the hyperbolic plane get miniscule amounts of space near     \\nthe edge of the disk.  Translating the hierarchy on the hyperbolic plane     \\nprovides a mechanism for controlling which portion of the structure     \\nreceives the most space without compromising the illusion of viewing the     \\nentire hyperbolic plane.  We have developed effective procedures for     \\nmanipulating the focus using pointer dragging and for smoothly animating     \\ntransitions across such manipulation.     \\n     \\n     \\nWe have implemented versions of the hyperbolic browser that run on Unix/X     \\nand on Macintoshes.  We conducted an experiment with 4 subjects to compare     \\nthe hyperbolic tree browser with a conventional browser on a node location     \\ntask.  Though no statistically significant performance difference was     \\nidentified, a strong preference for the hyperbolic tree browser was     \\nestablished and a number of design insights were gained.     \\n     \\n     \\nPROBLEM AND RELATED WORK     \\n     \\nMany hierarchies, such as organization charts or directory structures, are too     \\nlarge to display in their entirety on a computer screen.  The conventional     \\ndisplay approach maps all the hierarchy into a region that is     \\nlarger than the display and then uses scrolling to move around the region.     \\nThis approach has the problem that the user can't see the relationship of the     \\nvisible portion of the tree to the entire structure (without auxiliary views).     \\nIt would be useful to be able to see the entire hierarchy while focusing on any     \\nparticular part so that the relationship of parts to the whole can be seen and     \\nso that focus can be moved to other parts in a smooth and continuous way.     \\n     \\n     \\nA number of focus+context display techniques have been introduced in the last     \\nfifteen years to address the needs of many types of information structures     \\n(Leung and Apperley, 1994) (Sarkar and Brown, 1994).  Many of these     \\nfocus+context techniques, including the document lens (Robertson and Mackinlay     \\n1993), the perspective wall (Mackinlay, Robertson and Card, 1991), and the work     \\nof Sarkar et al, could be applied to browsing trees laid out using conventional     \\n2-d layout techniques.  The problem is that there is no satisfactory     \\nconventional 2-d layout of a large tree, because of its exponential growth. If     \\nleaf nodes are to be given adequate spacing, then nodes near the root must be     \\nplaced very far apart, obscuring the high level tree structure, and leaving no     \\nnice way to display the context of the entire tree.     \\n     \\n     \\nThe Cone Tree (Robertson, Mackinlay and Card, 1991) modifies the above approach     \\nby embedding the tree in a three dimensional space.  This embedding of the tree     \\nhas joints that can be rotated to bring different parts of the tree into focus.     \\nThis requires currently expensive 3D animation support.  Furthermore, trees     \\nwith more than approximately 1000 nodes are difficult to manipulate.  The     \\nhyperbolic browser is two dimensional and has relatively modest computational     \\nneeds, making it potentially useful on a broad variety of platforms.     \\n     \\n     \\nAnother novel tree browsing technique is treemaps (Johnson and Schneiderman,     \\n1991) which allocates the entire space of a display area to the nodes of the     \\ntree by dividing the space of a node among itself and its descendants according     \\nto properties of the node.  The space allocated to each node is then filled     \\naccording to the same or other properties of the node.  This technique utilizes     \\nspace efficiently and can be used to look for values and patterns amongst a     \\nlarge collection of values which agglomerate hierarchically, however it tends     \\nto obscure the hierarchical structure of the values and provides no way of     \\nfocusing on one part of a hierarchy without losing the context.     \\n     \\n     \\nSome conventional hierarchy browsers prune or filter the tree to allow     \\nselective display of portions of the tree that the user has indicated.  This     \\nstill has the problem that the context of the interesting portion of the tree     \\nis not displayed.  Furnas (1986) introduced a technique whereby nodes in the     \\ntree are assigned an interest level based on distance from a focus node (or its     \\nancestors).  Degree of interest can then be used to selectively display the     \\nnodes of interest and their local context.  Though this technique is quite     \\npowerful, it still does not provide a solution to the problem of displaying the     \\nentire tree.  In contrast, the hyperbolic browser is based on an underlying     \\ngeometry that allows for smooth blending of focus and context and continuous     \\nrepositioning of the focus.     \\n     \\n     \\nBertin (1983) illustrates that a radial layout of the tree could be uniform by     \\nshrinking the size of the nodes with their distance from the root.  The use of     \\nhyperbolic geometry provides an elegant way of doing this while addressing the     \\nproblems of navigation.  The fractal approach of Koike and Yoshihara (1993)     \\noffers a similar technique for laying out trees.  In particular, they have     \\nexplored an implementation that combines fractal layout with Cone Tree-like     \\ntechnique.  The hyperbolic browser has the benefit that focusing on a node     \\nshows more of the node's context in all directions (i.e. ancestors, siblings,     \\nand descendants).  The fractal view has a more rigid layout (as with other     \\nmultiscale interfaces) in which much of this context is lost as the viewpoint     \\nis moved to lower levels of the tree.     \\n     \\n     \\nThere have been a number of projects to visualize hyperbolic geometry,     \\nincluding an animated video of moving through hyperbolic space (Gunn, 1991).     \\nThe emphasis of the hyperbolic browser is a particular exploitation of     \\nhyperbolic space for information visualization.  We don't expect the user to     \\nknow or care about hyperbolic geometry.     \\n     \\nHYPERBOLIC BROWSER BASICS     \\n     \\nThe hyperbolic browser replaces the conventional approach of laying a tree out     \\non a Euclidean plane by doing  layout  on the hyperbolic plane and then     \\nmapping to the unit disk (which is straightforwardly mapped to the     \\ndisplay).  Change of focus is handled by performing a rigid     \\ntransformation of the hyperbolic plane, moving the laid out tree in the     \\nprocess.  Thus layout is only performed once.  Space for displaying node     \\ninformation is also computed during layout and automatically transformed     \\nwith each change of focus.     \\n     \\n     \\nThe implementation of points and transformations on the hyperbolic plane is     \\nbriefly discussed in the appendix.  The rest of this section presumes an     \\nimplementation of the hyperbolic plane and discusses higher level issues.     \\n     \\nLayout     \\n     \\nLaying a tree out in the hyperbolic plane is an easy problem, because the     \\ncircumference and area of a circle grow exponentially with its radius.     \\nThere is lots of room.  We use a recursive algorithm that lays out each     \\nnode based on local information.  A node is allocated a wedge of the     \\nhyperbolic plane, angling out from itself, to put its descendants in.  It     \\nplaces all its children along an arc in that wedge, at an equal distance     \\nfrom itself, and far enough out so that the children are some minimum     \\ndistance apart from each other.  Each of the children then gets a sub-wedge     \\nfor its descendants.  Because of the way parallel lines diverge in     \\nhyperbolic geometry, each child will typically get a wedge that spans about     \\nas big an angle as does its parent's wedge, yet none of the children's     \\nwedges will overlap.     \\n     \\n     \\nThe layout routine navigates through the hyperbolic plane in terms of     \\noperations, like moving some distance or turning through some angle, which     \\nare provided by the hyperbolic plane implementation.     \\n     \\n\\n\\n Full Size Image\\n     \\nFigure 4: A uniform tree of depth 5 and branching factor 3 (364     \\nnodes).     \\n     \\n\\n\\n Full Size Image\\n     \\nFigure 5: The initial layout of a tree with 1004 nodes using      \\na Poisson distribution for number of children. The origin of the tree is in the     \\ncenter.     \\n     \\n     \\nFigure 4 shows what the layout of a uniform tree looks like.  Notice how     \\nthe children of each node span about the same angle, except near the root,     \\nwhere a larger wedge was available initially.  To get a more compact layout for     \\nnon-uniform trees, we modify this simple algorithm slightly, so that siblings     \\nthat themselves have lots of children get a larger wedge than siblings that     \\ndon't (the wedge size grows logarithmically).  This effect can be seen in     \\nFigure 5 where, for example, the five children of the root get different     \\namounts of space.  This tends to decrease the variation of the distances     \\nbetween grandchildren and their grandparent.     \\n     \\n     \\nAnother option in layout (in contrast to all examples so far illustrated) is to     \\nuse less than the entire 360 degree circle for spreading out the children of     \\nthe root node.  With this option, children of the root could all be put in one     \\ndirection, for example to the right or below, as in conventional layouts.  An     \\nexample of this option, discussed below, appears in Figure 8.     \\n     \\nMapping and Representation     \\n     \\nOnce the tree has been laid out on the hyperbolic plane, it must be mapped     \\nin some way to a 2-d plane for display.  (We can barely imagine the     \\nhyperbolic plane, not to mention see it.)  There are two canonical ways of     \\nmapping the hyperbolic plane to the unit disk.  In both of them, one     \\nvicinity in the hyperbolic plane is in focus at the center of the disk     \\nwhile the rest of the hyperbolic plane fades off in a perspective-like     \\nfashion toward the edge of the disk, as we desire.  We use the conformal     \\nmapping, or Poincar\\\\'e model, which preserves angles but distorts lines in     \\nthe hyperbolic space into arcs on the unit disk, as can be seen in the     \\ntakes lines in the hyperbolic plane to lines in the unit disk, but distorts     \\nangles.  You can't have it both ways.     \\n     \\n     \\nWe tried the Klein model.  But points that are mapped to near the edge by     \\nthe Poincar\\\\'e model get mapped almost right on the edge by the Klein     \\nmodel.  As a result, nodes more than a link or two from the node in focus     \\nget almost no screen real-estate, making it very hard to     \\nperceive the context.     \\n     \\nChange of Focus     \\n     \\nThe user can change focus either by clicking on any visible point to bring it     \\ninto focus at the center, or by dragging any visible point interactively to any     \\nother position.  In either case, the rest of the display transforms     \\nappropriately.  Regions that approach the center become magnified, while     \\nregions that were in the center shrink as they are moved toward the     \\nedge. Figure 6 shows the same tree as Figure 5 with a different focus.  The     \\nroot has been shifted to the right, putting more focus on the nodes that were     \\ntoward the left.     \\n     \\n\\n\\n Full Size Image\\n     \\nFigure 6: A new focus.      \\n     \\nChanges of focus are implemented as rigid transformations of the hyperbolic     \\nplane that will have the desired effect when the plane is mapped to the     \\ndisplay; there is never a need to repeat the layout process.  A change of     \\nfocus to a new node, for example, is implemented by a translation in the     \\nhyperbolic plane that moves the selected node to the location that is     \\nmapped to the center of the disk.     \\n     \\n     \\nTo avoid loss of floating point precision across multiple transformations,     \\nwe compose successive transformations into a single cumulative     \\ntransformation, which we then apply to the positions determined in the     \\noriginal layout.  Further, since we only need the mapped positions of the     \\nnodes that will be displayed, the transformation only needs to be computed     \\nfor nodes whose display size will be at least a screen pixel.  This yields     \\na constant bound on redisplay computation, no matter how many nodes are in     \\nthe tree.  And, the implementation of translation can be fairly efficient;     \\nwe require about 20 floating point operations to translate a point,     \\ncomparable to the cost of rendering a node on the screen.     \\n     \\nNode Information     \\n     \\nAnother property of the Poincar\\\\'e projection is that circles on the hyperbolic     \\nplane are mapped into circles on the Euclidean disk, though they will shrink in     \\nsize the further they are from the origin.  We exploit this property by     \\ncalculating a circle in the hyperbolic plane around each node that is     \\nguaranteed not to intersect the circle of any other node.  When those circles     \\nare mapped onto the unit disk they provide a circular display region for each     \\nnode of the tree in which to display a represenation of     \\nthe node.  This can be combined with a facility that uses different     \\nrepresentations for nodes depending on the amount of real space they receive.     \\nFigure 7 shows the same tree as Figure 1 with the display region     \\nof each node indicated.     \\n     \\n\\n\\n Full Size Image\\n     \\nFigure 7: The display regions of nodes.      \\n     \\nPRESERVING ORIENTATION     \\n     \\nOrientation presents an interesting issue for the hyperbolic browser, because     \\nthings tend to get rotated.  For example, most nodes rotate on the display     \\nduring a pure translation.  There is a line that doesn't rotate, but the     \\nfarther nodes are on the display from that line, the more they rotate.  This     \\ncan be seen in the series of frames in Figure 3.  The node labeled ``Lowe'',     \\nfor example, whose children fan out to the upper right in the top frame ends up     \\nwith its children fanning out to the right in the bottom frame.  These     \\nrotations are reasonably intuitive for translations to or from the origin.  But     \\nif drags near the edge of the disk are interpreted as translations between the     \\nthe source and the destination of the drag, the display will do a     \\ncounter-intuitive pirouette about the point being dragged.     \\n     \\n\\n\\n Full Size Image\\n     \\nFigure 8: Putting children toward the right.      \\n     \\n     \\nThere is a fundamental property of hyperbolic geometry that is behind this     \\nand that also causes another problem.  In the usual Euclidean plane, if     \\nsome graphical object is dragged around, but not rotated, then is always     \\nkeeps its original orientation---not rotated.  But this is not true     \\nin the hyperbolic plane.  A series of translations forming a closed loop,     \\neach preserving the orientation along the line of translation, will, in     \\ngeneral, cause a rotation.  (In fact the amount of rotation is proportional     \\nto the area of the closed loop and is in the opposite direction to the     \\ndirection the loop was traversed.)  This leads to the counter-intuitive     \\nbehavior that a user who browses around the hierarchy can experience a     \\ndifferent orientation each time they revisit some node, even though all     \\nthey did was translations.     \\n     \\n     \\nWe address both of these problems by interpreting the user's manipulations as a     \\ncombination of both the most direct translation between the points the user     \\nspecifies and an additional rotation around the point moved, so that the     \\nmanipulations and their cumulative effects are more intuitive.  From the user's     \\nperspective, drags and clicks move the point that the user is manipulating     \\nwhere they expect.  The additional rotation also appears natural, as it is     \\ndesigned to preserve some other property that the user expects.  The user need     \\nnot even be particularly aware that rotation is being added.     \\n     \\n     \\nWe have found two promising principles for adding rotations.  In one approach,     \\nrotations are added so that the original root node always keeps its original     \\norientation on the display.  In particular, the edges leaving it always leave     \\nin their original directions.  Preserving the orientation of the root node also     \\nmeans that the node currently in focus also has the orientation it had in the     \\noriginal image.  The transformations in the examples presented so far all     \\nworked this way.  It seems to give an intuitive behavior both for individual     \\ndrags and for the cumulative effect of drags.     \\n     \\n     \\nThe other approach we have taken is to explicitly not preserve orientation.     \\nInstead, when a node is clicked on to bring it to focus, the display is rotated     \\nto have its children fan out in a canonical direction, such as to the right.     \\nThis is illustrated in Figure 8 and also in the animation sequence in Figure 9.     \\nThis approach is aided when the children of the root node are all laid out on     \\none side of that node, as also illustrated in the two figures, so that the     \\nchildren of the root node can also fan out in the canonical direction when it     \\nis in focus.     \\n     \\n\\n\\n\\n\\n\\n     \\nFull Size  Sequence  Strip\\n     \\nFigure 9: Animation with compromised rendering.      \\n     \\nANIMATED TRANSITIONS     \\n     \\nAs demonstrated by recent work on information visualizations, animated     \\ntransitions between different views of a structure can maintain object     \\nconstancy and help the user assimilate the changes across views.  The smooth     \\ncontinuous nature of the hyperbolic plane allows for performing smooth     \\ntransitions of focus by rendering appropriate intermediate views.     \\n     \\n     \\nAnimation sequences are generated using the so-called ``nth-root'' of a     \\ntransition transformation, i.e. the rigid transformation that applied n times     \\nwill have the same effect as the original.  Successive applications of the     \\n``nth-root'' generate the intermediate frames.  The sequences in     \\nFigures 3 and 9 were generated this way.     \\n     \\n     \\nResponsive display performance is crucial for animation and interactive     \\ndragging.  This can be a problem for large hierarchies on standard hardware.     \\nWe achieve quick redisplay by compromising on display quality during motion.     \\nThese compromises provide options for use in a system that automatically     \\nadjusts rendering quality during animation, e.g.  the Information Visualizer     \\ngovernor (Robertson, Card, and Mackinlay, 1989) or Pacers (Tang and Linton,     \\n1993).  Fortunately, there are compromises that don't significantly affect the     \\nsense of motion. Figure 9 shows an animation sequence with the compromises     \\nactive in the intermediate frames.  Unless specifically looked for, the     \\ncompromises typically go unnoticed during motion.     \\n     \\n     \\nOne compromise is to draw less of the fringe.  Even the full quality     \\ndisplay routine stops drawing the fringe once it gets below one pixel     \\nresolution.  For animation, the pruning can be strengthened, so that     \\ndescendants of nodes within some small border inside the edge of the disk     \\nare not drawn.  This tremendously increases display performance, since the     \\nvast majority of nodes are very close to the edge.  But it doesn't     \\nsignificantly degrade perceptual quality for a moving display, since those     \\nnodes occupy only a small fraction of the display, and not a part that the     \\nuser is typically focusing on.     \\n     \\n     \\nThe other compromise is to draw lines, rather than arcs, which are     \\nexpensive in the display environments we have been using.     \\nWhile arcs give a more pleasing and intuitive static display, they     \\naren't as important during animation.  This appears to be the case both     \\nbecause the difference between arcs and lines isn't as apparent in the     \\npresence of motion, and because the user's attention during motion tends to     \\nbe focused near the center of the display, where the arcs are already     \\nalmost straight.     \\n     \\n     \\nOne other possible compromise is to drop text during animation.  We     \\nfound this to be a significant distraction, however.  And text display     \\nhas not been a performance bottleneck.     \\n     \\n     \\nEVALUATION AND FUTURE WORK     \\n     \\nA laboratory experiment was conducted to contrast the hyperbolic browser     \\nagainst a conventional 2-d scrolling browser with a horizontal tree layout.     \\nOur subjects preferred the hyperbolic browser in a post-experimental survey,     \\nbut there was no significant difference between the browsers in performance     \\ntimes for the given task, which involved finding specific node locations.  The     \\nstudy has fueled our iterative design process as well as highlighted areas for     \\nfurther work and evaluation.     \\n     \\n     \\nThe two browsers in the study support mostly the same user operations.     \\n``Pointing'' provided feedback on the node under the cursor in a feedback area     \\nat the bottom of window.  ``Clicking'' moved a point to the center.     \\n``Grabbing'' any visible point allowed interactive dragging of the tree within     \\nthe window.  The 2-d scrolling browser provides conventional scrollbars as     \\nwell.     \\n     \\n     \\nThe experiment was based on the task of locating and ``double-clicking'' on     \\nparticular nodes in four World-Wide-Web hierarchies identified by their URLs     \\n(the application intent being that a Web browser would jump to that node).     \\nThough this particular task and application were adequate for a rough baseline     \\nevaluation, there are problematic aspects.  Typically, this task would better     \\nbe supported by query-by-name or even an alphabetical listing of the nodes.     \\nFurthermore the WWW hierarchy (based on breadth-first flattening of the     \\nnetwork) contained many similarly-named nodes and the hierarchy wasn't strongly     \\nrelated to a semantic nesting.     \\n     \\n     \\n     \\nAfter pilot trials, we added to both browsers a feature to rotate the names of     \\nchildren of a pointed-to node through the feedback area and then jump to the     \\ncurrent child.  We also added a toggleable ``long text'' mode in which all     \\nnodes beyond an allocated space threshold disregard their boundaries and     \\ndisplay up to 25 characters.  Despite the overlapping of the text, this leads     \\nto more text being visible and discernible on the screen at once (see Figure     \\n10).     \\n     \\n\\n\\n Full Size Image\\n     \\nFigure 10: Hyperbolic browser in long text mode in World Wide     \\nWeb hierarchy utilized in laboratory experiment.     \\n     \\n     \\nThe experiment used a within-subject design with four subjects, and tested for     \\nthe effects of practice.  We found no significant difference in time or number     \\nof user actions in performing the tasks across the browsers.  There was a     \\nsignificant practice effect in which practice produced a decrease in the number     \\nof user actions required to perform the task for both browsers, but there was     \\nno practice effect on task completion time for either browser.  These practice     \\neffects did not differ significantly between the browsers.     \\n     \\n     \\nOur post-experimental survey showed that all four subjects preferred the     \\nhyperbolic browser for ``getting a sense of the overall tree structure'' and     \\n``finding specific nodes by their titles,'' as well as ``overall.''  In     \\naddition, specific survey questions and our observations identified relative     \\nstrengths and weaknesses of the hyperbolic browser.  Three of the subjects     \\nliked the ability to see more of the nodes at once and two mentioned the     \\nability to see various structural properties and a better use of the space.     \\n     \\n     \\nThe amount of text that the hyperbolic browser displays was a problem.  The     \\nexperimental task was particularly sensitive to this problem because of the     \\nlength and overlap of URLs, and the ill-structured nature of the WWW hierarchy.     \\nThough the long text mode was introduced before the study, none of the subjects     \\nused this feature during the study, preferring to point at the parent and then     \\nrapidly rotate the children through the much larger feedback area.     \\n     \\n     \\nProblem areas mentioned by one subject were that the hyperbolic browser     \\nprovide a weaker sense of directionality of links and also of location of a     \\nnode in the overall space (because shapes changed).  Layout in a canonical     \\ndirection as shown in Figure 8 addresses the first of these problems,     \\nbut may worsen the second.  In particular, for applications in which access     \\nto ancestors or to the root node is particularly important, this layout     \\nmakes it easy to find and navigate toward these nodes.     \\n     \\n     \\nA number of refinements may increase the value of the browser for navigating     \\nand learning hierarchies.  For example, landmarks can be created in the space     \\nby utilizing color and other graphical elements (e.g.\\\\ we painted http, gopher,     \\nand ftp links using different colors).  Other possibilities are providing a     \\nvisual indication of where there are nodes that are invisible because of the     \\nresolution limit, using a ladder of multiscale graphical representations in     \\nnode display regions, and supporting user control of trade-off between node     \\ndisplay region size and number of visible nodes (i.e.\\\\ packing).  The effective     \\nuse of these variations are likely to be application or task dependent and so     \\nbest explored in such a design context.     \\n\\n\\n\\nCONCLUSION     \\n     \\nHyperbolic geometry provides an elegant solution to the problem of providing a     \\nfocus+context display for large hierarchies.  The hyperbolic plane has the room     \\nto lay out large hierarchies, and the Poincar\\\\'e map provides a natural,     \\ncontinuously graded, focus+context mapping from the hyperbolic plane to a     \\ndisplay.  The hyperbolic browser can handle arbitrarily large hierarchies, with     \\na context that includes as many nodes as are included by 3d approaches and with     \\nmodest computational requirements.  Our evaluation study suggested     \\nthis technique could be valuable, and has identified issues for     \\nfurther work.  We believe that the hyperbolic browser offers a promising new     \\naddition to the suite of available focus+context techniques.     \\n     \\nACKNOWLEDGEMENTS     \\n     \\nWe would like to thank the reviewers, our four subjects, and the colleagues who     \\nmade suggestions for the prototype.  Xerox Corporation is seeking patent     \\nprotection for technology described in this paper.     \\n\\n\\n\\nReferences     \\n     \\nJ. Bertin.  Semiology of Graphics , University of Wisconsin Press,     \\n1983.     \\n     \\n     \\nH. S. M. Coxeter. Non-Euclidean Geometry. University of Toronto Press,     \\n1965.     \\n     \\n     \\nGeorge W. Furnas.  Generalized fisheye views.  In Proceedings of the     \\nACM SIGCHI Conference on Human Factors in Computing Systems, pages     \\n16--23. ACM, April 1986.     \\n     \\n     \\nC. Gunn.  Visualizing hyperbolic space.  In Computer Graphics and     \\nMathematics, pages 299--311.  Springer-Verlag, October 1991.     \\n     \\n     \\nB. Johnson and B. Shnedierman.  Tree-maps: A space-filling approach to the     \\nvisualization of hierarchical information.  In Visualization 1991, pages     \\n284--291. IEEE, 1991.     \\n     \\n     \\nHideki Koike and Hirotaka Yoshihara.  Fractal approaches for visualizing     \\nhuge hierarchies.  In Proceedings of the 1993 IEEE Symposium on Visual     \\nLanguages.  IEEE, 1993.     \\n     \\n     \\nY.K. Leung and M.D.Apperley.  A review and taxonomy of distortion-oriented     \\npresentation techniques.  ACM Transactions on Computer-Human     \\nInteraction, 1(2):126--160, June 1994.     \\n     \\n     \\nJ. D. Mackinlay, G. G. Robertson, and S. K. Card.  The perspective wall: Detail     \\nand context smoothly integrated.  In Proceedings of the ACM SIGCHI     \\nConference on Human Factors in Computing Systems, pages 173--179. ACM,     \\nApril 1991.     \\n     \\n     \\nE. E. Moise.  Elementary Geometry from an Advanced Standpoint.     \\nAddison-Wesley, 1974.     \\n     \\n     \\nG. G. Robertson, S. K. Card, and J. D. Mackinlay.  The cognitive coprocessor     \\narchitecture for interactive user interfaces.  In Proceedings of the ACM     \\nSIGGRAPH Symposium on User Interface Software and Technology, pages     \\n10--18. ACM Press, Nov 1989.     \\n     \\n     \\nG. G. Robertson, S. K. Card, and J. D. Mackinlay.  Information visualization     \\nusing 3d interactive animation.  Communications of the ACM, 36(4), 1993.     \\n     \\n     \\nG. G. Robertson, J. D. Mackinlay, and S. K. Card.  Cone trees: Animated 3d     \\nvisualizations of hierarchical information.  In Proceedings of the ACM     \\nSIGCHI Conference on Human Factors in Computing Systems, pages     \\n189--194. ACM, April 1991.     \\n     \\n     \\nGeorge G. Robertson and J. D. Mackinlay.  The document lens.  In Proceedings     \\nof the ACM Symposium on User Interface Software and Technology. ACM Press,     \\nNov 1993.     \\n     \\n     \\nManojit Sarkar and Marc H. Brown.  Graphical fisheye views of graphs.  In     \\nProceedings of the ACM SIGCHI Conference on Human Factors in Computing     \\nSystems, pages 83--91. ACM, April 1992.     \\n     \\n     \\nManojit Sarkar and Marc H. Brown.  Graphical fisheye views.  Communications of the ACM,     \\n37(12):73--84, December 1994.     \\n     \\n     \\nManojit Sarkar, Scott Snibbe, and Steven Reiss.  Stretching the rubber sheet: A     \\nmetaphor for visualizing large structure on small screen.  In Proceedings of     \\nthe ACM Symposium on User Interface Software and Technology. ACM Press, Nov     \\n1993.     \\n     \\n     \\nSteven H. Tang and Mark A. Linton.  Pacers: Time-elastic objects.  In     \\nProceedings of the ACM Symposium on User Interface Software and     \\nTechnology. ACM Press, Nov 1993.     \\n    \\n\\n\\nAPPENDIX: IMPLEMENTING HYPERBOLIC GEOMETRY\\n\\nNote: If you have trouble linking to this Appendix, the following link is a scanned image \\nof the Appendix content:Scanned Image of Appendix\\n\\n\\n\\n\\n\",\n",
       " '\\n\\n\\n\\n \\nGradient based method\\n\\n\\n\\n\\n\\n\\n   \\n Next: gradient wrt transition probabilities\\nUp: Maximum Likelihood (ML) criterion\\n Previous: Baum-Welch Algorithm\\n \\nGradient based method\\n\\nIn the gradient based method, any parameter    of the HMM\\n   is updated according to the standard formula,\\n\\n\\xa0  \\n\\nwhere J is a quantity to be minimized. We define in this case, \\n\\xa0  \\nSince the minimization of    is equivalent to the maximization of\\n  , eqn.1.19 yields the required optimization criterion,\\nML. But the problem is to find the derivative    for any parameter    of the model. This can\\nbe easily done by relating J to model parameters via   . As a\\nkey step to do so,  using the  eqns.1.7 and 1.9\\nwe can obtain,\\n\\n\\xa0  \\n\\nDifferentiating the last equality in eqn. 1.20 wrt an arbitrary\\nparameter   ,\\n\\n\\xa0  \\n\\n Eqn.1.22 gives   , if we\\n know    which can be found\\n using eqn.1.21. However this derivative  is specific to the\\n actual parameter concerned. Since there are two main parameter sets in\\n the HMM, namely transition probabilities    and observation probabilities   , we can find the derivative\\n     for each of the parameter\\n sets and hence the gradient,   .\\n\\n \\n\\n gradient wrt transition probabilities\\n gradient wrt observation probabilities\\n\\n \\n\\nNarada Warakagoda \\nFri May 10 20:35:10 MET DST 1996\\n\\n\\n\\n\\n\\nHome Page\\n\\n',\n",
       " '\\n\\n\\n\\nFuzzy Logic Overview\\n\\n\\n\\n\\nFuzzy Logic Overview\\n\\nI\\'ve seen a lot of confusion in the first few articles posted\\nto this newsgroup about what, exactly, fuzzy logic is. Since I\\'ve\\nbeen working in the field for five years, I thought I\\'d help get\\nthings started by posting some introductory material. This\\narticle covers the question \"What is Fuzzy Logic?\" from\\na mathematical point of view. Succeeding articles will cover the\\nquestions \"What is a Fuzzy\\nExpert System?\" and \"What is\\nFuzzy Control?\". \\nWarning: If you\\'re not already familiar with fuzzy\\nlogic, you\\'re going to see a lot of new terms defined in this\\narticle. I\\'ll try to put _underscores_ around terms that are\\nlikely to be new. You may need to read it a few times, just to\\npick up all the terms. \\n\\nWhat is Fuzzy Logic?\\nFuzzy logic is a superset of conventional (Boolean) logic that\\nhas been extended to handle the concept of partial truth - truth\\nvalues between \"completely true\" and \"completely\\nfalse\". It was introduced by Dr. Lotfi Zadeh of U.C.\\nBerkeley in the 1960\\'s. \\n\\nFuzzy Subsets\\nThere is a strong relationship between Boolean logic and the\\nconcept of a subset. There is a similar strong relationship\\nbetween fuzzy logic and fuzzy subset theory (Note: there is no\\nfuzzy set theory, as far as I am aware - only a fuzzy subset\\ntheory). \\nA subset U of a set S can be defined as a set of ordered\\npairs, each with a first element that is an element of the set S,\\nand a second element that is an element of the set { 0, 1 }, with\\nexactly one ordered pair present for each element of S. This\\ndefines a mapping between elements of S and elements of the set {\\n0, 1 }. The value zero is used to represent non-membership, and\\nthe value one is used to represent membership. The truth or\\nfalsity of the statement \\n\\n    x is in U\\n\\nis determined by finding the ordered pair whose first element\\nis x. The statement is true if the second element of the ordered\\npair is 1, and the statement is false if it is 0. \\nSimilarly, a fuzzy subset F of a set S can be defined as a set\\nof ordered pairs, each with a first element that is an element of\\nthe set S, and a second element that is a value in the interval [\\n0, 1 ], with exactly one ordered pair present for each element of\\nS. This defines a mapping between elements of the set S and\\nvalues in the interval [ 0, 1 ]. The value zero is used to\\nrepresent complete non-membership, the value one is used to\\nrepresent complete membership, and values in between are used to\\nrepresent intermediate _degrees of membership_. The set S is\\nreferred to as the _universe of discourse_ for the fuzzy subset\\nF. Frequently, the mapping is described as a function, the\\n_membership function_ of F. The degree to which the statement \\n\\n    x is in F\\n\\nis true is determined by finding the ordered pair whose first\\nelement is x. The _degree of truth_ of the statement is the\\nsecond element of the ordered pair. \\nThat\\'s a lot of mathematical baggage, so here\\'s an example.\\nLet\\'s talk about people and \"tallness\". In this case\\nthe set S (the universe of discourse) is the set of people. Let\\'s\\ndefine a fuzzy subset TALL, which will answer the question\\n\"to what degree is person x tall?\" To each person in\\nthe universe of discourse, we have to assign a degree of\\nmembership in the fuzzy subset TALL. The easiest way to do this\\nis with a membership function based on the person\\'s height.\\n\\n    [erik - I hope this notation is clear]\\n\\n    tall(x) = { 0,                     if height(x) < 5 ft.,\\n                (height(x)-5ft.)/2ft., if 5 ft. <= height (x) <= 7 ft.,\\n                1,                     if height(x) > 7 ft. }\\n\\nA graph of this looks like: \\nWarning: put on your peril-sensitive sunglasses. Bad ASCII\\ngraphics follow! \\n\\n1.0 +                   +-------------------\\n    |                  /\\n    |                 /\\n0.5 +                /\\n    |               /\\n    |              /\\n0.0 +-------------+-----+-------------------\\n                  |     |\\n                 5.0   7.0\\n\\n                height, ft. ->\\n\\nGiven this definition, here are some example values: \\n\\nPerson    Height    degree of tallness\\n--------------------------------------\\nBilly     3\\' 2\"     0.00 [I think]\\nYoke      5\\' 5\"     0.21\\nDrew      5\\' 9\"     0.38\\nErik      5\\' 10\"    0.42\\nMark      6\\' 1\"     0.54\\nKareem    7\\' 2\"     1.00 [depends on who you ask]\\n\\nSo given this definition, we\\'d say that the degree of truth of\\nthe statement \"Drew is TALL\" is 0.38. \\nNote: Membership functions almost never have as simple a shape\\nas tall(x). At minimum, they tend to be triangles pointing up,\\nand they can be much more complex than that. Also, I\\'ve discussed\\nmembership functions as if they always are based on a single\\ncriterion, but this isn\\'t always the case, although it is the\\nmost common case. One could, for example, want to have the\\nmembership function for TALL depend on both a person\\'s height and\\ntheir age (he\\'s tall for his age). This is perfectly legitimate,\\nand occasionally used in practice. It\\'s referred to as a\\ntwo-dimensional membership function. It\\'s also possible to have\\neven more criteria, or to have the membership function depend on\\nelements from two completely different universes of discourse. \\n\\nLogic Operations\\nOk, we now know what a statement like\\n\\n    X is LOW\\n\\nmeans in fuzzy logic. The question now arises, how do we\\ninterpret a statement like \\n\\n    X is LOW and Y is HIGH or (not Z is MEDIUM)\\n\\nThe standard definitions in fuzzy logic are: \\n\\n    truth (not x)   = 1.0 - truth (x)\\n    truth (x and y) = minimum (truth(x), truth(y))\\n    truth (x or y)  = maximum (truth(x), truth(y))\\n\\nwhich are simple enough. Some researchers in fuzzy logic have\\nexplored the use of other interpretations of the AND and OR\\noperations, but the definition for the NOT operation seems to be\\nsafe. Note that if you plug just the values zero and one into\\nthese definitions, you get the same truth tables as you would\\nexpect from conventional Boolean logic. \\nSome examples - assume the same definition of TALL as above,\\nand in addition, assume that we have a fuzzy subset OLD defined\\nby the membership function: \\n\\n    old (x) = { 0,                      if age(x) < 18 yr.\\n                (age(x)-18 yr.)/42 yr., if 18 yr. <= age(x) <= 60 yr.\\n                1,                      if age(x) > 60 yr. }\\n\\nAnd for compactness, let \\n\\n    a = X is TALL and X is OLD\\n    b = X is TALL or X is OLD\\n    c = not X is TALL\\n\\nThen we can compute the following values. \\n\\nheight  age     X is TALL       X is OLD        a       b       c\\n------------------------------------------------------------------------\\n3\\' 2\"   65?     0.00            1.00            0.00    1.00    1.00\\n5\\' 5\"   30      0.21            0.29            0.21    0.29    0.79\\n5\\' 9\"   27      0.38            0.21            0.21    0.38    0.62\\n5\\' 10\"  32      0.42            0.33            0.33    0.42    0.58\\n6\\' 1\"   31      0.54            0.31            0.31    0.54    0.46\\n7\\' 2\"   45?     1.00            0.64            0.64    1.00    0.00\\n3\\' 4\"   4       0.00            0.00            0.00    0.00    1.00\\n\\n\\nWhere is Fuzzy Logic Used?\\nDirectly, very few places. The only pure fuzzy logic\\napplication I know of is the Sony PalmTop, which apparently used\\na fuzzy logic decision tree algorithm to perform handwritten\\n(well, computer lightpen) Kanji character recognition. \\nThe only common use of fuzzy logic, to my knowledge, is as the\\nunderlying logic system for fuzzy expert systems, which will be\\nthe subject of the next article in this series, titled \"What is a Fuzzy Expert System?\". \\nWhew. A lot of words, wasn\\'t it? \\n\\n---\\nErik Horstkotte, Togai InfraLogic, Inc.\\nThe World\\'s Source for Fuzzy Logic Solutions (The company, not me!)\\nerik@til.com, gordius!til!erik - (714) 975-8522\\ninfo@til.com for info, fuzzy-server@til.com for fuzzy mail-server\\n\\n\\n\\nWebMina@austinlinks.com\\n© 2000 SiteTerrific \\n  Web Solutions.  \\n  All rights Reserved \\n\\xa0\\n\\n\\n\\xa0\\n\\n\\n\\n',\n",
       " \" CIspace : tools for learning Computational Intelligence. We have applets for learning about graph searching , constraint satisfaction problem solving , stochastic local search , neural network learning , and robot control . Online Code for the book Solved exam-style problems ( not exercises from the book). Overhead Transparencies Errata Sample 12 week course based on the book. CILog (or PDF format ), a representation and reasoning system with declarative debugging and explanation tools. CI's unofficial home page --> Order a copy of the book Price Compare (put in your own country or state and currency then redisplay the result). We make no guarantees about this service, but it seems to be a good idea. accesses since 6 November 1997. Copyright © 1998, 1999, David Poole , Alan Mackworth , Randy Goebel . \\n\\t\\n\\n-->\\n\\n\\n\\nComputational Intelligence: A Logical Approach\\n\\n\\n\\n\\n\\n\\n\\nComputational Intelligence\\nA Logical Approach\\nDavid Poole\\nAlan Mackworth\\n\\nRandy Goebel\\nPublished by Oxford University\\nPress, New York.\\n\\n\\nComputational Intelligence: A Logical Approach is a textbook\\non artificial intelligence.\\nIt was published in January 1998. \\n\\n Table of Contents\\n(or front matter in PDF\\nformat).\\n  \\n Preface (or\\n    PDF format).\\n  \\n Chapter 1 (in PDF\\nformat)\\n  \\n CIspace: tools for\\nlearning Computational Intelligence. We have applets for learning about\\n    graph\\nsearching,\\n    constraint\\nsatisfaction problem solving, stochastic\\nlocal\\nsearch, neural\\nnetwork learning, and robot control.\\n  \\n Online Code for the book\\n\\n Solved\\nexam-style problems (not exercises from the book).\\n  \\n Overhead\\nTransparencies\\n\\n Errata\\n\\n Sample\\n12 week course based on the book.\\n  \\n CILog\\n(or PDF\\nformat),\\na representation and reasoning system with\\ndeclarative debugging and explanation tools.\\n\\n Order\\na copy of the book\\n\\n Price\\nCompare (put in your own country or state and currency then\\nredisplay the result). We make no guarantees about this service, but\\nit seems to be a good idea.\\n  \\n\\n\\naccesses since 6 November 1997.\\n\\n\\nCopyright © 1998, 1999, David  Poole, Alan\\nMackworth,\\nRandy Goebel.\\n\\n\\n\\n\",\n",
       " '\\n\\n\\n\\n\\n\\n\\nMapping phonemic notations onto IPA\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nweb hosting\\ndomain names\\nemail addresses\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0map-ipa-2\\n\\n\\n\\n\\n\\n\\n\\n\\nMapping\\nDifferent Orthographies on to IPA\\n\\xa0\\nThis\\npage was developed in response to a request received on\\xa0\\nthe simplified\\nspelling mailing list\\xa0 for\\na way to relate different\\nsolutions to\\nthe alphabet problem to the Int\\'l Phonetic Alphabet\\nSelect the orthography\\nyou want to see mapped from the list below:\\xa0 map-ipa-1.html\\n\\n1.BroadRomic\\n2.Chkt\\nSpelling 2\\n3.SaundSpel\\n4.\\nNew\\nSpelling\\n5.\\nALC\\nFonetic\\n6.Truespel\\n7.OGD-Positional\\n8.Anjel\\n9.Unigraf\\xa0\\xa0\\xa0\\xa0\\xa0\\n-\\xa0\\n\\xa0\\nText\\nsamples\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe Phoneme Inventory\\n. . .\\xa0\\n\\xa0What are the\\nsignificant\\nspeech sounds in English?\\nThis is easy to visualize and remember:\\xa0\\xa0\\xa0\\xa0\\n12\\xa0 pure\\xa0\\nvowels\\nplus 9 diphthongs\\nor vowel combinations:\\xa0 Total 21 in\\nRP (British BBC English)\\nWhat\\nis unessential for RP (received pronunciation)\\nmay be essential for another dialect\\n\\xa0\\xa0 By adding a few\\nunessential combinations, it is even easier to remember as simple multiples\\nof 6\\n.\\n6 checked, 6 unchecked,\\n6\\ndiphthongs, and 6 R-combinations or shwa combinations.\\nThere\\nis no\\nagreement on the total number of combinations.\\xa0 It depends on\\nthe dialect and the number of combinations that the orthographer thinks\\nare not self evident.\\xa0 If the sound of a combination is not obvious,\\nthere may be a need to make it explicit.\\xa0 The following table lists\\n13 bringing the total number of vowel phonemes to 25.\\n\\nVowel\\nPhoneme Table for British English (RP) with\\nkey words\\nThree\\nnotations for the 21 essential sounds\\nfor RP English -\\n4x6 table\\n(Jones\\nwas searching for the minimum number of phonemes and did not include 3\\xa0\\nlisted below (with peach background))\\n6 checked\\xa0\\n+\\xa0 6 unchecked\\xa0 + 5 diphthongs + 4 ending with schwa\\n= 21 essential vowels\\n\\n\\nIPA-SAMPA\\n& Broad Romic\\nVowel Notation\\n\\xa0\\xa0\\n6 checked\\xa0\\xa0 |\\xa0 6 unchecked\\xa0 |\\xa0 5 -6 diphthongs\\xa0\\n|\\xa0\\xa0 4 - 8 with shwa\\n\\n\\n\\xa0Chekt\\n-\\nshort\\n\\xa0Free\\n- long\\n\\xa0Difthongs\\n\\xa04\\nwith schwa\\n\\n\\næ{(æ)\\na:A:(a)\\naiaI(ay)\\naca@\\n| aic\\naI@\\n\\n\\nat, ax,\\nask,\\xa0 cat\\nalms,\\nwant, star\\xa0 5\\neye, ice,\\nbite\\nare, care\\xa0\\n|\\xa0 ire, fire\\n\\n\\nee(e)\\nc:3:()@@\\neieI(ey)\\nece@(e)\\n\\n\\nedge get,\\nelbow\\xa0 3\\nher, girl,\\nurban\\nace, ape,\\nvein\\nair, care,\\nthere, barely\\n\\n\\ni\\nI(\\ni )\\ni:\\ni:(iy)\\noi\\noI(oy)\\ni:c\\nI@(i)\\n\\n\\nit, in,\\nindex, ill\\neel, east,\\nvery\\noil, boy,\\nloyal\\near, fear,\\ndeer, mere\\n\\n\\no\\nturned\\naQ\\xa0\\no:o:(ao)\\nou\\n@U(oa)\\nor\\no@(o)\\n\\n\\nox, cot,\\notter\\nawe, call,\\ncost\\noh, oat,\\nlow\\nfor, four,\\nfloor, more\\n\\n\\nu\\nU\\n(u)\\nu:\\nu:(uu)\\n\\xa0ju\\nyU\\nuc\\nu@(u)\\n\\n\\nhook,\\nput, could\\nooze,\\nzulu, zoo\\nyou, few,\\nfuse\\nyour,\\xa0\\nsure, cure\\n\\n\\n^\\xa0\\xa0 V()\\nc\\nturned\\ne@\\xa0\\nau\\naU(a)\\nauc\\nau@\\xa0\\nau\\n\\n\\nup, cut,\\nputt\\nago, sofa,\\nunit\\nout, down\\nour, flower,\\npower\\n\\n\\n\\xa0\\n\\xa0 The IPA turned e,\\nturned a, and turned c are unavailable in ASCII\\nand Latin-1\\n\\xa0 The peachcolored\\ncells are not included in the Jones/Wijk essential phoneme inventory.\\n\\xa0 All checked\\nvowels (vowels which are checked or stopped by a consonant)\\nare short and all\\nfree\\nvowels, with the exception of schwa in an initial position, are long.\\xa0\\nIf a vowel occurs at the end of a word, it is by definition a free vowel\\nand does not need to be distinguished from a checked vowel. Thus \"Siy may\\nhælou\" could be \"Si may hælo.\"\\nin BR or \"Si m\\'y halo.\" in CKS\\xa0\\n\\n\\nOnly when the free vowel is\\nfollowed by a consonant does it need to be distinguished from a closely\\nrelated short vowel.\\nAn adequate orthography\\nfor English should have a unique character assigned to 10 of the 12 pure\\nvowels and a unique graf, digraf or trigraf for 24 of the 25 sound categories\\n(or phonemes)\\nlisted above.\\xa0\\nIdeally, the letters assigned to the\\npure vowels in the first two columns can be reused in the diphthongs and\\nother vowel blends.\\xa0\\xa0 Notice that most of the variants of New\\nSpelling below do not always reuse the pure vowels (ai=ie)\\n\\n1.\\nBroad\\nRomic IPA notation\\n\\xa0\\nSample\\nText 2\\n\\n2.\\nMapping of checkt\\nspelling including\\nconsonants. Chekt\\nSpeling\\xa0 Vowel\\nMap\\n\\xa0\\n\\n3.\\nMapping of Ian\\'s SaundSpel\\xa0\\xa0\\xa0 Sample\\nText\\nIan Ascott has spent over\\n30 years perfecting this particular IPA ASCII notation. Originally it was\\nan augmented alphabet that used Greek letters, eg, alpha for /æ/.\\nto provide a unique unigraph for all of the 34 pure vowels. The ASCII version\\n(below) has one Latin-1 character, the rest of the Greek characters has\\nbeen converted to a digraph or marked character. The notation includes\\na letter for schwa [c] but it is rarely used: eg,\\nDont puz hR araund. or Dont puz h3\\naraund.\\nIan\\'s SaundSpel is one of\\nthe few orthographies that clearly distinguishes between the 3 pronunciations\\nof R\\xa0 (r, R /3/, c). The notation adds an extra sound segment for\\nthe [long e] between [e] and [ei]. It is always possible to distinguish\\nsounds between any two phonemes in a minimalist phoneme inventory such\\nas the one created for broad romic. Thus [barely] is transcribed as [beeli]\\ninstead of [be@li].\\nThe based pronunciation for\\nSaundSpel is RP. For a discussion of the differences beween RP and the\\nmore traditional base pronunciations, see David Kelley\\'s page.\\n\\xa0\\n\\n\\nChekt\\nFree\\nDifthongs\\nw.schwa\\xa0\\nShort-chkt\\nExtended\\nDifthongs\\n\\xa0R-endings\\n\\n\\na\\'\\xa0 9\\naa\\nai\\naac,\\naic\\na\\'t\\xa0 9t\\xa0\\naalmz\\naiz (eyes)\\naa (are)\\naic (ire)\\n\\n\\ne\\nR\\xa0 3\\nee\\xa0\\n|\\xa0 ei\\nee\\xa0 ec\\nelbou\\nhR (her)\\neis (ace)\\nee (air)\\n\\n\\ni\\nj\\noi\\nic\\nit, index\\njst (east)\\noil, boy\\nic (ear)\\n\\n\\no\\noo\\nø\\nou\\n6\\noo\\xa0 oc\\nox, cot\\ncoot (caught)\\nøt (oat)\\nmoc (more)\\n\\n\\nu\\nuu\\niu\\nuc\\nhuk, gud\\nluut (lute)\\niu (you)\\npiuc (pure)\\n\\n\\na\\nc =\\nau\\nauc\\nap (up)\\ncgo (ago)\\naut (out)\\nauc (our)\\n\\n\\n[IA]\\nI wud now write \\'air\\' as \\'ee\\'.\\xa0 [SB]\\nI would write it as ea\\x92 or er (which amounts to the same thing)\\xa0 the\\nr can be the same. As /e:/\\xa0 so er = eh-uh.\\xa0 I see little justification\\nfor ee except for the fact that you say that many people can hear the difference\\nbetween ehh and eh-uh.\\xa0 I am going to follow Jones on this one.\\nSaundSpel\\nleaves out the R when it is not pronounced in RP\\n[IA]\\xa0\\nI kno that Americans sound the \\'r\\' in \\'air\\',\\xa0 but I go for\\nthe simplest skjm posibl. John wants tu leav the r\\'s in\\xa0 tu be ignored\\nby the non-rhotic community, whereas I ljv it out, putiq\\xa0 the onus\\non the Scots and Americanos tu add the \\'r\\' when impelled.something close\\nto ei - ee (SS) e: (NF)\\xa0 I sj no prob here: ei as in keik(cake), and\\nee as in beeli(barely)eat - iit (SS) i:t (NF) jt (SS)\\xa0 For me \\'j\\'\\nis the simplest option. \\'ii\\' is fiddly and \\'i: \\' hq tu put in\\xa0 colons\\nhinders the flow of writiqice - ais (SS) ys (NF)\\xa0 We both understand\\nmy use of IFA \\'ai\\'ire - air (SS) yr (NF) - aic (SS) ya\\' or aia\\' (NF)\\xa0\\nI spoze I wht (will have tu) stei with aic or aia, tho I\\'d prefer abolishiq\\xa0\\xa0\\nthe word \\'ire\\' altugether!oat - out (SS) o\\'t (NF)\\xa0\\noo for awe and\\xa0\\nor, ee for air.\\nMy IFA use of ou / dxou (though)\\nyuu\\'l understand.or - our? ooc? (SS) or (NF) (could be o:r)\\xa0 Followiq\\nmy use of \\'ee\\' for \\'air\\', I must use \\'oo\\' for \\'or\\'.\\xa0\\nMy com-\\xa0 ments abov\\nre Scots/American \\'r\\' apply here tuu.use - iuz or juuz (SS) iuz (NF)\\xa0\\nyuuz (verb) / yuus (noun)\\xa0\\nloot - luut\\xa0 lu:tlook\\n- luk lu\\'kluck - lxk luk\\xa0 luck - lak ...I agrj that my doubl yuus\\nof \\'a\\' is a bit haad tu teik...\\xa0 there is no-one in NZ with whom tu\\nargu it thruu with.\\xa0 PS duu yuu equate ei and e: ? Let mj kno...with\\nexamples.Ian\\nI have experimented with\\nequating ei with e:\\xa0 This is simply a prescription e: = ei and has\\nnothing to do with the way you may now pronounced an extended e.\\xa0\\nThis is based on the fact that many languages pronounce their E\\x92s two ways\\n- eh and ei.\\xa0 [comments by Ian Ascott, Jan, 1999]\\nNumbers as phonograms\\nFollowing the modest use of\\xa0 phononumerals\\nin CKS, Ian has come up with this extension\\npure\\nshort and long vowels...finiziq wi4 (with) 4 dif7oq2 (diphthongs).\\n2 = TO z,\\xa0\\xa0 as\\nin 2uu, niu2 (zoo, news)\\n3\\n= TO er, as in b3d, w3m, 3li (bird, worm, early)\\xa0\\n3: is also used in IPA, looks something like an R\\n4 = TO dh, as in 4is, wi4,\\n4 (this, with, the)\\n5 = TO zh, as in me5a, a5ua\\n(measure, azure)\\n6\\n= TO oh, as in k6k6, fol6 (cocoa, follow)\\n7 = TO th, as in 7ik, 7iqk,\\n79tz (thick, think, thatch)\\n9\\n= TO a,\\xa0 as in f9t, k9t, at9tz (fat, cat, attach)\\xa0\\nlooks like a script a or a\\xa0 turned a\\ntz =\\xa0 TO ch, - tch:\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\ntzip, itzi, butza\\xa0 (chip, itchy, butcher)\\nd5 = TO ge, -dge, j:\\xa0\\nd5amp, ed5i, sled5 (jump, edgy, sledge)\\nThe problem with numeric\\nphonograms is their inability to handle new fonts [e.g., shavian]\\n[A]\\nIF the converted /ae/ looks\\nnothing like a [9], then the script is without\\nnumeric representation.\\n\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nVAULS\\xa0\\xa0 (Vowels) ------ 12 simple vowels, saundspel\\nnotation, key words\\n\\xa0\\n\\n\\na\\naa\\ne\\nee\\ni\\nj\\no\\noo\\nu\\nuu\\n9\\n3\\nkat, ma4a, wari, sam (cut, mother,\\nworry, some)\\nkaat, faa4a, laafiq (cart, father,\\nlaughing)\\nferi, beli, meni (ferry, belly,\\nmany)\\nfeeri, beeli, zee (fairy, barely,\\nshare)\\npik, siti, likwid, risjv (pick,\\ncity, liquid, receive)\\npjk, sjtiq, pjpl, tzjri (peak,\\nseating, people, cheery)\\nkot, sori, soq, biloq (cot, sorry,\\nsong, belong)\\nkoot, gloori, brood, oo (caught,\\nglory, broad, or/oar/oar)\\nput, fut, zud, pudiq (put, foot,\\nshould, pudding)\\nfruut, muuv, skruu, tuu (fruit,\\nmove, screw, too/two)\\nm9t, 9ktiq, 9nt, fr9ntik (mat,\\nacting, ant, frantic)\\nf3n, b3d, t3n, w3m, 3li (fern,\\nbird, turn, worm, early)\\n\\n\\n\\n\\nD\\nai\\nei\\noi\\nau\\niu\\n6\\nDIF7OQ2\\xa0\\xa0 (Diphthongs)\\nail, fairi, ai-brau\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n(aisle, fiery, eye-brow)\\neibl, rein, teibl, vein\\xa0\\xa0\\n(able, rain, table, vain/vane/vein)\\nboi, oil, oista, boiliq\\xa0\\xa0\\xa0\\n(boy, oil, oyster, boiling)\\nkau, aut, haus, zaua\\xa0 (cow,\\nout, house, shower)\\nbiuti, fiu, hiuma\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n(beauty, few, humor)\\n46, fl6, k6k6, b6t\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n(though, flow, cocoa, boat)\\n\\n\\n\\n\\xa0\\n\\n4.\\nMapping of New Spelling onto IPA\\xa0\\xa0 3\\nSample\\nText\\nThere have been various attempts\\nto update this approach beginning in 1910 when it was circulated by Ellis\\nunder the name of Simplied Spelling.\\xa0 After 1930 the most common title\\nwas New Spelling.\\xa0 Walter Ripman and Walter Archer updated the notation\\nin 1955 and published a book on the topic.\\xa0 New Spelling is the basis\\nfor\\xa0 Anglic (1930), WES (World English Spelling), and ALC Fonetik.\\xa0\\nThe basic difference between these notational systems is in the number\\nand type of concessions to TO (tradtional orthography).\\xa0 New Spelling\\nused to have 11 word signs (including the, to, a, I, be, he, we,\\nme, she) which are used over 8,000 times in 100,000 running words. In Anglic\\nthe number of word signs was extended to 42. It could be extended to the\\n200 most frequent words since the point is to have a way to spell unfamiliar\\nwords.\\xa0 Another option is to find a system where the, me, and be are\\nnot irregular exceptions but this requires positional\\nspelling.\\nThe latest New Spelling\\n90\\xa0 brochure from the Simplified Spelling Society substitutes\\ny for /ai/.\\xa0 Most versions use [ie] to index this phoneme (see SpelRiet).\\nThe goal of New Spelling\\nwas to take one frequently used TO spelling pattern and use it consistently.\\xa0\\nThe system of orthography is easy to read for those familiar with TO.\\xa0\\nIt should be relatively easy to learn and spell.\\xa0 ITA (augmented roman)\\nwas essentially New Spelling with a special font that changed digrafs into\\njoined letters or ligatures [ ae became æ ].\\nThe chief failing of NS is\\nspace efficiency.\\xa0 A passage in NS tends to be longer than the same\\npassage in TO.\\xa0 In\\nNS, letter combinations are considered to be new symbols. It would be more\\nelegant if they were blends of the sounds of component letters (ae\\nis not ah+eh).\\n\\xa0\\n\\n\\nChekt\\nFree\\nDifthongs\\nw.schwa\\xa0\\nShort-chkt\\nExtended\\nDifthongs\\n\\xa0R-endings\\xa0\\n\\n\\na\\naa\\nie\\naar,\\nier\\nat\\naalmz (alms)\\niez (eyes)\\naar (are)\\nier (ire)\\n\\n\\ne\\neur\\nae\\ner\\nelboe\\nheur (her)\\naes (ace)\\naer (air)\\n\\n\\ni\\nee\\noi, oy\\neer\\nit, index\\neest (east)\\noil, boi\\neer (ear)\\n\\n\\no\\naw\\noe\\nor, oer\\nox, cot\\ncawt (caut)\\noet (oat)\\nmoer (more)\\n\\n\\noo\\nuu\\nue\\nur\\nhook (hook)\\nluut (lute)\\nue (you)\\npuer (pure)\\n\\n\\nu\\na e i u\\nou\\nour\\nup, but\\nugo (ago)\\nout (out)\\nour (our)\\n\\n\\n\\n\\n\\nQuiz\\nSentences in New Spelling,\\xa0\\nBroad Romik, and other systematic notations.\\xa0\\n(Can you guess which is which?):\\nThae sae it iz her faet.\\xa0 Ðey\\nsey it iz h\\xa0\\nfeyt.\\xa0\\xa0 Xei sei\\nit iz h\\'r feit.\\xa0\\xa0 Thay\\nsay it is her fayt.\\xa0\\n(click for answers)\\n\\n\\n\\n\\xa0List\\nof all New Spelling phonemes including consonants\\n\\n5.\\nMapping ALC Fonetik on to IPASample\\nText\\nFonetic is closely related\\nto WES (World English Spelling) which was based on Anglic and New Spelling.\\nFonetic is very well documented. The BTRSPL conversion dictionary contains\\nnearly 40,000 words and the Dictionary of American Spelling\\ncontains 43,000 words in this notation.\\nThe Fonetik table is almost\\nidentical to the one for New Spelling. However, Fonetik translations may\\nbe quite different. This is because Fonetik is less phonemic than New Spelling.\\nFonetik tries to minimize the differences between fonetik spelling and\\nTO by using various devices such as logograms (sight words) and positional\\nspelling. These compromises make the spelling less systematic and predictable\\nbut reduce the liklihood that the spelling will be unfamiliar and disturbing\\nfor those used to TO.\\nPositional spelling can be\\nsystematic and predictable. For example, [your highly..] can be spelled\\n[yur hyly..]. In positional spelling, the sound of Y depends on its position\\nin the word. In the initial position itis a consonant, in the medial position\\nit indicates the /ai/ vowel sound, and in the terminal position it represents\\nthe /i:/ sound.\\nFonetic does not have a separate\\nsymbol for the schwa and does not show stress. [ago] is spelled as if it\\nwere pronounced ah-goh (see chart). [more] is spelled as if it were pronounced\\n/mour/.\\n\\xa0\\n\\n\\nChekt\\nFree\\nDifthongs\\nw.schwa\\xa0\\nShort-chkt\\nExtended\\nDifthongs\\n\\xa0R-endings\\n\\n\\na\\naa, o\\nie, i\\naar, ier\\nat\\naalmz\\niez (eyes)\\naar (are)\\nier (ire)\\n\\n\\ne\\ner..\\nae, ay, a\\nair, er\\nelboe\\nher (her)\\naes (ace)\\naer (air)\\n\\n\\ni\\nee\\noi, oy\\neer, ir\\nit, index\\neest (east)\\noil, boi\\neer (ear)\\n\\n\\no\\naw\\noe, o\\noer, or\\nox, cot\\ncawt (caut)\\noet (oat)\\nmoer (more)\\n\\n\\noo\\nuu\\nue, u\\nur, uer\\nhook, good\\nluut (lute)\\nue (you)\\npuer (pure)\\n\\n\\nu\\na e i u\\nou\\nour\\nup, but\\nagoe (ago)\\nout (out)\\nour (our)\\n\\n\\nSample 5: Speling\\nReformers tipicaly wont to riet with a dicshunairy pronunsiaeshun gied\\nrather than tradishunal English speling. Thay wont the speling sistem to\\nbe neerly 100% alfabetic insted of 40%. English speling is hard becauz\\nthair ar too meny orthografic opshuns\\xa0\\n\\n6.\\nMapping Truespel\\nonto IPASample\\nText\\nThis table is based on the\\nBTRSPL translations, not on a grapheme/phoneme correspondence chart.\\nTS does not have a separate\\nsymbol for the schwa or ng. Truespel and Troospel are closely related.\\nBoth are derivitives of\\nWES (World English Spelling) which makes the GPC table very familiar.\\nIn tabular form, Truespel\\nlooks like ALC Fonetic. The unique aspect of TS is the use of double consonants\\nto mark stress: dubool kaansunint leed u stresd\\nsilubool.\\xa0 Fer instints thu TO werdz \"desert\" and \"dessert\".\\xa0\\nIn truespel theez wood bee speld dezert and dizzert. [CKS:\\ndesrt, dizert]. Thair iz noe reezin naat tue maek good yues uv dubool kaansunints.\\xa0\\nStres shood bee indikaetid in enee nue speleeng.\\xa0 -Taam\\n5/99\\nAlthough based on the same\\nprinciples, passages written in TS and Fonetic look quite different.\\xa0\\nTruespel is the most space inefficient of all the notations displayed on\\nthis page.\\xa0 To mark stress, a passage written in Truespel will be\\n10% longer than one written in ALC fonetic.\\xa0 See: Automated\\nSpelling Converter:\\n\\xa0\\n\\xa0\\n\\n\\nChekt\\xa0\\nFree\\nDifthongs\\nw.schwa\\nShort-chkt\\nExtended\\nDifthongs\\n\\xa0R-endings\\n\\n\\na\\naa\\nie\\xa0 i\\naar\\nat\\npaalm (palm)\\niez (eyes)\\nar (are) ier (ire)\\n\\n\\ne\\ner\\nae\\ner\\nelboe\\nher (her)\\naes (ace)\\naer (air)\\n\\n\\ni\\nee\\noi\\neer\\nit, index\\neest (east)\\noil, boi\\neer (ear)\\n\\n\\naa\\nau\\noe\\nor\\naax, caat\\ncaut (caught)\\noet (oat)\\nmor (more)\\n\\n\\noo\\nue\\nyue\\nuer\\nhuuk, guud\\nluet (lute)\\nyue (you)\\nyuer (your)\\n\\n\\nu\\na e i o\\nou\\nour\\nup, but\\nuggoe (ago)\\nout (out)\\nour (our)\\n\\n\\n\\xa0Sample 6: Speleeng reformers\\ntipikoollee waant tue riet withh u dikshunairee prununseeyyaeshun giede\\nrather than truddishunool Eenglish speleeng. Thae waant thu speleeng sistem\\ntue bee neerlee 100% alfubbetikool instted uv 40%. Eenglish speleeng iz\\nhaard beekkuz thair aar tue menee orthografik opshunzns.\\xa0\\n\\nsee\\nRES\\n7.\\nMapping of OGD Restored English Spelling onto IPA\\xa0\\xa0\\n(by John Reilly)\\xa0\\xa0 Sample\\nText 7\\nThis nu noataytion is wun\\nov the beter sceems a\\'round. Thair ar stil a fiu ambigiuities tu be werked\\nout. Moast ov the problems are relayted tu incorporayting morfemic speling:\\xa0\\nplurals=s [munky-munkees]\\xa0 past=ed\\xa0 [cairy-caireed]\\xa0\\nThe isiu on plurals is wether\\nthe s ending counts as a consonant.\\xa0 If so, munkys wuud\\nnot be a lejitimat speling and wuid hav tu be converted tu munkees.\\xa0\\nSince TO is a grafo-morfeemic\\nsistem of speling, so is OGD positional speling.\\xa0\\nThe notaytion dus not employ\\nthe majic-e or dubl consonants as short vowel (voul) markers.\\xa0\\nIn a rscent Speling Sosyity\\nJurnal articl, it was observd that Cut\\nSpeling ofen reprodooses Midl Inglish forms. Eeven wen respelings ar\\nnovl,\\xa0 the sceems that produess them ar jeneraly just reaserting prinsipels\\nthat wer part ov Inglish ortthografy frum its inseption, but that becaim\\nobsciured in erly modern tyms.\\xa0 OGD is a sistematic form of cut speling\\nwhich contrains the number of orthografic options for a particiular sound\\n(or foneem) tu three or less.\\xa0\\nThe foloweeng is an atempt\\nto condenss all the rools for positional speling into a singl chart. John\\nReilly is of the opinion that tu charts ar required, won for singl silabl\\nwerds and won for multisilaabic werds.\\xa0\\nLetrs such as Y and W cuud\\nposibly mark silabl boundarees as in OYster.\\xa0\\nStress is normaly falls on the penultimat silabl. Not every combyned voul\\noperayts exactly the saim way.\\xa0\\n\\xa0\\n\\n\\n\\xa0OGD\\nPositions\\xa0 OGD\\nspelling depends on the position of the sound in the word\\n1.\\nInitial letter of an initial syllable (-alone), ,\\xa0\\xa0\\xa0\\xa0\\n2. before a consonant,\\xa0\\xa0\\xa0\\xa0 3. at the end of word or\\nsyllable\\n\\n\\nChekt\\n\\xa0Free\\nDiphthongs\\nwith\\n-r\\n\\n\\n-\\na\\n- /æ/\\noaa,\\no\\xa0\\xa0 aa, ah\\ni-\\xa0\\xa0\\xa0\\xa0\\ni-e\\xa0\\xa0 y\\xa0\\xa0\\xa0\\xa0\\xa0 y\\xa0 ye\\naraar\\n|\\xa0 ier\\xa0 ire\\n\\n\\n\\nAT,\\nAL, PAT\\n\\nalms-oms,\\nwaant-wont\\nice-iess,\\neye-ie, like-lyk,\\xa0\\nmy-my\\nare-ar,\\nCAR\\xa0\\n|\\xa0 ire-ier,\\nfyr\\n\\n\\n-\\ne\\n-\\nur\\xa0\\xa0\\xa0\\xa0\\ner\\xa0\\xa0\\xa0\\xa0 er\\nay-\\xa0\\xa0\\nai\\xa0\\xa0\\xa0\\xa0 ay\\nairayrer\\n\\n\\n\\nedge-ej,\\nPET\\n\\n\\nURBAN,\\nbird-berd, HER\\n\\n\\nace-aiss,\\naip,\\nMAIL, faze-faiz,\\nMAY\\n\\n\\nAIR,\\nthere-thair,\\nswair, bair, FAIR\\n\\n\\n\\n-\\ni\\n-\\nea\\xa0\\xa0\\xa0\\xa0\\nee\\xa0\\xa0\\xa0\\xa0 e-y\\noy/oi\\xa0\\n-\\noi\\xa0\\xa0\\xa0\\xa0 oy\\neer\\nearir\\n\\n\\n\\nIT,\\npijen,\\nPIT\\n\\n\\neat,\\nneet, ME, VERY\\n\\n\\nOY-STER,\\nBOIL, loyal-loil,\\nBOY\\n\\n\\nEAR,\\nfear- feer, speer, neer\\n\\n\\n\\n-\\no\\n-\\naw-all\\xa0\\xa0\\xa0\\nau\\xa0\\xa0\\xa0 aw\\no-\\xa0\\xa0\\xa0\\noa\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 o\\nor\\nawr\\n\\n\\nOX,\\nPOT\\nall,\\nhauk,\\ncaust,\\npaw\\nobey-obay,\\nOAT, boal, silo-sylo,\\xa0\\nOR,\\nsoar -sor, ore-or,\\nfour-for, flor\\n\\n\\n-\\nui -\\noo\\xa0\\xa0\\xa0\\xa0\\noo\\xa0\\xa0\\xa0\\xa0\\xa0 u\\nu-\\xa0\\niu\\xa0\\xa0\\xa0\\xa0\\xa0 iu\\noor\\xa0\\n|\\xa0 iur\\n\\n\\nhook-huik,\\npuit\\nooz,\\nPOOL,\\xa0 zulu, do-du\\nUNIT,\\nuse-iuss, you-u,\\nsuit-siut\\npour-poor,\\ntour-toor | your-iur,\\nfiury\\n\\n\\n-\\nu\\xa0 -\\na\\'\\xa0\\xa0\\naeiou\\xa0\\xa0\\xa0 a-e\\no-\\xa0\\xa0\\nu\\xa0\\xa0\\xa0\\xa0\\xa0 ow\\nour\\nower\\n\\n\\nUP,\\nCUP, put\\na\\'go,\\nu\\'nder, SOFA,THE\\nOUTer,\\nhouse-houss,\\nNOW, COW\\xa0\\nOUR,\\nFLOUR,\\xa0 flower-flour\\n\\n\\n\\xa0consonants\\n\\nTHE\\xa0 o\\'ther\\xa0\\ntthin\\xa0 witth\\n\\n\\nz\\xa0\\xa0\\xa0\\nz\\xa0\\xa0\\xa0 s\\n\\n\\ns\\xa0\\xa0\\xa0\\ns\\xa0\\xa0\\xa0 ss\\n\\n\\n\\n\\npl=s,\\npast=ed\\n\\n\\nus-u\\'ss,\\nsofa, fiesta,\\no\\'ther\\nwhat-\\nwhaat-\\nwot,\\nof-ov\\n\\n\\nzoo-zu,\\nfroze-froaz,\\nnose-nos, does-dus,\\nBEERS,\\ntears-teers\\n\\n\\nSO,\\nuse-iuss, deuce-dooss,\\nmoose-mooss,\\nmice-myss\\n\\n\\n\\n\\xa0©1999\\nBETA\\xa0 OGD\\nwords that match TO\\nare in caps\\n\\n\\n\\xa0Grafic\\nversion\\nThe\\nschwa sound could be marked with a schwapostrophe to reduce ambiguity.\\na\\'go, u\\'nder, u-nity\\xa0\\nThe\\nshwaa sound cwd be marked with a\\' shwapostrofe tu re\\'dooss ambigiuity.\\nX\\n5wa saund cu.d bi markt with a\\' 5wapostrofi tu ridus ambigiuiti.\\n\\n\\n8.\\nMapping of ANJeL onto IPASample\\nText 8\\nThe ANJel notation has been optimized\\nfor video captions on television screens. Notice that there are no characters\\nwith descenders [gqpy] because the descenders are short, only one pixel\\nwide, and almost invisible in video captions. The system represents 9 of\\nthe 12 pure vowels with unique characters. [aa, o, and o:] are merged into\\na single phoneme which is represented with an x. [Shwa and 3:] are\\nboth rerpresented with [r]. U is used for /^/ and shwa: [apple/ aPUL].\\nThe diphthongs or vowel blends are all represented with a single letter.\\nThe most frequently used letters are displayed in upper case. The small\\nu\\nis not used. Instead m and k are used for the other U vowels.\\n\\xa0\\n\\n\\nChekt\\nFree\\nDifthongs\\nw.schwa\\xa0\\nShort-chkt\\nExtended\\nDifthongs\\n\\xa0R-endings\\n\\n\\na\\nx\\nI\\nxr, Ir\\naT\\nxLMZ (alms)\\nIZ (eyes)\\nxr (are)\\nIr (ire)\\n\\n\\ne\\nr\\nA\\ner\\neLBX\\nHR (her)\\nAS (ace)\\ner (air)\\n\\n\\ni\\nE\\nb\\nir, Er\\niT (it)\\nEST (east)\\nbL (oil)\\xa0\\nEr (ear)\\n\\n\\nx\\nx\\nX\\nXr\\nxKS (ox)\\nCxT (caught)\\nXT (oat)\\nMXr (more)\\n\\n\\nm\\nk\\nYk\\nkr\\nHmK\\n(hook)\\nLkT (lute)\\nYk (you)\\nPYkr (pure)\\n\\n\\nU\\nU\\nd\\ndr\\nUP (up)\\nUGX (ago)\\ndT (out)\\ndr (our)\\n\\n\\napple/aPUL, church/crc, boil/BbL, vicious/VisUS,\\nshow\\nher around/ sX Hr URdnD\\nSample 8: SPeLn REFXRMrZ TiPiKLE\\nWxNT Tk RIT Wih U DiKsUNerE PRUNUNSIAsUN GID Rahr haN TRUDisUNUL EnGLis\\nSPeLin. hA WxNT U SPeLin SiSTUM Tk BE NiRLE 100% aLFUBeTiK iNSTeD UV 40%.\\nEnGLis SPeLin iZ HarD BEKxZ her xR Tk MeNE XRhXGRaFiK xPsUNZ.\\xa0\\n\\xa0\\n\\n9.\\nMapping of Unigraf\\nonto IPA (under construction)\\xa0\\xa0\\xa0\\nSample\\nText 9\\nUnigraf means one mark and implies\\none\\nsymbol per sound.\\xa0 Unigraf\\nsounds\\nThis notational system is\\nquite workable but is not quite as easy to read as\\none might\\nexpect.\\xa0 While it is\\ntrue that the most frequent representation of the long vowels in TO\\nis with a single letter,\\nusing single letters doesn\\'t always make for easy reading.\\nThere is a good case for\\neliminating digraphs for non-combinations but a much weaker case for\\nreplacing true combinations\\n(diphthongs).\\xa0 dVsIz is much more\\ndifficult to read than daunsaiz.\\n(That\\'s downsyz in\\nOGD).\\nUnigraf has several variations.\\nThe one below is optimized for Keyboard\\nShavian.\\xa0 The original keyboard Shavian is shown in gray brackets.\\xa0\\nThe modifications are marked\\nin yellow.\\xa0\\nWe ate good food at the\\ntable = wE At gcd fCd æt\\nx tAbL\\n\\xa0\\n\\n\\n\\nChekt\\n\\n\\nFree\\n\\n\\nDifthongs\\n\\n\\nwith\\nschwa\\n\\n\\n\\n\\n@\\xa0 /æ/\\n[A]\\n\\n\\nq V\\xa0 [y]\\n\\n\\nY /ai/\\xa0 [F]\\n\\n\\nVr/qr H R\\xa0\\nYr F X\\n\\n\\n\\n\\ne\\n\\n\\nR\\xa0 [D]\\n\\n\\nA /ei/\\xa0\\n[E]\\n\\n\\ner\\xa0 B\\xa0 [xX]\\n\\n\\n\\n\\ni\\n\\n\\nE\\xa0 [I]\\xa0\\n\\n\\nQ /oi/\\xa0 [q]\\n\\n\\nir\\xa0 D\\xa0 [C]\\n\\n\\n\\n\\nq\\xa0 [o]\\n\\n\\no [Y]\\n\\n\\nO /ou/\\n\\n\\nor\\xa0 P\\n\\n\\n\\n\\nc\\xa0 [U]\\n\\n\\nC\\xa0 [M]\\n\\n\\nU /ju/\\n\\n\\nUr\\n\\n\\n\\n\\nu\\n\\n\\na\\n\\n\\nW /au/ [Q]\\n\\n\\nWr\\n\\n\\n\\n\\xa0\\n\\n\\nChekt\\nFree\\nDifthongs\\nw.schwa\\xa0\\nShort-chkt\\nExtended\\nDifthongs\\n\\xa0R-endings\\n\\n\\na\\nq\\nI\\nqr, Ir\\nat\\nqmz (alms)\\nIz (eyes)\\nqr (are)\\nIr (ire)\\n\\n\\ne\\nR\\nA\\nAr, er\\nelbO\\nHR (her)\\nAs (ace)\\nAr (air)\\n\\n\\ni\\nE\\nQ\\nEr, ir\\nit\\xa0 (it)\\nEst (east)\\nQL (oil)\\xa0\\nEr (ear)\\n\\n\\nq\\no\\nO\\nor\\nqX (ox)\\nkot (caught)\\nOt (oat)\\nmor (more)\\n\\n\\nc\\nC\\nU, yC\\nUr\\nhck (hook)\\nlCt (lute)\\nU (you)\\nPUr (pure)\\n\\n\\nu\\n^\\nV\\nVr\\nup (up)\\n^gO (ago)\\ndT (out)\\ndr (our)\\n\\n\\napple/apl, church/KrK, boil/bQL, vicious/Visus,\\nshow\\nher around/ SO hR a\\'rVnd\\xa0 Sample\\n9\\n\\n\\n\\n\\n\\nMapping\\ndifferent orthographies on to IPA\\n\\xa0This\\npage was developed in response to a request received on the simplified\\nspelling mailing list\\n\\xa0\\nfor\\na way to relate different solutions to the alphabet problem to the Int\\'l\\nPhonetic Alphabet (l897)\\n1Broad\\nRomic\\xa0 - 2 Chkt Spelling2\\xa0\\n-\\xa0 3\\nSaundSpel 4New\\nSpelling\\xa0 -\\n5ALC\\nFonetic\\xa0 -\\xa0\\n6Truespel\\xa0\\n-\\xa0\\xa0 7.\\nOGD8.\\nANJeL9.\\nUnigraf\\n\\n\\n\\n\\n\\n\\n\\n\\n2.\\nBroad\\nRomic IPA notation\\n\\n\\nSample 1 (broad romic)\\n\\nSpeling rifomz\\ntipikli wont tu rayt wið\\xa0\\ndikSneri\\nprnnsieySn\\ngayd rað\\nðæn tradiSanl\\nIngliS speling. ðey wont ð\\nspeling sistm\\ntu bi niyrli 100% ælfbetic\\ninsted\\xa0v\\n40%. IngliS speling iz hard bikaoz ther\\nar tuu meni orthogræfik opSnz.\\n\\n\\n\\nSample 2 (chkt\\nspeling) [does not include x for th and 5 for sh]\\n\\nSpeling riform\\'rz tipikly want tu r\\'yt with \\'a\\ndiksh\\'anery pr\\'anu\\'nsieish\\'an g\\'yd rathr tha.n tr\\'adish\\'anl Inglish speling.\\xa0\\nThey want th\\' speling sistm tu bi nirly 100% alfabetik insted ov 40%.\\xa0\\nInglish speling iz hard bikoz ther ar tu meny orthografik opsh\\'anz.\\n\\n\\n\\nSample 3 (saund\\nspel)\\n\\nSpeliq rifoomcz tipikli wont tu rait widx a dikzcneri\\nprcnansieezcn gaid rcdxc dxa\\'n tradizanl Iqgliz speliq. dxei wont dxc speliq\\nsistcm tu bi nirli 100% a\\'lfabetik insted cv 40%. Iqgliz speliq iz haard\\nbikooz dxer aar tuu meni oortxougra\\'fik opzcnz.\\n\\n\\xa0\\n\\nSample 4 (nu speling) [see spelriet]\\n\\nSpeling Reformerz tipikli wont tuu riet with a\\ndikshunairi pronunsiaeshun gied rathr than tradishunul English speling.\\nThay wont the speling sistem to be neerly 100% alfabetic insted of 40%.\\nEnglish speling is hard becauz thair ar too meny orthografic opshuns.\\n\\n\\xa0\\n\\nSample 5 (alc fonetik)\\n[this text was converted by the automated BTRSPL conversion program]\\n\\nSpeling Reformers tipicaly wont to riet with a\\ndicshunairy pronunsiaeshun gied rather than tradishunal English speling.\\nThay wont the speling sistem to be neerly 100% alfabetic insted of 40%.\\nEnglish speling is hard becauz thair ar too meny orthografic opshuns.\\n\\n\\n\\nSample 6 (truespel)\\n\\nSpeleeng reformers tipicoollee waant tue riet\\nwithh u dikshunairee prununseeyyaeshun gied rather than truddishunool Eenglish\\nspeleeng. Thae waant thu speleeng sistem tue bee neerlee 100% alfubbetikool\\ninstted uv 40%. Eenglish speleeng iz haard beekkuz thair aar tue menee\\northografik opshuns.\\n\\nSample 7 (OGD\\n- Restored English Spelling)\\n\\nSpeling reformers tipicaly waant tu ryt witth\\na dictionairy pronunseaytion gyd rather than traditional English speling.\\nThay waant the speling sistem tu be neerly 100% alfabetic insted of 40%.\\nEnglish speling is hard becauz thair ar tu meny orthografic options.\\n\\n\\xa0Sample 8 (ANJeL)\\n[see www.webpal.org]\\n\\nSPeLin REFXRMrZ TiPiKULE WxNT Tk RIT Wit U DiKsUNerE\\nPRUNUNSIAsUN GID Rahr haN TRUDisUNUL EnGLis SPeLin. hA WxNT hU SPeLin SiSTUM\\nTk BE NERLi 100% aLFUBeTiKUL iNSTeD UV 40%. EnGLis SPeLin iZ HarD BEKxZ\\nher xR Tk MeNE XRhOGRAFiK xPsUNZ.\\n\\n\\nSample 9 (Unigraf)\\ndownsize\\xa0 (Cc = lazy Uu,\\xa0 x=dh, u=up and schwa)\\n\\nspeliN riformRz tipikli wqnt tC rIt wiT u dikSuneri\\nprununsIASun gId raxr xan trudiSunul EngliS speliN.\\xa0 xA wqnt xu speliN\\nsistum tC bE nErli 100% alfubetikul insted uv 40%.\\xa0 engliS speliN\\niz hqrd bEkoz xer qr tC meni orTografik qpSunz.\\n\\n\\n\\n\\nbase:\\xa0 http://pages.whowhere.com/community/sbett/map-ipa-2.html\\nMore\\nalternate\\nnotations and transcriptions:\\xa0\\nI say nu boy go out (RES)\\nFor\\nmore spelling links go to the Spelreform\\nforum. www.delphi.com/spelreform\\nor go directly to the message\\nboard at\\nhttp://forums.delphi.com/m/main.asp?sigdir=spelreform\\nThe\\ndelphi site reached its message limit in 1998.\\xa0 The new message board\\nwith threaded messages is located at http://www.egroups.com/group/saundspel\\nGlossary\\xa0\\xa0\\xa0\\xa0\\n\\n\\n\\nlast revised:\\xa0\\nApril, 1999\\n\\n\\n\\n\\n\\nLocal (relative)\\nlinks\\n\\n\\nSpelling\\nLinks\\n\\n\\nSiteMap-index\\n\\n\\nAddress\\n\\n\\n\\n\\nRemote (absolute)\\nlinks\\n\\n\\nSpelling\\nLinks\\n\\n\\nSiteMap-index\\n\\n\\nMap-IPA\\n\\n\\n\\n\\nComments\\nor Problems? Contact\\n\\n\\nSteve\\nBett\\n\\n\\nAlt.\\nnotations\\n\\n\\nCKS\\n\\n\\n\\n\\nOriginal\\npage elements:\\xa0 Copyright ©1998 BETA Interactive\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nweb hosting • domain names • web designonline games • online dating  • long distancedigital cameras  • advertising online\\n\\n\\n\\n\\n\\n',\n",
       " '\\n\\n\\n\\n\\n\\nNIST Ground levels and ionization energies for the neutral atoms\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nVersion History \\xa0 | \\xa0 \\nDisclaimer\\n\\n\\n\\n\\nW.C. Martin, A. Musgrove, S. Kotochigova, and J.E. Sansonetti\\n\\nNational Institute of Standards and Technology \\nGaithersburg, MD 20899 \\n\\n\\n\\nTable of Ground Levels and \\nIonization Energies for the Neutral Atoms*\\n  or\\nPeriodic Table Format (NIST SP 966)\\n[292\\xa0kB\\xa0PDF file - download the reader]\\n\\n \\nThe ionization energies in the table are based on a recent survey of the \\nliterature. A reference to one or another data compilation is given for a \\nnumber of elements; the cited compilation gives the reference(s) for the \\noriginal ionization-energy data. The uncertainties are mainly in the range from \\nless than one to several units in the last decimal place, but a few of the \\nvalues may be in error by 20 or more units in the final place; i.e., the error \\nof some of the two place values could be greater than 0.2\\xa0eV. Estimated \\nuncertainties of the ionization energies are usually given in the references. \\nAlthough no more \\nthan four decimal places are given, the accuracies of some of the better known \\nvalues would, in eV units, be limited only by the uncertainty in the conversion \\nfactor, \\n1.239\\xa0842\\xa044(37)\\xa0×\\xa010-4\\xa0eV/cm-1.\\n\\nThe ground-state electron configurations of elements heavier than neon are \\nshortened in the table by using rare-gas element symbols in brackets to \\nrepresent the corresponding electrons. The ground levels of all neutral atoms \\nhave reasonably meaningful LS-coupling names, the corresponding \\neigenvector percentages lying in the range from ~55% to 100%.  These names \\nare listed in the table, except for Pa, U, and Np; the lowest few \\nground-configuration levels of these atoms comprise better \\n5f\\xa0N(L1S1J1), \\n6dj7s2\\xa0(J1\\xa0j) terms than LS-coupling terms. The \\nrelatively large spin-orbit interaction of the 6p electrons produces \\njj-coupling structures for the \\n(6p21/2)0, \\n(6p21/26p3/2)o3/2, \\nand \\n(6p21/26p23/2)2 \\nground levels of the 6p2, 6p3, and \\n6p4 configurations of neutral Pb, Bi, and Po, respectively. \\nThese jj-coupling names are more appropriate for these atoms than the \\nalternative LS-coupling designations in the table.\\n\\n\\n\\n\\n*Based on the article \"Atomic Spectroscopy,\" by\\nW.\\xa0C.\\xa0Martin and W.\\xa0L.\\xa0Wiese in Atomic, Molecular, & \\nOptical Physics Handbook, ed. by G.W.F.\\xa0Drake (AIP, Woodbury, NY, \\n1996) Chapter\\xa010, pp.\\xa0135-153.\\n\\n\\n\\n Inquiries or comments: www@physics.nist.gov. \\nOnline: April 1998 \\xa0 - \\xa0 Last update: September 2003 \\n\\n\\n\\n\\n',\n",
       " ' Are You Wanted by the Recording Industry? By Chris Sherman , Associate Editor August 11, 2003 Concerned that information about your file-sharing user name may have been subpoenaed by the Recording Industry Association of America? Check this database to see if you\\'re a potential target. Ever since Napster made it easy for music lovers to share files, the Recording Industry Association of America (RIAA) has been on a crusade to put an end to what they allege is illegal file sharing. The RIAA successfully sued Napster, resulting in the demise of the popular service. The battle has escalated this year, with the RIAA shifting its targets from companies to individual users. On June 25, 2003, the RIAA announced that it will begin suing users of peer-to-peer (P2P) file-sharing systems beginning at the end of August. Targets of RIAA actions are users of services like Grokster, Morpheus, KaZaA, Aimster, Gnutella, and others. According to the RIAA, it plans to selectively target users who upload or share \"substantial\" amounts of copyrighted music. How does the RIAA identify those users? By using software that scans users\\' publicly available P2P directories, and then identifies the ISP of each user. Then, under the provisions of the controversial Digital Millennium Copyright Act (DMCA), the RIAA subpoenas ISPs for each user\\'s name, address, and other personal information. The RIAA will then sue those individuals. The Electronic Frontier Foundation (EFF) is helping users deal with the draconian tactics of the music industry cartel. It has created a Subpoena Database that lets you check the user names and IP addresses used on a file sharing service against a database of those user names specified in hundreds of subpoenas the RIAA issued to Internet Service Providers (ISPs). \"The recording industry continues its futile crusade to sue thousands of the over 60 million people who use file sharing software in the U.S.,\" said EFF Senior Intellectual Property Attorney Fred von Lohmann. \"We hope that EFF\\'s subpoena database will give people some peace of mind and the information they need to challenge these subpoenas and protect their privacy.\" \"EFF is also documenting the scope of privacy invasions committed by the RIAA,\" explained EFF Staff Attorney Jason Schultz. \"EFF\\'s subpoena database will help document the damage done to innocent people misidentified as copyright infringers in the RIAA\\'s overzealous campaign.\" If you\\'re a user of a P2P system, it\\'s important to realize that these systems are legal, and using them is not a crime, despite the RIAA\\'s heavy-handed tactics. If you are a P2P user, take a look at How Not to Get Sued by the RIAA for File Sharing . If you feel you\\'ve been wrongly targeted by the RIAA, visit the Subpoena Defense Alliance , a joint project of the Electronic Frontier Foundation, the US Internet Industry Association and other organizations that began in April of 2003. Its purpose is to assist consumers and Internet Service Providers who have been served subpoenas seeking the identity of customers who use the Internet for private communications. What\\'s Your IP Address? An IP address is a unique identifier used by your computer to let it communicate with other computers on the Internet. It\\'s similar to a telephone number. Your browser announces your computer\\'s IP address to every web site it visits, as does file sharing software. This is how the recording industry is developing its set of targets. You can find out what your own computer\\'s IP address is by visiting a browser header check utility. This shows all of the information your browser reveals to sites you visit. Note: If you\\'re on a network, behind a firewall, or use a large ISP, your IP address may be temporary, assigned only for your current online session. Search Headlines NOTE: Article links often change. In case of a bad link, use the publication\\'s search facility, which most have, and search for the headline. ISPs unite against RIAA attacks ... Netimperative Aug 11 2003 12:59PM GMT Can\\'t spot a spoof? Meet Google ... Guardian Unlimited Aug 11 2003 11:31AM GMT Yahoo chief executive gains $ 11 million ... SiliconValley.com Aug 11 2003 10:32AM GMT Google pulls AdSense feature after backlash ... Netimperative Aug 11 2003 9:34AM GMT Ask Jeeves Drops Butler in New Ad Campaign ... SiliconValley.com Aug 11 2003 7:33AM GMT Search Engine For JavaScript ... Research Buzz Aug 11 2003 6:21AM GMT Saudis Restrict Internet To Stop Al Qaida ... Middle East Newsline Aug 11 2003 3:51AM GMT Google ads a threat to eBay trademark? ... ZDNet Aug 10 2003 11:40PM GMT DMA-Delayed Best Practices Skirt Spam, Permission Issues ... Boston.Internet.com Aug 9 2003 3:54AM GMT Google girds for big challenge as its influence widens ... Singapore Business Times Aug 8 2003 11:40PM GMT TV: the Next Portal War ... Internet News Aug 8 2003 6:03PM GMT Do Not Spam list and filtering firms join hands ... The Register Aug 8 2003 4:02PM GMT New top-level domain goes .pro ... ZDNet Aug 8 2003 11:09AM GMT Reed Elsevier, the unsung star of the internet revolution ... Independent Aug 8 2003 2:21AM GMT SearchDay Archives Search Engine Watch www.searchenginewatch.com Danny Sullivan, Editor; Chris Sherman, Associate Editor Jupitermedia is publisher of the internet.com and EarthWeb.com networks. Copyright 2004 Jupitermedia Corporation All Rights Reserved. Legal Notices , \\xa0 Licensing , Reprints , & Permissions , \\xa0 Privacy Policy . \\n\\t\\n\\n-->\\n\\n\\n\\n\\nAre You Wanted by the Recording Industry?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMembers Area With Exclusive Content\\n\\n\\n\\n\\nAlready a member?\\n\\n\\n\\nLearn about the benefits:\\n                     \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n>> Departments\\n\\n\\n\\n\\nSearch Engine Submission Tips\\n\\n\\n\\n\\n\\nWeb Searching Tips\\n\\n\\n\\n\\n\\nSearch Engine Listings\\n\\n\\n\\n\\n\\nReviews, Ratings & Tests\\n\\n\\n\\n\\n\\nSearch Engine Resources\\n\\n\\n\\n\\n\\nSearchDay\\n\\n\\n\\n\\n\\nSearch Engine Report\\n\\n\\n\\n\\n\\nMembers Area\\n\\n\\n\\n\\n\\n>> Site Info\\n\\n\\n\\n\\nAdvertising Info\\n\\n\\n\\n\\n\\nAbout The Site\\n\\n\\n\\n\\n\\nSite Map\\n\\n\\n\\n\\n\\n\\n>> Search\\n\\n\\n\\n\\n\\n\\n\\n\\nMembers Area\\n\\nFree Area\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch Engine Watchis part of\\nThe Internet & IT Network\\n\\n\\n\\n\\ninternet.commerce\\n\\n\\n\\n\\nSearch Optimization\\n\\n\\nFree Virus Scan\\n\\nPromote Your Website\\n\\nTech Magazines - FREE\\n\\nReference\\nLibrary\\n\\n\\n\\nDigital Camera Deals\\n\\n\\nBest Fruit Baskets\\n\\n\\nCompare Prices & Shop\\n\\nSend a Press Release\\n\\n\\nPayment Solutions\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ninternet.com\\n\\n\\n\\n\\nDeveloper\\nDevX\\nDownloads\\nEarthWeb\\nGraphics\\nInteractive Marketing\\nInternational\\nInternet Lists\\nInternet News\\nInternet Resources\\nIT\\nLinux/Open Source\\nSmall Business \\nWindows Technology\\nWireless Internet\\nxSP Resources \\n\\nSearch internet.com\\nAdvertise\\nCorporate Info\\nNewsletters\\nTech Jobs\\nE-mail Offers\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                        More resources from internet.com:\\nAd Resource\\nArtToday.com\\nChannelSeven\\nClickZ\\nCyberAtlas\\nAdvertising Report\\nJupiterdirect\\nInternetPRGuide\\nNewMedia\\nRefer-it\\nSEMList\\nSearchEngineWatch\\nTurboAds\\nWirelessAdWatch\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch Engine Watch\\'s conference on search engine marketing comes to:\\n\\n\\x95 March 1 - 4, 2004 New York, NY\\n\\x95 April 20 - 21, 2004 Tokyo, Japan\\n\\x95 May 11 - 12, 2004 Toronto, Canada\\n\\x95 June 2 - 3, 2004 London, England\\n\\x95 August 2 - 5, 2004 San Jose, CA\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n>> Search Engine Watch Marketplace\\n\\n\\n\\n\\nSign-up for Mamma\\'s non-bidded PPC Program\\n\\n\\n\\n\\nThe Best SEO Software: Free Download!\\n\\n\\n\\n\\n\\nDrive targeted traffic to your web site\\n\\n\\n\\n\\nWant More Customers? Click Here!\\n \\n\\n\\n\\n\\nGUARANTEED inclusion- all major engines!\\n \\n\\n\\nReach nearly 70% of all search traffic!\\n \\n\\n\\n\\n\\nSearch Engine Optimization Tools and Services\\n\\n\\n\\n\\nSearch Engine Marketing Services Free Report\\n\\n\\n\\n\\n\\n\\nUnleash the Revenue Potential of Your Website\\n\\n\\n\\n\\nFind quality traffic and sales here!\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n>> Search Engine Newsletters\\n\\n\\n\\nFREE!\\nOver 150,000 readers depend on our free newsletters to keep up\\nwith search engines. To join them, enter your email below:\\n\\n\\n\\n\\nDaily\\n\\t\\t\\t\\t\\t\\t\\t\\tMonthly\\nLearn more about the newsletters\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n>> \\n\\n\\n\\n\\nOnline Search Industry Research\\nGet deep analysis and actionable advice from expert analysts\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n>> Search Engine Research Reports\\n\\n\\n\\nSearch Engine Watch publisher Jupitermedia offers research reports and briefing papers on various topics about search engines.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAre You Wanted by the Recording Industry?\\nBy  Chris Sherman, Associate EditorAugust 11, 2003\\n\\n\\n\\n\\n\\n\\nConcerned that information about your file-sharing user name may have been subpoenaed by the Recording Industry Association of America? Check this database to see if you\\'re a potential target.\\nEver since Napster made it easy for music lovers to share files, the Recording Industry Association of America (RIAA) has been on a crusade to put an end to what they allege is illegal file sharing.  The RIAA successfully sued Napster, resulting in the demise of the popular service.\\nThe battle has escalated this year, with the RIAA shifting its targets from companies to individual users.  On June 25, 2003, the RIAA announced that it will begin suing users of peer-to-peer (P2P) file-sharing systems beginning at the end of August. \\nTargets of RIAA actions are users of services like Grokster, Morpheus, KaZaA, Aimster, Gnutella, and others.  According to the RIAA, it plans to selectively target users who upload or share \"substantial\" amounts of copyrighted music. \\nHow does the RIAA identify those users?  By using software that scans users\\' publicly available P2P directories, and then identifies the ISP of each user. Then, under the provisions of the controversial Digital Millennium Copyright Act (DMCA), the RIAA subpoenas ISPs for each user\\'s name, address, and other personal information.  The RIAA will then sue those individuals. \\nThe Electronic Frontier Foundation (EFF) is helping users deal with the draconian tactics of the music industry cartel.  It has created a Subpoena Database that lets you check the user names and IP addresses used on a file sharing service against a database of those user names specified in hundreds of subpoenas the RIAA issued to Internet Service Providers (ISPs). \\n\"The recording industry continues its futile crusade to sue thousands of the over 60 million people who use file sharing software in the U.S.,\" said EFF Senior Intellectual Property Attorney Fred von Lohmann. \"We hope that EFF\\'s subpoena database will give people some peace of mind and the information they need to challenge these subpoenas and protect their privacy.\" \\n\"EFF is also documenting the scope of privacy invasions committed by the RIAA,\" explained EFF Staff Attorney Jason Schultz. \"EFF\\'s subpoena database will help document the damage done to innocent people misidentified as copyright infringers in the RIAA\\'s overzealous campaign.\" \\nIf you\\'re a user of a P2P system, it\\'s important to realize that these systems are legal, and using them is not a crime, despite the RIAA\\'s heavy-handed tactics.  If you are a P2P user, take a look at How Not to Get Sued by the RIAA for File Sharing.  \\nIf you feel you\\'ve been wrongly targeted by the RIAA, visit the Subpoena Defense Alliance, a joint project of the Electronic Frontier Foundation, the US Internet Industry Association and other organizations that began in April of 2003.  Its purpose is to assist consumers and Internet Service Providers who have been served subpoenas seeking the identity of customers who use the Internet for private communications.\\n\\nWhat\\'s Your IP Address?\\n\\nAn IP address is a unique identifier used by your computer to let it communicate with other computers on the Internet.  It\\'s similar to a telephone number.  \\n\\nYour browser announces your computer\\'s IP address to every web site it visits, as does file sharing software.  This is how the recording industry is developing its set of targets.\\n\\nYou can find out what your own computer\\'s IP address is by visiting a browser header check utility.  This shows all of the information your browser reveals to sites you visit.\\n\\nNote: If you\\'re on a network, behind a firewall, or use a large ISP, your IP address may be temporary, assigned only for your current online session.\\n\\nSearch Headlines\\n\\nNOTE: Article links often change. In case of a bad link, use the publication\\'s search facility, which most have, and search for the headline.\\n\\n\\n\\n\\n\\nISPs unite against RIAA attacks...\\nNetimperative  \\n \\xa0 Aug 11 2003 12:59PM GMT\\n\\n\\n\\n\\n\\nCan\\'t spot a spoof? Meet Google...\\nGuardian Unlimited  \\n \\xa0 Aug 11 2003 11:31AM GMT\\n\\n\\n\\n\\n\\nYahoo chief executive gains $11 million...\\nSiliconValley.com  \\n \\xa0 Aug 11 2003 10:32AM GMT\\n\\n\\n\\n\\n\\nGoogle pulls AdSense feature after backlash...\\nNetimperative  \\n \\xa0 Aug 11 2003 9:34AM GMT\\n\\n\\n\\n\\n\\nAsk Jeeves Drops Butler in New Ad Campaign...\\nSiliconValley.com  \\n \\xa0 Aug 11 2003 7:33AM GMT\\n\\n\\n\\n\\n\\nSearch Engine For JavaScript...\\nResearch Buzz  \\n \\xa0 Aug 11 2003 6:21AM GMT\\n\\n\\n\\n\\n\\nSaudis Restrict Internet To Stop Al Qaida...\\nMiddle East Newsline  \\n \\xa0 Aug 11 2003 3:51AM GMT\\n\\n\\n\\n\\n\\nGoogle ads a threat to eBay trademark?...\\nZDNet  \\n \\xa0 Aug 10 2003 11:40PM GMT\\n\\n\\n\\n\\n\\nDMA-Delayed Best Practices Skirt Spam, Permission Issues...\\nBoston.Internet.com  \\n \\xa0 Aug 9 2003 3:54AM GMT\\n\\n\\n\\n\\n\\nGoogle girds for big challenge as its influence widens...\\nSingapore Business Times  \\n \\xa0 Aug 8 2003 11:40PM GMT\\n\\n\\n\\n\\n\\nTV: the Next Portal War...\\nInternet News  \\n \\xa0 Aug 8 2003 6:03PM GMT\\n\\n\\n\\n\\n\\nDo Not Spam list and filtering firms join hands...\\nThe Register  \\n \\xa0 Aug 8 2003 4:02PM GMT\\n\\n\\n\\n\\n\\nNew top-level domain goes .pro...\\nZDNet  \\n \\xa0 Aug 8 2003 11:09AM GMT\\n\\n\\n\\n\\n\\nReed Elsevier, the unsung star of the internet revolution...\\nIndependent  \\n \\xa0 Aug 8 2003 2:21AM GMT\\n\\n\\n\\n\\n\\n\\n\\nSearchDay Archives\\n\\n\\n\\n\\n\\nSearch Engine Watch \\nwww.searchenginewatch.com\\nDanny Sullivan, Editor; Chris Sherman, Associate Editor\\n\\n\\n\\nJupitermedia is publisher of the internet.com and EarthWeb.com networks.\\n\\nCopyright 2004 Jupitermedia Corporation All Rights Reserved.\\nLegal Notices, \\xa0Licensing, Reprints, & Permissions, \\xa0Privacy Policy.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " ' Using and Porting the GNU Compiler Collection (GCC): Objective C [ < ] [ > ] [ << ] [ Up ] [ >> ] [ Top ] [ Contents ] [ Index ] [ ? ] 7. GNU Objective-C runtime features This document is meant to describe some of the GNU Objective-C runtime features. It is not intended to teach you Objective-C, there are several resources on the Internet that present the language. Questions and comments about this document to Ovidiu Predescu ovidiu@cup.hp.com . 7.1 +load : Executing code before main 7.2 Type encoding 7.3 Garbage Collection 7.4 Constant string objects 7.5 compatibility_alias [ < ] [ > ] [ << ] [ Up ] [ >> ] [ Top ] [ Contents ] [ Index ] [ ? ] 7.1 +load : Executing code before main The GNU Objective-C runtime provides a way that allows you to execute code before the execution of the program enters the main function. The code is executed on a per-class and a per-category basis, through a special class method +load . This facility is very useful if you want to initialize global variables which can be accessed by the program directly, without sending a message to the class first. The usual way to initialize global variables, in the +initialize method, might not be useful because +initialize is only called when the first message is sent to a class object, which in some cases could be too late. Suppose for example you have a FileStream class that declares Stdin , Stdout and Stderr as global variables, like below: FileStream *Stdin = nil; FileStream *Stdout = nil; FileStream *Stderr = nil; @implementation FileStream + (void)initialize { Stdin = [[FileStream new] initWithFd:0]; Stdout = [[FileStream new] initWithFd:1]; Stderr = [[FileStream new] initWithFd:2]; } /* Other methods here */ @end In this example, the initialization of Stdin , Stdout and Stderr in +initialize occurs too late. The programmer can send a message to one of these objects before the variables are actually initialized, thus sending messages to the nil object. The +initialize method which actually initializes the global variables is not invoked until the first message is sent to the class object. The solution would require these variables to be initialized just before entering main . The correct solution of the above problem is to use the +load method instead of +initialize : @implementation FileStream + (void)load { Stdin = [[FileStream new] initWithFd:0]; Stdout = [[FileStream new] initWithFd:1]; Stderr = [[FileStream new] initWithFd:2]; } /* Other methods here */ @end The +load is a method that is not overridden by categories. If a class and a category of it both implement +load , both methods are invoked. This allows some additional initializations to be performed in a category. This mechanism is not intended to be a replacement for +initialize . You should be aware of its limitations when you decide to use it instead of +initialize . 7.1.1 What you can and what you cannot do in +load [ < ] [ > ] [ << ] [ Up ] [ >> ] [ Top ] [ Contents ] [ Index ] [ ? ] 7.1.1 What you can and what you cannot do in +load The +load implementation in the GNU runtime guarantees you the following things: you can write whatever C code you like; you can send messages to Objective-C constant strings ( @\"this is a constant string\" ); you can allocate and send messages to objects whose class is implemented in the same file; the +load implementation of all super classes of a class are executed before the +load of that class is executed; the +load implementation of a class is executed before the +load implementation of any category. In particular, the following things, even if they can work in a particular case, are not guaranteed: allocation of or sending messages to arbitrary objects; allocation of or sending messages to objects whose classes have a category implemented in the same file; You should make no assumptions about receiving +load in sibling classes when you write +load of a class. The order in which sibling classes receive +load is not guaranteed. The order in which +load and +initialize are called could be problematic if this matters. If you don\\'t allocate objects inside +load , it is guaranteed that +load is called before +initialize . If you create an object inside +load the +initialize method of object\\'s class is invoked even if +load was not invoked. Note if you explicitly call +load on a class, +initialize will be called first. To avoid possible problems try to implement only one of these methods. The +load method is also invoked when a bundle is dynamically loaded into your running program. This happens automatically without any intervening operation from you. When you write bundles and you need to write +load you can safely create and send messages to objects whose classes already exist in the running program. The same restrictions as above apply to classes defined in bundle. [ < ] [ > ] [ << ] [ Up ] [ >> ] [ Top ] [ Contents ] [ Index ] [ ? ] 7.2 Type encoding The Objective-C compiler generates type encodings for all the types. These type encodings are used at runtime to find out information about selectors and methods and about objects and classes. The types are encoded in the following way: char c unsigned char C short s unsigned short S int i unsigned int I long l unsigned long L long long q unsigned long long Q float f double d void v id @ Class # SEL : char* * unknown type ? bit-fields b followed by the starting position of the bit-field, the type of the bit-field and the size of the bit-field (the bit-fields encoding was changed from the NeXT\\'s compiler encoding, see below) The encoding of bit-fields has changed to allow bit-fields to be properly handled by the runtime functions that compute sizes and alignments of types that contain bit-fields. The previous encoding contained only the size of the bit-field. Using only this information it is not possible to reliably compute the size occupied by the bit-field. This is very important in the presence of the Boehm\\'s garbage collector because the objects are allocated using the typed memory facility available in this collector. The typed memory allocation requires information about where the pointers are located inside the object. The position in the bit-field is the position, counting in bits, of the bit closest to the beginning of the structure. The non-atomic types are encoded as follows: pointers `^\\' followed by the pointed type. arrays `[\\' followed by the number of elements in the array followed by the type of the elements followed by `]\\' structures `{\\' followed by the name of the structure (or `?\\' if the structure is unnamed), the `=\\' sign, the type of the members and by `}\\' unions `(\\' followed by the name of the structure (or `?\\' if the union is unnamed), the `=\\' sign, the type of the members followed by `)\\' Here are some types and their encodings, as they are generated by the compiler on an i386 machine: Objective-C type Compiler encoding int a[10]; [10i] struct { int i; float f[3]; int a:3; int b:2; char c; } {?=i[3f]b128i3b131i2c} In addition to the types the compiler also encodes the type specifiers. The table below describes the encoding of the current Objective-C type specifiers: Specifier Encoding const r in n inout N out o bycopy O oneway V The type specifiers are encoded just before the type. Unlike types however, the type specifiers are only encoded when they appear in method argument types. [ < ] [ > ] [ << ] [ Up ] [ >> ] [ Top ] [ Contents ] [ Index ] [ ? ] 7.3 Garbage Collection Support for a new memory management policy has been added by using a powerful conservative garbage collector, known as the Boehm-Demers-Weiser conservative garbage collector. It is available from http://www.hpl.hp.com/personal/Hans_Boehm/gc/ . To enable the support for it you have to configure the compiler using an additional argument, `--enable-objc-gc\\' . You need to have garbage collector installed before building the compiler. This will build an additional runtime library which has several enhancements to support the garbage collector. The new library has a new name, `libobjc_gc.a\\' to not conflict with the non-garbage-collected library. When the garbage collector is used, the objects are allocated using the so-called typed memory allocation mechanism available in the Boehm-Demers-Weiser collector. This mode requires precise information on where pointers are located inside objects. This information is computed once per class, immediately after the class has been initialized. There is a new runtime function class_ivar_set_gcinvisible() which can be used to declare a so-called weak pointer reference. Such a pointer is basically hidden for the garbage collector; this can be useful in certain situations, especially when you want to keep track of the allocated objects, yet allow them to be collected. This kind of pointers can only be members of objects, you cannot declare a global pointer as a weak reference. Every type which is a pointer type can be declared a weak pointer, including id , Class and SEL . Here is an example of how to use this feature. Suppose you want to implement a class whose instances hold a weak pointer reference; the following class does this: @interface WeakPointer : Object { const void* weakPointer; } - initWithPointer:(const void*)p; - (const void*)weakPointer; @end @implementation WeakPointer + (void)initialize { class_ivar_set_gcinvisible (self, \"weakPointer\", YES); } - initWithPointer:(const void*)p { weakPointer = p; return self; } - (const void*)weakPointer { return weakPointer; } @end Weak pointers are supported through a new type character specifier represented by the `!\\' character. The class_ivar_set_gcinvisible() function adds or removes this specifier to the string type description of the instance variable named as argument. [ < ] [ > ] [ << ] [ Up ] [ >> ] [ Top ] [ Contents ] [ Index ] [ ? ] 7.4 Constant string objects GNU Objective-C provides constant string objects that are generated directly by the compiler. You declare a constant string object by prefixing a C constant string with the character `@\\' : id myString = @\"this is a constant string object\"; The constant string objects are usually instances of the NXConstantString class which is provided by the GNU Objective-C runtime. To get the definition of this class you must include the `objc/NXConstStr.h\\' header file. User defined libraries may want to implement their own constant string class. To be able to support them, the GNU Objective-C compiler provides a new command line options `-fconstant-string-class= class-name \\' . The provided class should adhere to a strict structure, the same as NXConstantString \\'s structure: @interface NXConstantString : Object { char *c_string; unsigned int len; } @end User class libraries may choose to inherit the customized constant string class from a different class than Object . There is no requirement in the methods the constant string class has to implement. When a file is compiled with the `-fconstant-string-class\\' option, all the constant string objects will be instances of the class specified as argument to this option. It is possible to have multiple compilation units referring to different constant string classes, neither the compiler nor the linker impose any restrictions in doing this. [ < ] [ > ] [ << ] [ Up ] [ >> ] [ Top ] [ Contents ] [ Index ] [ ? ] 7.5 compatibility_alias This is a feature of the Objective-C compiler rather than of the runtime, anyway since it is documented nowhere and its existence was forgotten, we are documenting it here. The keyword @compatibility_alias allows you to define a class name as equivalent to another class name. For example: @compatibility_alias WOApplication GSWApplication; tells the compiler that each time it encounters WOApplication as a class name, it should replace it with GSWApplication (that is, WOApplication is just an alias for GSWApplication ). There are some constraints on how this can be used--- WOApplication (the alias) must not be an existing class; GSWApplication (the real class) must be an existing class. [ << ] [ >> ] [ Top ] [ Contents ] [ Index ] [ ? ] This document was generated by GCC Administrator on October, 25 2001 using texi2html \\n\\t\\n\\n-->\\n\\n\\n\\n\\n\\nUsing and Porting the GNU Compiler Collection (GCC): Objective C\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[ < ]\\n[ > ]\\n \\xa0 [ << ]\\n[ Up ]\\n[ >> ]\\n \\xa0  \\xa0  \\xa0  \\xa0 [Top]\\n[Contents]\\n[Index]\\n[ ? ]\\n\\n 7. GNU Objective-C runtime features \\n\\n\\n\\nThis document is meant to describe some of the GNU Objective-C runtime\\nfeatures.  It is not intended to teach you Objective-C, there are several\\nresources on the Internet that present the language.  Questions and\\ncomments about this document to Ovidiu Predescu\\novidiu@cup.hp.com.\\n\\n\\n7.1 +load: Executing code before main\\xa0\\xa0\\n7.2 Type encoding\\xa0\\xa0\\n7.3 Garbage Collection\\xa0\\xa0\\n7.4 Constant string objects\\xa0\\xa0\\n7.5 compatibility_alias\\xa0\\xa0\\n\\n\\n\\n\\n\\n\\n[ < ]\\n[ > ]\\n \\xa0 [ << ]\\n[ Up ]\\n[ >> ]\\n \\xa0  \\xa0  \\xa0  \\xa0 [Top]\\n[Contents]\\n[Index]\\n[ ? ]\\n\\n 7.1 +load: Executing code before main \\n\\n\\n\\nThe GNU Objective-C runtime provides a way that allows you to execute\\ncode before the execution of the program enters the main\\nfunction.  The code is executed on a per-class and a per-category basis,\\nthrough a special class method +load.\\n\\n\\nThis facility is very useful if you want to initialize global variables\\nwhich can be accessed by the program directly, without sending a message\\nto the class first.  The usual way to initialize global variables, in the\\n+initialize method, might not be useful because\\n+initialize is only called when the first message is sent to a\\nclass object, which in some cases could be too late.\\n\\n\\nSuppose for example you have a FileStream class that declares\\nStdin, Stdout and Stderr as global variables, like\\nbelow:\\n\\n\\xa0\\nFileStream *Stdin = nil;\\nFileStream *Stdout = nil;\\nFileStream *Stderr = nil;\\n\\n@implementation FileStream\\n\\n+ (void)initialize\\n{\\n    Stdin = [[FileStream new] initWithFd:0];\\n    Stdout = [[FileStream new] initWithFd:1];\\n    Stderr = [[FileStream new] initWithFd:2];\\n}\\n\\n/* Other methods here */\\n@end\\n\\n\\n\\nIn this example, the initialization of Stdin, Stdout and\\nStderr in +initialize occurs too late.  The programmer can\\nsend a message to one of these objects before the variables are actually\\ninitialized, thus sending messages to the nil object.  The\\n+initialize method which actually initializes the global\\nvariables is not invoked until the first message is sent to the class\\nobject.  The solution would require these variables to be initialized\\njust before entering main.\\n\\n\\nThe correct solution of the above problem is to use the +load\\nmethod instead of +initialize:\\n\\n\\xa0\\n@implementation FileStream\\n\\n+ (void)load\\n{\\n    Stdin = [[FileStream new] initWithFd:0];\\n    Stdout = [[FileStream new] initWithFd:1];\\n    Stderr = [[FileStream new] initWithFd:2];\\n}\\n\\n/* Other methods here */\\n@end\\n\\n\\n\\nThe +load is a method that is not overridden by categories.  If a\\nclass and a category of it both implement +load, both methods are\\ninvoked.  This allows some additional initializations to be performed in\\na category.\\n\\n\\nThis mechanism is not intended to be a replacement for +initialize.\\nYou should be aware of its limitations when you decide to use it\\ninstead of +initialize.\\n\\n\\n7.1.1 What you can and what you cannot do in +load\\xa0\\xa0\\n\\n\\n\\n\\n\\n\\n[ < ]\\n[ > ]\\n \\xa0 [ << ]\\n[ Up ]\\n[ >> ]\\n \\xa0  \\xa0  \\xa0  \\xa0 [Top]\\n[Contents]\\n[Index]\\n[ ? ]\\n\\n 7.1.1 What you can and what you cannot do in +load \\n\\n\\n\\nThe +load implementation in the GNU runtime guarantees you the following\\nthings:\\n\\n\\n\\nyou can write whatever C code you like;\\n\\n\\nyou can send messages to Objective-C constant strings (@\"this is a\\nconstant string\");\\n\\n\\nyou can allocate and send messages to objects whose class is implemented\\nin the same file;\\n\\n\\nthe +load implementation of all super classes of a class are executed before the +load of that class is executed;\\n\\n\\nthe +load implementation of a class is executed before the\\n+load implementation of any category.\\n\\n\\n\\n\\nIn particular, the following things, even if they can work in a\\nparticular case, are not guaranteed:\\n\\n\\n\\nallocation of or sending messages to arbitrary objects;\\n\\n\\nallocation of or sending messages to objects whose classes have a\\ncategory implemented in the same file;\\n\\n\\n\\n\\nYou should make no assumptions about receiving +load in sibling\\nclasses when you write +load of a class.  The order in which\\nsibling classes receive +load is not guaranteed.\\n\\n\\nThe order in which +load and +initialize are called could\\nbe problematic if this matters.  If you don\\'t allocate objects inside\\n+load, it is guaranteed that +load is called before\\n+initialize.  If you create an object inside +load the\\n+initialize method of object\\'s class is invoked even if\\n+load was not invoked.  Note if you explicitly call +load\\non a class, +initialize will be called first.  To avoid possible\\nproblems try to implement only one of these methods.\\n\\n\\nThe +load method is also invoked when a bundle is dynamically\\nloaded into your running program.  This happens automatically without any\\nintervening operation from you.  When you write bundles and you need to\\nwrite +load you can safely create and send messages to objects whose\\nclasses already exist in the running program.  The same restrictions as\\nabove apply to classes defined in bundle.\\n\\n\\n\\n\\n\\n[ < ]\\n[ > ]\\n \\xa0 [ << ]\\n[ Up ]\\n[ >> ]\\n \\xa0  \\xa0  \\xa0  \\xa0 [Top]\\n[Contents]\\n[Index]\\n[ ? ]\\n\\n 7.2 Type encoding \\n\\n\\n\\nThe Objective-C compiler generates type encodings for all the\\ntypes.  These type encodings are used at runtime to find out information\\nabout selectors and methods and about objects and classes.\\n\\n\\nThe types are encoded in the following way:\\n\\n\\nchar\\n c\\n\\nunsigned char\\n C\\n\\nshort\\n s\\n\\nunsigned short\\n S\\n\\nint\\n i\\n\\nunsigned int\\n I\\n\\nlong\\n l\\n\\nunsigned long\\n L\\n\\nlong long\\n q\\n\\nunsigned long long\\n Q\\n\\nfloat\\n f\\n\\ndouble\\n d\\n\\nvoid\\n v\\n\\nid\\n @\\n\\nClass\\n #\\n\\nSEL\\n :\\n\\nchar*\\n *\\n\\nunknown type\\n ?\\n\\nbit-fields\\n b followed by the starting position of the bit-field, the type of the bit-field and the size of the bit-field (the bit-fields encoding was changed from the NeXT\\'s compiler encoding, see below)\\n\\n\\n\\nThe encoding of bit-fields has changed to allow bit-fields to be properly\\nhandled by the runtime functions that compute sizes and alignments of\\ntypes that contain bit-fields.  The previous encoding contained only the\\nsize of the bit-field.  Using only this information it is not possible to\\nreliably compute the size occupied by the bit-field.  This is very\\nimportant in the presence of the Boehm\\'s garbage collector because the\\nobjects are allocated using the typed memory facility available in this\\ncollector.  The typed memory allocation requires information about where\\nthe pointers are located inside the object.\\n\\n\\nThe position in the bit-field is the position, counting in bits, of the\\nbit closest to the beginning of the structure.\\n\\n\\nThe non-atomic types are encoded as follows:\\n\\n\\npointers\\n `^\\' followed by the pointed type.\\n\\narrays\\n `[\\' followed by the number of elements in the array followed by the type of the elements followed by `]\\'\\n\\nstructures\\n `{\\' followed by the name of the structure (or `?\\' if the structure is unnamed), the `=\\' sign, the type of the members and by `}\\'\\n\\nunions\\n `(\\' followed by the name of the structure (or `?\\' if the union is unnamed), the `=\\' sign, the type of the members followed by `)\\'\\n\\n\\n\\nHere are some types and their encodings, as they are generated by the\\ncompiler on an i386 machine:\\n\\n\\nObjective-C type\\n Compiler encoding\\n\\n\\n\\xa0int a[10];\\n [10i]\\n\\n\\n\\xa0struct {\\n  int i;\\n  float f[3];\\n  int a:3;\\n  int b:2;\\n  char c;\\n}\\n {?=i[3f]b128i3b131i2c}\\n\\n\\n\\nIn addition to the types the compiler also encodes the type\\nspecifiers.  The table below describes the encoding of the current\\nObjective-C type specifiers:\\n\\n\\nSpecifier\\n Encoding\\n\\nconst\\n r\\n\\nin\\n n\\n\\ninout\\n N\\n\\nout\\n o\\n\\nbycopy\\n O\\n\\noneway\\n V\\n\\n\\n\\nThe type specifiers are encoded just before the type.  Unlike types\\nhowever, the type specifiers are only encoded when they appear in method\\nargument types.\\n\\n\\n\\n\\n\\n[ < ]\\n[ > ]\\n \\xa0 [ << ]\\n[ Up ]\\n[ >> ]\\n \\xa0  \\xa0  \\xa0  \\xa0 [Top]\\n[Contents]\\n[Index]\\n[ ? ]\\n\\n 7.3 Garbage Collection \\n\\n\\n\\nSupport for a new memory management policy has been added by using a\\npowerful conservative garbage collector, known as the\\nBoehm-Demers-Weiser conservative garbage collector.  It is available from\\nhttp://www.hpl.hp.com/personal/Hans_Boehm/gc/.\\n\\n\\nTo enable the support for it you have to configure the compiler using an\\nadditional argument, `--enable-objc-gc\\'.  You need to have\\ngarbage collector installed before building the compiler.  This will\\nbuild an additional runtime library which has several enhancements to\\nsupport the garbage collector.  The new library has a new name,\\n`libobjc_gc.a\\' to not conflict with the non-garbage-collected\\nlibrary.\\n\\n\\nWhen the garbage collector is used, the objects are allocated using the\\nso-called typed memory allocation mechanism available in the\\nBoehm-Demers-Weiser collector.  This mode requires precise information on\\nwhere pointers are located inside objects.  This information is computed\\nonce per class, immediately after the class has been initialized.\\n\\n\\nThere is a new runtime function class_ivar_set_gcinvisible()\\nwhich can be used to declare a so-called weak pointer\\nreference.  Such a pointer is basically hidden for the garbage collector;\\nthis can be useful in certain situations, especially when you want to\\nkeep track of the allocated objects, yet allow them to be\\ncollected.  This kind of pointers can only be members of objects, you\\ncannot declare a global pointer as a weak reference.  Every type which is\\na pointer type can be declared a weak pointer, including id,\\nClass and SEL.\\n\\n\\nHere is an example of how to use this feature.  Suppose you want to\\nimplement a class whose instances hold a weak pointer reference; the\\nfollowing class does this:\\n\\n\\xa0\\n@interface WeakPointer : Object\\n{\\n    const void* weakPointer;\\n}\\n\\n- initWithPointer:(const void*)p;\\n- (const void*)weakPointer;\\n@end\\n\\n\\n@implementation WeakPointer\\n\\n+ (void)initialize\\n{\\n  class_ivar_set_gcinvisible (self, \"weakPointer\", YES);\\n}\\n\\n- initWithPointer:(const void*)p\\n{\\n  weakPointer = p;\\n  return self;\\n}\\n\\n- (const void*)weakPointer\\n{\\n  return weakPointer;\\n}\\n\\n@end\\n\\n\\n\\nWeak pointers are supported through a new type character specifier\\nrepresented by the `!\\' character.  The\\nclass_ivar_set_gcinvisible() function adds or removes this\\nspecifier to the string type description of the instance variable named\\nas argument.\\n\\n\\n\\n\\n\\n[ < ]\\n[ > ]\\n \\xa0 [ << ]\\n[ Up ]\\n[ >> ]\\n \\xa0  \\xa0  \\xa0  \\xa0 [Top]\\n[Contents]\\n[Index]\\n[ ? ]\\n\\n 7.4 Constant string objects \\n\\n\\n\\nGNU Objective-C provides constant string objects that are generated\\ndirectly by the compiler.  You declare a constant string object by\\nprefixing a C constant string with the character `@\\':\\n\\n\\xa0  id myString = @\"this is a constant string object\";\\n\\n\\nThe constant string objects are usually instances of the\\nNXConstantString class which is provided by the GNU Objective-C\\nruntime.  To get the definition of this class you must include the\\n`objc/NXConstStr.h\\' header file.\\n\\n\\nUser defined libraries may want to implement their own constant string\\nclass.  To be able to support them, the GNU Objective-C compiler provides\\na new command line options `-fconstant-string-class=class-name\\'.\\nThe provided class should adhere to a strict structure, the same\\nas NXConstantString\\'s structure:\\n\\n\\xa0\\n@interface NXConstantString : Object\\n{\\n  char *c_string;\\n  unsigned int len;\\n}\\n@end\\n\\n\\n\\nUser class libraries may choose to inherit the customized constant\\nstring class from a different class than Object.  There is no\\nrequirement in the methods the constant string class has to implement.\\n\\n\\nWhen a file is compiled with the `-fconstant-string-class\\' option,\\nall the constant string objects will be instances of the class specified\\nas argument to this option.  It is possible to have multiple compilation\\nunits referring to different constant string classes, neither the\\ncompiler nor the linker impose any restrictions in doing this.\\n\\n\\n\\n\\n\\n[ < ]\\n[ > ]\\n \\xa0 [ << ]\\n[ Up ]\\n[ >> ]\\n \\xa0  \\xa0  \\xa0  \\xa0 [Top]\\n[Contents]\\n[Index]\\n[ ? ]\\n\\n 7.5 compatibility_alias \\n\\n\\n\\nThis is a feature of the Objective-C compiler rather than of the\\nruntime, anyway since it is documented nowhere and its existence was\\nforgotten, we are documenting it here.\\n\\n\\nThe keyword @compatibility_alias allows you to define a class name\\nas equivalent to another class name.  For example:\\n\\n\\xa0@compatibility_alias WOApplication GSWApplication;\\n\\n\\ntells the compiler that each time it encounters WOApplication as\\na class name, it should replace it with GSWApplication (that is,\\nWOApplication is just an alias for GSWApplication).\\n\\n\\nThere are some constraints on how this can be used---\\n\\n\\nWOApplication (the alias) must not be an existing class;\\n\\nGSWApplication (the real class) must be an existing class.\\n\\n\\n\\n\\n\\n\\n[ << ]\\n[ >> ]\\n \\xa0  \\xa0  \\xa0  \\xa0  \\xa0 [Top]\\n[Contents]\\n[Index]\\n[ ? ]\\n\\n\\n\\nThis document was generated\\nby GCC Administrator on October, 25  2001\\nusing texi2html\\n\\n\\n',\n",
       " '\\n\\n\\nSpelling tolerant textsearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\xa0\\xa0\\n  gibney.de >> fuzzy textsearch  \\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSpelling tolerant text search\\n\\nOne of the basic issues in user-software interaction is to deal with free text input. \\nPeople more and more expect to be corrected if they do typos. \\nMost of the big players like Amazon and Google who deal with user-input have already integrated quite good fault-tolerance into their systems.\\n\\nRecently I started writing an add-on for gnod.net that is able to search gnod\\'s ten thousands of artist names for\\nsimilar occurrences of a chosen name. It\\'s still beta, but already produces results of quality similar to the search implemented by Amazon.\\n\\nI\\'m not displaying the link to the test-page here. The algorithm is not yet completely optimized for speed, and \\n(dependant on the current search-depth settings) sometimes still needs too much processing resources, especially when\\nsearching for terms like \"a\" or something. Guess you can imagine why :o)\\n\\nIn case you want to try it, please contact me, and I will mail you the link.\\n\\nUpdate: since I did this code in asp, the textsearch live-demo is currently not available because I ported Gnod to php lately.\\n\\nHowever, here are some \"screenshots\" (well copy & paste actually):\\n\\nResults for elfis pressli:\\nno exact match\\nsimilar names:\\n[x]    Elvis Presley\\n\\nResults for madona:\\nfound: Madonna\\n\\nResults for john sebustin buch:\\nno exact match\\nno similar names found.\\n\\nResults for john sebustin bach:\\nno exact match\\nsimilar names:\\n[x]    J. S. Bach\\n\\nResults for rem:\\nfound: R.E.M.\\n\\nResults for tschaikowski:\\nno exact match\\nsimilar names:\\n[x]    Tchaikowsky\\n\\nResults for tscheikofsky:\\nno exact match\\nno similar names found.\\n\\nResults for depesche mote:\\nno exact match\\nsimilar names:\\n[x]    Depeche Mode\\n\\nResults for irfana:\\nno exact match\\nsimilar names:\\n[x]    Nirvana\\n[x]    Bananarama\\n[x]    Arcana\\n[x]    Garmana\\n[x]    Rosana\\n\\nResults for peter and the test tube babies:\\nfound: Peter & The Test Tube Babies\\n\\nResults for peter\\'s testtube babies:\\nno exact match\\nsimilar names:\\n[x]    Peter & The Test Tube Babies\\n[x]    Backyard Babies\\n\\nResults for mosart:\\nno exact match\\nsimilar names:\\n[x]    Mozart\\n\\nResults for mohsard:\\nno exact match\\nno similar names found.\\n\\n\\n\\n\\nSearch\\xa0the\\xa0Web.Type it and go!\\n\\n\\n',\n",
       " '\\n\\n\\n\\n\\nLogistic Regression Calculating Page\\n\\n\\n\\n\\nLogistic Regression\\n\\n\\nRevised 11/18/2003\\n\\nBackground ||| Techie-Stuff\\n  ||| Instructions\\n\\n\\nThis page performs logistic regression, in which a dichotomous (two-value)\\noutcome is predicted by one or more variables. It generates the coefficients\\nof a prediction formula (and standard errors of estimate and significance\\nlevels), and odds ratios (with their 95% confidence intervals).\\n\\n\\n\\nBackground Info (just what is\\n  logistic regression, anyway?):\\n\\n\\nOrdinary regression deals with finding a function that relates a\\ncontinuous outcome variable (dependent variable y) to one or\\nmore predictors (independent variables x1,\\nx2, etc.). Simple linear regression assumes a function\\nof the form:\\ny = c0 + c1 * x1 +\\nc2 * x2 +...\\nand finds the values of c0, c1, c2, etc.\\n(c0 is called the \"intercept\" or \"constant term\").\\n\\nLogistic regression is a variation of ordinary regression, useful\\nwhen the observed outcome is restricted to two values, which usually\\nrepresent the occurrence or non-occurrence of some outcome event, (usually\\ncoded as 1 or 0, respectively). It produces a formula that predicts the\\nprobability of the occurrence as a function of the independent variables.\\n\\nLogistic regression fits a special s-shaped curve by taking the linear regression\\n(above), which could produce any y-value between minus infinity and\\nplus infinity, and transforming it with the function:\\np = Exp(y) / ( 1 + Exp(y) )\\nwhich produces p-values between 0 (as y approaches minus infinity)\\nand 1 (as y approaches plus infinity). This now becomes a special\\nkind of non-linear regression, which is what this page performs.\\n\\nLogistic regression also produces Odds Ratios (O.R.) associated with\\neach predictor value. The odds of an event is defined as the probability\\nof the outcome event occurring divided by the probability of the event\\nnot occurring. The odds ratio for a predictor tells the relative amount\\nby which the odds of the outcome increase (O.R. greater than 1.0) or decrease\\n(O.R. less than 1.0) when the value of the predictor value is increased by\\n1.0 units.\\n\\n\\n\\n\\nTechie-stuff (for those who might\\n  be interested):\\n\\n\\nThis page contains a straightforward JavaScript implementation of\\na standard iterative method to minimize the Log Likelihood Function (LLF),\\ndefined as the sum of the logarithms of the predicted probabilities of occurrence\\nfor those cases where the event occurred and the logarithms of the predicted\\nprobabilities of non-occurrence for those cases where the event did not occur.\\n\\nMinimization is by Newton\\'s method, with a very simple elimination algorithm\\nto invert and solve the simultaneous equations. Central-limit estimates of\\nparameter standard errors are obtained from the diagonal terms of the inverse\\nmatrix. Odds Ratios and their confidence limits are obtained by exponentiating\\nthe parameters and their lower and upper confidence limits (approximated\\nby +/- 1.96 standard errors).\\n\\nNo special convergence-acceleration techniques are used. For improved precision,\\nthe independent variables are temporarily converted to \"standard scores\"\\n( value - Mean ) / StdDev. The Null Model is used as the starting\\nguess for the iterations -- all parameter coefficients are zero, and the\\nintercept is the logarithm of the ratio of the number of cases with\\ny=1 to the number with y=0. Convergence is not guaranteed,\\nbut this page should work properly with most practical problems that arise\\nin real-world situations.\\n\\nThis implementation has no predefined limits for the number of independent\\nvariables or cases. The actual limits are probably dependent on your web\\nbrowser\\'s available memory and other browser-specific restrictions.\\n\\nThe fields below are pre-loaded with a very simple example.\\n\\n\\n\\n\\nInstructions:\\n\\n\\n\\n      Enter the number of data points:\\n       (or number of lines of\\n      date, if data is summarized).\\n    \\n      Enter the number of independent variables:\\n      \\n\\n      Indicate whether each row contains just one case, or summarized\\n      data (check \\n      here if data is\\n      summarized).\\n    \\n      Type (or paste) the [x,y] data:\\n1,0\\n2,0\\n3,0\\n4,0\\n5,1\\n6,0\\n7,1\\n8,0\\n9,1\\n10,1\\n\\nUse a separate row for each data point. For each\\n      row, enter the independent variable or variables (which must be numeric),\\n      followed by the dependent variable (outcome). \\n      If the data is not summarized (that is, if each row contains the values for\\n      one individual observation), then enter a single outcome number, coded as\\n      0 (if the event did not happen) or 1 (if the event did\\n      happen).\\n      If your data is summarized (that is, if each row provides information\\n      about multiple subjects having identical predictor values), then enter the\\n      outcome as two numbers: the first is the number of subject who did not\\n      experience the event (that is, had a \"0\" outcome); the second is the number\\n      of subjects who did experience the event (that is, had a \"1\"\\n      outcome).\\n      Values should be separated by commas or tabs. You can copy data from another\\n      program, like a spreadsheet, and paste it into the window\\n      above.\\n\\nClick the\\n       button.\\n      The results will appear below:\\n\\n\\n      To print out results, copy and paste the contents of the Output window above\\n      into a word processor or text editor, then Print. For best appearance, specify\\n      a fixed-width font like Courier.\\n  \\n\\n\\n\\n  Reference: Applied Logistic Regression, by D.W. Hosmer and S. Lemeshow.\\n  1989, John Wiley & Sons, New York\\n  \\n\\n\\n  Return to the Interactive Statistics page or\\n  to the JCP Home Page\\n\\n  Send e-mail to John C. Pezzullo at\\n  johnp71@aol.com\\n\\n\\n',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Language Overview\\n\\n\\n\\n\\n\\n\\n     \\n1 Introduction\\n\\n\\n\\n Language Overview\\nDylan is written in a very regular syntax. In addition to making the language easier to read and write, the layered composition of the syntax supports a macro system that is language-aware. The macro system does not simply perform text substitution, but rather performs syntax fragment substitution. This allows the extension of the language within bounds that are safe, semantically well-defined, and in accord with the \"syntactic flavor\" of the language.\\nBindings (Dylan\\'s analog to variables) are lexically scoped and fully resolved at compile time. Binding names are not retained in running programs. The module system allows bindings to be private or shared. Names can be changed upon import to a module, so the possibility of irreconcilable name conflicts among separately developed modules is eliminated. Modules can provide multiple interfaces to the same code base, decreasing the chance of exposing a client to inappropriate interfaces.\\nFlow of control is supported through polymorphic function calls, a variety of conditional and iteration constructs, and a nonlocal transfer mechanism with protected regions. \\nAll objects are first class, including numbers, classes and functions. This means that  all objects can be used as arguments to functions, returned as values, and stored in data structures, and all are subject to introspection. All objects are typed, and type-safety is guaranteed, either through compile-time or runtime type checking. There are no facilities for deallocating objects. Objects are deallocated automatically when they can no longer be reached by a program.\\nTypes are used to categorize and specify the behavior of objects. An object may be an instance of any number of types. Classes are a particular kind of type used to define the structure and inheritance of instances. Every Dylan object is a direct instance of exactly one class, and a general instance of that class and each of its superclasses. The root of the class hierarchy (and of the type hierarchy) is a class called <object>.\\nValues associated with an instance are stored in slots of the instance.\\nClasses do not define scopes for names. Names are scoped by modules and local binding declarations.\\nFunctions are the active portions of Dylan programs. Functions accept zero or more arguments and return zero or more values. Functions are specialized to accept arguments of particular types, and will signal an error if they are called with arguments that are not instances of those types. The return values of functions are similarly type-checked.\\nA method is a basic unit of callable code. When a method is called, it creates local bindings for its arguments and executes a body in the resulting environment. A method can be called directly by a program or indirectly through a generic function that contains it.\\nA generic function contains a number of methods. When a generic function is called, it finds the methods that are applicable to the arguments, and passes control to the most specific of those methods.\\nSlots are accessed through functions. This ensures that instances present an abstract interface to their clients, which assists both in polymorphism and in program redesign.\\nSealing declarations allow the programmer to declare portions of the class hierarchy and set of functions to be invariant. This supports the enforcement of protocols, compile-time resolution of polymorphic behavior, and efficient inline slot access. Portions of a program that are not sealed can be extended at run time or by additional libraries.\\nDylan includes a number of predefined libraries, including an exception system, collections, arithmetic, higher-order functions, and introspection.\\nThe exception system is object-based. It uses calling semantics (thereby allowing recovery)  but also provides exiting handlers.\\nThe collection system includes a number of predefined collection classes and operations built on a simple iteration protocol. Additional classes defined in terms of this protocol have access to the full suite of collection operations.\\nArithmetic is fully object-based and extensible.\\nA library of higher-order operations on functions supports function composition.\\nA library of introspective functions supports the run time examination of objects, including classes and functions.\\n\\n\\n\\n\\n\\n\\nThe Dylan Reference Manual - 7 Apr 1998\\n     \\nCopyright Apple Computer, Inc. 1996. Apple® and the Apple logo are registered trademarks of Apple Computer, Inc. Used with permission. All Rights Reserved.\\nYou can order a bound copy of this book from Harlequin.\\nGenerated with Harlequin WebMaker®\\n\\n\\n\\nCopyright © 1995-1999 Harlequin Limited. All rights reserved. Last update July 02 1998\\n',\n",
       " \"\\n\\n\\n\\n4.1 Overview\\n\\n\\n\\n\\n\\n\\n\\n     \\n Next: 4.2 Syntactic Generation\\nUp: 4 Language Generation\\n Previous: 4 Language Generation\\n  \\nChapter 4:  Language Generation\\n4.1 Overview\\n\\nEduard Hovy \\nUniversity of Southern California, Marina del Rey, California, USA\\n\\n\\nThe area of study called natural language generation\\n(NLG)\\ninvestigates how computer programs can be made to produce high-quality\\nnatural language text from computer-internal representations of\\ninformation.  Motivations for this study range from entirely\\ntheoretical (linguistic, psycholinguistic) to entirely practical (for the production of output systems for computer\\nprograms). Useful overviews of the research are\\n[DHRS92,PSM90,Kem87,BH92,MS87,MBG81].  The stages\\nof language generation for a given application, resulting in speech output, are\\nshown in Figure 4.1.\\n\\nFigure 4.1: The stages of language generation.\\n\\n\\nThis section discusses the following:\\n the overall state of the art in generation,\\n ignificant gaps of knowledge, and\\n new developments and infrastructure.\\n\\nFor more detail, it then turns to two major areas of generation theory and\\npractice: single-sentence generation (also called realization or tactical generation) and multisentence generation (also called text planning or strategic generation).  \\n4.1.1 State of the Art\\n\\nNo field of study can be described adequately using a single perspective.\\nIn order to understand NLG it is helpful to consider independently the \\ntasks of generation and the process of generation.  Every \\ngenerator addresses one or more tasks and embodies one (or sometimes two) \\ntypes of process.  One can identify three types of generator task: \\ntext planning, sentence planning, and surface realization.  Text planners \\nselect from a knowledge pool what information to include in the output, \\nand out of this create a text structure to ensure coherence.  On a more \\nlocal scale, sentence planners organize the content of each sentence, \\nmassaging and ordering its parts.  Surface realizers convert sentence-sized \\nchunks of representation into grammatically correct sentences.  Generator \\nprocesses can be classified into points on a range of sophistication \\nand expressive power, starting with inflexible canned methods and ending \\nwith maximally flexible feature combination methods.  For each point on \\nthis range, there may be various types of implemented algorithms.\\n18 \\nThe simplest approach,  canned text systems, is used in the majority \\nof software: the system simply prints a string of words without any change \\n(error messages, warnings, letters, etc.).  The approach can be used equally \\neasily for single-sentence and for multi-sentence text generation.  Trivial \\nto create, the systems are very wasteful.   Template systems, the next \\nlevel of sophistication, are used as soon as a message must be produced \\nseveral times with slight alterations.  Form letters are a typical template \\napplication, in which a few open fields are filled in specified constrained \\nways.  The template approach is used mainly for multisentence generation,\\nparticularly in applications whose texts are fairly regular in structure \\nsuch as some business reports.  The text planning components of the U.S. \\ncompanies CoGenTex (Ithaca, NY) and Cognitive Systems Inc. (New Haven, \\nCT) enjoy commercial use.  On the research side, the early template-based \\ngenerator ANA [Kuk83] produced stock market reports from a news\\nwire by filling appropriate values into a report template.  More \\nsophisticated, the multisentence component of TEXT [McK85] could \\ndynamically nest instances of four stereotypical paragraph templates called \\nschemas to create paragraphs.  TAILOR [Par93a] generalized TEXT by \\nadding schemas and more sophisticated schema selection criteria.\\n\\n Phrase-based systems employ what can be seen as\\ngeneralized templates, whether at the sentence level (in\\nwhich case the phrases resemble phrase structure grammar\\nrules) or at the\\ndiscourse level (in which case they are often called text plans).  In such systems, a phrasal pattern is first selected to match the top\\nlevel of the input (say, [SUBJECT VERB OBJECT]), and then each\\npart of the pattern is expanded into a more specific phrasal pattern\\nthat matches some subportion of the input (say, [DETERMINER ADJECTIVES HEAD-NOUN MODIFIERS]), and so on; the cascading process\\nstops when every phrasal pattern has been replaced by one or more\\nwords.  Phrase-based systems can be powerful and robust, but are very\\nhard to build beyond a certain size, because the phrasal\\ninterrelationships must be carefully specified to prevent\\ninappropriate phrase expansions.  The phrase-based approach has mostly\\nbeen used for single-sentence generation (since linguists' grammars\\nprovide well-specified collections of phrase structure rules).  A\\nsophisticated example is MUMBLE\\n[McD80,MMA87], built at the University of Massachusetts,\\nAmherst.  Over the past five years, however, phrase-based multisentence text structure generation (often called text planning) has received considerable attention in the research community, with the development of the RST text structurer [Hov88a], the EES text planner\\n[Moo89], and several similar systems\\n[Dal90,Caw89,Sut93], in which each\\nso-called text plan is a phrasal pattern that specifies the\\nstructure of some portion of the discourse, and each portion of the\\nplan is successively refined by more specific plans until the\\nsingle-clause level is reached.  Given the lack of understanding of\\ndiscourse structure and the paucity of the discourse plan libraries,\\nhowever, such planning systems do not yet operate beyond the\\nexperimental level.\\n\\n Feature-based systems represent, in a sense, the limit\\npoint of the generalization of phrases. In feature-based systems, each\\npossible minimal alternative of expression is represented by a single\\nfeature; for example, a sentence is either POSITIVE or NEGATIVE, it is a QUESTION or an IMPERATIVE or a STATEMENT, its tense is PRESENT or PAST and so on.  Each\\nsentence is specified by a unique set of features.  Generation\\nproceeds by the incremental collection of features appropriate for\\neach portion of the input (either by the traversal of a\\nfeature selection network or by unification), until the sentence is fully determined.  Feature-based systems are among the\\nmost sophisticated generators built today.  Their strength lies in the\\nsimplicity of their conception: any distinction in language is defined\\nas a feature, analyzed, and added to the system.  Their strength lies\\nin the simplicity of their conception: any \\ndistinction in language can be added to the system as a feature.  Their \\nweakness lies in the difficulty of maintaining feature interrelationships \\nand in the control of feature selection (the more features available, \\nthe more complex the input must be).  No feature-based multisentence\\ngenerators have\\nbeen built to date. The most advanced single-sentence generators of this type include PENMAN [Mat83,MM85] and its descendant \\n  KPML\\n[BMTW91], the Systemic generators developed at \\n  USC/ISI and IPSI;\\nCOMMUNAL [Faw92] a\\nSystemic generator developed at Wales; the Functional Unification\\nGrammar framework (FUF)\\n[Elh92] from Columbia University; SUTRA [VHHJW80]\\ndeveloped at the University of Hamburg; SEMTEX\\n[R86] developed at the University of Stuttgart; and\\nPOPEL [Rei91] developed at the University of the\\nSaarland.  The two generators most widely distributed, studied, and\\nused are PENMAN/KPML and FUF.  None of these systems is in\\ncommercial use.\\n\\n4.1.2 Significant Gaps and Limitations\\n\\nIt is safe to say that at the present time one can fairly easily build\\na single-purpose generator for any specific application, or with some\\ndifficulty adapt an existing sentence generator to the application, with\\nacceptable results.  However, one cannot yet build a general-purpose\\nsentence generator or a non-toy text planner.  Several significant\\nproblems remain without sufficiently general solutions:\\n lexical selection\\n sentence planning\\n discourse structure\\n domain modeling\\n generation choice criteria\\n\\n\\n Lexical Selection: Lexical selection is one of the most\\ndifficult problems in generation.  At its simplest, this\\nquestion involves selecting the most appropriate single word for a\\ngiven unit of input.  However, as soon as the semantic model\\napproaches a realistic size, and as soon as the lexicon is\\nlarge enough to permit alternative locutions, the problem\\nbecomes very complex.  In some situation, one might have to choose\\namong the phrases John's car, John's sports car, his speedster,\\nthe automobile, the red vehicle, the red Mazda for referring to a\\ncertain car.  The decision depends on what has already been said, what\\nis referentially available from context, what is most salient, what\\nstylistic effect the speaker wishes to produce, and so on.\\nA considerable amount of work has been devoted to this question, and \\nsolutions to various aspects of the problem have been suggested (see \\nfor example [Gol75,ER92,MRT93]).\\nAt this\\ntime no general methods exist to perform lexical selection.\\nMost current generator systems simply finesse the problem by linking a\\nsingle lexical item to each representation unit.  What is\\nrequired: Development of theories about and implementations of\\nlexical selection algorithms, for reference to objects, event, states,\\netc., and tested with large lexica.\\n\\n Discourse Structure: One of the most exciting recent\\nresearch developments in generation is the automated planning of\\nparagraph structure.  The state of the art in discourse research is\\ndescribed in chapter 6. So far no text planner exists that can\\nreliably plan texts of several paragraphs in general.  What is\\nrequired: Theories of the structural nature of discourse, of the\\ndevelopment of theme and focus in discourse, and of coherence and\\ncohesion; libraries of discourse relations, communicative goals, and\\ntext plans; implemented representational paradigms for characterizing\\nstereotypical texts such as reports and business letters; implemented\\ntext planners that are tested in realistic non-toy domains.\\n\\n Sentence Planning: Even assuming the text planning\\nproblem solved, a number of tasks remain before well-structured\\nmultisentence text can be generated.  These tasks, required for\\nplanning the structure and content of each sentence, include:\\npronoun specification, theme signaling, focus signaling, content aggregation to remove unnecessary redundancies, the ordering of prepositional phrases, adjectives, etc.  An elegant system that addressed some of these tasks\\nis described in [App85].  While to the nonspecialist these\\ntasks may seem relatively unimportant, they can have a significant\\neffect and make the difference between a well-written and a poor text.\\nWhat is required: Theories of pronoun use, theme and focus\\nselection and signaling, and content aggregation; implemented\\nsentence planners with rules that perform these operations; testing in\\nrealistic domains.\\n\\n Domain Modeling: A significant shortcoming in\\ngeneration research is the lack of large well-motivated application\\ndomain models, or even the absence of clear principles by which to\\nbuild such models.  A traditional problem with generators is that the\\ninputs are frequently hand-crafted, or are built by some other system\\nthat uses representation elements from a fairly small hand-crafted\\ndomain model, making the generator's inputs already highly oriented\\ntoward the final language desired.  It is very difficult to link a\\ngeneration system to a knowledge base or database that was originally\\ndeveloped for some non-linguistic purpose. The mismatches between the\\nrepresentation schemes demonstrate the need for clearly articulated\\nprinciples of linguistically appropriate domain modeling and\\nrepresentational adequacy (see also [Met90]).  The use of\\nhigh-level language-oriented concept taxonomies such as the Penman Upper Model [BMW90] to act as a bridge between the\\ndomain application's concept organization and that required for\\ngeneration is becoming a popular (though partial) solution to this\\nproblem.  What is required: Implemented large-size (over 10,000\\nconcepts) domain models that are useful both for some non-linguistic\\napplication and for generation; criteria for evaluating the internal\\nconsistency of such models; theories on and practical experience in\\nthe linking of generators to such models; lexicons of commensurate\\nsize.\\n\\n Generation Choice Criteria: Probably the problem least\\naddressed in generator systems today is the one that will take the\\nlongest to solve.  This is the problem of guiding the generation\\nprocess through its choices when multiple options exist to handle any\\ngiven input.  It is unfortunately the case that language, with its\\nalmost infinite flexibility, demands far more from the input to a\\ngenerator than can be represented today.  As long as generators remain\\nfairly small in their expressive potential then this problem does not\\narise.  However, when generators start having the power of saying\\nthe same thing in many ways, additional control must be exercised\\nin order to ensure that appropriate text is produced.  As shown in\\n[Hov88b] and\\n[Jam87], different texts generated from the same input carry\\nadditional, non-semantic import; the stylistic variations serve to express\\nsignificant interpersonal and situational meanings (text can be formal or\\ninformal, slanted or objective, colorful or dry, etc.).  In order to\\nensure appropriate generation, the generator user has to specify not only\\nthe semantic content of the desired text, but also its\\npragmatic---interpersonal and\\nsituational---effects.  Very little research has been\\nperformed on this question beyond a handful of small-scale pilot studies.\\nWhat is required: Classifications of the types of reader\\ncharacteristics and goals, the types of author goals, and the interpersonal\\nand situational aspects that affect the form and content of language;\\ntheories of how these aspects affect the generation process; implemented\\nrules and/or planning systems that guide generator systems' choices;\\ncriteria for evaluating appropriateness of generated text in specified\\ncommunicative situations.\\n\\n4.1.3 Future Directions\\n\\n Infrastructure Requirements: The overarching challenge for generation\\nis scaling up to the ability to handle real-world, complex domains.\\nHowever, given the history of relatively little funding support, hardly\\nany infrastructure required for generation research exists today.\\n\\nThe resources most needed to enable both high-quality research and large-scale\\ngeneration include the following:\\n Large well-structured lexicons of various languages.  Without such\\n lexicons, generator builders have to spend a great deal of redundant\\n effort, collecting standard morphological and syntactic information\\n to include in lexical items.  As has been shown recently in the\\n construction of the Penman English lexicon of 90,000+ items, it is\\n possible to extract enough information from online dictionaries to\\n create lexicons, or partial lexicons, automatically.\\n Large well-structured knowledge bases.  Paralleling the recent\\n knowledge base construction efforts centered around WordNet\\n [Mil85] in the U.S., a large general-purpose knowledge\\n base that acts as support for domain-specific application oriented\\n knowledge bases would help to speed up and enhance generator porting\\n and testing on new applications.  An example is provided by the\\n ontology construction program of the Pangloss\\n machine translation effort [HK93].\\n Large grammars of various languages.\\n The general availability of such grammars would free generator\\n builders from onerous and often repetitive linguistic work, though\\n different theories of language naturally result in very different\\n grammars.  However, a repository of grammars built according to\\n various theories and of various languages would constitute a valuable\\n infrastructure resource.\\n Libraries of text plans.  As discussed above, one of the major\\n stumbling blocks in the ongoing investigation of text planning is the\\n availability of a library of tested text plans.  Since no consensus\\n exists on the best form and content of such plans, it is advisable to\\n pursue several different construction efforts.\\n\\n\\n Longer-term Research Projects: Naturally, the number and variety of\\npromising long-term research projects is large.  The following directions\\nhave all been addressed by various researchers for over a decade and\\nrepresent important strands of ongoing investigation:\\n stylistically appropriate generation\\n psycholinguistically realistic generation\\n reversible multilingual formalisms and algorithms\\n continued development of grammars and generation methods\\n generation of different genres/types of text\\n\\n\\n Near- and Medium-term Applications with Payoff Potential:  Taking into\\naccount the current state of the art and gaps in knowledge and capability,\\nthe following applications (presented in order of increasing difficulty)\\nprovide potential for near-term and medium-term payoff:\\n\\n  Database Content Display:\\n The description of database contents in natural language is not a new\\n problem, and some such generators already exist for specific\\n databases.  The general solution still poses problems, however, since\\n even for relatively simple applications it still includes unsolved\\n issues in sentence planning and text planning.\\n  Expert System Explanation:\\n This is a related problem, often however requiring more interactive\\n ability, since the user's queries may not only elicit more\\n information from a (static, and hence well-structured) database, but\\n may cause the expert system to perform further reasoning as well, and\\n hence require the dynamic explanation of system behavior, expert\\n system rules, etc.  This application also includes issues in\\n text planning, sentence planning, and  lexical choice.\\n  Speech Generation:\\n Simplistic text-to-speech synthesis systems have been\\n available commercially for a number of years, but naturalistic speech\\n generation involves unsolved issues in discourse and\\n interpersonal pragmatics (for example, the\\n intonation contour of an utterance can express dislike,\\n questioning, etc.).  Only the most advanced speech synthesizers today\\n compute syntactic form as well as intonation  contour and pitch level.\\n  Limited Report and Letter Writing: As mentioned in the previous\\n section, with increasingly general representations for text structure,\\n generator systems will increasingly be able to produce standardized\\n multiparagraph texts such as business letters or monthly reports.  The\\n problems faced here include text plan libraries, sentence planning, adequate\\n lexicons, and robust sentence generators.\\n  Presentation Planning in Multimedia Human-Computer\\nInteraction:\\n By generalizing text plans, [HA91] showed that it is\\n possible also to control some forms of text formatting, and then\\n argued that further generalization will permit the planning of\\n certain aspects of multimedia presentations.  Ongoing research in the\\n WIP project at Saarbrücken [WAGR91] and the\\n COMET project at Columbia University\\n [FM90] have impressive demonstration systems for\\n multimedia presentations involving planning and\\n language generation.\\n  Automated Summarization:\\n A somewhat longer-term functionality that would make good use of\\n language generation and discourse knowledge is the automated\\n production of summaries.  Naturally, the major problem to be solved\\n first is the identification of the most relevant information.\\n\\n\\nDuring the past two decades, language generation technology\\nhas developed to the point where it offers general-purpose\\nsingle-sentence generation capability and limited-purpose\\nmultisentence paragraph planning capability.  The possibilities for\\ngrowth and development of useful applications are numerous and\\nexciting.  Focusing new research on specific applications and on\\ninfrastructure construction will help turn the promise of current text\\ngenerator systems and theories into reality.\\n\\n     \\n Next: 4.2 Syntactic Generation\\nUp: 4 Language Generation\\n Previous: 4 Language Generation\\n  \\n\\n\",\n",
       " '\\n\\n\\n\\n\\n\\nUnified Modeling Language Examples (UML)\\n\\n\\n\\xa0\\n\\n\\n\\xa0\\n\\xa0UML\\nBy Examples\\xa0\\n\\n\\xa0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\nHome\\xa0\\n\\xa0\\n\\n\\nUML\\xa0\\n\\xa0\\n\\n\\nResources\\xa0\\xa0\\n\\xa0\\n\\n\\n\\xa0\\n\\n\\n\\xa0\\n\\n\\n\\xa0\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\xa0\\xa0\\xa0 Table of\\nContents\\xa0\\n1 Elevator Problem\\xa0\\n2 Unified Modeling Language\\xa0\\n3 Analysis\\xa0\\n3.1 Use Case Dagram\\xa0\\n3.2 Class Diagram\\xa0\\xa0\\n3.3 State Diagram\\xa0\\n4 Design\\xa0\\xa0\\n4.1 Sequence Diagram\\xa0\\xa0\\n4.2 Collaboration Diagram\\xa0\\n5 Detail Design\\xa0\\n5.1 Detail Class Diagram\\xa0\\n5.2 Detail Opreation Description\\xa0\\n5.3 Pseud-Code\\n6. Acknowledgement\\n\\n0. Introduction\\xa0\\n\\nThe aim of this tutorial is to show how to use\\nUML in \"real\" software development environment.\\xa0\\xa0\\n1. Elevator Problem\\xa0\\n\\nA product is to be installed to control elevators in a building with\\nm floors. The problem concerns the logic required to move elevators between\\nfloors according to the following constraints:\\xa0\\n\\xa0\\n\\n\\n\\xa0Each elevator has a set of m buttons, one for each floor. These illuminate\\nwhen pressed and cause the elevator to visit the corresponding floor. The\\nillumination is canceled when the elevator visits the corresponding floor.\\xa0\\n\\n\\xa0\\n\\n\\nEach floor, except the first floor and top floor has two buttons, one to\\nrequest and up-elevator and one to request a down-elevator. These buttons\\nilluminate when pressed. The illumination is canceled when an elevator\\nvisits the floor and then moves in the desired direction.\\xa0\\n\\n\\xa0\\n\\n\\n\\xa0When an elevator has no requests, it remains at its current floor\\nwith its doors closed.\\n\\n\\n\\xa0\\n2. Unified Modeling Language\\xa0\\n\\nUML is a modeling language that only specifies semantics and notation\\nbut no process is currently defined. Thus, we decided to do the analysis\\nas follows;\\xa0\\n\\xa0\\n\\n\\nUse Case Diagram\\xa0\\n\\n\\xa0Class Diagram\\xa0\\n\\n\\xa0Sequence Diagram\\xa0\\n\\n\\xa0Collabration Diagram\\xa0\\n\\n\\xa0State Diagram\\n\\n3.\\xa0 Analysis\\xa0\\n\\n3.1. Use case diagram\\xa0\\n\\nUse case description:\\xa0\\n\\xa0\\n\\n\\n\\xa0A generalized description of how a system will be used.\\xa0\\n\\n\\xa0Provides an overview of the intended functionality of the system.\\xa0\\n\\n\\xa0Understandable by laymen as well as professionals.\\xa0\\n\\nUse Case Diagram:\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\nElevator basic scenario that can be extracted from Use Case Diagram:\\xa0\\n\\xa0\\n\\n\\n\\xa0Passenger pressed floor button\\xa0\\n\\n\\xa0Elevator system detects floor button pressed\\xa0\\n\\n\\xa0Elevator moves to the floor\\xa0\\n\\n\\xa0Elevator doors open\\xa0\\n\\n\\xa0Passenger gets in and presses elevator button\\xa0\\n\\n\\xa0Elevator doors closes\\xa0\\n\\n\\xa0Elevator moves to required floor\\xa0\\n\\n\\xa0Elevator doors open\\xa0\\n\\n\\xa0Passenger gets out\\xa0\\n\\nElevator doors closes\\xa0\\n\\n\\xa0\\n3.2.\\xa0 Class Diagram\\xa0\\n\\nClass diagrams show the static structure of the object, their internal\\nstructure, and their relationships.\\xa0\\n\\nClass diagram:\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n3.3.\\xa0 State diagram\\xa0\\xa0\\n\\nA state diagram shows the sequences of states an object goes through\\nduring it\\'s life cycle in response to stimuli, together with its responses\\nand actions.\\xa0\\n\\xa0\\n\\xa0\\n4.\\xa0 Design\\xa0\\n\\nThe design phase should produce the detailed class diagrams, collaboration\\ndiagrams, sequence diagrams, state diagrams, and activity diagram. However,\\nthe elevator problem is too simple for an activity diagram. Thus, we are\\nnot using an activity diagram for the elevator problem.\\xa0\\n\\n4.1. Sequence Diagram\\xa0\\n\\nA sequence diagram and collaboration diagram conveys similar information\\nbut expressed in different ways. A Sequence diagram shows the explicit\\nsequence of messages suitable for modeling a real-time system, whereas\\na collobration diagram shows the relationships between objects.\\xa0\\n\\nSequence Diagrams:\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\xa0\\nSequence Diagram for Serving Elevator Button\\n\\xa0\\n\\xa0\\nSequence Diagram for Serving Door Button\\n\\xa0\\n\\xa0\\n\\xa0\\n4.2. Collaboration diagram\\xa0\\n\\xa0\\n\\n\\nDescribes the set of interactions between classes or types\\xa0\\n\\n\\xa0Shows the relationships among objects\\xa0\\n\\nCollabration diagrams:\\xa0\\n\\xa0\\n\\xa0\\nCollabration Digaram for Serving Elevator Button\\xa0\\n\\xa0\\xa0\\nCollabration Digaram for Serving Door Button\\xa0\\n\\xa0\\n\\xa0\\n5. Detail Design\\xa0\\n\\xa0\\n5.1. Detail Class Diagram\\xa0\\n\\xa0\\n\\xa0\\n\\n\\xa05.2. Detail Operation Description\\xa0\\xa0\\n\\xa0\\nModule Name\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nElevator_Control::Elevator_control_loop\\xa0\\nModule Type\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nMethod\\xa0\\nInput Argument\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nNone\\xa0\\nOutput Argument\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nNone\\xa0\\nError Message\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nNone\\xa0\\nFile Access\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nNone\\xa0\\nFile Change\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nNone\\xa0\\nMethod Invoke\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nbutton::illuminate, button::cancel_illumination,\\xa0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\ndoor::open, door::close, elevator::move, elevator::stop\\xa0\\nNarative\\xa0\\n\\xa0\\n\\xa05.3. Pseudo-Code\\xa0\\xa0\\xa0\\n\\xa0\\nvoid elevator_control (void)\\xa0\\n{\\xa0\\n\\xa0\\xa0 while a button has been pressed\\xa0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if button not on\\xa0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 {\\xa0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nbutton::illuminate;\\xa0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nupdate request list;\\xa0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 }\\xa0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 else if elevator is moving\\nup\\xa0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 {\\xa0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nif there is no request to stop at floor f\\xa0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nElevator::move one floor up;\\xa0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nelse\\xa0\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\n}\\xa0\\n\\n6. Acknowledgement\\xa0\\n\\nThis example was developed for topic in software\\nengineering in Vanderbilt University\\nby myself and my best friends:\\xa0\\n\\n\\nHelen\\nXioa\\n\\nValeria Amburge\\n\\nParvathi RajaGopal\\n\\n\\n\\n\\xa0\\n\\n\\n\\xa0\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\n\\xa0\\n\\n\\n\\xa0\\n\\n\\n\\n\\n',\n",
       " '\\n\\n\\nKerberos Authentikation\\n\\n\\nKerberos Autentifikation\\n\\n\\n\\nInhalt:\\n1 Grundlegende Prinzipien\\n1.1 Was ist Kerberos?\\n1.2 Software Komponenten\\n1.3 Funktionsweise\\n1.4 Datenbank\\n2 Kerberos in Windows NT 5\\n2.1 Komponenten des NT Sicherheitsmodells\\n2.2 Kerberos Autentifikation in Win NT\\n2.3 Kerberos Protokoll und Win NT Autorisation\\n\\xa0\\n\\xa0\\n1 Grundlegende Prinzipien\\n1.1 Was ist Kerberos?\\nKerberos ist in der Griechischen Mythologie der dreiköpfige Wachhund\\nan den Pforten zur Unterwelt, aber im folgenden soll es nicht um Griechische\\nMythologie sondern um die Autentifikation von Client und Servern in offenen\\nNetzwerken gehen.\\nKerberos für offene Netzwerke wurde von Miller und Neuman entwickelt.\\nDas entscheidende Grundprinzip lautet: Jeder glaubt an das Urteil von Kerberos\\nbei der Autentifikation. Das heißt jeder vertraut auf einen Dritten,\\nan der eigentlichen Arbeit unbeteiligten. Der wesentliche Bestandteil von\\nKerberos ist die Datenbank. In ihr sind alle Clients und deren private\\nkeys gespeichert. Ein private key ist eine große Zahl, die nur dem\\nClient selbst und Kerberos bekannt ist. Ist der Client ein Nutzer, ist\\nder private key sein verschlüsseltes Paßwort.\\nAutentifikation ist nur die Bestätigung, daß der in einer\\nAnfrage genannte Client auch der ist, für den er sich ausgibt. Hierbei\\nspielen die Rechte desjenigen keine Rolle. (Bei der Autorisation spielen\\ndie Rechte die entscheidende Rolle.)\\nJeder Client (gemeint ist hier sowohl der User als auch der Server)\\nhat einen eindeutigen Kerberos-Namen. Dieser Kerberos-Name besteht aus\\neinem Namen (bei Usern der Login-Name, bei Servern der Service-Name), einer\\nInstanz (dient zur Unterscheidung des Hauptnamens, z. B. bei Personen mit\\nAdministrator und Normalen Login) und einem Bereich (damit ist die administratorische\\nEinrichtung gemeint). Der allgemeine Aufbau sieht folgendermaßen\\naus:\\n\\nname.instanz@bereich\\nEs werden drei unterschiedliche Stufen der Sicherheit angeboten.\\n1. Nur zu Beginn einer jeden Kommunikation wird die Authentizität\\ngeprüft. Bei allen weiteren Kommunikationen wird davon ausgegangen,\\ndaß die Nachricht von dieser Netzadresse von dem richtigen Partner\\nkommt.\\n2. Jede Nachricht wird durch Autentifikation überprüft. Hierbei\\nhandelt es sich um die sogenannte \\x91save massage\\x92.\\n3. Zusätzlich zu 2. wird die Nachricht selbst geschützt. Dies\\ngeschieht durch Verschlüsselung. Die sogenannte \\x91private massage\\x92\\nwird unter anderem durch Kerberos selbst genutzt, wenn Paßworte über\\ndas Netz übertragen werden müssen.\\n1.2 Software Komponenten\\nKerberos application library bietet ein Interface zwischen Programmanwendern\\nund Programmservern, z. B. Routinen zum Erstellen oder Lesen von Autentifikationsanfragen,\\nRoutinen zum Erstellen von save oder private massages.\\nEncryption library basiert auf dem Data Encryption Standard.\\nSie bietet unterschiedliche Methoden der Verschlüsselung (unterschiedlich\\nfür hohe Sicherheit oder große Geschwindigkeit) und ist ein\\nunabhängiges Modul, das dadurch durch andere Module ersetzt werden\\nkann.\\nDatabase library ist ein weiterer Bestandteil der Software Komponenten.\\nSie wird vor allem von den database administration progams genutzt. Auch\\ndiese Bibliothek ist ersetzbar.\\nDatabase administration programs bieten alle benötigten\\nTools zur Administration der Datenbank.\\nAdministration server ist das Lese- und Schreibinterface zu der\\nDatenbank. Er läuft nur auf dem Rechner, auf dem auch die Kerberos\\nDatenbank läuft, wohingegen die Clients des Servers auf jeden beliebigen\\nRechner laufen kann.\\nAuthentication server (Kerberos Server) dient zur Authentication\\nvon allen Nutzern und zur Generierung von Session Keys. Er führt nur\\nLeseoperationen auf der Kerberos Datenbank aus und kann auf jeder Maschine\\nlaufen, wo es eine Kopie (read only) der original Kerberos Datenbank gespeichert\\nist.\\nDatabase propagation software hat die Aufgabe, der Verteilung\\nvon Kopien der Kerberos Datenbank. Es ist möglich, Kopien der Datenbank\\nund des authentication servers auf vielen verschiedenen Rechnern laufen\\nzu haben (Ausfallsicherheit bei nur einem Rechner mit der Datenbank Kontra\\nSicherheit bei der Verbreitung von Kopien der Datenbank). Jede sogenannte\\nSlave-Machine erhält in regelmäßigen Abständen Updates\\nvon der Master Datenbank.\\nApplications sind z. B. Programme für das Einloggen in Kerberos,\\nfür das Ändern von Paßworten oder das Anzeigen oder Zerstören\\nvon Tickets.\\n1.3 Funktionsweise\\nDie Funktionsweise von Kerberos basiert auf Needham und Schroeder\\x92s\\nkey distribution protocol. Die Arbeitsweise gliedert sich in drei Phasen\\nzur Autentifikation:\\n\\n\\nNutzer erhält Beglaubigungsschreiben\\n\\nAnfrage nach Autentifikation für einen speziellen Service\\n\\nNutzer zeigt Beglaubigung dem End-Server\\n\\nEs gibt zwei Arten von Beglaubigungen: Tickets und Autentifikatoren. Tickets\\ndienen der sicheren Identifikation und stellen außerdem sicher, daß\\ndie Person, die das Ticket benutzt dieselbe ist, der das Ticket ausgestellt\\nwurde. Ein Autentifikator enthält zusätzliche Informationen,\\nwelche bestätigt, daß der Client derselbe ist, für welchen\\ndas Ticket ausgestellt wurde.\\nEin Ticket enthält:\\n\\n\\nName des Servers\\n\\nName des Client\\n\\nInternetadresse des Client\\n\\nZeitstempel\\n\\nZeitdauer der Gültigkeit\\n\\nSession key\\n\\nAlle Daten werden mit dem private key des Servers verschlüsselt. Ein\\nTicket ist mehrmals verwendbar (vom Client zu dem angegebenen Server).\\nEin Autentifikator kann nur einmal verwendet werden, aber der Client\\nstellt diesen selber her. Er enthält:\\n\\n\\nName des Client\\n\\nInternetadresse des Client\\n\\naktuelle Urzeit\\n\\nDiese Daten werden mit dem Session key, welcher Teil des Tickets ist, verschlüsselt.\\nErhalt eines Tickets\\n\\nAls erstes sendet der Client seinen Namen und den Namen des ticket granting\\nservers an Kerberos. Kerberos überprüft diese Informationen.\\nFalls sie richtig sind, generiert er einen Session key, welcher später\\nfür die Kommunikation zwischen Client und dem ticket granting server\\ngenutzt werden wird. Als nächstes erstellt Kerberos ein Ticket, wobei\\nder Name des Servers der Name des angeforderten ticket granting servers\\nist. Das Ticket wird mit dem Private key des TGS verschlüsselt. Dieses\\nTicket sendet Kerberos dann, gemeinsam mit dem generierten Session key,\\nzurück zu dem Client, wobei die gesamte Sendung mit dem Schlüssel\\ndes Client verschlüsselt wurde.\\nHandelt es sich bei dem Client um einen User, wird er nach Erhalt der\\nAntwortsendung von Kerberos aufgefordert sein Paßwort einzugeben.\\nAus diesem Paßwort wird der Private key des Users ermittelt, um die\\nSendung von Kerberos zu entschlüsseln. In allen anderen Fällen\\nist der Private key dem Client bekannt.\\nErhalt eines Tickets für einen speziellen Server\\n\\nUm einen speziellen Service in Anspruch nehmen zu können, braucht\\nder Client ein spezielles Ticket für diesen Server. Dieses Ticket\\nbekommt er von dem ticket granting server. Dazu sendet er den Servernamen,\\ndas Ticket für den TGS und einen selbst erstellten Autentifikator,\\nwelcher mit dem Session key (gültig für Kommunikationen zwischen\\nClient und TGS) verschlüsselt wurde, an den ticket granting server.\\nDieser überprüft die Informationen (zuerst entschlüsseln\\ndes Tickets mit eigenem Private key und anschließendes entschlüsseln\\ndes Autentifikators mit dem im Ticket enthaltenen Session key). Wenn alle\\nInformationen gültig sind, generiert TGS einen neuen Session key für\\ndie Kommunikation zwischen Client und angefordertem Server und anschließend\\nein neues Ticket (verschlüsselt mit dem Private key des angeforderten\\nServers). Die Lebenszeit dieses Tickets ist das Minimum zwischen der Lebenszeit\\ndes Tickets für TGS und der Standard Lebenszeit eines neuen Tickets.\\nTicket und neuer Session key werden verschlüsselt mit dem Session\\nkey von Client und TGS zurück an den Client geschickt.\\nAnfrage an einen Server\\n\\nUm einen Service des Servers in Anspruch zu nehmen, muß der Client\\nsich selbstverständlich bei dem Server autentifizieren. Dies tut er\\nmit dem Erhaltenen Ticket und einem selbst erstellten Autentifikator, verschlüsselt\\nmit dem Session key. Der Server entschlüsselt die Sendung und überprüft\\nderen Inhalt. Falls alles o. k. ist, läßt er die Anfrage zu.\\nAlle Uhren eines Netzwerkes müssen synchronisiert sein, da Anfragen\\nvon einem Client mit einem Zeitstempel, der zu alt ist oder in der Zukunft\\nliegt zurückgewiesen wird. Außerdem werden auch Anfragen mit\\neinem Zeitstempel, der schon mal benutzt wurde nicht zugelassen.\\nMöchte nun auch der Client über die Autentität des Servers\\nsicher sein, sendet der Server den um eins erhöhten Zeitstempel des\\nClient an ihn zurück. Diese Nachricht wird mit dem gemeinsamen Session\\nkey verschlüsselt.\\n1.4 Datenbank\\nEin entscheidender Punkt für die Sicherheit von Kerberos ist die\\nSicherheit der Kerberos Datenbank. Ein schreibender Zugang zu dieser ist\\nnur durch einen administrativen Service, dem Kerberos Database Management\\nService (KDMS) möglich. Jede Änderung ist nur auf der Master\\nDatenbank erlaubt und KDMS läuft nur auf dem Rechner mit der Master\\nDatenbank.\\nKDMS hat zwei Hauptaufgaben:\\n\\n\\nBearbeitung von Anfragen zur Paßwortänderung\\n\\nHinzufügen neuer Nutzer\\n\\nJedesmal, wenn ein Dienst des KDMS angefordert wird, ist eine zusätzliche\\nAutentifikation notwendig, d. h. die nochmalige Eingabe des Paßwortes.\\nDies führt zu der Sicherheit, daß falls doch einmal ein Nutzer\\nseine Workstation verläßt ohne sich abzumelden, es nicht möglich\\nist von einem zufällig vorbeikommenden das Paßwort zu ändern.\\nFolgender Ablauf wird zur Überprüfung durchgeführt: Zuerst\\nwird der Name des Anfragenden mit dem, dessen Paßwort geändert\\nwerden soll, verglichen. Stimmen dies Namen nicht überein, folgt ein\\nVergleich mit allen Namen der Zugangskontrolliste.\\nAlle Anfragen an diesen Service, ob erfolgreich oder nicht, werden protokolliert.\\nWie bereits erwähnt, kann es von der Master Datenbank auch noch\\nKopien geben. Dies ist ein kritischer Punkt, da hierzu die Übertragung\\nder Datenbankinhalte über das Netz erfolgen muß und auch die\\nFrage der Konsistenz der Kopien steht. Allerdings gibt es auch Vorteile.\\nFalls die Master Datenbank mal ausfallen sollte, gibt es im Netz immer\\nnoch Stellen, die ein Autentifikation durchführen können, so\\nfällt für diese Zeit nur der Service des Paßwortänderns\\naus. Außerdem kann es bei nur einen Autentifikationsservices zu einem\\nEngpaß bei der Bearbeitung von Anfragen kommen.\\nAuf folgende Art und Weise soll die Konsistenz aller Kerberos Datenbanken\\nerreicht werden: Die Master Datenbank sendet in regelmäßigen\\nIntervallen Updates zu den Slave Datenbanken. Zuerst ermittelt die Masterdatenbank\\neine Prüfsumme. Diese sendet sie dann, verschlüsselt mit dem\\nMaster Datenbank key (nur Master Datenbank und Kopien bekannt), den Kopien.\\nDanach wird das Update übertragen. Die Kopie überprüft die\\nPrüfsumme und falls sie korrekt ist, wird das Update genutzt. Natürlich\\nwerden alle Paßworte nur mit dem Master Datenbank key verschlüsselt\\nübertragen.\\n\\xa0\\n2 Kerberos in Windows NT 5\\n\\xa0\\nFür die Sicherheit in Windows NT 5 Domänen wird das Kerberos\\nVersion 5 Autentifikationsprotokoll und ein Active Directory verwendet.\\nDie Implementation von Kerberos basiert hierbei auf RFC1510, wobei Microsoft\\nzusätzliche Erweiterungen implementiert hat. Kerberos ist nur eines\\nder in Win NT 5 enthaltenen Sicherheitsprotokolle. Andere sind u. a.:\\n\\n\\nNTLM für die Kompatibilität mit älteren Versionen von NT\\n\\nSSL und IETF Standard Transport Layer Security\\n\\nSimple Protected Negotiation\\n\\nIP security\\n\\n2.1 Komponenten des NT Sicherheitsmodells\\nDas NT Sicherheitsmodell basiert auf 3 wesentlichen Komponenten:\\n\\n\\njede Workstation und jeder Server besitzt einen \\x91trust path\\x92 (Vertrauens\\nPfad) zu einem Domain Controler (DC); trust path wird durch eine autentifizierte\\nRPC Verbindung beim Netlogon eingerichtet; Sicherheitskanäle stellen\\nVerbindungen zu anderen Domänen des Netzwerkes her (interdomain trust\\nrelationships); Sicherheitkanal werden zur Überprüfung und dem\\nErhalt von Sicherheitsrelevanten Daten (einschließlich von Security\\nIdentifieres) genutzt\\n\\njede Operation wird mit den Ausführungsrechten des den Dienst anfordernden\\nClients ausgeführt; dies basiert auf den security access token, welcher\\ndurch die Local Security Authority erstellt wurde; wird durch alle Win\\nNT Dienste unterstützt.\\n\\nWin NT Kernel unterstützt objektorientierte Zugangskontrolle; dies\\ngeschieht durch die Überprüfung der SID im access token mit den\\ngarantierten Rechten definiert in der access control list (ACL); jedes\\nObjekt besitzt eigene ACL; eine Überprüfung erfolgt bei jedem\\nZugriff auf das Objekt\\n\\n2.2 Kerberos Autentifikation in Win NT\\nDer prinzipielle Ablauf bei der Autentifikation läuft wie im ersten\\nAbschnitt beschrieben ist ab. Win NT hat dies nur durch einen public key\\nerweitert.\\nVerfügbar ist Kerberos für DCOM, autentifizierte RPC und für\\njede Anwendung, die SSPI benutzt. SSPI ist ein Win32 Sicherheitinterface,\\nwelches unter Win NT seit der Version 3.5 verfügbar ist und auch von\\nWin 95 unterstützt wird. SSPI nutzt die selben Architektur-Konzpte,\\nwie Generic Security Services API, so müssen Dienste keine Details\\ndes Sicherheitsprotokolls wissen, um es nutzten zu können.\\nJeder Domain Conroler bietet ein Kerberos Key Distribution Center (KDC)\\nund ein Active Directory an. KDC und Active Directory sind priviligierte\\nProzesse, beide gehen mit geheimen Informationen um (z. B. Paßworte).\\nDas Aktive Directory regelt das Update von Kopien der DB, das Erstellen\\nvon Kopien der DB, erstellen neuer Nutzer, ändern von User-, Gruppenzugehörigkeiten,\\nändern von Paßworten .... Das Ticket enthält zusätzlich\\nAutorisationsdaten.\\n2.3 Kerberos Protokoll und Win NT Autorisation\\nDie Impersonifizierung benötigt Informationen über User- und\\nGruppenmitglieder SID\\x92s. SID\\x92s werden von einer Domäne mit Vertrauensverhältnis\\nausgegeben. Bei der Verwendung von NTLM erhält der Server diese SID\\x92s\\ndirekt von dem DC unter Verwendung des Netlogon Secure Chanels. Wird dagegen\\nKerberos verwendet, enthält das Ticket zusätzlich diese Informationen.\\nBereits beim ersten Login werden Autoristationsdaten an das Ticket für\\nden Ticket Granting Server angehängt. Die Autoristationsdaten werden\\nfür die Session Tickets einfach kopiert, oder bei einem mehrdomänen\\nNetzwerk können auch weitere Gruppenmitglieder SID\\x92s durch den KDC\\nangehängt werden.\\nIn alten Netzwerken stellt NTLM ein großes Sicherheitsrisiko dar.\\nDas Ziel sollte daher sein NTLM in reinen NT 5 Netzwerken auszuschalten.\\n\\n\\n',\n",
       " '\\n\\n\\n\\nSimple URLs for Search Engine Robots: SearchTools Report\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHome\\n\\n\\n\\n\\nGuide\\n\\n\\n\\n\\nTools List\\n\\n\\n\\n\\nNews\\n\\n\\n\\nBackground\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nAbout Us\\n\\n\\n\\n\\n  \\nSearch Tools Reports\\nGenerating Simple URLs for Search Engines \\n\\n\\t \\n\\n\\nSearch engines generally use robot crawlers to locate searchable \\n\\n    pages on web sites and intranets (robots are also called crawlers, \\n\\n    spiders, gatherers or harvesters). These robots, \\n\\n    which use the same requests and responses as web browsers, read pages and \\n\\n    follow links on those pages to locate every page on the specified servers. \\n\\n  \\n Dynamic URLs\\n\\nSearch engine robots follow standard links with slashes, but dynamic pages, \\n\\n      generated from databases or content management systems, have dynamic URLs \\n\\n      with question marks (?) and other command punctuation such as &, %, \\n\\n      + and $. \\nHere\\'s an example of a dynamic URL which queries a database with specific \\n\\n      parameters to generate a page:\\n\\nhttp://store.britannica.com/escalate/store/CategoryPage?\\n\\n        pls=britannica&bc=britannica&clist=03258f0014009f&cc\\n\\n        =eb_online&startNum=0&rangeNum=15\\n\\nAll those elements in the URL? They\\'re parameters to the program that generates \\n\\n      the page. You probably don\\'t even need most of them.\\nHere is a theoretical version of that URL rewritten in a simple form: \\n\\nhttp://store.britannica.com/Cat/B-online/03258f0014009f/s0-e15/\\n\\nIf you look at Amazon\\'s URLs, you\\'ll see they contain no indication to \\n\\n      a robot that they\\'re pointing into a database, but of course they are.\\n\\nProblems with Dynamic URLs\\n\\nSome public search engines and most site and intranet search engines will \\n\\n      index URLs with dynamic URLS, but others will not. And because it\\'s difficult \\n\\n      to link to these pages, they will be penalized by engines which use link \\n\\n      analysis to improve relevance ranking, such as Google\\'s PageRank algorithm. \\n\\n      In Summer 2003, Google had very few dynamic URL pages in the first 10 \\n\\n      pages of results for test searches. \\nWhen pages are hidden behind a form, they are even less accessible to search \\n\\n      spiders. For example, Internet Yellow Pages sites often require users to \\n\\n      type a business name and city. For a search engine to index these pages \\n\\n      requires special-case programming to fill in those fields. Most webwide \\n\\n      search engines will simply skip the content. This is sometimes referred \\n\\n      to as the \"deep web\" or the \"invisible web\" -- valuable \\n\\n      content pages that are invisible to search engine robots, (internal and \\n\\n      external), do not get indexed and therefore can\\'t be found by potential \\n\\n      customers and users.\\nSearch engine robot writers are concerned about their robot programs getting \\n\\n      lost on web sites with infinite listings. Search engine developers call \\n\\n      these \"spider traps\" or \"black holes\" -- sites where \\n\\n      each page has links to many more programmatically-generated pages, without \\n\\n      any useful content on them. The classic example is a calendar that keeps \\n\\n      going forward through the 21st Century, although it has no events set after \\n\\n      this year. This can cause the search engine to waste time and effort, or \\n\\n      even crash your server.\\nReadable URLs are good for more than being found by local and webwide search \\n\\n      engine robots. Humans feel more comfortable with consistent and intuitive \\n\\n      paths, recognizing the date or product name in the URL. \\nFinally, by abstracting the public version of the URL, it will not be dependent \\n\\n      on the backend software. If your site changes from Perl to Java or from \\n\\n      CFM to .Net, the URLs will not change, so all links to your pages will remain \\n\\n      live. \\n\\nStatic Page Generation or URL Rewriting?\\n\\nThe simplest solution is to generate static pages from your dynamic data \\n\\n      and store them in the file system, linking to them using simple URLs. Site \\n\\n      visitors and robots can access these files easily. This also removes a load \\n\\n      from your back end database, as it does not have to gather content every \\n\\n      time someone wants to view a page. This process is particularly appropriate \\n\\n      for web sites or sections with archival data, such as journal back issues, \\n\\n      old press releases, information on obsolete products, and so on. \\nFor rapidly-changing information, such as news, product pages with inventory, \\n\\n      special offers, or web conferencing, you should set up automatic conversion \\n\\n      system. Most servers have a filter that can translate incoming URLs with \\n\\n      slashes to internal URLs with question marks -- this is called URL \\n\\n      rewriting. \\nFor either system, you must make sure that of the rewritten pages has at \\n\\n      least one incoming link. Search engine robots will follow these links, and \\n\\n      index your page. \\nIf you have database access forms, dynamic menus, Java, JavaScript or Flash \\n\\n      links, you should set up a system to generate listings with entries for \\n\\n      everything in your database. This can be chronological, alphabetical, but \\n\\n      product ID, or any other order that suits you. Search engine robots can \\n\\n      only follow links they can find, so be sure to keep this listing up to date.\\n\\nURL Rewriting and Relative Link Problems\\n\\nPages which include images and links to other pages should use relative \\n\\n      links from the root rather than from the current page. This is because the \\n\\n      browser sends a request for each image by putting together the host and \\n\\n      domain name (www.example.com) with the relative link \\n\\n      (images/design44.gif). This is based on the Unix file name \\n\\n      conventions of current directory, child and parent (../) directories. Because \\n\\n      URL rewriting mimics directory path structures, it confuses the browsers.\\n If your original file linked to the local images directory, \\n\\n      then rewritten the link would break. \\nOriginal URL for this page\\n\\nwww.example.com/prod=?int+7=2&2&4\\n\\nRewritten URL looks like this\\n\\nwww.example.com/prod/int/7/224/\\n\\nIf the relative link to the dynamic location images directory would be:\\n\\ndyn/images/design44.gif\\n\\nin the rewritten path, this would be interpreted as\\n\\nwww.example.com/prod/int/7/224/dyn/images/design44.gif \\n\\ninstead of the correct path, which would be something like this:\\n\\nwww.example.com/dyn/images/design44.gif \\n\\n Because the path to the images subdirectory is wrong, the image is lost.\\nSolution One: Use Absolute Links\\nTo avoid confusion, use links that start at the host root, that means paths \\n\\n      which all start at the main directory of your site. A slash at the start \\n\\n      tells the browser that it should not try to find things in the local directory, \\n\\n      but just put the host name and the path together. The disadvantage is that \\n\\n      if you move or change the directory hierarchy, you\\'ll need to change every \\n\\n      link which includes that path.\\nUsing the same example above, change the relative (local) link within the \\n\\n      generated page from this:\\n\\nimages/design44.gif \\n\\n to an absolute link that points at the correct directory:\\n\\n/dyn/images/design44.gif\\n\\nSimilarly, a link to a page in a related directory (either static or rewritten)\\n\\nperpetualmotion/moreinfo.html\\n\\nWould require the entire path:\\n\\n/prod/int/perpetualmotion/moreinfo.html\\n\\nNote that if you change the directory name from \"prod\" to \"products\" \\n\\n      or take the \"int\" directory out of the hierarchy, you\\'ll have \\n\\n      to change every one of those URLs.\\nSolution Two: Rewrite the Links Dynamically\\nUsing a mechanism like URL rewriting, you can generate a path to the correct \\n\\n      directory and program your server to create absolute links within the pages \\n\\n      as it\\'s generating them. \\nFor example, if all the images are in the directory \\n\\nwww.example.com/dyn/images/\\n\\nYou could create a variable with the path \"/dyn/images/\" \\n\\n      and the server would put that before all the relative urls to images.\\n\\nChecking URLs on Your Site\\n\\n\\n\\nPerform a sanity check - make sure that your site does not generate \\n\\n      infinite information. For example, if you have a calendar, see if it shows \\n\\n      pages for years far in the future. If it does, set limits.\\n\\nGenerate pages or Choose and implement a URL rewriting system \\n\\n      - see below for articles and products.\\n\\n\\nCheck relative links for images and other pages.\\n\\n\\nCreate automatic link list pages so there are links to the pages \\n\\n      generated dynamically.\\n\\n\\n Test with your browser.\\n\\n\\nTest with a robot You can use a linkchecker, local search crawler \\n\\n      or site mapping tool to make sure these URLs work properly, don\\'t have duplicates, \\n\\n      and don\\'t generate infinite loops.\\n\\n\\n\\n\\nArticles About URL Rewriting\\n\\n\\nSearch \\n\\n      Engines and Dynamic Pages (members only) SearchEngineWatch.com, \\n\\n      updated April, 2003 by Danny Sullivan \\n\\n      Very clear description of the process of rewriting URLs for getting pages \\n\\n      indexed by public search engines, and includes links to articles and rewrite \\n\\n      tools.\\n\\n\\nTowards \\n\\n      Next Generation URLs Port 80 Software Archive, March 27, 2003 by \\n\\n      Thomas A. Powell & Joe Lima  \\n\\n      Helpful explanation of how to address problems with URLs, from domain name \\n\\n      spelling through generating static pages and rewriting query strings.\\n\\n\\nMaking \\n\\n      Dynamic and E-Commerce Sites Search Engine Friendly SearchDay (SearchEngineWatch), \\n\\n      October 29, 2002 by Catherine Seda \\nA report from a panel at the Search Engine Strategies 2002 conference \\n\\n      provides strong justification for simplifying URLs, and strategies to work \\n\\n      around the problem when the dynamic URLs must remain.\\n\\n\\nUsing \\n\\n      ForceType For Nicer Page URLs DevArticles.com, June 5 2002 by Joe \\n\\n      O\\'Donnell \\n\\n      Apache\\'s ForceType directive doesn\\'t require access to the main configuration \\n\\n      file, rather it uses a local \".htaccess\" file for rewriting the \\n\\n      URLs. The article includes excellent examples for implementing this with \\n\\n      PHP. \\n\\n\\n Making \\n\\n      \"clean\" URLs with Apache and PHP evolt.org, March 29, 2002 by stef \\n\\n      \\nGives some context for dynamic sites, describes a solution using both \\n\\n      Apache ForceType and PHP. Comments offer some interesting thoughts about \\n\\n      the value of stable URLs.\\n\\n\\nSearch \\n\\n      Engine Friendly URLs (Part II) evolt.org, November 5, 2001 by Bruce \\n\\n      Heerssen\\n\\n      Describes how to generate dynamic absolute path links using PHP.\\n\\n\\nHow to Succeed with \\n\\n      URLs A List Apart; October 21, 2001 by Till Quack\\n\\n      Using the Apache .htaccess to direct requests to a PHP script which converts \\n\\n      the URL to an array, ready to send the query to the database or other dynamic \\n\\n      source. Includes helpful comments, checks for static pages, default index \\n\\n      pages, skipping password-protected URLs, and handling non-matching requests. \\n\\n      Also covers security protections, showing how to strip inappropriate hacking \\n\\n      commands from URLs.\\n\\n\\nSearch \\n\\n      Engine Friendly URLs with PHP and Apache evolt.org, August 21 2001 \\n\\n      by Garrett Coakley\\n\\n      Very simple and easy to understand introduction.\\n\\n\\n Optimization \\n\\n      for Dynamic Web Sites Spider Food site, August 13, 2001\\nOverview, with examples, of the value of converting dynamic URLs to \\n\\n      static ones.\\n\\n\\n Search Engine-Friendly \\n\\n      URLs PromotionBase SitePoint, August 10, 2001 by Chris Beasley \\n\\n      Three ways to convert dynamic URLs to simple URLs using PHP with Apache \\n\\n      on Linux. These include using the $PATH_INFO variable, the .htaccess error \\n\\n      handling, or Apache\\'s .htaccess ForceType directive, using the path text \\n\\n      to filter certain URLs to the PHP application handler.\\n\\n\\nInvite \\n\\n      Search Engine Spiders Into Your Dynamic Web Site Web Developer\\'s \\n\\n      Journal; February 28, 2001 by Larisa Thomason\\n\\n      Nice introduction to simple URL rewriting, useful warning about relative \\n\\n      link problems.\\n\\n\\n Building \\n\\n      Dynamic Pages With Search Engines in Mind PHPBuilder.com, June 2000 \\n\\n      by Tim Perdue\\n\\n      Details of a PHP setup which can scale up to 200,000 pages and 150,000 page \\n\\n      views per day. Automatic generation of the page header also includes meta \\n\\n      tags. In this example, the pages are arranged by country, state, city and \\n\\n      topic, so the URLs generate those parameters for the database queries. Comments \\n\\n      recommend sending a success HTTP status header before every page returned: \\n\\n      Header(\"HTTP/1.1 200 OK\"); running as an Apache modules \\n\\n      vs. CGI on Windows, use of the ForceType directive, and Apache 2 compatibility.\\n\\n\\nURLs! URLs! URLs! A \\n\\n      List Apart; June 30, 2000 by Bill Humphries\\n\\n      Recommends creating a simple system for URLs and mapping them to the backend. \\n\\n      Describes using Apache\\'s mod_rewrite component, optionally using \\n\\n      .htaccess. \\n\\n\\n URL Rewriting Tools\\n\\n Apache\\n\\nApache mod_rewrite \\n\\n          documentation - canonical documentation\\nA Users \\n\\n          Guide to URL Rewriting with the Apache Webserver - useful but does \\n\\n          not quite explain the general case.\\nModule \\n\\n          mod_rewrite Tutorial - Part 3 explains how to convert dynamic URLs \\n\\n          to simple ones\\nSearch Engine Friendly \\n\\n          URLs with mod_rewrite -  \\n\\n          (April 2003) - simple instructions, very clear.\\n\\n\\nPerl\\n\\nUsing the environment variables Path_Info and Script_Name provides \\n\\n          access to the dynamic URL including the \"query string\" (the \\n\\n          part after the question mark). Converting the query information into \\n\\n          a single code and adding it to the path creates a static URL.\\n\\n\\nPHP\\n\\nsee articles above, especially the one on Using ForceType, \\n\\n          and PortalPageFilter below\\n\\n\\nMicrosoft IIS and Active Server Pages (ASP) \\n\\n      These filters recognize slash-delimited URLs and convert them to internal \\n\\n      formats before the server gets them. \\n\\n      \\nASPSpiderBait \\n\\n          - package converts the PATH_INFO part of an HTTP header request: the \\n\\n          user doesn\\'t see the punctuation, just placeholder letters. The filter \\n\\n          replaces the placeholders with punctuation them before the server can \\n\\n          see them. $100 per server.\\nISAPI_Rewrite - IIS ISAPI \\n\\n          filter written in C/C++, simple rule for dynamic conversion. Lite version \\n\\n          is free, Full version, single server, $46, enterprise license, $418\\nIISRewrite \\n\\n          - IIS filter, works much like mod_rewrite, examples include converting \\n\\n          dynamic to simple URLs. $199.00 per server\\nPortalPageFilter \\n\\n          - a high-priority C++ ISAPI filter for ASP\\nXQASP - high-performance \\n\\n          NT IIS C++ or Unix Java filter for ASP. $99 to $2,100 depending on scope\\n\\n\\nColdFusion (CFM) \\n\\nMay have an option to reconfigure the setup, replacing the ? with \\n\\n          a /\\nUse the Fusebox framework, <cf_formurl2attributessearch> \\n\\n          tag. \\n\\n\\nLotus Domino Servers\\n\\nWeb \\n\\n          Site rules and global Web settings Lotus Domino Administrator \\n\\n          6 Help \\n\\n\\nWebSTAR, AppleShareIP, etc. (Macintosh OS 9 and OS X) \\n\\nWelcome \\n\\n          module by Andreas Pardeike (included in WebSTAR 4.5 and 5)\\n\\n\\n\\n\\nPage Updated 2003-07-29\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHome\\n\\n\\n\\n\\nGuide\\n\\n\\n\\n\\nTools Listing\\n\\n\\n\\n\\nNews\\n\\n\\n\\nBackground\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nAbout Us\\n\\n\\n\\n\\n\\nSearchTools.com\\n\\n\\tCopyright © 2002-2003 Search Tools \\n\\n\\tConsulting \\n\\n\\n\\n',\n",
       " '\\n\\n\\n\\n\\nPRESS RELEASE\\n\\n\\nPRESS RELEASE\\n\\nFor Immediate Release\\n\\n\\xa0\\nElsevier Science Launches Scirus.com \\nThe First Comprehensive Search Engine Dedicated to Science\\n\\nAmsterdam, April 1, 2001 \\x96 Elsevier Science, the leading international publisher of scientific information, announced the launch of Scirus (www.scirus.com) \\x96 a search engine dedicated to science \\x96 as part of the ScienceDirect® family of products.  Scirus is the world\\'s most comprehensive search engine specifically designed for finding highly relevant scientific information. Using the latest in search engine technology \\x96 developed by Fast Search & Transfer, ASA (FAST) \\x96 Scirus pinpoints both free and access-controlled scientific information sources that conventional, generic search engines cannot find. \\n\\n\"We supply a tailor-made tool specifically for the scientist,\" said Derk Haank, CEO Elsevier Science.  \"It really fulfills the desire of scientists to put important refereed research in the context of everything else that\\x92s available.  People don\\x92t like to search in boxes \\x96 they want to search the universe.  The way I see it, Scirus is a targeted entrance to the universe.\"  \\nScirus searches the whole world-wide-web, including access-controlled sites.  It currently covers more than 60 million science-related pages and is capable of reading non-text files in formats such as PDF and Postscript.  Scirus yields more precise results because it indexes complete documents.  It can direct users to more peer-reviewed articles than any other search engine.\\nScirus is powered by the same core technology behind FAST\\x92s AllTheWeb.com, the world\\x92s freshest and most comprehensive search engine.  Scirus also leverages specialized linguistic search algorithms specifically designed to target scientific content.  FAST is providing hosting services for the Scirus search engine.\\nThe Internet has revolutionized the research process. Scientists and researchers have a whole new set of expectations for information delivery.  Scientists around the world tested the beta version in months preceding the launch of Scirus.  Their feedback was used to create a user experience specifically tailored to meet the needs of scientists.  \\n\"The advantages of being able to find large amounts of information using general search engines are counteracted by the potential of a tremendous loss of precision,\" said Dr. Franz Guenthner, Professor at Ludwig-Maximilians University in Munich, Germany.  \"Leveraging FAST search technology, Scirus bridges this gap by providing \\x96 in a common index \\x96 the possibility of querying both web documents and access controlled material through the same search window.\"  \\nIn addition to scientific information that is freely available on the Web, Scirus will cover Elsevier Science\\x92s leading information resources: ScienceDirect, BioMedNet and Chemweb.  Elsevier Science is currently in final negotiations with other scientific publishing companies to make their proprietary databases searchable through Scirus.\\nScirus is also available as a Web search capability on ScienceDirect (www.sciencedirect.com), complementing both its breadth of scientific literature and the scientific content available via its extensive linking capabilities. \\nAbout Fast Search & Transfer ASA\\n\\nFast Search & Transfer ASA (FAST) powers the \"information-on-demand\" economy through a powerful platform of search and real-time filter solutions.  FAST technology unlocks the ever-expanding volume of information on the wired and wireless Internet, and within enterprise networks and intranets.  AllTheWeb.com, FAST\\x92s showcase product, is the world\\x92s most comprehensive and freshest Internet search engine.  Information-on-demand market leaders relying on FAST\\'s highly scalable offerings include Reed Elsevier, Dell, Ericsson, KPNQwest, LookSmart, Lycos and TIBCO Software.  Shares in FAST are traded OTC in Oslo, Norway.\\n\\nAbout Elsevier Science \\nElsevier Science (www.elsevier.com) is the world\\x92s largest scientific, technical and medical information provider, publishing journals as well as books and secondary databases.  ScienceDirect offers its subscribers desktop access to the full text of nearly 1,200 journals, representing more than 3 million full-text articles, published by Elsevier Science and other leading STM publishers. Elsevier Science is part of the Reed Elsevier plc group (www.reedelsevier.com), a leading international publisher and information provider.  Operating in the scientific, legal and business-to-business industry sectors, Reed Elsevier provides information solutions to professional end users, with increasing emphasis on the Internet as a delivery method. \\n# # #\\nContacts:  \\nPaula Reeves\\tSandra de Gelder\\nPurple Cow\\tElsevier Science\\n+31-(0)20-3051382\\t+31-(0)20-4853851\\npaula@purple-cow.com s.gelder@elsevier.com\\n\\n\\n\\n\\n\\n',\n",
       " '\\n\\n\\n\\n The ABC of adaptation: Towards a software architecture for \\n    adaptation-centered CBR systems\\nThe ABC\\xa0of adaptation: \\nTowards a software architecture for \\n    adaptation-centered CBR systems \\n  Enric Plaza and Josep-Lluí s Arcos        \\n        IIIA - Artificial Intelligence Research Institute\\n        CSIC - Spanish Council for Scientific Research\\n        Campus UAB, 08193 Bellaterra, Catalonia, Spain.\\n        Vox: +34-93-5809570, Fax: +34-93-5809661\\n{enric,arcos}@iiia.csic.es\\nhttp://www.iiia.csic.es \\nNov 12, 1999\\n  \\n\\n\\n IIIA-RR-99-22 -  November 1999, IIIA Research Report N. 22 \\n\\n A shorter version of this article has been published: \\n\\nE. Plaza, J.-L. Arcos (2000), \"Towards a software architecture for case-based reasoning systems\", \\nFoundations of Intelligent Systems, 12th International Symposium, ISMIS \\n2000. Ras, Z. W. and Ohsuga, S., (Eds.), Lecture Notes in Computer \\nScience 1932. \\n\\nhttp://www.springer.de/cgi-bin/search_book.pl?isbn=3-540-41094-5\\n\\n Abstract\\n        We present a software architecture model of adaptation in CBR.  A \\nsoftware \\n    architecture is defined by its components and their connectors.  We \\npresent \\n    a software architecture for CBR systems based on three components (a \\ntask \\n    description, a domain model, and adaptors) connected by a type of \\n    connectors called bridges.\\n    Adaptors are basic inference components that perform specific\\n    transformations to cases.\\n    Two kinds of adaptors are introduced: domain adaptors \\n(d-adaptors) and\\n    case-based adaptors (c-adaptors).\\n    Adaptors are applied to a given problem, performing search until a \\n    sequence of adaptor instantiations is found such that a solution \\n    is achieved.\\n    Thus, in the ABC\\xa0architecture \\nadaptation is viewed as a search process \\n    on the space of adaptors.\\n    We describe how the ABC\\xa0components \\nhave been used in the SaxEx\\xa0\\n    application, a CBR system for generating expressive musical phrases.\\n\\n\\n\\n1\\xa0\\xa0Introduction\\n\\nThe goal of software architectures is learning from system developing \\nexperience in order to provide the abstract recurring patterns for \\nimproving further system development.  As such, software architectures \\ncontribution is mainly methodological in providing a way to specify \\nsystems.\\nIn this paper we present a software architecture for adaptation in \\nCBR-called Adaptors-Bridges-Connectors software \\narchitecture (ABC)-based on the notion of \\nconnectors and inspired on \\nobject-oriented and component-based methodologies.\\n\\n\\nThe three main elements of the ABC\\xa0software architecture are (i) a task \\ndescription-characterizing the goal that a CBR system pursues; (ii) a \\ndomain model-characterizing the ontology and properties of the knowledge \\ncontent; and (iii) a library of adaptors-performing transformations to \\ncase-specific models.  These three elements are connected with a special \\nkind of connectors called bridges.\\n\\n\\nProblem solving in ABC\\xa0is considered as \\nthe construction of a \\ncase-specific model.  ABC\\xa0follows the ``problem solving as \\nmodeling\\'\\' view, that is to say, solving a problem consists of building a \\nmodel specific to the problem that satisfies the task requirements.  In \\nthis view, a knowledge system uses a domain model to enlarge the input \\nmodel until a complete and correct case-specific model is built-where \\n``complete and correct\\'\\' are with respect to the requirements of the task.\\n\\n\\nWe have considered two kinds of adaptors: domain adaptors \\n(d-adaptors) and case-based adaptors (c-adaptors).  \\nD-adaptors use some domain-specific knowledge to transform the \\ncase-specific model (in a way specified by the adaptor\\'s \\ncompetence).  C-adaptors also transform the case-specific \\nmodel but use domain knowledge that includes precedent cases retrieved \\nfrom \\ncase memory.\\n\\n\\nAdaptors are applied to the case-specific model, performing search until a \\nsequence of adaptor instantiations is found such that transforms the \\ninitial case-specific model into a correct case-specific model that \\nsatisfies the task goals.  Thus, adaptation is viewed as a search process \\non the space of adaptors.  Since new adaptors can be applied to the first \\nadapted object, several search strategies (such as depth-first, \\nbreadth-first, and beam search) are possible.\\n  \\n\\n    We will show how the ABC\\xa0theory has \\nbeen applied in the SaxEx\\xa0\\napplication, a complex real-world case-based reasoning system for \\ngenerating expressive performances of melodies based on examples of human \\nperformances that are represented as structured cases.\\n\\n\\n\\n1.1\\xa0\\xa0A software architecture approach\\n\\n\\nSoftware engineering is actively developing methodologies for software \\nreuse based on components, design patterns and software architectures.  \\nThe aim of these approaches is to learn to reuse the effort spent in \\ndeveloping (designing and implementing) software systems.  While \\ncomponent-based software focuses on reuse of implementation, design \\npatterns and software architectures focus on reuse of the design \\nresults.  It is a long term goal to develop such methodology for CBR \\nsystems, and a first step is the collection of case experiences \\ndeveloping CBR systems as done in the CBR-Product Experience Base \\nCBR-PEB1.  A \\nsecond step is developing a vocabulary to describe CBR \\nsystem designs. In this paper we propose a draft of such a \\ndescription scheme based on software architectures. Specifically, in \\nthis paper we will present a summary of our current approach and a \\nspecific example of applying it to the phase of reuse and adaptation \\nin CBR.\\n\\n\\nIn general terms, a software architecture describes the (i) \\ncomponents, (ii) connectors, and (iii) a configuration of how the \\ncomponents should be connected [9]. We can consider CBR \\nsystems as a specific variant of knowledge systems that furthermore \\nuse experiential knowledge [5]. Because of this we have \\ntaken  UPML, a software architecture being developed for reuse of \\nknowledge systems, and we are developing a variant adequate for CBR \\nsystems. The Unified Problem-solving Method Development Language UPML \\nis currently in development by the IBROW\\n3\\xa0consortium, and the first \\nversion is currently released [8]. \\nAlthough UPML can still \\nhave future modifications we expect them to be minor and maintain \\nstable the core ideas.\\n\\n\\nA software engineering concept of interest, from the CBR stance, are \\nconnectors, and a specific kind of connectors called ``component \\nadaptors\\'\\'.  A \\nconnector is a device that specifies, in an abstract way, the \\ninteraction process and properties between two components, denoted X \\n\\\\leftrightarrows Y.  In general, however, it may be the case that \\ntheir interaction protocols are not (directly) compatible.  In this \\nsituation, a kind of connector called an component adaptor \\n[17] is needed \\nto bridge the differences in \\ninteraction protocol between two objects.  The most common kind of \\ncomponent adaptor is a wrapper W, denoted X \\\\leftrightarrows W(Y).  \\nA wrapped component W(Y) can now interact with the X component.  \\nWhat is hidden here is that there is a new component Z interacting \\nwith Y using its protocol (Z \\\\leftrightarrows Y).  Since it also \\ninteracts with X in its own protocol the wrapper effect is that of \\nan intermediary or a broker: X \\\\leftrightarrows (Z \\\\leftrightarrows Y).  A \\nsecond kind of adaptor is one that produces a new component \\nfrom Y given X, namely A(Y,X) Æ Y¢ such that X \\\\leftrightarrows Y¢. In what follows we propose to view case reuse \\n(adaptation) in the light of software component reuse and come up \\nwith a software architecture adequate for this objective.\\n\\n\\nThe organization of this paper is as follows.  In Section\\xa02 we \\npresent the \\nABC\\xa0architecture.  Section\\xa03 \\ndescribes how two different families of \\nadaptors (c-adaptors and d-adaptors) are incorporated in the Noos\\xa0\\nlanguage.  Section\\xa04 shows the use of c-adaptors and d-adaptors in \\nthe \\nSaxEx\\xa0application.  Finally, in \\nSection\\xa05 we present the conclusions and \\ndiscuss related work.\\n\\n\\n\\n2\\xa0\\xa0The ABC\\xa0architecture\\n\\n\\nThe three main elements of the ABC\\xa0software architecture are (i) a \\ntask description, (ii) a domain model, and (iii) a library of adaptors.  \\nFigure 1 shows these three elements connected with \\na special \\nkind of connector called bridge.  In addition, the problem to be solved is \\ncalled input in the figure and for \\nsimplicity we will include the \\ncase base into the domain model element.  More specifically, we will \\nconsider that each solved problem is a model per se, and we will call it \\ncase-specific model-in other words, it is the model of an \\nepisode of solving that problem [5].\\n\\n\\nThese three elements are taken from UPML where the main goal is the \\nreuse of Problem Solving Methods (PSMs); since our goal is the \\nreuse of cases we propose the specific architectural \\nvariation where adaptors play the role of PSMs.  This transformation \\nmakes sense since PSMs are the components that perform the inferences \\nfor a knowledge system to build the case-specific model of \\nthe problem (i.e. the ``solution\\'\\' to the problem).  In our approach \\nthe final case-specific model is build by the adaptors that transform \\nthe case-specific model imported from the case(s) retrieved \\nfrom the case-base.\\n\\n\\n\\n  Figure\\n      Figure 1: The ABC\\xa0software architecture consists of three \\n    elements: a task description, a domain model, and a library of \\nadaptors. \\n    These three elements are connected by connectors called bridges. \\n    The problem to be solved is called input \\nin this picture.\\n\\n\\n\\n\\nTasks, domain models, and adaptors are conceptually distinct entities, \\nalthough in practice CBR systems use an implicit description of the \\ntask and domain knowledge is tightly integrated with the CBR engine.  \\nFrom a methodological stance, however, it is better to consider they \\nseparate and possibly coming from distinct sources.  For instance, \\nconcerning domain models, current work on ontologies2 provide shared and reusable \\ncomponents that can be in principle very useful to CBR development.\\n\\n\\nIn a similar manner, specification of tasks is also being studied \\n[15] to provide a vocabulary \\ncapable of describing tasks \\nacross a range of different domains of application, i.e. independent \\nof the domain-specific vocabulary.  For instance, a task description \\nof diagnosis [8] is specified in terms \\nof findings \\nand hypothesis.  A bridge is then \\nneeded to connect \\nin a meaningful way task descriptions and domain models.  For \\ninstance, in a medical diagnosis domain the bridge from the task \\ndescription to the domain model maps findings and \\nhypothesis to the terms manifestation and \\ncause respectively.\\n\\n\\nTherefore the methodological approach we are endorsing takes two main \\naspects of knowledge modeling techniques: explicit representation and \\nconceptual separation of tasks, domain knowledge, and adaptors. From \\nUPML software architecture [8] we adopt \\nthe bridge \\nconnectors among ABC\\xa0architecture \\ncomponents but we change the main \\nelements of the architecture for CBR systems. In the rest of this \\nsection we make explicit those components, while in later sections we \\nshow a particular adaptation engine developed following this \\nmethodology.\\n\\n\\nBefore considering in detail the components of the ABC\\xa0software \\narchitecture a point needs to be clarified.  It may seem ABC\\xa0deals \\nonly with the adaptation phase of CBR and not with the retrieval \\nphase.  In a certain way this is true, but we are not dismissing or \\nignoring case retrieval.  The ABC\\xa0software architecture focuses on \\ncase reuse and the rest of the CBR phases (retrieve, reuse, \\nrepair, and retain) is abstracted away, simply being considered part \\nof the knowledge contained in the domain model.  For instance, the \\nSaxEx\\xa0system shown in section \\n§\\xa04 has two main processes \\nbefore adaptation: given an input case-specific model SaxEx\\xa0uses \\ndomain knowledge (two musical theories) to analyze it and build \\na more complex case-specific model-however in the ABC\\xa0architecture \\nthis process is just considered part of the domain model.  Also, since \\nABC\\xa0does not specify control then it is \\nnot reflected that the \\nmusical analysis being performed previously to retrieval and \\nadaptation.  Concerning retrieval of cases (and subparts of cases, \\nalso considered cases in SaxEx) we consider \\nthat the set of \\ncase-specific models (the case base) is part of the domain model and \\nwe also consider that the criteria for assessing similitude and \\nrelevance of these precedent cases for the current problem is \\nknowledge contained in the domain model.\\n \\n\\n\\n2.1\\xa0\\xa0The ABC\\xa0components\\n\\n\\nTasks\\n\\n\\nA task provides a way to characterize what a CBR system is intended to \\nachieve. \\n\\n\\n\\nTask Description\\xa0\\xa0 consist of a task name and\\n    \\n\\n\\n  pragmatics (author, explanation, URL, last change date)\\n        \\n  ontology (the vocabulary)\\n        \\n  specification\\n    \\n\\n\\n  goals (expressions characterizing the output case-models)\\n    \\n\\n  preconditions (expressions characterizing valid input \\n        case-models)\\n    \\n\\n   assumptions (expressions characterizing requirements on \\n        domain knowledge)\\n    \\n\\n   The main elements for characterizing a task are \\ngoals, \\npreconditions and assumptions. These elements are described in some \\nlogical language, the option of which is open to the designer. \\nPreconditions state constraints to be satisfied by the problems to be \\nsolved (input case models). Goals specify properties to be satisfied \\nby the solved problem, i.e. by the output case-specific model. Finally \\nassumptions determine assumptions made by the task description upon \\nthe  content of the domain model.\\n\\n\\nDomain Models\\n\\n\\nDomain models are specified using a specific vocabulary (domain \\nontology) and is characterized by properties, assumptions, and \\ndomain knowledge. \\n\\n\\n\\nDomain Model Description\\xa0\\xa0 consist of a domain model \\nname and\\n\\n\\n\\n  pragmatics (author, explanation, URL, last change date) \\n    \\n  ontology (the vocabulary)\\n    \\n  specification\\n\\n\\n\\n  properties (meta-expressions characterizing domain \\n    knowledge)\\n    \\n  domain knowledge (expressions describing knowledge)\\n    \\n  assumptions (expressions characterizing assumptions on \\n    domain model)\\n\\n\\n    Properties and assumptions both are used to characterize \\nthe knowledge \\ncontent of a KB. These characteristics can be directly inferred from \\nthe domain knowledge or can be derived from requirements introduced by \\nother components of the specification.  While properties deal with \\ncharacteristics of the knowledge content assumptions deal with \\nexternal requirements like the environment of the system.  As before, \\nproperties, assumptions, and domain knowledge are expressed in a \\nspecific formal language of choice.\\n\\n\\nTask-domain bridge\\n\\nThe td-bridge is a connector that translates (refines) the \\ntask specification to a particular domain specified in domain-model.  \\nThis bridge may add assumptions (on domain knowledge) to ensure that \\nthe translation result is valid. The only formal requirement is the \\nunion of both task and domain specifications is logically consistent.\\n\\n\\nAdaptors\\n\\n\\nAn adaptor is a special kind of connector between case-specific \\nmodels-i.e. between ``models of cases\\'\\'. \\n\\n\\n\\nAdaptor Description\\xa0\\xa0 consist of an adaptor name \\nand\\n\\n\\n\\n  pragmatics (author, explanation, URL, last change date, \\n    \\n  ontology (the vocabulary)\\n    \\n  specification\\n    \\n\\n\\n  preconditions (expressions characterizing valid input \\n    case-models)    \\n        \\n  assumptions (expressions characterizing domain \\n        knowledge needed by the adaptor to succeed)    \\n\\t\\n competence (expressions characterizing the output \\n\\tcase-models)\\n\\n\\n    The preconditions of an adaptor specify the \\nrequirements to be \\nsatisfied by the input case-specific model for the adaptor\\'s result be \\na valid one.  The competence is a description of the transformation \\nresulting from the application of the adaptor.  Finally, the \\nassumptions express the kind of domain knowledge the adaptor requires \\nin order to be able to function.  These assumptions may enlarge the \\nrequirements on domain knowledge already specified by the task.  Since \\nthe case base is considered as a specific type of domain knowledge, \\ncase-based adaptation is considered to be realized by adaptors that \\nuse the experiential knowledge of the case base.  Case-based adaptors \\nare later discussed on §\\xa03.\\n\\n\\n\\n  Figure       \\nFigure 2: The adaptor is a connector between ``models of cases\\'\\', here \\ncalled \\n    case-specific models.\\n\\n\\n\\n\\nTask-Adaptor bridge\\n\\nThe ta-bridge works like the tb-bridge above but now is \\nconnecting the task goals with the adaptors competence.  Since the task \\ngoals specify the conditions for a problem to be correctly and \\ncompletely solved the problem solving process is finished when an \\nadaptor with a corresponding competence is available. \\n\\n\\nThere are different ways to realize the ta-bridge depending on the \\nstrategy used to implement adaptors.  A common strategy is designing a \\ncomponent library of adaptors.  Moreover, depending on the complexity \\nof the application domain the designers may implement one-shot \\nadaptors-i.e. adaptors with a competence that directly fulfills the \\ntask goals.  In more complex situations, the ``total\\'\\' adaptor need to \\nbe constructed from the elementary components in the adaptor library \\nto fit the needs of each particular problem.  Section 4 \\nbelow show that this is the implementation we have chosen for the \\nSaxEx\\xa0system. In this setting, \\nadaptation is then a search problem \\nover the space of adaptors whose goal is finding a combination of \\nadaptor instantiations such that the final competence satisfies, via \\nthe bridge, the task goals.\\n\\n\\nFrom the software architecture stance what is formally required to \\nestablish a ta-bridge is only that the adaptor competence logically \\nimplies the task goals. The ABC\\xa0architecture does not establish \\ncontrol constraints on the implementation nor distinguishes the \\nsituations where the adaptors already exist or have to be constructed \\nfrom elementary adaptor components (and whether this construction is \\nautomated or performed by hand).\\n\\n\\nAdaptor-Domain bridge\\n\\nThe da-bridge connects the assumptions upon domain knowledge \\nspecified by the adaptor with the domain model, in a similar way to \\nhow td-bridge maps task assumptions to the domain model. \\nSome requirements of PSM upon domain models have already been \\nestablished by connecting method with task and task with a domain.  \\nNow we only need to map the knowledge requirements that are \\nexclusively for the method.\\n\\n\\n\\n2.2\\xa0\\xa0ABC\\xa0and CBR systems \\ndesign\\n\\nThe very idea of software architectures is learning from system \\ndeveloping experience in order to provide the abstract recurring \\npatterns for improving further system development.  As such, software \\narchitectures contribution in mainly methodological in providing a way \\nto specify systems.  If we consider existing CBR systems and the ABC\\xa0\\narchitecture we can observe that ABC\\xa0is \\nmaking explicit issues that \\nCBR system developers already know but treat implicitly when developing \\nnew systems and that they are not explicit either on the actual CBR \\nsystem.  Let\\'s take the task of a CBR system, for instance.  The \\nspecific task a CBR system has always to be specified, albeit \\ninformally, in the system design phase. The ABC\\xa0approach considers this \\na specification of the task but also provides a specific way to relate \\nthat \\nspecification to  each other component of the architecture: \\npreconditions relate to the input problem, assumptions relates with \\nthe availability of knowledge, and goals relate to the search process \\nperformed by the CBR system.\\n\\n\\nFurthermore, let us consider domain knowledge.  Some CBR systems use \\ncases (and similarity) as the unique source of knowledge available to \\nsolve problems-e.g. instance-based learning approaches.  However, a \\ngreat number of CBR systems use domain knowledge, for different \\npurposes and in different ways, in addition to cases. Commonly, this \\ndomain knowledge is not described as such, but it is described by \\nexplaining the implementation of the CBR system. In other words, what \\nis described is the representation used to encode it (rules, \\nconstraints) and the role it plays in the system implementation \\n(mainly concerning control issues). It is our personal opinion that a \\nclarification of the role of domain knowledge in CBR systems is needed \\nto improve the understanding of CBR and the development of CBR systems.\\n\\n\\nAs a result of focusing on the adaptation process, ABC\\xa0suggests that \\nretrieval (and similarity assessment) is also a type of domain \\nknowledge.  In our approach, solving a problem is constructing a \\ncase-specific model of the ``input problem\\'\\'-as was established in \\nthe knowledge-level description of CBR [5].  A software \\narchitecture is a much refined level of description, so solving a \\nproblem in ABC\\xa0involves building a \\ncase-specific model that satisfies \\nthe task description goals.  Domain knowledge is used to perform the \\ninference necessary to build this model3. The ABC\\xa0architecture does not deal with \\ncontrol aspects of the implementation, thus the order in which \\ndomain-specific inference and case retrieval are performed is \\nunspecified. \\n\\n\\nIn the next section we show an example of the use of adaptors in a \\nparticular CBR system.  Since the SaxEx\\xa0system is already \\nimplemented we are not fully using ABC: we \\nare focusing on improving \\nSaxEx\\'s adaptation process so we will use \\nthe top half of Figure \\n1.  In other words, we have added an explicit \\nspecification of tasks, adaptors, and ta-bridges.  The lower half of \\nFig.  1 is implicit in the system, e.g. there is no \\nda-bridge in SaxEx\\xa0but we have used the \\nda-bridge during the design \\nphase to establish the knowledge needed-in the form of new \\nmusical knowledge that had to be added for the adaptors to \\nwork.  Other CBR systems can take a similar approach towards using the \\nABC\\xa0architecture: some parts of it are \\nused in the system design \\nphase but they are not explicitly present in the final system \\nimplementation.\\n\\n\\n\\n3\\xa0\\xa0Implementing Adaptors\\n\\n\\nWe will considered two kinds of adaptors: domain adaptors \\n(d-adaptors) and case-based adaptors (c-adaptors).  \\n``Transformational adaptation\\'\\' is realized by d-adaptors, i.e. \\nby adaptors that use some domain-specific knowledge to transform the \\ncase-specific model (in a way specified by the adaptor\\'s \\ncompetence).  Moreover, that domain \\nknowledge is the one \\nexplicitly required by the adaptor\\'s assumptions.\\n\\n\\n``Derivational replay\\'\\' is realized by c-adaptors.  Case-based \\nadaptors also transform the case-specific model but use some domain \\nknowledge that includes a precedent case retrieved from case \\nmemory4.  In the \\nsimplest \\nscenario there is only on retrieved case, but in CBR systems where \\nparts of cases are also cases each part can be adapted in a case-based \\nway by c-adaptors.  Derivational replay in planning is one example and \\nthe SaxEx\\xa0system below is another \\nexample.  As shown below, \\nadaptation in the SaxEx\\xa0system combines \\nd-adaptors and c-adaptors.\\n\\n\\nThe main issue to go from a specification like ABC\\xa0to an actual \\nimplementation is deciding how is 1) the representation of components \\nand bridges, and 2) the control scheme.  We are implementing adaptors \\nin Noos, a representation language designed \\nfor supporting knowledge \\nmodeling approaches to problem solving and learning [4] in \\nwhich different CBR systems have been built, including SaxEx.  \\nIn Noos\\xa0cases are represented as \\nfeature terms [13], a \\nformalism for representing structured cases in which any subpart of a \\ncase (feature term) is also a term-and thus is also a case.  \\nInference is provided by problem solving methods (PSMs) that use \\ndomain knowledge to build models (or parts of models).  A problem is \\nsolved when a a case-specific model is completed, and then it is \\nretained in the case base.  Retrieval is performed by specialized \\nPSMs, retrieval methods, that use domain knowledge or heuristic \\nprinciples to search the case base.  Concerning the control scheme, \\nNoos\\xa0inference is on demand, i.e. \\nfollows a lazy evaluation \\nstrategy. The chain of control is thus backwards: retrieval methods \\ndetermine the features of a case that they need, thus forcing the \\nevaluation of the PSMs that infer those features needed that were not \\npart of the input problem model. Moreover, c-adaptors use retrieval \\nmethods so  the retrieval process is in fact directed by the \\nadaptation strategy.\\n\\n\\nThe main ABC\\xa0elements incorporated in \\nNoos\\xa0are i) an explicit \\ndescription of a task, ii) adaptors, iii) and ta-bridges.  Since the \\nrest of the ABC\\xa0elements is obviated, \\nsome parts of this elements \\nneed not be represented explicitly: the reason being that Noos\\xa0will \\nnot be reasoning about them.  Thus, a task holds only goals and \\npreconditions, while adaptors holds only competence and preconditions.  \\nAssumptions are not present since we are not representing td-bridges \\nnor da-bridges.  The contents of these slots (goals, competence, \\npreconditions) are expressed by feature terms.  Satisfaction is \\nrepresented as feature term subsumption (\\\\sqsubseteq), thus a \\ncase-specific model C satisfies an adaptor preconditions \\nAPi when \\nAPi \\\\sqsubseteq C (APi \\nsubsumes C).\\n\\n\\nThe overall adaptation process is realized following an ``Adaptation as \\nSearch\\'\\' strategy.  The initial state is the case-specific model of \\nthe problem; this begins with the information given as input, but the \\ndomain PSMs can enlarge this model performing inference as needed.  \\nThe goal state is a complete and correct case-specific model CF \\nthat satisfies the task goals TG.  The ta-bridge provides a \\ntranslation from the task description vocabulary to the domain \\nvocabulary used in adaptors and case specific models.  Thus, the task \\ngoals expressed in domain vocabulary are obtained applying the bridge \\nBTA to the task goals BTA(TG) and \\ntherefore a solution is \\ndefined as a case-specific model CF such that\\nBTA(TG) \\\\sqsubseteq CF.\\n\\n\\nAdaptors are applied to the case-specific model, performing search \\nuntil a sequence of adaptor instantiations is found such that \\ntransforms the initial case-specific model into CF.  A \\nclassical \\nmeans ends analysis technique is used with the adaptors, where \\npreconditions establish if the adaptor is applicable to a \\nparticular case-specific model, and competence establishes \\nthe goals or subgoals achievable by instantiating the adaptor.  Since \\nNoos\\xa0provides automatic backtracking, \\nselection of adaptors and \\nadaptor instantiation following several search strategies -such as \\ndepth-first, breadth-first, and beam search- can be easily \\nimplemented for a particular CBR system.  An interesting issue left \\nfor future work is performing a case-based search of adaptor selection \\nand instantiation: since adaptors are feature terms, they are stored \\nin memory by Noos\\xa0and they are thus \\namenable to be retrieved.  This \\ncase-based adaptation process would be able to use both c-adaptors and \\nd-adaptors, unifying ``transformational\\'\\' and ``generative\\'\\' \\nadaptation in a case-based reuse of cases.\\n\\n\\n\\n4\\xa0\\xa0Adaptors in SaxEx\\n\\n\\nSaxEx\\xa0reuse process  uses both \\nc-adaptors and d-adaptors, thus unifying \\n``transformational\\'\\' and ``generative\\'\\' adaptation. Currently, \\nthe reuse process has a  first phase using c-adaptors and \\na second one using d-adaptors. We need now to summarize the SaxEx\\xa0\\nsystem in order to later focus on the use of the adaptors.\\n\\n\\n\\n  Figure       \\nFigure 3: The transformations of the case-specific model of the \\n    current problem in SaxEx. First, domain \\nknowledge adds musical \\n    analysis models; then adaptors generate an expressive score; next \\n    SMS manipulates the sound track to  add the expressivity features; \\n    and finally the reusable parts of the case-specific model are \\n    stores in the case base.\\n\\n\\n\\n\\nSaxEx\\xa0[3] is a system for generating expressive performances of\\nmelodies based on examples of human performances (for the moment SaxEx\\xa0is \\nfocused in tenor saxophone interpretations of standard jazz ballads).  \\nSaxEx\\xa0consists of two modules: a) a SMS \\nmodule of sound analysis and \\nsynthesis, and b) a CBR module implemented on Noos.\\nThe input of SaxEx\\xa0is musical phrase \\nwith a sound track and a score in \\nMidi format.  Thus the input case-specific model is C(sound,score), where \\nthe sound track is analyzed by SMS while the score is \\ntranslated by Noos\\xa0to feature terms.\\nDomain knowledge consists of two musical theories: \\nNarmour\\'s implication/realization (IR) model [12]  and Lerdahl and \\nJackendoff\\'s generative theory of tonal music (GTTM) [11].  \\nNoos\\xa0employs IR and GTTM to construct \\ntwo complementary models of the musical \\nstructure of the phrase. While the IR model holds an analysis of melodic \\nsurface, \\nthe GTTM model is concentrated on the hierarchical structures associated \\nwith a piece.\\n\\n\\nThus, an enlarged case-specific model is constructed: C(sound, score, ir, \\ngttm).  Models IR and GTTM are highly relational models that \\ninfer the most relevant relations among notes and groups of notes (see \\nFig.\\xa03).  These relational structure is \\nthen used by \\nretrieval methods to find in the case base other (parts of) phrases \\nthat share some musical structure.  The musical structure to be shared \\nis declared in the form of patterns that are used by \\nperspectives [1] to extract and \\nretrieve the parts of \\nrelevant musical phrases. \\nA retrieved case has a complete case-specific model C(score, ir, gttm, \\nexpression, performance) where i) expression is a symbolic \\ndescription of the expressive parameters-such as dynamics, rubato, \\nvibrato, and articulation-applied to each note; and ii) \\nperformance is a sound track with expressive performance.\\nThe goal of SaxEx\\xa0is now to infer by \\nCBR the expression model, \\nand later pass it to SMS that will perform the specified changes in the \\nsound track outputting a new expressive performance.\\n\\n\\nThe expression model holds knowledge such as: sound amplitude (dynamics); \\nnote anticipations/delays (rubato); note durations (rubato); attack and \\nrelease times (rubato and articulation); vibrato frequency and vibrato \\namplitude of notes; articulation mode of each note (from legato to \\nstaccato); and note attacks (allowing effects such as reaching the pitch \\nof \\na note starting from a lower pitch or increasing the noise component of \\nthe \\nsound).\\n\\n\\nRetrieval and the first phase of adaptation (using c-adaptors) are closely \\ncoupled.  In this phase, SaxEx\\xa0uses \\nc-adaptors considering one note, and \\nfinishes after treating all notes in the phrase.  C-adaptors use \\nperspectives to extract a particular context around the note consisting of \\nthe closer notes, where ``closer\\'\\' means those notes related to the \\ncurrent \\nnote either on the score or on the IR and GTTM models.  Retrieval methods \\ninstantiate their patterns on these contexts and use perspectives to find \\ncases that satisfy them.  The c-adaptor uses the retrieved (portions of) \\ncases to determine the expressivity features of the current note based on \\nthe expressivity patterns of retrieved cases.  \\nThere is one c-adaptor for each expressivity parameter.  Examples of \\nc-adaptors are majority, minority, strict \\nmajority and strict minority, continuity, \\nnon-continuity, and random.\\nFor instance, the majority c-adaptor chooses \\nthe values that were \\napplied in the majority of precedents, while the continuity \\nc-adaptor gives priority to precedent notes belonging to the same musical \\nsubphrase in the case base.\\nThe reuse strategy determines which adapter is used for each situation: \\nmusical expression being more an art than a science there is clearly no \\nunique correct solution for the adaptation process.  Reuse strategy can \\nalso be interactively set by SaxEx\\'s user, \\nas shown in [2].\\n\\n\\nAlthough the first phase of the reuse process focuses on individual \\nnotes it takes into account its immediate context as given by musical \\nknowledge.  However, there are other considerations that can be only \\nobserved taking a broader view and taking into account groups of \\nnotes.  Thus, the task description goals of SaxEx\\xa0specify conditions \\nthat have to be satisfied by the expressive features of note groups.  \\nThese goals establish two kinds of main criteria: smoothness and \\nvariation.  Moreover this criteria are established both over single \\nexpressive features (e.g. pitch, attack) and over the relationships \\namong expressive features (e.g. the relation between pitch and attack).  \\nSmoothness and variation are basically contradictory: the first tends \\nto iron out strong variations, while the second, variation, is against \\nrepetition of structures and thus strengthens variations.  The \\nresulting expressive performance deals with the trade-offs among them \\nwith the aim of striking an overall balance pleasant to the ear.  \\nMoreover, the criterion used in each part of the musical piece depends \\non the musical model. \\nFor instance, according to the melodic structure of a given melody, rough \\nchanges can be enforced in some notes and prevented in other.\\n\\n\\nThe ta-bridge instantiates these goals for the current problem in new \\nfeature terms that describe the states to be achieved; these \\ndescriptions are in terms of the vocabulary used in adaptors and the \\ndomain model.  Those descriptions that are not satisfied by the \\ncurrent case-specific model are matched with d-adapter\\'s competence to \\nselect those applicable.  Each adaptor selected is then instantiated \\non the current case-specific model and produces a new model where the \\nincorrections have been straighten out.  Notice that adaptors work on \\nnote groups, so the same adaptor may be instantiated on a number of \\noccasions over different sequences of the musical phrase.  \\n\\n\\nAn example of d-adaptor (based on ``smoothness\\'\\') is RAA that works upon a \\nrepetitive sequence of notes where attack time has been advanced-the \\nanticipation of the note attack produces an expressive effect that is \\ndestroyed by the iteration of the same effect.  The result produced by the \\nRAA d-adaptor is a new sequence where the attack is maintained in the \\nfirst \\nnote and less advanced in the rest of notes.\\nThere is a family of similar adaptors whose effect is, for specific \\nsituations, increasing or decreasing the value of an expressive feature \\nupon a sequence of notes.\\nA second example of d-adaptor (based on ``variation\\'\\') is ND that works \\nupon a sequence of descending notes where dynamics and articulation \\nis the same-because of this, the passage will be perceived as mechanical. \\nThe result produced by the ND d-adaptor is a new sequence where \\ndynamics is successively decreased and the first note is emphasized \\n(changing articulation and attack mode).\\n\\n\\nSince Noos\\xa0has a lazy and on demand \\nevaluation mode, the actual control flux \\nfollows an order opposite to the order in which we have explained the \\ndifferent phases and steps. At the start we have the input and the \\ntask specification that is not satisfied-causing the activation of \\nadaptors. Since d-adaptors use the expressive features of notes, and \\nthey are not in the input, the c-adaptors are activated. Finally, since \\nc-adaptors use musical concepts, the corresponding domain \\nknowledge (embodied in PSMs) will be activated. This activation chain \\nhas nodes where more than one option is available, so in fact Noos\\xa0\\nspawns an activation tree performing backtracking on choice nodes. \\nChoice and backtracking is not chronological, since a language of \\npreferences is used to specify a partial order upon \\nalternatives at choice points. Preferences are also part of the domain \\nknowledge and they are the basic mechanism used to control adaptor \\nselection at all points.\\n\\n\\nWhen the adaptation process finishes, the expressive model is passed to \\nthe \\nSMS module starting the synthesis procedure and generating an expressive \\ninterpretation (a sound file) that can be listened and judged by the \\nuser.\\n\\n\\n\\n5\\xa0\\xa0Discussion and Related work\\n\\nA conceptual framework for describing CBR systems is Richter\\'s \\nknowledge containers [14].  An \\napproach towards a formal \\nmodel of transformational adaptation based on the knowledge containers \\nframework is presented in [6].  The purpose of \\nBergmann and Wilke\\'s paper is to characterize when properties such as \\nsoundness and completeness can be formally proven to hold in \\ntransformational adaptation.  Interestingly, their approach centered \\non adaptation also seems to downplay the importance of retrieval (and \\nsimilarity) in CBR systems, in a similar way as the ABC\\xa0architecture \\nconceives of retrieval as a part of domain knowledge.  In our approach \\nit is up to the designer of a CBR system to decide whether \\ncompleteness is required or possible. Moreover, the designer may \\ndecide to use a logical language for specifying a ABC\\xa0architecture \\nand then formally prove that certain formal properties hold. For an \\napproach of using UPML with automated reuse see [7]. \\nIt is an interesting question whether the knowledge containers framework \\ncould be refined to provide a software architecture with the \\ncontainers as components-in which case appropriate connectors should \\nbe defined.\\n\\n\\nThere are several lines of research relevant to the work presented \\nhere that we will presently summarize.  As already mentioned, the \\nCBR-Product Experience Base CBR-PEB developed by Klaus-Dieter Althoff, \\nProf.  Dr.  Michael M. Richter is a complementary and necessary effort \\nfor CBR methodology.  The objective of CBR-PEB is to archive a \\ncollection of experience situations in building CBR system.  The \\nvocabulary used by CBR-PEB is of a descriptive nature but too shallow \\nfor certain purposes.  Our main goal in this paper is to argue that \\nconcepts from software engineering, such software architectures, and \\nconcepts from knowledge modeling, such as bridges and adaptors, can be \\nsuitably adapted to the CBR field and provide a vocabulary in which \\ndescribe CBR systems. It is clear that a real methodology for \\ndeveloping CBR systems can only be achieved by a community of \\nresearchers and practitioners that work upon both sides: 1) the \\ntheoretical side for developing and refining a descriptive language \\nand vocabulary, and 2) the empirical side that applies the theoretical \\nconcepts to real systems and provides the necessary feedback to \\nimprove (or reject) the theoretical side. Thus, an article as this \\none can never propose a methodology, but merely propose a starting \\npoint. \\n\\n\\nAcknowledgments\\n\\n\\nThis research has been supported by the Esprit Long Term Research \\nProject 27169: IBROW 3\\xa0An Intelligent Brokering Service \\nfor \\nKnowledge-Component Reuse\\xa0 on the World-Wide Web, and the CICYT \\nProject SMASH:  Systems of Multiagents \\nfor Medical \\nServices in Hospitals.\\n\\n\\nReferences\\n\\n\\n[1]\\nJosep\\xa0Lluís Arcos and Ramon López de Mántaras.\\n Perspectives: a declarative bias mechanism for case retrieval.\\n In David Leake and Enric Plaza, editors, Case-Based Reasoning.\\n  Research and Development, number 1266 in Lecture Notes in Artificial\\n  Intelligence, pages 279-290. Springer-Verlag, 1997.\\n\\n\\n[2]\\nJosep\\xa0Lluís Arcos and Ramon López de Mántaras.\\n An interactive cbr approach to generate expressive music.\\n Submitted, 1999.\\n\\n\\n[3]\\nJosep\\xa0Lluís Arcos, Ramon López de Mántaras, and Xavier \\nSerra.\\n Saxex : a case-based reasoning system for generating expressive\\n  musical performances.\\n Journal of New Music Research, 27 (3):194-210, 1998.\\n\\n\\n[4]\\nJosep\\xa0Lluís Arcos and Enric Plaza.\\n Inference and reflection in the object-centered representation\\n  language Noos.\\n Journal of Future Generation Computer Systems, 12:173-188,\\n  1996.\\n\\n\\n[5]\\nEva Armengol and Enric Plaza.\\n A knowledge level model of case-based learning.\\n In S.\\xa0Wess, K.D. Althoff, and M.\\xa0Richter, editors, Topics in\\n  Case-Based Reasoning, number 837 in Lecture Notes in Artificial\\n  Intelligence, pages 53-64. Springer-Verlag, 1993.\\n\\n\\n[6]\\nR.\\xa0Bergmann and W.\\xa0Wilke.\\n Towards a new formal model of transformational adaptation in\\n  case-based reasoning.\\n In European Conference on Artificial Intelligence (ECAI\\'98),\\n  1998.\\n\\n\\n[7]\\nD.\\xa0Fensel and V.\\xa0R. Benjamins.\\n Key issues for automated problem-solving methods reuse.\\n In Proceedings of the 13th European Conference on Artificial\\n  Intelligence (ECAI-98), pages 63-67, 1998.\\n\\n\\n[8]\\nD.\\xa0Fensel, V.\\xa0R. Benjamins, M.\\xa0Gaspari S.\\xa0Decker, \\nR.\\xa0Groenboom, W.\\xa0Grosso,\\n  M.\\xa0Musen, E.\\xa0Motta, E.\\xa0Plaza, G.\\xa0Schreiber, \\nR.\\xa0Studer, and B.\\xa0Wielinga.\\n The component model of upml in a nutshell.\\n In Proceedings of the International Workshop on Knowledge\\n  Acquisition KAW\\'98, 1998.\\n\\n\\n[9]\\nD.\\xa0Garland and D.\\xa0Perry (Eds.).\\n Special issue on software architectures.\\n IEEE Transactions on Software Engineering, 1995.\\n\\n\\n[10]\\nT.\\xa0R. Gruber.\\n A translation approach to portable ontology specifications.\\n Knowledge Acquisition, (5):188-220, 1993.\\n\\n\\n[11]\\nFred Lerdahl and Ray Jackendoff.\\n An overview of hierarchical structure in music.\\n In Stephan\\xa0M. Schwanaver and David\\xa0A. Levitt, editors, \\nMachine\\n  Models of Music, pages 289-312. The MIT Press, 1993.\\n Reproduced from Music Perception.\\n\\n\\n[12]\\nEugene Narmour.\\n The Analysis and cognition of basic melodic structures : the\\n  implication-realization model.\\n University of Chicago Press, 1990.\\n\\n\\n[13]\\nEnric Plaza.\\n Cases as terms: A feature term approach to the structured\\n  representation of cases.\\n In Manuela Veloso and Agnar Aamodt, editors, Case-Based\\n  Reasoning, ICCBR-95, number 1010 in Lecture Notes in Artificial\\n  Intelligence, pages 265-276. Springer-Verlag, 1995.\\n\\n\\n[14]\\nM.\\xa0M. Richter.\\n The knowledge contained in similarity measures. invited talk to\\n  iccbr-95, 1995.\\n http://wwwagr.informatik.uni-kl.de/\\xa0lsa/CBR/.\\n\\n\\n[15]\\nK.\\xa0Seta, M.\\xa0Ikeda, T.\\xa0Shima, O.\\xa0Kakusho, and \\nR.\\xa0Mizoguchi.\\n Clepe: a task ontology based conceptual level programming\\n  environment.\\n Trans. of IEICE, (9), 1999.\\n\\n\\n[16]\\nG\\xa0van Heijst, A\\xa0T Schreiber, and B\\xa0J Wielinga.\\n Using explicit ontologies in knowledg based systems development.\\n International Journal on Human-Computer Studies, 6(46), 1997.\\n\\n\\n[17]\\nDaniel\\xa0M. Yellin and Robert\\xa0E. Strom.\\n Protocol specifications and component adaptors.\\n ACM Transactions on Programming Languages and Systems,\\n  19(2):292-333, 1997.\\n\\n\\nFootnotes:\\n1 Developed \\nby Klaus-Dieter Althoff and Prof.  Dr.  \\nMichael M. Richter, located at URL \\nhttp://demolab.iese.fhg.de:8080/\\n2 Seminal \\nwork on ontologies was developed in the Knowledge Sharing Initiative \\n[10].  Currently, the HPKB Project \\nis continuing this effort; \\nthe High Performance Knowledge Bases website is at \\nhttp://www.teknowledge.com/HPKB/.  European \\nwork related to \\nKADS is reported on [16]\\n3 Some CBR \\npapers \\ndistinguish between primary and derived feature cases.  Primary \\nfeatures are those appearing on the ``input case\\'\\' and derived \\nfeatures are inferred by the system from primary features.  In our \\napproach, inference uses domain knowledge (including cases) to build \\na model of the problem.\\n4 Recall \\nthat, for the ABC\\xa0architecture, the \\nbase of \\ncases is also part and parcel of the domain model.\\nFile translated from TEX \\nby TTH, version 2.25.On 12 Nov 1999, \\n11:55.\\n\\n',\n",
       " '\\n\\n\\nResearch \\n\\n\\n\\nResearch\\n\\n\"Copy from one, it\\'s plagiarism; copy from two, it\\'s research.\"\\n\\n\\nInterests\\n\\n\\nMy research interests include probabilistic reasoning with uncertainty and\\nimcomplete knowledge, machine learning, data mining, decision making under\\nreal-time constraints, computational models of intelligence, search and NP-hard\\nproblem solving techniques, kolmogorov complexity and algorithmic complexity theory. \\n\\n\\nBayesian networks\\n\\nMy current research focuses on Bayesian network learning and inference\\ntechniques. A Bayesian Network, also called Bayes Belief Network (BBN), is a\\nconcise representation of a joint probability distribution defined on a finite\\nset of random variables. It is a directed acyclic graph (DAG) with conditional\\nprobabilities for each node. In a BBN nodes represent random variables in a\\nproblem domain and arcs represent conditional dependence relationship among\\nthese variables. Each node contains a CPT(Conditional Probabilistic Table) that\\ncontains probabilities of the node being a specific value given the values of\\nits parents. \\nBBN is a powerful common knowledge representation and reasoning tool for\\npartial beliefs under uncertainty. It combines graph theory and probability\\ntheory to provide a practical tool for representing and updating\\nprobabilities(beliefs) about events of interest. The framework of BBNs offers a\\ncompact, intuitive, and efficient graphical representation of dependence relationships\\nbetween entities of a problem domain. \\nFor more information about Bayesian networks, I refer you to Kevin Murphy\\'s\\npage: A Brief Introduction to Graphical Models and Bayesian\\nNetworks, and E. Charniak\\'s tutorial paper \"Bayesian\\nNetworks without Tears\", AI magazine, 1991. \\n\\n\\nComplexity of Inference in Bayesian Networks\\n\\n\\n[Co90] G.F. Cooper. `The computational complexity of probabilistic inference using Bayesian belief networks, Artificial Intelligence, vol. 42, pp. 393-405, 1990.\\n[DL93] P. Dagum and M. Luby. ``Approximating probabilistic inference in Bayesian belief networks is NP-hard,\\'\\' Artificial Intelligence, vol. 60, pp. 141-153, 1993.\\n[Sh94] S. E. Shimony. Finding MAPs for belief networks is NP-hard. Artificial Intelligence, vol. 68, pp. 399--410, 1994.\\n[Ro96] D. Roth. On the Hardness of Approximate Reasoning. Artificial Intelligence, 82(1/2):273--302, 1996.\\n[AH98] A. M. Abdelbar and S. M. Hedetniemi. Approximating MAPs for belief networks in NP-hard and other theorems. Artificial Intelligence vol. 102, pp. 21-38, 1998.\\n[LMP00] M.L. Littman, S.M. Majercik, and T. Pitassi. Stochastic Boolean satisfiability. Journal of Automated Reasoning, 2000.\\n[Pa02] J. D. Park. MAP Complexity Results and Approximation Methods. In Proceedings of the 18th Conference on Uncertainty in Artifical Intelligence(UAI) pp 388-396, 2002.\\n[SD02] Solomon E. Shimony and Carmel Domshlak, Complexity of Probabilistic Reasoning in Singly Connected\\n            (Not Polytree!) Bayes Networks. 2002.\\n\\n\\n\\n\\nDevelopments and Downloads\\n\\nSince the spring of 2000, I have been working with the BNJ(Bayesian\\nNetwork tools in Java ) development team at the KDD Lab on an open-source software\\ndevelopment toolkit for research in probabilistic learning and inference using\\nBayesian networks. To date, we have implemented the following modules into BNJ:\\n\\n\\nCore classes for representing main data structures\\n\\nA graphic Bayesian network editor for loading and\\nmanipulating the network\\n\\nA network format converter for converting network file formats\\n\\nAn exact inference algorithm (the clique-tree\\npropagation algorithm by Lauritzen and Spiegelhalter 1988)\\n\\nSeveral important sampling inference algorithms,\\ninclduing logic sampling, liklihood weighting, self-importance sampling, and\\nadaptive importance sampling\\n\\nK2, a greedy search-based Bayesian network learning\\nalgorithm using Bayesian score\\n\\n\\nData generator, generating training data set by\\nsimulating the network using forward sampling\\n\\nOther useful utilities\\n\\nThe binary code of BNJ can be downloaded from our BNJ\\nannouncement page. The user manual is located at here.\\nAnd the latest source release (bnjsource-alpha1.02.zip) can also be downloaded from SourceForge. If you would like to be part of any\\ndiscussions relating to the future development of this toolkit, please feel\\nfree to join our Yahoo! Group, the\\nBNT Development Team. \\n\\nPresentations\\n\\nIJCAI-2003 \\nWorkshop on AI & Motonomic Computing, Acapulco, Mexico.\\nComplexity Results of BN Inferences KDD Seminar, Friday, March 14, 2003.\\nK2 algorithm: Learning Bayesian Networks from Data \\nBayesian Networks Inference and LS Clique-tree Propagation Algorithm\\nDynamic Bayesian networks: representation, inference, and learning KDD seminar series, Friday, August 23,fall 2002.\\n\\n\\n\\nWorkshops\\nIJCAI-01\\nWorkshop on Wrappers\\nfor Performance Enhancement in Knowledge Discovery in Databases(KDD). Saturday,\\n04 August 2001 Seattle, Washington, USA. (Program Committee) \\nAAAI/KDD/UAI-2002 Joint Workshop on Real-Time Decision\\nSupport and Diagnosis Systems. Monday, 29 July 2002. Edmonton, Alberta,\\nCanada. (Organizing Chair, Program Committee) \\nAAAI2002 Doctoral Consortium\\n\\nIJCAI-2003 workshop on Learning Graphical Models for Computational Genomics . Saturday, 09 August, 2003. Acapulco, Mexico.  \\n\\n\\nLast updated : June 27, 2002 \\n\\n\\n',\n",
       " \"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUPML page\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\n\\n\\nlast update:\\n8 MAR 01\\n\\n\\n\\n\\nUPML\\nUnified Problem-solving Method description Language\\n\\xa0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNews\\n\\n\\nIbrow\\n\\n\\nTools\\n\\n\\nWeb\\nsyntax\\n\\n\\nPapers\\n\\n\\n\\n\\n\\n\\n\\nNews\\n\\xa0\\n\\n\\nFull\\nProtégé\\nsupport for UPML is available now!\\nDuring the 14 Feb meeting\\nin Melton Keynes we decided to stack to RDF component markup to be done\\nby the Protégé tool. Hence,\\nyou may use it to encode your libraries and do not care about their DTD/XMLS\\nrepresentations.\\xa0\\n\\n\\nDeliverable 5: UPML version 2.0,\\nrelease.\\xa0\\n\\nWeb syntax for UPML\\n\\n\\n\\nFull Deliverable\\n\\n\\n\\xa0DTD\\n\\n\\n\\nXML Schema\\n\\n\\n\\nRDF Schema\\n\\n\\n\\n\\nWordDocument or PDF\\n\\n\\nUPML 2.0\\nUPML 2.0 p\\n\\n\\nUPML 2.0\\n\\n\\nUPML 2.0\\n\\n\\n\\n\\nIf you downloaded XML/XSD/DTD/RDF file\\nand see nothing, then try to select 'View'|'Page Source' in your browser\\n\\n\\nXSL stylesheet\\nfor visualization of the Protégé-UPML RDF output.\\xa0\\n\\n\\ntop\\n\\n\\nIbrow Project\\nProject title: An\\nIntelligent Brokering Service for Knowledge-Component Reuse on the World-Wide\\nWeb\\nProject number: IST-1999-19005\\nKeywords: World-Wide Web brokering service, Knowledge-based Systems,\\nProblem-solving methods, Ontologies\\n\\nObjectives and approach\\nThe objective of IBROW is to develop intelligent brokers that are able\\nto distributively configure reusable components into knowledge systems\\nthrough the World-Wide Web. The WWW is changing the nature of software\\ndevelopment to a distributive plug & play process, which requires a\\nnew kind of managing software: intelligent software brokers.\\n\\nResults\\nPart of IBROW is the development of the The Unified Problem-solving\\nMethod Development Language UPML. UPML has been developed to describe\\nand implement such architectures and components to facilitate their semiautomatic\\nreuse and adaptation. Spoken in a nutshell, UPML is a framework for developing\\nknowledge-intensive reasoning systems based on libraries of generic problem-solving\\ncomponents. UPML provides a Protégé-based\\neditor for writing specifications.\\ntop\\n\\nTools for UPML\\n\\xa0\\n\\n\\nProtégé\\nsupport for UPML.\\n\\n\\nWe used Protégé-2000\\nto develop the meta-ontology of UPML and a graphical editor for creating\\ninstances of UPML specifications. Background and download information\\non Protégé can be found here.\\nWe defined the meta-ontology of UPML in Protégé as a hierarchy\\nof classes, with slots and facets attached to them. Then, we customized\\nthe knowledge acquisition tool generated by Protégé to create\\nthe\\xa0 UPML editor for authoring instances of UPML specifications. For\\nexample, we defined specific kinds of diagrams to enter the task decomposition\\nof a complex PSM and the structure of its corresponding operational control.\\nFinally, we entered the specification of the case study of D. Fensel et\\nal.: The Unified Problem-solving Method description Language UPML. Using\\nProtégé-2000's support for RDF(S), we can also generate RDF\\ndocuments out of UPML specifications.\\nThis work helped us better identify the concepts and relations of UPML\\nnecessary to provide a guided framework for defining UPML specifications.\\nBesides\\nfurther experimenting UPML usability by creating additional instances of\\nproblem-solving components, we are currently developing a PSM\\nbrowser and editor as a plug-in\\nto Protégé.\\ntop\\n\\nPapers on UPML\\n\\n\\n\\n\\nDieter Fensel, V. Richard Benjamins, Stefan Decker, Mauro Gaspari, Rix\\nGroenboom, William Grosso, Mark Musen, Enrico Motta, Enric Plaza, Guus\\nSchreiber, Rudi Studer, and Bob Wielinga: The Component Model of UPML in\\na Nutshell. In WWW Proceedings of the 1st Working IFIP Conference\\non Software Architectures (WICSA1) , San Antonio, Texas, USA, February\\n1999.\\n\\nProblem-solving methods provide reusable architectures\\nand components for implementing the reasoning part of knowledge-based systems.\\nThe Unified Problem-solving Method description Language UPML has been developed\\nto describe such architectures and components to facilitate their semiautomatic\\nreuse and adaptation. This paper sketches the components and connectors\\nprovided by UPML.\\n\\n\\n\\nDieter Fensel, V. Richard Benjamins, Stefan Decker, Mauro Gaspari, Rix\\nGroenboom, W. Grosso, Enrico Motta, Enric Plaza, Guus Schreiber, Rudi Studer,\\nBob Wielinga: The Unified Problem-solving Method description Language UPML.\\nESPRIT Report.\\n\\nProblem-solving methods provide reusable architectures\\nand components for implementing the reasoning part of knowledge-based systems.\\nThe Unified Problem-solving Method description Language UPML has been developed\\nto describe such architectures and components to facilitate their semi-automatic\\nreuse and adaptation. UPML descriptions are used by an intelligent brokering\\nservice for knowledge-component reuse on the World-Wide Web. This paper\\ndescribes the architecture style and the language primitives and semantics\\nof UPML.\\n\\n\\n\\nDieter Fensel, V. Richard Benjamins, Enrico Motta, and Bob Wielinga: UPML:\\nA Framework for knowledge system reuse. In Proceedings of the International\\nJoint Conference on AI (IJCAI-99), Stockholm, Sweden, July 31\\n- August 5, 1999.\\n\\nProblem-solving methods provide reusable architectures\\nand components for implementing the reasoning part of knowledge-based systems.\\nThe Unified Problem-solving Method Development Language UPML has been developed\\nto describe and implement such architectures and components and to facilitate\\ntheir semiautomatic reuse and adaptation. Spoken in a nutshell, UPML is\\na framework for developing knowledge-intensive reasoning systems based\\non libraries of generic problem-solving components. The paper describes\\nthe components and adapters, architectural constraints, development guidelines,\\nand tools provided by UPML. Our focus is hereby on the meta ontology that\\nhas been developed to formalize the architectural structure and elements\\nof UPML.\\n\\ntop\\nMaintained by Borys\\nOmelayenko\\n\\n\\n\",\n",
       " '\\n\\n\\n\\n\\nDERIVING ONTOLOGY FROM DESIGN CASES\\n\\n\\nDERIVING ONTOLOGY\\nFROM DESIGN CASES\\n\\n\\nSimeon Simoff and Mary\\nLou Maher\\n\\n\\nKey Centre of Design Computing\\n\\n\\nUniversity of Sydney, NSW\\n\\n\\nAustralia\\n1. Knowledge Discovery\\nfrom Design Cases\\nSince the early seventies considerable research in artificial\\nintelligence (AI) have been focused on the implementation of various knowledge-base\\ncomputing models, which was inspired by a common initial idea, namely,\\nto formalise and represent human expertise in machine-oriented form, and\\nemploy it in computer support of human problem solving activities. Case-based\\nreasoning is one of the techniques, which implements this idea. A \"case\"\\nrepresents the experience accumulated during the solution of relevant problems\\nin the past (Aamodt and Plaza, 1994). The collection\\nof data forms the case base or case library. Past experience\\ncan be described in variety of forms, which span from a row or a number\\nof rows in a database table to a number of volumes of documentation. Considering\\ncomputer implementations of the CBR paradigm, the representation of cases\\nrequires an abstraction of the experience into a form that can be manipulated\\nby the reasoner, where the reasoner comprises procedural or heuristic\\nmodules for retrieving and selecting relevant cases and for adapting a\\nselected case for a new problem. Sometimes the reasoner is assumed to be\\nthe user, rather than a computational process.\\nCase-based reasoning, as a design process, is illustrated\\nin Figure 1. A case library provides several examples of designs and the\\nbasis for finding relevant designs to a new design problem. A new design\\nproblem provides some information that serves as the basis for recalling\\none or more design cases. A selected design case can then be adapted to\\nbe a new design. The resulting new design can be added to the case library,\\nallowing the library to grow with use. This accumulation of experience\\nis considered as the machine learning part of case-based design computing\\nmodel.\\n\\xa0\\n\\nFigure 1.\\nDesign process using case-based reasoning\\n\\xa0\\nDesign case representations - using informal representations\\nto manage complexity\\nWhat exactly is denoted by a case and how it is represented\\nare major issues in CBR. The application of case-based reasoning to structural\\ndesign (Maher et al., 1995), has shown\\nthat in the early systems, attribute-value pairs and object-oriented representations\\nwere the dominating approaches to case representation. Similar to the rule-based\\nsystems, these representations have a limited expressive power. Case models\\nbased on hypermedia representations are alternatives to the strict format\\nof object-oriented and attribute-value representations. The hypermedia\\nrepresentations comprise a collection of \"natural\" descriptions represented\\nas text in free or table format and other multimedia data, such as images,\\nvideo, sound, etc. Another characteristic of hypermedia is the use of links,\\nwhere the links can connect information within a case, between different\\ncases, or links to data that lies outside the case library.\\nCase-based design, as an information rich process, readily\\nmade the shift towards hypermedia case representations. However, reasoning\\nalgorithms are based on attribute-value representations. Therefore, hypermedia\\ndesign cases include an additional structured layer, and case indexing\\nand selection is still based on the comparison of attribute values. Following\\nthis paradigm, we have developed a hypermedia case library of buildings\\nthat focus on structural design. The library is referred to as SAM [HREF1],\\nfor its use in teaching Structures And Materials to undergraduate architecture\\nstudents. In developing SAM, we consider the issues raised by the need\\nto organise the material within a multimedia case library of structural\\ndesigns, while presenting the material using multimedia.\\nDesign in any domain usually involves the development\\nand understanding of complex systems. A general approach to addressing\\ndomain complexity is the representation and reuse of parts of cases, typically\\norganised as hierarchies of \"subcases.\" This supports case-based reasoning\\nbecause sub-dividing designs in this way allows reasoning to focus only\\non the relevant parts of a design. By processing only some of the knowledge\\nassociated with a case, reasoning can become more efficient. The development\\nof a case-base that has a hierarchical structure usually requires defining\\na typical decomposition of a design experience. The representation of design\\ncases in SAM follows the structural design principals that are taught in\\nthe Structures and Materials course. The overall organisation of design\\ninformation falls into three categories:\\n\\n\\nproject information,\\n\\nfunctional decomposition of the structural design, and\\n\\nstructural system types.\\n\\nFigure 2 illustrates the decomposition of the library into separate cases\\n(Figure 2a), and cases into subcases (Figure 2b). Each subcase is a separate\\nweb page.\\n\\xa0\\n\\n\\n\\n\\xa0\\n\\n\\n\\xa0\\n\\n\\n\\n\\na. structure of the case site\\xa0\\n\\n\\nb. structure of the individual case\\xa0\\n\\n\\n\\nFigure 2. Case site and individual case\\nHypermedia case mining\\nThe multimedia representation of design cases makes it easier for people\\nto understand and reason about a case, but it does not support automated\\nreasoning. Case retrieval is based on matching strings rather than based\\non reasoning about the concepts in common between the new design problem\\nand previous designs. Case adaptation in a multimedia representation has\\nno support from the CBR system other than provision of an editor so the\\nperson can edit a previous case to transform it to a new design. In order\\nto address this, we consider knowledge discovery techniques to derive a\\nformal representation from the multimedia representation of design cases.\\n\\nMultimedia and hypermedia representation makes automated\\nknowledge discovery and organization of discovered information difficult.\\nTraditionally hypermedia representations are indexed manually on the basis\\nof provided set of key words, page titles, image labels and other descriptions.\\nThis problem has been recognised in the Web research community where a\\nrecent research provides statistically rigorous comparative evaluation\\nof several well-known indexing and search engines [HREF2].\\nThe general taxonomy of hypermedia case data mining is\\nillustrated in Figure 3. The assumption is that the information in each\\ncase is organised as a set of pages. In our particular example the case\\nhas a pre-defined structure, i. e. a standard set of pages. The structure\\nof a design case, as subcases where each subcase is a separate web page,\\nlends itself to a first approximation as a set of objects. Here we present\\nthe case page text analysis, the most left branch, highlighted in the taxonomy\\non Figure 3, and the transformation from unstructured text format to ontology-based\\nstructured representation.\\n\\n\\nFigure 3. Taxonomy of hypermedia case mining\\n2. Ontology as a structured\\nform of knowledge representation\\nResearchers in artificial intelligence borrowed the term \\'ontology\\'\\nto delineate the part of the problem or domain relevant knowledge that\\ncan be computationally represented in a program. This broad view gave birth\\nto variety of definitions of ontology which span from taxonomy of concepts,\\nwhich defines the semantic interpretation of the relevant knowledge (Alberts\\n1993), to a formal explicit specification of a shared conceptualisation\\n(Gruber, 1993; Borst, 1997).\\nThus, within a given domain ontology is not just a representation of that\\ndomain, but it also reflects some consensus about the knowledge in that\\ndomain (Studer, Benjamins and Fensel, 1998, p.\\n184). Agreeing to accept an ontology means committing to that ontology\\nand its representational power. Therefore, we understand ontology as domain\\ninformation model, comprehensible both by humans and computers. In agreement\\nwith this understanding we use a frame-oriented representation of ontology.\\n\\xa0\\n3. Deriving ontology\\nfrom design cases\\nWe consider the case structure as a first approximation, i.e. we assume\\na taxonomy of terms that is equivalent to the decomposition of the design\\ncase. This taxonomy constitutes the backbone of our initial ontology. The\\nrole of the text analysis is to find the words that characterise the object\\nthat the web page describes. A simple text analysis takes the paragraphs\\nin the subcase description and produces a set of words and their frequency\\nof occurrence. These words are the source of properties and values for\\nderiving the ontology.\\n\\nFigure 4 shows a web page that describes the lateral load resisting\\nsystem in a high rise building. We assume an object called Lateral Load\\nResisting System, and use a text analysis to find the properties of this\\nobject.\\n\\n\\nFigure 4. Part of the Lateral Load Resisting system\\nThe result of a text analysis, as illustrated below, provides a list\\nof words in the description and their frequencies. The complete list for\\nthe Lateral Load Resisting System is shown in the Table\\n1. Below is an excerpt from the word list, sorted by frequency, excluding\\nthe stop words.\\n\\n\\nwords with higher occurrence (frequency > 10)\\n\\n\\nlateral\\n(15)\\n\\nload\\n(13)\\n\\nloads\\n(12)\\n\\ncore\\n(11)\\n\\n\\nwords with \"midrange\" occurrence (frequency between 6 and 10)\\n\\n\\nsystem\\n(8)\\n\\nfloor\\n(7)\\n\\nbuilding\\n(6)\\n\\nfacade\\n(6)\\n\\n\\nwords with low occurrence (frequency < 6 (for this illustration the\\nlower bound is 4))\\n\\n\\nacting\\n(4)\\n\\nresisting\\n(4)\\n\\nstructural\\n(4)\\n\\ntransferred\\n(4)\\n\\nwalls\\n(4)\\n\\nwind\\n(4)\\n\\n\\nConverting the list of words to properties and values requires establishing\\na relationship between words. In ontological engineering this step is done\\nstill manually. The complete automation of this process is below the scope\\nof this paper. Here we present the initial steps of transformation from\\ntext to object definitions. We do this by looking for co-occurrence (the\\ncomplete list is presented in Table\\n2), looking for word patterns such as \"word\"\\n_is_ \"word\", and the association of a word with a number. Some properties\\nare not adequately labeled with one word, such as \"load path\". The frequencies\\nof the adjacency of these two words indicates a compound word as a property.\\nIn general, compound properties can have labels which consist of more than\\ntwo words.\\xa0 For compound properties we use word intersection between\\npairs of words from the list of co-occurrences, illustrated in Figure 5.\\nThe growth of the chain is restricted by stop words. We inspect word chains\\nwhich consist only of nouns and adjectives.\\n\\n\\n\\nFigure 5. Finding compound properties.\\nThe initial object definition derived from the text analysis of the\\nweb page is illustrated in Figure 6. It is generated on the basis of word\\nfrequencies, co-occurrences and adjacency of more than two words. Taking\\nin account the compact description of the case we considered all words\\nthat have frequency non-less than 2.\\n\\xa0\\n\\n\\n\\n\\nSubcase text\\n\\n\\nList of words\\n\\n\\nObject representation\\n\\n\\n\\n\\nLateral Load\\nResisting System Grosvenor Place Primary Structural System The lateral\\nloads on this building arise mainly from wind pressure effects and their\\nmagnitudes increase with the height of the building. The lateral load resisting\\nsystems should not only have adequate strength and stiffness against lateral\\nloads, but also be able to resist tendencies to become unstable due to\\ntoppling, sliding or uplift. The core forms the main lateral load resisting\\nsystem for this building. The core is located centrally to the building\\nand has outer walls with elliptical plan shape, and houses the lifts, toilets,\\nkitchen areas and building services. The structural part of the core consists\\nof the two outer elliptical walls tied together by a number of cross walls,\\nall cast out of reinforced concrete and supported on a raft slab. The raft\\nslab, that supports the core, is 2.7 m thick and has sufficient lateral\\ndimension to prevent toppling of core due to lateral loads. The overall\\nstrategy for transferring lateral loads is to collect all the lateral loads\\nacting on the facade and transfer them horizontally along planes at the\\nfloor levels to the main lateral load resisting element at the centre of\\nthe building. The load is then transferred in the vertical direction -\\nalong the shortest path - to the foundation. The strategy for overcoming\\nthe toppling effect is to use the dead load on the core to counteract the\\nlateral load effects. The lateral load transferred to the floor planes\\nby the secondary system is first picked up by the perimeter composite beam,\\nspanning between the columns. The loads are then transferred horizontally\\nby the floor system acting as a diaphragm to the central core. The floor\\nsystem is stiffer along the sections where the main beams occur, and hence\\nmore of the loads will tend to flow along those sections. The loads that\\nreach the core then flow down its walls to reach the raft slab, which then\\ntransfers and spreads the loads on to the foundation. Lateral load acting\\non core as an overturning moment. Secondary Structural System The secondary\\nstructural systems for carrying the wind load to the core include the facade\\nand the composite floor system acting as a diaphragm. The floor to floor\\nheight is 3.5m and forms the shorter dimensions of typical facade panels.\\nThe strategy is thus to transfer the wind loads, applied normal to the\\nfacade, vertically and in the shorter direction along its facade. The wind\\nload is first picked up by the glazing on the facade which then transfers\\nthe loads to the mullions to the spandrel beams and window frames, and\\nthe loads are then transferred to the core. This diagram shows a plan view\\nof the lateral load path. This diagram shows the side view of the lateral\\nload path.\\xa0\\n\\n\\nlateral\\n-- 15\\xa0load -- 13\\xa0loads -- 12\\xa0core\\n-- 11\\xa0system -- 8\\xa0floor -- 7\\xa0building\\n-- 6\\xa0facade -- 6\\xa0acting -- 4\\xa0resisting\\n-- 4\\xa0structural -- 4\\xa0transferred -- 4\\xa0walls\\n-- 4\\xa0wind -- 4\\xa0main -- 3\\xa0path --\\n3\\xa0raft -- 3\\xa0secondary -- 3\\xa0slab\\n-- 3\\xa0strategy -- 3\\xa0toppling -- 3\\xa0beams\\n-- 2\\xa0composite -- 2\\xa0diagram -- 2\\xa0diaphragm\\n-- 2\\xa0direction -- 2\\xa0due -- 2\\xa0effects\\n-- 2\\xa0elliptical -- 2\\xa0flow -- 2\\xa0forms\\n-- 2\\xa0foundation -- 2\\xa0height -- 2\\xa0horizontally\\n-- 2\\xa0outer -- 2\\xa0picked -- 2\\xa0plan\\n-- 2\\xa0planes -- 2\\xa0reach -- 2\\xa0sections\\n-- 2\\xa0shorter -- 2\\xa0shows -- 2\\xa0systems\\n-- 2\\xa0transfer -- 2\\xa0transfers -- 2\\xa0view\\n-- 2\\xa0which -- 2\\xa0adequate -- 1\\xa0against\\n-- 1\\xa0applied -- 1\\xa0areas -- 1\\xa0arise\\n-- 1\\xa0beam -- 1\\xa0become -- 1\\xa0between\\n-- 1\\xa0carrying -- 1\\xa0cast -- 1\\xa0central\\n-- 1\\xa0centrally -- 1\\xa0centre -- 1\\xa0collect\\n-- 1\\xa0columns -- 1\\xa0concrete -- 1\\xa0consists\\n-- 1\\xa0counteract -- 1\\xa0cross -- 1\\xa0dead\\n-- 1\\xa0dimension -- 1\\xa0dimensions -- 1\\xa0eads\\n-- 1\\xa0effect -- 1\\xa0element -- 1\\xa0frames\\n-- 1\\xa0glazing -- 1\\xa0grosvenor -- 1\\xa0houses\\n-- 1\\xa0include -- 1\\xa0increase -- 1\\xa0kitchen\\n-- 1\\xa0levels -- 1\\xa0lifts -- 1\\xa0located\\n-- 1\\xa0magnitudes -- 1\\xa0mainly -- 1\\xa0moment\\n-- 1\\xa0more -- 1\\xa0mullions -- 1\\xa0normal\\n-- 1\\xa0number -- 1\\xa0occur -- 1\\xa0overall\\n-- 1\\xa0overcoming -- 1\\xa0overturning -- 1\\xa0panels\\n-- 1\\xa0part -- 1\\xa0perimeter -- 1\\xa0place\\n-- 1\\xa0pressure -- 1\\xa0prevent -- 1\\xa0primary\\n-- 1\\xa0reinforced -- 1\\xa0resist -- 1\\xa0services\\n-- 1\\xa0shape -- 1\\xa0shortest -- 1\\xa0should\\n-- 1\\xa0side -- 1\\xa0sliding -- 1\\xa0spandrel\\n-- 1\\xa0spanning -- 1\\xa0stiffer -- 1\\xa0stiffness\\n-- 1\\xa0strength -- 1\\xa0sufficient -- 1\\xa0supported\\n-- 1\\xa0supports -- 1\\xa0tend -- 1\\xa0tendencies\\n-- 1\\xa0thick -- 1\\xa0tied -- 1\\xa0together\\n-- 1\\xa0toilets -- 1\\xa0transferring -- 1\\xa0two\\n-- 1\\xa0typical -- 1\\xa0unstable -- 1\\xa0uplift\\n-- 1\\xa0use -- 1\\xa0vertical -- 1\\xa0vertically\\n-- 1\\xa0where -- 1\\xa0window -- 1\\xa02 --\\n1\\xa03 -- 1\\xa05m -- 1\\xa07 -- 1\\xa0-----------------\\xa0the\\n-- 69\\xa0to -- 23\\xa0and -- 15\\xa0of -- 11\\xa0is\\n-- 10\\xa0on -- 7\\xa0then -- 6\\xa0a -- 5\\xa0along\\n-- 5\\xa0by -- 5\\xa0for -- 4\\xa0this -- 4\\xa0as\\n-- 3\\xa0all -- 2\\xa0are -- 2\\xa0at -- 2\\xa0first\\n-- 2\\xa0has -- 2\\xa0in -- 2\\xa0its -- 2\\xa0m\\n-- 2\\xa0that -- 2\\xa0up -- 2\\xa0with -- 2\\xa0able\\n-- 1\\xa0also -- 1\\xa0an -- 1\\xa0be -- 1\\xa0but\\n-- 1\\xa0down -- 1\\xa0from -- 1\\xa0have --\\n1\\xa0hence -- 1\\xa0not -- 1\\xa0only -- 1\\xa0or\\n-- 1\\xa0out -- 1\\xa0their -- 1\\xa0them --\\n1\\xa0those -- 1\\xa0thus -- 1\\xa0will -- 1\\xa0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0Figure 6. Object definition of Lateral Load Resisting System\\nAt this stage we did not perform morphological analysis, thus some of\\nthe properties in the initial object definition can express relations between\\nother properties of the same object. For example, property \"Transferred\"\\nhas been derived based on its frequency (4) and the fact that it is not\\na stop word. However, the manual examination of the context (the chain\\nof words surrounding examined word - in this case we have selected no more\\nthan 5 words before and after the word in consideration) of \"transferred\"\\nshows relations between properties like \"Load\" and \"Floor_System\". The\\n4 cases of occurrence of \"transferred\" are shown below.\\nThe load is then transferred in the vertical\\ndirection\\n\\nThe lateral load transferred to the floor\\nplanes by\\n\\nThe loads are then transferred horizontally\\nby the floor system\\n\\nand the loads are then transferred to the\\ncore\\n4. Conclusions and directions\\nThe craftwork in ontology building is a very expensive empirical task.\\nIn this paper we have presented the application of text data mining methods\\nfor automating the initial step in ontology derivation. Current text data\\nmining methods used the techniques, developed in text and content analysis.\\nThe paper shows the limitations of these techniques. There is a lack of\\nformal methods for transfer from unstructured text description to structured\\nrepresentation. Even a simple step like the one presented in this paper\\ninvolves considerable amount of hand crafting. However, increasing the\\nsize of the text does not necessarily means proportional increasing of\\nthe hand crafting involved. We expect an increase in the frequencies of\\nthe representative words and increase of the words with very low frequencies.\\nThe direction for further work includes the incorporation of morphological\\nanalysis and analysis of the information provided by the tags in the html\\ndocument .\\n\\nReferences\\nAamodt, A. and Plaza, E. (1994). Case-based reasoning:\\nFoundational issues, methodological variations and system approaches. AI\\nCommunications, 7(1), 39-59.\\nAlberts, L. K. (1993). YMIR: an Ontology for Engineering\\nDesign. University of Twente, Enschede.\\nBorst, W. N. (1997). Construction of Engineering Ontologies,\\nPhD Thesis, University of Twente, Enschede.\\nGruber, T. R. (1993). A translation approach to portable\\nontology specifications. Knowledge Acquisition, 5(2), 199-220.\\nMaher, M.L., Balachandran, B., Zhang, D.M. (1995). Case-Based\\nReasoning in Design, Lawrence Erlbaum Associates, New Jersey.\\nMinsky, M. (1975). A framework for representing knowledge.\\nIn P. H. Winston (ed.), The Psychology of Computer Vision, McGraw-Hill,\\nNew York.\\nStuder, R., Benjamins, V. R. and Fensel, D. (1998). Knowledge\\nEngineering: Principles and methods. Data & Knowledge Engineering,\\n25 (1-2), 161-197.\\n\\xa0\\n\\nWeb\\nreferences\\n[HREF1] SAM - Case based design browser, http://www.arch.usyd.edu.au/kcdc/caut/index.html\\n[HREF2] Vernon Leighton, H.\\xa0 and Srivastava, J.\\xa0\\n(1997). Precision among WWW search services (search engines): Alta Vista,\\nExcite, Hotbot, Infoseek, Lycos. www.winona.msus.edu/library/webind2/webind2.htm.\\n\\n\\n',\n",
       " \" GNU Octave: Table of Contents [ Top ] [ Contents ] [ Index ] [ ? ] Table of Contents Preface Acknowledgements How You Can Contribute to Octave Distribution 1. A Brief Introduction to Octave 1.1 Running Octave 1.2 Simple Examples Creating a Matrix Matrix Arithmetic Solving Linear Equations Integrating Differential Equations Producing Graphical Output Editing What You Have Typed Help and Documentation 1.3 Conventions 1.3.1 Fonts 1.3.2 Evaluation Notation 1.3.3 Printing Notation 1.3.4 Error Messages 1.3.5 Format of Descriptions 1.3.5.1 A Sample Function Description 1.3.5.2 A Sample Command Description 1.3.5.3 A Sample Variable Description 2. Getting Started 2.1 Invoking Octave 2.1.1 Command Line Options 2.1.2 Startup Files 2.2 Quitting Octave 2.3 Commands for Getting Help 2.4 Command Line Editing 2.4.1 Cursor Motion 2.4.2 Killing and Yanking 2.4.3 Commands For Changing Text 2.4.4 Letting Readline Type For You 2.4.5 Commands For Manipulating The History 2.4.6 Customizing readline 2.4.7 Customizing the Prompt 2.4.8 Diary and Echo Commands 2.5 How Octave Reports Errors 2.6 Executable Octave Programs 2.7 Comments in Octave Programs 3. Data Types 3.1 Built-in Data Types 3.1.1 Numeric Objects 3.1.2 Missing Data 3.1.3 String Objects 3.1.4 Data Structure Objects 3.2 User-defined Data Types 3.3 Object Sizes 4. Numeric Data Types 4.1 Matrices 4.1.1 Empty Matrices 4.2 Ranges 4.3 Logical Values 4.4 Predicates for Numeric Objects 5. Strings 5.1 Creating Strings 5.2 Searching and Replacing 5.3 String Conversions 5.4 Character Class Functions 6. Data Structures 7. Containers 7.1 Lists 7.2 Cell Arrays 8. I/O Streams 9. Variables 9.1 Global Variables 9.2 Status of Variables 9.3 Summary of Built-in Variables 9.4 Defaults from the Environment 10. Expressions 10.1 Index Expressions 10.2 Calling Functions 10.2.1 Call by Value 10.2.2 Recursion 10.3 Arithmetic Operators 10.4 Comparison Operators 10.5 Boolean Expressions 10.5.1 Element-by-element Boolean Operators 10.5.2 Short-circuit Boolean Operators 10.6 Assignment Expressions 10.7 Increment Operators 10.8 Operator Precedence 11. Evaluation 12. Statements 12.1 The if Statement 12.2 The switch Statement 12.3 The while Statement 12.4 The do-until Statement 12.5 The for Statement 12.5.1 Looping Over Structure Elements 12.6 The break Statement 12.7 The continue Statement 12.8 The unwind_protect Statement 12.9 The try Statement 12.10 Continuation Lines 13. Functions and Script Files 13.1 Defining Functions 13.2 Multiple Return Values 13.3 Variable-length Argument Lists 13.4 Variable-length Return Lists 13.5 Returning From a Function 13.6 Function Files 13.7 Script Files 13.8 Dynamically Linked Functions 13.9 Organization of Functions Distributed with Octave 14. Error Handling 15. Debugging 16. Input and Output 16.1 Basic Input and Output 16.1.1 Terminal Output 16.1.2 Terminal Input 16.1.3 Simple File I/O 16.2 C-Style I/O Functions 16.2.1 Opening and Closing Files 16.2.2 Simple Output 16.2.3 Line-Oriented Input 16.2.4 Formatted Output 16.2.5 Output Conversion for Matrices 16.2.6 Output Conversion Syntax 16.2.7 Table of Output Conversions 16.2.8 Integer Conversions 16.2.9 Floating-Point Conversions 16.2.10 Other Output Conversions 16.2.11 Formatted Input 16.2.12 Input Conversion Syntax 16.2.13 Table of Input Conversions 16.2.14 Numeric Input Conversions 16.2.15 String Input Conversions 16.2.16 Binary I/O 16.2.17 Temporary Files 16.2.18 End of File and Errors 16.2.19 File Positioning 17. Plotting 17.1 Two-Dimensional Plotting 17.2 Specialized Two-Dimensional Plots 17.3 Three-Dimensional Plotting 17.4 Plot Annotations 17.5 Multiple Plots on One Page 17.6 Multiple Plot Windows 17.7 Interaction with gnuplot 18. Matrix Manipulation 18.1 Finding Elements and Checking Conditions 18.2 Rearranging Matrices 18.3 Special Utility Matrices 18.4 Famous Matrices 19. Arithmetic 19.1 Utility Functions 19.2 Complex Arithmetic 19.3 Trigonometry 19.4 Sums and Products 19.5 Special Functions 19.6 Coordinate Transformations 19.7 Mathematical Constants 20. Linear Algebra 20.1 Basic Matrix Functions 20.2 Matrix Factorizations 20.3 Functions of a Matrix 21. Nonlinear Equations 22. Quadrature 22.1 Functions of One Variable 22.2 Orthogonal Collocation 23. Differential Equations 23.1 Ordinary Differential Equations 23.2 Differential-Algebraic Equations 24. Optimization 24.1 Linear Programming 24.2 Quadratic Programming 24.3 Nonlinear Programming 24.4 Linear Least Squares 25. Statistics 25.1 Basic Statistical Functions 25.2 Tests 25.3 Models 25.4 Distributions 26. Financial Functions 27. Sets 28. Polynomial Manipulations 29. Control Theory 29.1 System Data Structure 29.1.1 Variables common to all OCST system formats 29.1.2 tf format variables 29.1.3 zp format variables 29.1.4 ss format variables 29.2 System Construction and Interface Functions 29.2.1 Finite impulse response system interface functions 29.2.2 State space system interface functions 29.2.3 Transfer function system interface functions 29.2.4 Zero-pole system interface functions 29.2.5 Data structure access functions 29.2.6 Data structure internal functions 29.3 System display functions 29.4 Block Diagram Manipulations 29.5 Numerical Functions 29.6 System Analysis-Properties 29.7 System Analysis-Time Domain 29.8 System Analysis-Frequency Domain 29.9 Controller Design 29.10 Miscellaneous Functions (Not yet properly filed/documented) 30. Signal Processing 31. Image Processing 32. Audio Processing 33. Quaternions 34. System Utilities 34.1 Timing Utilities 34.2 Filesystem Utilities 34.3 Controlling Subprocesses 34.4 Process, Group, and User IDs 34.5 Environment Variables 34.6 Current Working Directory 34.7 Password Database Functions 34.8 Group Database Functions 34.9 System Information A. Tips and Standards A.1 Writing Clean Octave Programs A.2 Tips for Making Code Run Faster. A.3 Tips for Documentation Strings A.4 Tips on Writing Comments A.5 Conventional Headers for Octave Functions B. Known Causes of Trouble B.1 Actual Bugs We Haven't Fixed Yet B.2 Reporting Bugs B.3 Have You Found a Bug? B.4 Where to Report Bugs B.5 How to Report Bugs B.6 Sending Patches for Octave B.7 How To Get Help with Octave C. Installing Octave C.1 Installation Problems D. Emacs Octave Support D.1 Installing EOS D.2 Using Octave Mode D.3 Running Octave From Within Emacs D.4 Using the Emacs Info Reader for Octave E. Grammar E.1 Keywords F. GNU GENERAL PUBLIC LICENSE F.1 Preamble F.2 Appendix: How to Apply These Terms to Your New Programs Concept Index Variable Index Function Index Operator Index This document was generated by John W. Eaton on November, 18 2003 using texi2html \\n\\t\\n\\n-->\\n\\n\\n\\n\\n\\nGNU Octave: Table of Contents\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Top]\\n[Contents]\\n[Index]\\n[ ? ]\\n\\nTable of Contents\\n\\nPreface\\n\\n\\nAcknowledgements\\n\\nHow You Can Contribute to Octave\\n\\nDistribution\\n\\n\\n1. A Brief Introduction to Octave\\n\\n\\n1.1 Running Octave\\n\\n1.2 Simple Examples\\n\\n\\nCreating a Matrix\\n\\nMatrix Arithmetic\\n\\nSolving Linear Equations\\n\\nIntegrating Differential Equations\\n\\nProducing Graphical Output\\n\\nEditing What You Have Typed\\n\\nHelp and Documentation\\n\\n\\n1.3 Conventions\\n\\n\\n1.3.1 Fonts\\n\\n1.3.2 Evaluation Notation\\n\\n1.3.3 Printing Notation\\n\\n1.3.4 Error Messages\\n\\n1.3.5 Format of Descriptions\\n\\n\\n1.3.5.1 A Sample Function Description\\n\\n1.3.5.2 A Sample Command Description\\n\\n1.3.5.3 A Sample Variable Description\\n\\n\\n\\n\\n2. Getting Started\\n\\n\\n2.1 Invoking Octave\\n\\n\\n2.1.1 Command Line Options\\n\\n2.1.2 Startup Files\\n\\n\\n2.2 Quitting Octave\\n\\n2.3 Commands for Getting Help\\n\\n2.4 Command Line Editing\\n\\n\\n2.4.1 Cursor Motion\\n\\n2.4.2 Killing and Yanking\\n\\n2.4.3 Commands For Changing Text\\n\\n2.4.4 Letting Readline Type For You\\n\\n2.4.5 Commands For Manipulating The History\\n\\n2.4.6 Customizing readline\\n\\n2.4.7 Customizing the Prompt\\n\\n2.4.8 Diary and Echo Commands\\n\\n\\n2.5 How Octave Reports Errors\\n\\n2.6 Executable Octave Programs\\n\\n2.7 Comments in Octave Programs\\n\\n\\n3. Data Types\\n\\n\\n3.1 Built-in Data Types\\n\\n\\n3.1.1 Numeric Objects\\n\\n3.1.2 Missing Data\\n\\n3.1.3 String Objects\\n\\n3.1.4 Data Structure Objects\\n\\n\\n3.2 User-defined Data Types\\n\\n3.3 Object Sizes\\n\\n\\n4. Numeric Data Types\\n\\n\\n4.1 Matrices\\n\\n\\n4.1.1 Empty Matrices\\n\\n\\n4.2 Ranges\\n\\n4.3 Logical Values\\n\\n4.4 Predicates for Numeric Objects\\n\\n\\n5. Strings\\n\\n\\n5.1 Creating Strings\\n\\n5.2 Searching and Replacing\\n\\n5.3 String Conversions\\n\\n5.4 Character Class Functions\\n\\n\\n6. Data Structures\\n\\n7. Containers\\n\\n\\n7.1 Lists\\n\\n7.2 Cell Arrays\\n\\n\\n8. I/O Streams\\n\\n9. Variables\\n\\n\\n9.1 Global Variables\\n\\n9.2 Status of Variables\\n\\n9.3 Summary of Built-in Variables\\n\\n9.4 Defaults from the Environment\\n\\n\\n10. Expressions\\n\\n\\n10.1 Index Expressions\\n\\n10.2 Calling Functions\\n\\n\\n10.2.1 Call by Value\\n\\n10.2.2 Recursion\\n\\n\\n10.3 Arithmetic Operators\\n\\n10.4 Comparison Operators\\n\\n10.5 Boolean Expressions\\n\\n\\n10.5.1 Element-by-element Boolean Operators\\n\\n10.5.2 Short-circuit Boolean Operators\\n\\n\\n10.6 Assignment Expressions\\n\\n10.7 Increment Operators\\n\\n10.8 Operator Precedence\\n\\n\\n11. Evaluation\\n\\n12. Statements\\n\\n\\n12.1 The if Statement\\n\\n12.2 The switch Statement\\n\\n12.3 The while Statement\\n\\n12.4 The do-until Statement\\n\\n12.5 The for Statement\\n\\n\\n12.5.1 Looping Over Structure Elements\\n\\n\\n12.6 The break Statement\\n\\n12.7 The continue Statement\\n\\n12.8 The unwind_protect Statement\\n\\n12.9 The try Statement\\n\\n12.10 Continuation Lines\\n\\n\\n13. Functions and Script Files\\n\\n\\n13.1 Defining Functions\\n\\n13.2 Multiple Return Values\\n\\n13.3 Variable-length Argument Lists\\n\\n13.4 Variable-length Return Lists\\n\\n13.5 Returning From a Function\\n\\n13.6 Function Files\\n\\n13.7 Script Files\\n\\n13.8 Dynamically Linked Functions\\n\\n13.9 Organization of Functions Distributed with Octave\\n\\n\\n14. Error Handling\\n\\n15. Debugging\\n\\n16. Input and Output\\n\\n\\n16.1 Basic Input and Output\\n\\n\\n16.1.1 Terminal Output\\n\\n16.1.2 Terminal Input\\n\\n16.1.3 Simple File I/O\\n\\n\\n16.2 C-Style I/O Functions\\n\\n\\n16.2.1 Opening and Closing Files\\n\\n16.2.2 Simple Output\\n\\n16.2.3 Line-Oriented Input\\n\\n16.2.4 Formatted Output\\n\\n16.2.5 Output Conversion for Matrices\\n\\n16.2.6 Output Conversion Syntax\\n\\n16.2.7 Table of Output Conversions\\n\\n16.2.8 Integer Conversions\\n\\n16.2.9 Floating-Point Conversions\\n\\n16.2.10 Other Output Conversions\\n\\n16.2.11 Formatted Input\\n\\n16.2.12 Input Conversion Syntax\\n\\n16.2.13 Table of Input Conversions\\n\\n16.2.14 Numeric Input Conversions\\n\\n16.2.15 String Input Conversions\\n\\n16.2.16 Binary I/O\\n\\n16.2.17 Temporary Files\\n\\n16.2.18 End of File and Errors\\n\\n16.2.19 File Positioning\\n\\n\\n\\n17. Plotting\\n\\n\\n17.1 Two-Dimensional Plotting\\n\\n17.2 Specialized Two-Dimensional Plots\\n\\n17.3 Three-Dimensional Plotting\\n\\n17.4 Plot Annotations\\n\\n17.5 Multiple Plots on One Page\\n\\n17.6 Multiple Plot Windows\\n\\n17.7 Interaction with gnuplot\\n\\n\\n18. Matrix Manipulation\\n\\n\\n18.1 Finding Elements and Checking Conditions\\n\\n18.2 Rearranging Matrices\\n\\n18.3 Special Utility Matrices\\n\\n18.4 Famous Matrices\\n\\n\\n19. Arithmetic\\n\\n\\n19.1 Utility Functions\\n\\n19.2 Complex Arithmetic\\n\\n19.3 Trigonometry\\n\\n19.4 Sums and Products\\n\\n19.5 Special Functions\\n\\n19.6 Coordinate Transformations\\n\\n19.7 Mathematical Constants\\n\\n\\n20. Linear Algebra\\n\\n\\n20.1 Basic Matrix Functions\\n\\n20.2 Matrix Factorizations\\n\\n20.3 Functions of a Matrix\\n\\n\\n21. Nonlinear Equations\\n\\n22. Quadrature\\n\\n\\n22.1 Functions of One Variable\\n\\n22.2 Orthogonal Collocation\\n\\n\\n23. Differential Equations\\n\\n\\n23.1 Ordinary Differential Equations\\n\\n23.2 Differential-Algebraic Equations\\n\\n\\n24. Optimization\\n\\n\\n24.1 Linear Programming\\n\\n24.2 Quadratic Programming\\n\\n24.3 Nonlinear Programming\\n\\n24.4 Linear Least Squares\\n\\n\\n25. Statistics\\n\\n\\n25.1 Basic Statistical Functions\\n\\n25.2 Tests\\n\\n25.3 Models\\n\\n25.4 Distributions\\n\\n\\n26. Financial Functions\\n\\n27. Sets\\n\\n28. Polynomial Manipulations\\n\\n29. Control Theory\\n\\n\\n29.1 System Data Structure\\n\\n\\n29.1.1 Variables common to all OCST system formats\\n\\n29.1.2 tf format variables\\n\\n29.1.3 zp format variables\\n\\n29.1.4 ss format variables\\n\\n\\n29.2 System Construction and Interface Functions\\n\\n\\n29.2.1 Finite impulse response system interface functions\\n\\n29.2.2 State space system interface functions\\n\\n29.2.3 Transfer function system interface functions\\n\\n29.2.4 Zero-pole system interface functions\\n\\n29.2.5 Data structure access functions\\n\\n29.2.6 Data structure internal functions\\n\\n\\n29.3 System display functions\\n\\n29.4 Block Diagram Manipulations\\n\\n29.5 Numerical Functions\\n\\n29.6 System Analysis-Properties\\n\\n29.7 System Analysis-Time Domain\\n\\n29.8 System Analysis-Frequency Domain\\n\\n29.9 Controller Design\\n\\n29.10 Miscellaneous Functions (Not yet properly filed/documented)\\n\\n\\n30. Signal Processing\\n\\n31. Image Processing\\n\\n32. Audio Processing\\n\\n33. Quaternions\\n\\n34. System Utilities\\n\\n\\n34.1 Timing Utilities\\n\\n34.2 Filesystem Utilities\\n\\n34.3 Controlling Subprocesses\\n\\n34.4 Process, Group, and User IDs\\n\\n34.5 Environment Variables\\n\\n34.6 Current Working Directory\\n\\n34.7 Password Database Functions\\n\\n34.8 Group Database Functions\\n\\n34.9 System Information\\n\\n\\nA. Tips and Standards\\n\\n\\nA.1 Writing Clean Octave Programs\\n\\nA.2 Tips for Making Code Run Faster.\\n\\nA.3 Tips for Documentation Strings\\n\\nA.4 Tips on Writing Comments\\n\\nA.5 Conventional Headers for Octave Functions\\n\\n\\nB. Known Causes of Trouble\\n\\n\\nB.1 Actual Bugs We Haven't Fixed Yet\\n\\nB.2 Reporting Bugs\\n\\nB.3 Have You Found a Bug?\\n\\nB.4 Where to Report Bugs\\n\\nB.5 How to Report Bugs\\n\\nB.6 Sending Patches for Octave\\n\\nB.7 How To Get Help with Octave\\n\\n\\nC. Installing Octave\\n\\n\\nC.1 Installation Problems\\n\\n\\nD. Emacs Octave Support\\n\\n\\nD.1 Installing EOS\\n\\nD.2 Using Octave Mode\\n\\nD.3 Running Octave From Within Emacs\\n\\nD.4 Using the Emacs Info Reader for Octave\\n\\n\\nE. Grammar\\n\\n\\nE.1 Keywords\\n\\n\\nF. GNU GENERAL PUBLIC LICENSE\\n\\n\\nF.1 Preamble\\n\\nF.2 Appendix: How to Apply These Terms to Your New Programs\\n\\n\\nConcept Index\\n\\nVariable Index\\n\\nFunction Index\\n\\nOperator Index\\n\\n\\n\\n\\n\\nThis document was generated\\nby John W. Eaton on November, 18 2003\\nusing texi2html\\n\\n\\n\\n\",\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAppendix B: Brief Intro. to Bayesian Inference\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCSU\\n\\n\\nHayward\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\xa0\\n\\n\\nStatistics\\n\\n\\nDepartment\\n\\n\\n\\n\\n\\n\\nSession\\n4: A Brief Introduction\\nTo Bayesian Estimation\\n\\n\\n\\nNote:\\nThis session is intended to be a free-standing introduction to certain aspects\\nof Bayesian inference. The material is of interest in its own right, but is\\nalso necessary background for Session\\xa04. This session expands considerably\\nupon the very brief Appendix\\xa0B for Suess, Fraser, and Trumbo:\\n\"Elementary Uses of the Gibbs Sampler: Applications to Medical Screening\\nTests,\" in STATS\\xa0#27, Winter\\xa02000. It contains more examples,\\nmore detailed explanations, some additional topics, and extensive exercises\\nsuitable for instructional use. For another introduction to Bayesian inference,\\nsee the article by Hal Stern: \"A Primer on the Bayesian Approach to\\nStatistical Inference,\" in STATS\\xa0#23, Fall\\xa01998, pages\\xa03-9.\\n\\n\\n\\n3.1. Introduction\\nBayesian and frequentist statistical inference\\ntake fundamentally different viewpoints toward statistical decision making. The\\nfrequentist view of probability, and thus of statistical inference, is based on\\nthe idea of an experiment that can be repeated very many times. The Bayesian\\nview of probability and of inference is based on personal assessments of\\nprobability and on data from a single performance of an experiment. In\\npractical application, both ways of thinking have advantages and disadvantages,\\nsome of which we will explore here.\\nStatistics is a young science. For example,\\ninterval estimation and hypothesis testing have become common in scientific\\nresearch and business decision making only within the past 75 years, and then\\nonly gradually. On this time scale it seems strange to talk about\\n\"traditional\" approaches. Nevertheless, frequentist viewpoints are currently\\nmuch better established, particularly in scientific research, than Bayesian\\nones. Recently, the use of Bayesian methods has been increasing, partly because\\nimprovements in computation have made these methods easier to apply in practice\\nand partly because the Bayesian approach seems to be able to get useful\\nsolutions in some applications where frequentist approaches cannot. The Gibbs\\nsampler, examined more fully in Session 4, is one example of a broadly\\napplicable, computationally intensive Bayesian method.\\nWe will see later on that, for the very simple\\nexamples considered here, Bayesian and frequentist methods give similar\\nresults. But that is not the main point. We hope you will gain some\\nappreciation that Bayesian methods are sometimes the most natural useful ones\\nin practice.\\nFor most people, the starkest contrast between\\nfrequentist and Bayesian approaches is that Bayesian inference provides the\\nopportunity \\x97 even makes it a requirement \\x97 to take into account\\n\"information\" that is available before any data is collected. That is\\nwhere we begin.\\n3.2. Prior Distributions: Personal Opinions\\nand Expert Knowledge\\nThe Bayesian approach to statistical inference\\ntreats population parameters as random variables (not as fixed, unknown\\nconstants). The distributions of these parameters are called prior\\ndistributions. Often both expert knowledge and mathematical convenience\\nplay a role in selecting a particular type of prior distribution.\\n3.2.1. Example\\n1: Election polling\\nSuppose that Proposition A is on the ballot for an\\nupcoming statewide election, and that a political consultant has been hired to\\nhelp manage the campaign for its passage. The proportion P of prospective voters who currently favor\\nProposition A is the population parameter of interest here. Based on her knowledge\\nof the politics of the state, the consultant\\'s judgment is that the proposition\\nis almost sure to pass, but not by a large margin. She believes that the most\\nlikely proportion of voters in favor is 55% and that the percentage is not\\nlikely to be below 51% or above\\xa059%.\\nIt is reasonable to consider the beta distribution\\nto model the expert\\'s opinion of the proportion in favor because distributions\\nin the beta family take values in the interval (0,\\xa01) as do proportions.\\nThis family of distributions has density functions of the form\\nf(p) = K1pa\\x961(1 \\x96 p)b\\x961,\\n0 < p < 1.\\nwhere a,\\xa0b\\xa0>\\xa00, and where K1\\nis a constant chosen so that f(p)\\nintegrates to 1 over (0,\\xa01).\\nA member of the beta family that corresponds\\nroughly to the expert\\'s opinion has a\\xa0=\\xa0331\\nand b\\xa0=\\xa0271. \\no\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nThis density curve has its mode at (a\\xa0\\x96\\xa01)\\xa0/\\xa0(a\\xa0+\\xa0b\\xa0\\x96\\xa02)\\n=\\xa00.55. \\no\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nNumerical integration shows that P(0.51\\xa0<\\xa0P\\xa0<\\xa00.59) =\\xa00.95. \\nOf course, many other distributional shapes share\\nthese two numerical properties, but we choose a member of the beta family\\nbecause it makes the mathematics relatively easy and because we have no reason\\nto believe its shape is inappropriate here.\\nIf the political consultant\\'s judgments about the\\npolitical situation are correct, then they may be helpful in managing the\\ncampaign. If she too often brings bad judgment to her clients, her reputation\\nwill suffer and she will be out of the political consulting business before\\nlong. Fortunately, as we will see, the details of her judgments become less\\nimportant if we also have polling data to rely upon.\\n3.2.2. Example 2: Weighing an object\\nA construction company buys steel beams with a\\nnominal weight of 200\\xa0lb. Experience with a particular supplier of these\\nbeams has shown that their beams very seldom weigh less than 180 or more than\\n220\\xa0lb. In these circumstances it may be convenient and reasonable to\\nsuppose that the distribution of weights of beams from this supplier is normal\\nwith mean\\xa0200 and standard deviation\\xa010. \\nUsually, the exact weight of a beam is not\\nespecially important, but there are some situations in which it is crucial to\\nknow the weight of a beam more precisely. Then a particular beam is selected\\nand weighed several times on a scale. The scale is known to give results that\\nare normally distributed without bias, but with a standard deviation of\\n1\\xa0lb. Here it seems reasonable to use N(200,\\xa0102) as the\\nprior distribution of the weight m of a\\nbeam. The hope is that the weighing process will help us to know its true\\nweight more accurately.\\nTheoretically, the frequentist statistician would\\nignore \"prior\" or background experience in doing statistical\\ninference, basing statistical decisions only on the data collected when a beam\\nis weighed. In real life it is not so simple. For example, the design of the\\nweighing experiment will very likely take past experience into account in one\\nway or another. For the Bayesian statistician the explicit codification of some\\nkinds of background information into a prior distribution is a required first\\nstep.\\n3.2.3. Example\\n3: Counting mice\\nAn island in the middle of a river is one of the\\nlast known habitats of an endangered kind of mouse. The mice rove about the\\nisland in ways that are not fully understood and so are taken as random.\\nEcologists are interested in the average number of mice to be found in\\nparticular regions of the island. To do the counting in a region they set many\\ntraps there at night, using bait that is irresistible to mice at close range.\\nIn the morning they count and release the mice caught. It seems reasonable to\\nsuppose that almost all of the mice in the region the previous night were\\ncaught and that the number of them on any one night has a Poisson distribution.\\nThe purpose of the trapping is to estimate the mean l of this distribution.\\nEven before the trapping is done the ecologists\\ndoing this study have some information about l.\\nFor example, even though the mice are quite shy, there have been occasional\\nsightings of them in almost all regions of the island, so it seems likely that l > 1. On the other hand, from what is\\nknown of the habits of the mice and the food supply in the regions, it seems\\nunlikely that there would be as many as 25 of them in any one region at a given\\ntime. In these circumstances, it may be reasonable to use a gamma distribution\\nwith shape parameter a\\xa0=\\xa04\\nand scale parameter b\\xa0=\\xa03 as\\na prior distribution for values\\xa0l\\nof the random variable\\xa0L. This\\ngamma distribution has density function f(x)\\xa0=\\xa0K1\\xa0xa\\x961e\\x96bx (for x\\xa0>\\xa00),\\nmean ab\\xa0=\\xa012, mode (a\\xa0\\x96\\xa01)b\\xa0=\\xa09, variance ab2\\xa0=\\xa036,\\nand standard deviation\\xa06. \\nA member of the gamma family may be a reasonable\\nprior because l must be positive and\\nmembers of the gamma family take only positive values. We will see later that\\nchoosing a gamma prior simplifies some important mathematical computations. \\nThese technicalities of the prior distribution\\naside, it is clear that the experience of the ecologists with the island and\\nits endangered mice will influence the course of this investigation in many\\nways: dividing the island into meaningful regions; modeling the randomness of\\nmouse movement as Poisson, the number of traps to use and where to place them,\\nwhat to use for bait so as to attract mice from a region of interest but not\\nfrom all over the island, and so on. The expression of some of their background\\nknowledge as a prior distribution is perhaps a relatively small use of their\\nexpertise. But it is a necessary first step in Bayesian inference, and it is\\nperhaps the only aspect of their expert opinion that will be explicitly\\ntempered by the data that are collected.\\nProblems\\n3.2.1. In Example 1, show that for appropriate\\nvalues of a and b, the density function has a unique mode at (a\\xa0\\x96\\xa01)/(a\\xa0+\\xa0b\\xa0\\x96\\xa02).\\n[Hint: Differentiate the density function, set the derivative equal to\\xa00,\\nand note the values of\\xa0a and\\xa0b for which the solution of this equation\\nyields an absolute maximum.]\\n3.2.2. In Example\\xa03, verify the stated value\\nof the mode, and say for what values of\\xa0a\\nand\\xa0b the mode exists.\\n3.2.3. In Example\\xa01, suppose that a\\xa0=\\xa02 and b\\xa0=\\xa01. Find the value of the constant\\xa0K1\\nthat makes this beta density function integrate to\\xa01. Find the mean and\\nvariance. Find the values\\xa0a and\\xa0b such that P(P\\xa0<\\xa0a)\\xa0= P(P\\xa0>\\xa0b)\\xa0=\\xa00.025.\\nSketch the density function. Repeat for a\\xa0=\\xa01\\nand b\\xa0=\\xa01\\n3.2.4. In Example 3, suppose that a\\xa0=\\xa01\\nand b\\xa0=\\xa010. Find\\xa0K1, E(L), V(L),\\nand a\\xa0and\\xa0b such that P(L\\xa0<\\xa0a)\\xa0=\\nP(L\\xa0>\\xa0b) = 0.025.\\n3.2.5. For each of the three examples of this\\nsection sketch the given prior distribution (for the specific parameter values\\nstated). Then use tables, Minitab, S-Plus, or other statistical software to\\nfind values a and b that cut off the upper and lower 2.5% of the\\nstated prior distributions. Shade in the corresponding areas. An example of a\\ntypical Minitab instruction is as follows: \\nMTB\\n> invcdf 0.975;\\nSUBC> beta 331 271.\\nFor Example\\xa02, it is obvious that the true\\nweight cannot be negative. Why is it safe to ignore the fact that the suggested\\nnormal prior distribution includes negative values?\\n3.2.6. Suggest reasonable prior distributions in\\nthe following cases:\\n(a) In Example 1, suppose that a very successful\\nprofessional political fundraiser from another state has been hired. He doesn\\'t\\nknow what Proposition\\xa0A is about and has no experience with the politics\\nof this state. Before he sees some polling data he has absolutely no idea what\\npercentage of the voters favor Proposition\\xa0A.\\n(b) In Example\\xa02, the mean weight of the\\nbeams from the supplier is around 200\\xa0lb. and we suppose that about half of\\nthem weigh between 195 and\\xa0205\\xa0lb.\\n(c) In Example\\xa03, we seek a prior with P(L\\xa0<\\xa00.5)\\xa0» \\xa00.025, P(L\\xa0>\\xa075)\\xa0»\\n\\xa00.025 and E(L)\\xa0» \\xa020.\\n(d) In Example\\xa01, now suppose the fundraiser\\nis unfamiliar with opinions in the state, does know what Proposition\\xa0A is\\nabout, is sure that it will be extremely controversial with a very low\\nprobability that the percentage in favor is anywhere near\\xa01/2, but he has\\nno opinion whether the sentiment will be overwhelmingly for or overwhelmingly\\nagainst. (Suggest general ranges of parameter values.)\\n3.3. Data and Posterior Distributions \\nThe second step in Bayesian inference is to\\ncollect data and to combine the information in the data with the expert opinion\\nrepresented by the prior distribution. The result is a posterior distribution\\nthat can be used for inference. The computation of the posterior distribution\\nuses Bayes\\' Theorem. You have probably seen Bayes\\' Theorem stated for an\\nevent E and a partition {B1,\\xa0B2,\\xa0...,\\xa0Bk}\\nof a sample space\\xa0S. (The\\xa0Bi are mutually\\nexclusive events whose union is\\xa0S):\\n\\nfor j\\xa0=\\xa01, ..., k. Even in\\nthis most elementary setting, the quantities P(Bj) are called\\nprior probabilities and P(Bj|E) are called posterior\\nprobabilities. \\nA more general version of Bayes\\' Theorem for\\ndistributions involving data\\xa0x and a parameter\\xa0p is as follows:\\n\\nwhere the integral is taken over the region where\\nthe integrand is positive. The denominator of this fraction is a constant\\xa0J.\\nThe conditional density f(p|x)\\nis the posterior distribution of\\xa0P\\ngiven\\xa0X, evaluated for the specific numerical values X\\xa0=\\xa0x\\nand\\xa0P\\xa0=\\xa0p. \\n3.3.1. Example 1\\n(election) continued.\\nIn our election example, suppose n\\xa0=\\xa01000\\nsubjects are selected at random. Assuming the specific parameter value P\\xa0=\\xa0p,\\nthe number\\xa0X of them in favor of Proposition\\xa0A is a random\\nvariable with the binomial distribution\\nf(x|p) = K2 px(1\\xa0\\x96\\xa0p)n\\x96x,\\nfor x\\xa0=\\xa00,\\xa01,\\xa02, ..., n,\\nand where K2\\xa0=\\xa0n!/[x!(n\\x96x)!]\\nis the constant that makes the distribution sum to\\xa01. Then the general\\nversion of Bayes\\' Theorem gives the posterior distribution\\nf(p|x) = K3px(1\\xa0\\x96\\xa0p)n\\x96x\\xa0pa\\x961(1\\xa0\\x96\\xa0p)b\\x961\\xa0=\\xa0K3px+a\\x961(1\\xa0\\x96\\xa0p)n\\x96x+b\\x961,\\nwhere K3\\xa0=\\xa0K1K2/J.\\nIf x\\xa0=\\xa0621 of the n\\xa0=\\xa01000 subjects\\ninterviewed favor Proposition\\xa0A, it is then clear that the posterior has a\\nbeta distribution with parameters x\\xa0+\\xa0a\\xa0=\\xa0952 and n\\xa0\\x96\\xa0x\\xa0+\\xa0b\\xa0=\\xa0650. According to this\\nposterior distribution, P(0.570\\xa0<\\xa0P\\xa0<\\xa00.618)\\n=\\xa00.95, so that a 95% posterior probability interval for the\\npercent in favor is (57.0%,\\xa061.8%). \\nNote the following three aspects of this\\ndevelopment: \\no\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nThe interval estimate for\\xa0P is a straightforward probability statement. Unlike frequentist\\n\"confidence intervals,\" it does not require the user to imagine a\\nrepeatable experiment. \\no\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nThe mathematical forms of the beta prior and the\\nbinomial data are similar, making it especially easy to find the posterior. In\\nsuch cases we say that the beta is a \"conjugate prior\" to the\\nbinomial. \\nOften in Bayesian distributional computations it\\nis unnecessary to know the actual values of the constants involved. Then the\\nproportionality symbol µ can be used to\\nshow relationships. For example, the first equation in this subsection could\\nhave been written as f(x|p)\\xa0µ\\xa0px(1\\xa0\\x96\\xa0p)n\\x96x,\\nwhere the expression on the right is the kernel of the density function. The kernel\\nis the part of a density function that displays necessary variables and\\nparameters, but does not display the constant of integration.\\n3.3.2. Example 2\\n(weighing a beam) continued.\\nSuppose that a particular beam is selected from\\namong the beams available. The prior distribution is N(200,\\xa0102)\\nand so f(m)\\xa0µ \\xa0exp\\xa0[(m\\xa0\\x96\\xa0m0)2/t2], where m0\\xa0=\\xa0100 and t\\xa0=\\xa010.\\nThe data x\\xa0=\\xa0(x1,\\xa0...,\\xa0x5)\\nprovide the likelihood function \\nf(x|m) µ\\nexp {\\x96S[(xi \\x96 m)2/2s2]},\\nwhere m\\ndetermined by the prior distribution, the xi are five\\nobserved numbers, and s2\\xa0=\\xa01.\\nThen, after some algebra (see Problem\\xa03.3.6), the posterior can be written\\nas\\nf(m|x) µ\\nf(m)f(x|m) µ\\nexp [\\x96(m \\x96 mn)2/2Vn],\\nwhich is the kernel of N(mn,\\xa0Vn), where\\n\\nIt is common to use the term precision to\\nrefer to the reciprocal of a variance. Using this terminology, we say that mn is a weighted average of\\nthe prior mean and the sample mean, where the precisions are the weights.\\nHere the precision of each observation is much\\ngreater than the precision of the prior. In this case even our small sample of\\nfive observations is enough to diminish the impact of the prior on the\\nposterior distribution.\\n3.3.3. Example 3\\n(counting mice) continued\\nSuppose that a region of the island is selected\\nwhere the gamma prior distribution with parameters a\\xa0=\\xa04 and b\\xa0=\\xa03\\nis reasonable. This prior has density f(l)\\xa0µ \\xa0la\\n- 1e\\x96l/b.\\nOver a period of about a year traps are set out on n = 50 nights with\\ntotal number of captures T\\xa0=\\xa0S\\xa0xi\\xa0=\\xa0256,\\nfor an average of 5.12 mice captured per night. This gives the likelihood\\n\\nThus, the posterior distribution f(l|x) is gamma with parameters given by\\nan\\xa0= T\\xa0+\\xa0a\\xa0=\\xa0260 and 1/bn\\xa0=\\xa0n\\xa0+\\xa01/b\\xa0= 50\\xa0+\\xa01/3, so that bn\\xa0=\\xa00.0199. A 95%\\nBayesian probability interval for l\\nbased on this posterior distribution is (4.56,\\xa05.82).\\nProblems\\n3.3.1. This problem deals with point and interval\\nestimates of the population proportion in the continuation of Example\\xa01.\\n(a) It might be reasonable to use the mode of the\\nposterior distribution as a point estimate of the proportion in favor of\\nProposition\\xa0A in the population. What is its numerical value?\\n(b) Use a computer package to verify the endpoints\\nof the Bayesian probability interval given in the Section\\xa03.3.2. This is a\\nprobability-symmetric interval which excludes 2.5% of the probability in each\\ntail of the posterior distribution. \\n(c) Taking a frequentist point of view, use the\\nnormal approximation to the binomial distribution to find an approximate 95%\\nconfidence interval for the population proportion. What is the main difference\\nbetween this interval and the one verified in part\\xa0(a)?\\n(d) What Bayesian point estimate (mode) and 95%\\nprobability interval would have resulted from using the uniform distribution on\\n(0,\\xa01) as the prior distribution?\\n3.3.2. This problem deals with point and interval\\nestimates of m in Example\\xa02.\\nSuppose that the five measurements of the weight of the beam are: 198.14,\\n198.45, 196.59, 197.64, 198.12 (mean\\xa0197.79).\\n(a) What is the point estimate of m. In this instance, does it matter whether\\nwe choose the mean, the median, or the mode of the posterior distribution as\\nour point estimate? Explain.\\n(b) Find a 95% Bayesian probability estimate\\nof\\xa0m.\\n(c) Suppose we would be unwilling to use this beam\\nfor a particular purpose if we thought it weighed less than 197\\xa0lb. What\\nare the chances of that?\\n(d) Taking a frequentist point of view, use the\\nfive observations and the known variance of measurements produced by the scale\\nto give a 95% confidence interval for the true weight of the beam.\\n(e) What Bayesian point estimate and 95%\\nprobability interval would have resulted from using a normal distribution with\\nmean 202 and standard deviation 5 as the prior?\\n3.3.3. This problem deals with point and interval\\nestimates of l in Example\\xa03 based\\non the data given in Sec.\\xa03.3.3.\\n(a) Find the mean, median, and mode of the\\nposterior distribution.\\n(b) Verify the 95% Bayesian probability interval\\ngiven in Sec.\\xa03.3.3. Strictly speaking, shorter 95% probability intervals\\ncan be found because of the skewness of the distribution. For such a shorter\\ninterval would slightly more or slightly less that 2.5% be omitted from the\\nright tail?\\n(c) Taking a frequentist point of view, find an\\napproximate 95% confidence interval for\\xa0nl and hence for\\xa0l.\\nUse the fact that T has a Poisson distribution that is well-approximated\\nby an appropriate normal distribution.\\n(d) Suppose that the prior distribution had been\\ngamma with parameters a\\xa0=\\xa03\\nand b\\xa0=\\xa01. Give an interval that\\ncontains 95% of the probability under this prior distribution. Use the same\\ndata as above (T\\xa0=\\xa0256) and find the corresponding 95%\\nBayesian probability interval for\\xa0l.\\n3.3.4. Derive the posterior distribution resulting\\nfrom a beta prior and binomial data. Verify the specific parameter values given\\nin Sec.\\xa03.3.1.\\n3.3.5. Derive the posterior distribution resulting\\nfrom a gamma prior and Poisson data. Verify the specific parameter values given\\nin Section\\xa03.3.3. (The gamma prior is conjugate to the Poisson\\ndistribution of the data.)\\n3.3.6. In this problem we invite you to derive the\\nposterior distribution for Example\\xa02 in Section\\xa03.3.2.\\n(a) Show that the expression f(x|m)\\xa0µ\\nexp {\\x96S[(xi\\xa0\\x96\\xa0m)2/2s2]} for the likelihood\\nfunction is correct and can be re-expressed as\\n\\nThe likelihood function is the joint density\\nfunction of the data viewed as a function of\\xa0m (rather than of the\\xa0xi). By independence,\\nthe joint density function is proportional to P\\xa0exp{\\x96[(xi\\xa0\\x96\\xa0m)2/2s2]}, which is\\nclearly equal to the first expression for the likelihood function. To put the\\nfirst expression in the form displayed just above, write \\n\\nexpand the square and sum over\\xa0i. On\\ndistributing the sum you should obtain three terms. One of the terms provides\\nthe desired result; another is\\xa00, and the third is irrelevant because it\\ndoes not contain the variable\\xa0m.\\n(A constant term in the exponential amounts to a constant factor in the\\ndensity, which is not included in the kernel.)\\n(b) Derive the expression for the kernel of the posterior\\ngiven in Section\\xa03.3.2. Multiply the kernels of the prior and the\\nlikelihood, and expand the squares in each. Put everything in the exponential\\nover a common denominator. Then, remembering that m is the variable, collect terms in m2 and\\xa0m.\\nTerms in the exponent that do not involve m\\nare constant factors in the posterior density that may be adjusted at will to\\ncomplete the square in order to obtain the desired kernel.\\n3.4. More About Prior Distributions\\nOne issue of concern in Bayesian inference is how\\nstrongly the particular selection of a prior distribution influences the\\nresults of the inference. Particularly if results are to be used by people who\\nmay question the expert\\'s opinion, it is desirable to have enough data that the\\ninfluence of the prior is slight.\\nAn uninformative prior (sometimes called a\\nflat prior) is one that provides little or no information. Depending on the\\nsituation, uninformative priors may be quite disperse, may avoid only\\nimpossible or quite preposterous values of the parameter, or may not have\\nmodes. Uninformative priors sometimes give results similar to those obtained by\\ntraditional frequentist methods.\\nReturning once again to our election example, the\\nfirst row of the table below summarizes a prior that matched the expert\\'s\\nopinion, the posterior, and our inference. Now suppose that another expert also\\nbelieves that Proposition\\xa0A is more likely than not to pass, but his views\\nabout current public opinion are more vague. Perhaps his beta prior has\\nparameters 56 and\\xa046. This prior also has mode\\xa055%, but here the\\nprior has P(0.45\\xa0<\\xa0P\\xa0<\\xa00.64)\\xa0=\\xa00.95.\\nWithout seeing any data, the second expert gives Proposition\\xa0A a\\nnon-trivial chance of losing if the election were held today: P(P\\xa0<\\xa00.5)\\xa0=\\xa00.16. \\nAfter he sees the results of the poll with 621 out\\nof 1000 in favor, his posterior beta distribution has parameters 677 and 425,\\nso that his 95% posterior interval estimate is (59.5%,\\xa065.2%), as recorded\\nin the second row of the table. \\nThe reasonable choice for an uninformative prior\\nin this situation is the uniform distribution (which is beta with parameters 1\\nand\\xa01, and which has no mode); it gives the 95% posterior interval\\nestimate (59.1%,\\xa065.1%).\\nIn this example, the 1000 observations are enough\\ndata that the two expert priors and the uninformative prior all give somewhat\\nsimilar results. (The agreement would not be so good with a small amount of\\ndata.) A non-Bayesian 95%\\xa0confidence interval, based on the normal\\napproximation, is (59.1%.\\xa065.1%).\\n\\n\\n\\n\\nBeta Prior\\n  Parameters\\n\\n\\nMode of Prior\\n  Distribution\\n\\n\\nPrior 95%\\n  Prob. Interval\\n\\n\\nMode of Posterior\\n  Distribution\\n\\n\\nPosterior 95% \\n  Prob. Interval\\n\\n\\n\\n\\na\\xa0=\\xa0331,\\n  b\\xa0=\\xa0271\\n\\n\\n55\\n\\n\\n(51.0%, 59.0%)\\n\\n\\n56.4%\\n\\n\\n(57.0%, 61.8%)\\n\\n\\n\\n\\na\\n  = 45, b = 46\\n\\n\\n55\\n\\n\\n(45.0%, 64.0%)\\n\\n\\n61.5%\\n\\n\\n(59.5%, 65.2%)\\n\\n\\n\\n\\na\\n  = 1, b = 1\\n\\n\\nNo Mode\\n\\n\\n(2.5%, 97.5%)\\n\\n\\n62.1%\\n\\n\\n(59.1%, 65.1%)\\n\\n\\n\\n\\nFrequentist\\n\\n\\n(No Prior)\\n\\n\\n(No Prior)\\n\\n\\n^p = 62.1%\\n\\n\\nCI: (59.1%, 65.1%)\\n\\n\\n\\n\\nNotice that, in view of the polling results, both\\nexperts were a little too pessimistic about the prospects for Proposition A.\\nHowever, the first expert\\'s \"sharper\" prior (first row of the table)\\nis roughly equivalent in influence to surveying 600 people and so the results\\nof the actual survey of 1000 people are not enough to override this expert\\'s\\nview that the true percentage in favor is unlikely to be above 60%. The second\\nexpert\\'s somewhat \"flatter\" prior (second row) is roughly equivalent\\nin influence to surveying 100 people and the actual survey results from 1000\\npeople nearly override his opinion. \\nThe uninformative prior (uniform distribution in\\nthe third row) gives a mode and probability interval that agree with the\\nfrequentist point estimate and confidence interval. However, the philosophical\\nbases of Bayesian and frequentist methods differ, as do the interpretations of\\nthe results. The Bayesian results are statements about a posterior distribution\\nfor the particular situation at hand, whereas the frequentist results must be\\ninterpreted in terms of many hypothetical repetitions of the sampling\\nexperiment. If the frequentist results depend on prior experience, it is by way\\nof the design of the survey, whereas the Bayesian results can depend explicitly\\non personal probability assessments. \\nProblems\\n3.4.1. The table of this section shows four rows\\nwith various point and interval estimates for the election example. Make a\\nsimilar table for the beam weight example. Use the data of Problem\\xa03.3.2: 198.14,\\n198.45, 196.59, 197.64, 198.12 (mean\\xa0197.79). In the first row show\\nresults for the prior N(200,\\xa0102), and in the second row\\nresults for the prior N(202,\\xa052). There is no such thing as a\\ntruly uninformative normal prior, but select a normal prior for the third row\\nthat gives results numerically similar (to two decimal places) to frequentist\\nresults, which you should show in the fourth row. Summarize and comment upon\\nthe results shown in the table, making any extra rows that you think would add\\nto your understanding of this example.\\n3.4.2. The table of this section shows four rows\\nwith various point and interval estimates for the election example. Make a\\nsimilar table for the mouse counting example. Suppose, as in Section\\xa03.3.3\\nthat 256 mice altogether are trapped in 50 nights. In the first row show\\nresults for the gamma prior distribution with parameters a\\xa0=\\xa04 and b\\xa0=\\xa02, and in the second row results for the gamma\\nprior with a\\xa0=\\xa02 and b\\xa0=\\xa05. There is no such thing as a\\ntruly uninformative gamma prior, but see if you can find a gamma prior for the\\nthird row that gives results numerically similar (to two decimal places) to\\nfrequentist results, which you should show in the fourth row. Summarize and\\ncomment upon the results shown in the table, making any extra rows that you\\nthink would add to your understanding of this example.\\n\\xa0\\n\\n\\n\\nCopyright © 2000 Bruce E. Trumbo. All rights\\nreserved. Intended mainly for instructional use at California State University,\\nHayward. This is a draft; comments and corrections are welcome. To request\\npermission for other uses please contact btrumbo@csuhayward.edu.\\n\\n\\n\\n\\n\\n\\n',\n",
       " \"\\n\\n\\n\\n\\nExploring Large Graphs in 3D Hyperbolic Space\\n\\n\\n\\n\\n\\n\\n\\n\\nExploring Large Graphs in 3D Hyperbolic Space\\n\\nTamara Munzner, Stanford University\\n\\n\\nAbstract:\\n\\nDrawing graphs as nodes connected by links is visually compelling but\\n    computationally difficult. Hyperbolic space and spanning trees can \\n    reduce visual clutter, speed up layout, and provide fluid\\n    interaction. \\n\\n\\nIntroduction\\n\\nA graph is a simple, powerful, and elegant abstraction which has broad\\napplicability in computer science and many related fields. Algorithms\\nthat operate on graphs see heavy use in both theoretical and\\npractical contexts. Graphs have a very natural visual representation\\nas nodes and connecting links arranged in space. Seeing this structure\\nexplicitly can aid tasks in many domains. Many people\\nautomatically sketch such a picture when thinking about small graphs,\\noften including simple annotations.\\n\\nThe pervasiveness of visual representations of small graphs testifies\\nto their usefulness. On the other hand, although many large data sets\\ncan be expressed as graphs, few such visual representations exist.\\nWhat causes this discrepancy? For one thing, graph layout poses a hard\\nproblem [1], one that current tools just can't\\novercome. Conventional systems often falter when handling hundreds of\\nedges, and none can handle more than a few thousand edges\\n[2].\\n\\nHowever, nonvisual manipulation of graphs with 50,000 edges is\\ncommonplace, and much larger instances exist. We can consider the Web\\nas an extreme example of a graph with many millions of nodes and\\nedges. Although many individual Web sites stay quite small, \\na significant number have more than 20,000 documents.\\nThe Unix file system reachable from a single networked workstation\\nmight include over 100,000 files scattered across dozens\\nof gigabytes worth of remotely mounted disk drives.\\n\\nComputational complexity is not the only reason that software to\\nvisually manipulate large graphs has lagged behind software to\\ncomputationally manipulate them. Many previous graph layout systems\\nhave focused on fine-tuning the layout of relatively small graphs in\\nsupport of creating polished presentations. A graph drawing system\\nthat focuses on the interactive browsing of large graphs can instead\\ntarget the quite different tasks of browsing and exploration.\\nMany researchers in scientific visualization have recognized the split\\nbetween explanatory and exploratory goals. This distinction proves equally\\nrelevant for graph drawing.\\n\\nContribution\\n\\nThis article briefly describes a software system that explicitly\\nattempts to handle much larger graphs than previous systems and\\nsupport dynamic exploration rather than final presentation. I'll then\\ndiscuss the applicability of this system to goals beyond simple\\nexploration.\\n\\nA software system that supports graph exploration should include both\\na layout and an interactive drawing component. I have developed new\\nalgorithms for both layout and drawing - H3 and H3Viewer\\n. A paper from InfoVis 97 contains a more extensive\\npresentation of the H3 layout algorithm [3]. The H3Viewer\\ndrawing algorithm remains under development, so this article presents\\npreliminary results.\\n\\nI have implemented a software library that uses these algorithms. It\\ncan handle graphs of more than 100,000 edges by using a spanning tree as\\nthe backbone for the layout and drawing algorithms. We draw the graph\\nstructure in 3D hyperbolic space to show a large neighborhood around a\\nnode of interest. This also allows for quick, fluid changes of the focus\\npoint. The H3Viewer drawing algorithm uses both graph-theoretic and\\nview-dependent information to achieve a high guaranteed frame rate.\\n\\nThe library has been incorporated into Site Manager\\n(http://www.sgi.com/software/sitemgr.html), a free Web publishing\\nproduct from Silicon Graphics aimed at Webmasters and content creators\\nThe first\\nversion of Site Manager incorporated only the H3 layout algorithm,\\nwhile the current release also includes the H3Viewer adaptive drawing\\nfunction. Users an also access the library from a stand-alone\\nviewer.\\n\\nSpanning Trees\\n\\nThe H3 layout algorithm finds a spanning tree from an input graph and\\nthen computes its layout. A spanning tree touches every\\nnode in a graph, but only a subset of the links. In a graph a node can\\nhave many incoming links, but in a tree a canonical parent is chosen for each\\nchild. We call links which appear in the graph but not in the spanning\\ntree non-tree links. These links do not affect the layout\\ncomputation and are drawn for a selected node or nodes only on demand.\\n\\nThe backbone spanning tree used by the layout and drawing algorithms\\nstrongly influences our system's visual impact. As a fallback, we can\\nalways find a default spanning tree using breadth-first search\\nfrom a root node. However, exploiting a small amount of\\ndomain-specific knowledge lets us construct a better spanning\\ntree, one that provides a more useful mental model. If the node identifier\\nhas a hierarchical structure, the library will determine parentage\\nbased on a best match rather than a breadth-first search. Such\\nstructure is available for Web sites like the one shown in Figure\\n1. In this domain the URL encodes the site's directory\\nstructure (often a deliberate choice by the site creator).\\nThat directory structure serves to determine which of the\\nincoming hyperlinks to a document should be chosen as its main parent\\nin the spanning tree.\\n\\nA hierarchical identifier is not trivially available in the case of a\\nfunction call graph -- we must construct it. We can use a combination\\nof compiler analysis and runtime profiling to find the calling\\nprocedure responsible for the majority of the child's execution time.\\nFigure 2 shows an example of a graph where this\\ntechnique was used to construct hierarchical identifiers for the\\nnodes. Software engineers who must modify or optimize unfamiliar code\\ncan browse through a call graph's H3 layout to discover a complex\\nprogram's structure. Such graphs are notoriously difficult to\\nunderstand when all the links appear in a 2D nonplanarized layout.\\n\\nOur reliance on a spanning tree is both the algorithm's strength and\\nits weakness. If we can use domain-specific information to\\nderive a good spanning tree, then our methods work very well up to the\\nlimits of main memory. If we must fall back to a breadth-first search\\nfor a fully connected graph, the resulting visualization may not\\nconvey any useful information. In such cases a spring-force graph\\ndrawing system like Frick's Gem3D [4] would be more\\nappealing in principle. However, in practice this class of methods\\ndoes not scale --  Frick's measure of ``large'' is only 500 edges.\\n\\nThe key idea is that many non-tree graphs exist for which the\\nright spanning tree can provide a useful mental model of the entire\\nstructure. Given a good domain-specific way to decide \\nwhich incoming link should be a node's parent in the spanning tree,\\nour method would work well. Many graphs, densely\\nconnected by graph-theoretic standards, fall into this category. In\\nthe extreme, trivial case of a tree, our system certainly suits the\\ntask well. On the other hand, a bipartite graph will almost certainly\\nresult in a misleading picture.\\n\\nThe particular domain-specific methods for finding spanning trees\\nmentioned here offer only one possibility. We want mainly to\\ndevelop fast, robust layout and drawing algorithms. If and when\\nother researchers determine more appropriate spanning trees for these or\\nother domains, they can explore those trees using the infrastructure\\npresented here.\\n\\nLayout\\n\\nOnce we have determined a spanning tree we must find positions in\\nspace for the nodes and edges. Our approach uses the influential cone\\ntree method as a springboard [5]. The standard cone\\ntree method lays out nodes on the linear circumference of a cone's\\nmouth. The H3 layout makes two changes. First, \\nthe nodes are laid out on the surface of a hemisphere instead of a\\nlinear circumference. We use hemispheres, not full spheres, since the\\nprocess is recursive. If a child used an entire hemisphere, the\\nback half would intersect the area used by the parental hemisphere.\\nSecond, the cone widens to its maximum extent, spanning\\na full 180 degrees. The cone body proper no longer takes\\nup space but flattens out into a disk at the base of the\\nhemisphere. The child hemispheres lie directly on the parental\\nhemisphere's tangent plane, with no visible intervening cone body.\\n\\nThe layout algorithm requires two passes: a bottom-up pass to estimate\\nthe radius needed for each hemisphere to accommodate all of its\\nchildren, and a top-down pass to place each child node on its parental \\nhemisphere's surface. These steps cannot be combined because we\\nneed the radius of the parental hemisphere before we can compute the\\nfinal position of the children.\\n\\nLaying out child hemispheres on the surface of their parent introduces\\nthe circle packing problem, which has received much\\nattention from the mathematical community [6]. We strike a\\nbalance between optimality and simplicity by laying out the children\\nin concentric bands around the hemisphere's pole. The\\namount of room each node needs is directly proportional to the total\\nnumber of its descendants. We lay them out in sorted order to avoid\\nwasting space within the bands, and thus the disk at the pole is the\\nnode with the most progeny (either direct children or indirect\\ndescendants). A full exposition of the layout algorithm appears elsewhere\\n[3], along with an appendix including a detailed\\nderivation.\\n\\nOur hemispherical layout is particularly effective because we lay out\\nour tree in a mathematical space where there is an exponential\\n``amount of room'' in the direction of the hemisphere's growth. The\\narea of a hemisphere,   , increases polynomially with respect\\nto its radius in Euclidean space. In hyperbolic space -- one of the\\nnon-Euclidean geometries -- the formula for hemisphere area is   . Since the hyperbolic sine and cosine functions (sinh and\\ncosh) are exponential, this space can easily accomodate a layout of an\\nexponential number of nodes (a classic problem in Euclidean\\ntree layout). Hyperbolic space is infinite in extent, just like\\nEuclidean space. However, it is possible to map the entire infinite\\nspace into a finite portion of Euclidean space. It might surprise you\\nthat you can map an infinite amount of space with ``more room'' into a\\nfinite piece of a space with ``less room'', but non-Euclidean\\ngeometries have many unexpected consequences for Euclidean intuitions.\\nThere are several standard mappings used in the mathematical\\nliterature [7]. We chose the projective (Klein) model,\\nwhich supports fast drawing because motions can be expressed as\\nstandard    matrices [8]. Figure\\n3 shows navigation in 3D hyperbolic space through a\\nUnix file system of more than 31,000 nodes.\\n\\nWhile not yet commonplace, hyperbolic space has appeared in\\nthe information visualization literature. A hyperbolic browser from\\nXerox PARC handled trees in two dimensions [9]. The\\nWebviz system from the Geometry Center drew graphs in three\\ndimensions, but the layout algorithm did not exploit 3D hyperbolic\\nspace to its full potential. The amount of information displayed was\\nquite sparse compared to the amount of white space\\n[10].\\n\\nThe H3 layout strikes a reasonable balance between information density\\nand clutter. The traditional cone tree layout in both the Xerox PARC\\nCone Tree and the Geometry Center Webviz system places nodes on\\na circle - a 1D line. In H3, nodes are placed on a hemisphere - a 2D\\nsurface.  Carpendale et\\nal [11] placed nodes in a 3D grid - a 3D volume.\\nIn all three examples, the space in which nodes are laid out\\nis a 3D volume. When the dimension of the surrounding space equals\\nthe dimension of the node structures, occlusion becomes the\\noverriding issue. We \\nlay out nodes on a surface, which offers a happy medium between the\\nsparseness of a line and the density of a volume. An excessively\\nsparse layout like the Webviz system wastes screen real estate.\\nWith too dense a layout, the leaf nodes near the ball's surface\\nwould block our view of the rest of the structure, since we are\\noutside of the ball looking in.\\n\\nDrawing\\n\\n\\xa0\\n\\nThe H3Viewer drawing algorithm depends on the number of visible, not\\ntotal, nodes and edges. The projection from hyperbolic to\\nEuclidean space guarantees that nodes sufficiently far from the center\\nwill project to less than a single pixel. Thus the visual complexity\\nof the scene has a guaranteed bound - only a local neighborhood of\\nnodes in the graph will be visible at any given time.\\n\\nA guaranteed frame rate is extremely important for a fluidly interactive\\nuser experience. We designed our adaptive drawing algorithm to always\\nmaintain a target frame rate even on low-end graphics systems. A high\\nconstant frame rate results from drawing only as much of the\\nneighborhood around a center point as the allotted time permits.\\nWhen the user is idle, the system fills in more of the\\nsurrounding scene. A slow graphics system will simply show less of the\\ncontext surrounding the node of interest during interactive\\nmanipulation, as in Figure 4.\\n\\nThe drawing algorithm incorporates knowledge of both the graph\\nstructure and the current viewing position. We use the spanning tree's \\nlink structure to guide additions to a pool of candidate nodes\\nand the nodes' projected screen area to choose from among the\\ncandidates. The largest projected area node from the previous frame \\nserves as a seed for the tree traversal on the next frame.\\n\\nThe current version of the drawing algorithm succeeds in\\nmaintaining the target interactive frame rate nearly all the time.\\nHowever, we have only partially addressed one important issue\\nmentioned in the H3 layout paper [3].\\nAny single mapping from hyperbolic to Euclidean space permits drawing only a\\nlimited number of nodes before the drawing system succumbs\\nto precision problems. The previous drawing system simply truncated\\nnodes beyond this limit. The H3Viewer system will instead compute a\\nremapping from hyperbolic to Euclidean space as necessary when the\\ncumulative error becomes too great. This remapping is a global\\noperation that depends on the total number of nodes in the scene\\ninstead of only the visible nodes. When drawing large graphs, this\\nremapping will cause a temporary interruption in the otherwise smooth\\nframe rate or a jump instead of an animated transition. A true\\nsolution to this problem would require an incremental mapping\\nalgorithm, which falls under the heading of future work.\\n\\nDiscussion\\n\\n\\xa0\\n\\nOur layout algorithm's computed overview of the graph structure\\nprovided by the geometry of nodes and links offers a way\\nfor a user to explore a graph too large to manipulate with traditional\\nmethods. However, an interactive graph exploration system can offer a\\nuser more than a global overview. Our layout and drawing system has\\napplicability for the\\nfollowing tasks: \\n\\n\\n Scaffolding for attributes\\n\\n Local orientation\\n\\n Context of part in whole\\n\\n Graph as index\\n\\n Scaffolding for attributes\\n\\nWhen used as a scaffolding, the structure can show static or dynamic\\nattributes. Many graph drawing systems support color and line-width\\ncoding, text labels, and filtering -- as does our own. These filtering\\nand coding capabilities can be very powerful when used to show dynamic\\ndata. For instance, in the new Site Manager release a Web site's\\ntraffic logs can help show the paths taken by Web users. A hit\\nfrom one page to another is shown by briefly highlighting those nodes\\nand the link between them, for a laser-like effect.\\n\\n Local Orientation\\n\\nOur drawing and layout approaches also support finding\\ninteresting places when browsing through an unfamiliar graph. When the\\nuser clicks on a node, it is highlighted and undergoes an animated\\ntransition to the center of the sphere. Animation proves critical in\\nhelping the user maintain a sense of context.\\n\\nThe transition includes\\nboth a translational and rotational component. Thus, when a node\\nreaches the origin, its ancestors always appear on its left and its\\ndescendants on the right. This ``butterfly'' configuration \\nprovides a canonical local orientation and also serves to minimize\\nocclusion of both nodes and their text labels. If the structure were\\naligned with the principal axes of the window, the text labels would\\noften occlude each other, so we add a slight tilt. Our layout\\nalgorithm always places the node with the most descendants at the\\n``pole'' of the hemisphere along the same axis as the incoming link\\nfrom the parent node. Thus a simple and effective navigation strategy\\nfor finding potentially interesting complexity is to click on polar\\nnodes after their parents move into their canonical orientation.\\n\\nIn our drawing algorithm, we explicitly chose to draw the links into\\nand out of a node, even if we don't have time to draw the node at\\nthe other end. The presence of an unterminated link during motion \\nhints to the user of something interesting in that\\ndirection. This situation occurs frequently when drawing nontree\\nlinks, whose other end often lies far enough away from the center that\\nthe drawing loop ends before the terminating node can be drawn.\\n\\n Context of part in whole\\n\\nHyperbolic space very effectively presents a large amount of\\nspace around a focus node. For instance, in Figure 1 the\\nuser can see enough of the distant subtrees to identify dense and\\nsparse ones. The destinations of nontree links are\\ndistorted, but the rough sense of their destination helps the user\\nconstruct and maintain a mental model of the larger graph structure.\\nFigure 3 shows how the details become clear in a\\nsmooth transition when an area of the structure moves towards the\\ncenter. The context shows up on several levels: the local parent-child\\nrelationships of the spanning tree, the nontree links between\\ndisparate nodes in the graph, and the rough structure far away from the\\ncurrent focus of interest.\\n\\n Graph as index\\n\\nAlthough you could use the H3Viewer to create a stand-alone application,\\nit is most effective when integrated with other tools. If a graph\\nviewer is one of several views which all support linked selection and\\nfiltering, the graph structure becomes one way to index the\\ninformation. Such an index proves useful for selecting items in a\\nknown graph in addition to discovering patterns in an unfamiliar one.\\nIn the Site Manager system, we tightly integrate the 3D hyperbolic\\nbrowser with a 2D file browser and a search window that shows all the\\nmatches of some string as a 1D list. The user can choose the\\nappropriate display -- the one having the appropriate attributes for the task\\nat hand.\\n\\nConclusion\\n\\n\\xa0\\n\\nOur implementation can handle graphs two orders of magnitude\\nlarger than previous systems by manipulating a backbone spanning tree\\ninstead of the full graph. Carrying out both layout and drawing in 3D\\nhyperbolic space lets us see a large amount of context around a\\nfocus point. Our layout is tuned for a good balance between\\ninformation density and clutter, and our adaptive drawing algorithm\\nprovides a fluid interactive experience for the user by maintaining a\\nguaranteed frame rate.\\n\\nAcknowledgements\\n\\nWe appreciate the efforts of the following people and organizations in\\ncollecting the data used here: function call graph data from Anwar\\nGhuloum of the Stanford University Intermediate Format (SUIF)\\ncompilers group; Autonomous Systems data \\nfrom Hans-Werner Braun of the National Laboratory for Applied Network\\nResearch (NLANR) and David M. Meyer of the University\\nof Oregon Route Views Project. Thanks to François Guimbretière\\nand Pat Hanrahan for their advice and ideas. I gratefully\\nacknowledge the efforts of the rest of the Site Manager team at\\nSilicon Graphics: Ken Kershner, Greg Ferguson, Alan Braverman, Donna\\nScheele, and Doug O'Morain. This work was supported in part by Silicon\\nGraphics, the National Science Foundation (NSF) Graduate Research\\nFellowship Program, and Advanced Research Projects Agency (ARPA).\\n\\nReferences\\n\\n1\\nFranz\\xa0J. Brandenburg.\\nNice drawing of graphs are computationally hard.\\nIn P.\\xa0Gorney and M.\\xa0J. Tauber, editors, Visualization in\\n  Human-Computer Interaction, Lecture Notes in Computer Science 439, pages\\n  1-15, Berlin, 1988. Springer-Verlag.\\n\\n2\\nGiuseppe\\xa0Di Battista, Peter Eades, Roberto Tamassia, and Ioannis Tollis.\\nAnnotated bibliography on graph drawing algorithms.\\nComputational Geometry: Theory and Applications, 4(5):235-282,\\n  1994.\\n\\n3\\nTamara Munzner.\\nH3: Laying out large directed graphs in 3D hyperbolic space.\\nProceedings of the 1997 IEEE Symposium on Information\\n  Visualization, pages 2-10, 1997.\\n\\n4\\nIngo Bruss and Arne Frick.\\nFast interactive 3-D graph visualization.\\nIn Proceedings of Graph Drawing '95, Lecture Notes in Computer\\n  Science 1027, pages 99-110, Berlin, 1995. Springer-Verlag.\\n\\n5\\nGeorge Robertson, Jock Mackinlay, and Stuart Card.\\nAnimated 3D visualizations of hierarchical information.\\nIn Proceedings of the ACM SIGCHI Conference on Human Factors in\\n  Computing Systems, pages 189-194. ACM, April 1991.\\n\\n6\\nJohn\\xa0Horton Conway and Neil\\xa0J.A. Sloane.\\nSphere packings, lattices, and groups.\\nSpringer-Verlag, Berlin, 1988.\\n\\n7\\nGeorge\\xa0E. Martin.\\nThe Foundations of Geometry and the Non-Euclidean Plane.\\nSpringer-Verlag, Berlin, 1975.\\n\\n8\\nMark Phillips and Charlie Gunn.\\nVisualizing hyperbolic space: Unusual uses of 4x4 matrices.\\nIn 1992 Symposium on Interactive 3D Graphics, volume\\xa025,\\n  pages 209-214, New York, 1992. ACM SIGGRAPH.\\nspecial issue of Computer Graphics.\\n\\n9\\nJohn Lamping, Ramana Rao, and Peter Pirolli.\\nA Focus+Content technique based on hyperboic geometry for viewing\\n  large hierarchies.\\nIn Proceedings of the ACM SIGCHI Conference on Human Factors in\\n  Computing Systems, New York, May 1995. ACM.\\n\\n10\\nTamara Munzner and Paul Burchard.\\nVisualizing the structure of the World Wide Web in 3D hyperbolic\\n  space.\\nIn Proceedings of the VRML '95 Symposium, pages 33-38, New\\n  York, 1995. ACM SIGGRAPH.\\n\\n11\\nM.\\xa0Sheelagh\\xa0T. Carpendale, David\\xa0J. Cowperthwaite, and F.\\xa0David Fracchia.\\nExtending distortion viewing from 2D to 3D.\\nComputer Graphics and Applications, pages 42-51, 1997.\\n\\n\\nFigures\\n\\n\\xa0\\n\\n  \\n\\nFigure 1: Part of the Stanford graphics group Web site drawn as\\na graph in 3D hyperbolic space. The entire site has more than 20,000\\nnodes. 4000 of them in the neighborhood of the papers archive appear\\nin this frame. In addition to the main spanning tree, the image shows\\nthe nontree outgoing links from an index of every paper by title. The\\ntree is oriented so that ancestors of a node appear on the left and\\nits descendants grow to the right.\\n\\n\\xa0\\n\\n  \\n\\nFigure 2: The function call graph structure for a FORTRAN scientific\\ncomputing benchmark, where compiler and run-time analysis determined\\nthe spanning tree. The node coloring indicates whether a\\nparticular global variable was untouched (cyan), referenced (blue), or\\nmodified (pink).\\n\\n\\xa0\\n\\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n\\nFigure 3: Hyperbolic motion over a 30,000 element\\nUnix file system. Many nodes and edges project to subpixel areas and\\nare not visible. The left column of images shows translation of a\\nnode to the center, while the right shows rotation around that node.\\nThe rotation clarifies that objects lie inside a ball, not on\\nthe surface of a hemisphere. The file system has a strikingly large\\nbranching factor when compared with the Web sites in Figure\\n1 or the call graphs in Figure 2. The\\ndirectory that\\napproaches the center, /usr/lib, contains a large number\\nof files and subdirectories.\\n\\n\\xa0\\n  \\n  \\n\\nFigure 4:\\nThe H3Viewer guaranteed frame rate mechanism ensures\\ninteractive response for large graphs, even on slow machines. On the\\nleft is a frame drawn in 1/20th of a second during user interaction.\\nOn the right is a frame filled in by the idle callbacks\\nfor a total of 2 seconds after user activity stopped. The graph shows\\nthe peering relationships between the  Autonomous\\nSystems, which comprise the backbone of the Internet.  The 3000 routers\\nshown here are connected by over 10,000 edges in the full graph.\\n\\nAbout the Author\\n\\nTamara Munzner is currently a PhD candidate at Stanford University,\\nwhere she received a BS in computer science in 1991. In the\\nintervening years she was a member of the technical staff at the\\nGeometry Center, a mathematical visualization research group at the\\nUniversity of Minnesota. Her technical interests include graph drawing,\\ninformation visualization, mathematical visualization, interactive 3D\\ngraphics systems, and pedagogical video creation.\\n\\nContact Munzner at the Department of Computer Science, 360 Gates\\nBuilding 3B, Stanford University, Stanford, CA 94305 USA, email \\nmunzner@cs.stanford.edu, \\nhttp://graphics.stanford.edu/~munzner.\\n\\n  \\nReference\\n\\nMunzner, Tamara. \\n``Exploring Large Graphs in 3D Hyperbolic Space'',\\nIEEE Computer Graphics and Applications, Vol. 18, No. 4, pp. 18-23, July/August 1998.\\n\\n \\n\\nTamara Munzner Mon Jul  6 18:37:12 PDT 1998\\n\\n\\n\\n\",\n",
       " \"\\n\\n\\n\\n\\n Advances in the Theory and  Practice of Graph Drawing \\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\tAdvances in the Theory and \\n Practice of Graph Drawing\\n\\nRoberto Tamassia\\n\\n\\nDepartment of Computer Science\\n Brown University\\n Providence, RI 02912-1910\\n rt@cs.brown.edu\\n\\n\\n\\nAbstract:\\nThe visualization of conceptual structures is a key component of\\nsupport tools for complex applications in science and engineering.\\nForemost among the visual representations used are drawings of graphs\\nand ordered sets.  In this talk, we survey recent advances in the\\ntheory and practice of graph drawing.  Specific topics include bounds\\nand tradeoffs for drawing properties, three-dimensional\\nrepresentations, methods for constraint satisfaction, and experimental\\nstudies.\\n\\n\\nIntroduction\\n\\xa0\\n\\nIn this paper, we survey selected research trends in graph drawing, and\\noverview some recent results of the author and his collaborators.\\n\\nGraph drawing addresses the problem of constructing geometric\\nrepresentations of graphs, a key component of support tools for complex\\napplications in science and engineering.  Graph drawing is a young\\nresearch field that has growth very rapidly in the last decade.  One of\\nits distinctive characteristics is to have furthered collaborative\\nefforts between computer scientists, mathematicians, and applied\\nresearchers.\\n\\nA comprehensive bibliography on graph drawing\\nalgorithms\\xa0[21] cites more than 300 papers written\\nbefore 1993.  Most papers on graph drawing are cited in geom.bib,\\nthe computational geometry BibTeX bibliography available from \\nftp://cs.usask.ca/pub/geometry/ (search for keyword ``graph\\ndrawing'').\\nSurveys on various aspects of graph drawing appear in\\xa0[24, 32, 41, 44, 74, 75, 78, 82, 83].\\n\\nThe proceedings of the annual Symposium on Graph Drawing are published\\nby Springer-Verlag in the LNCS series\\xa0[88, 5].  Three\\nspecial issues of journals dedicated to graph drawing have been\\nrecently assembled\\xa0[14, 26, 25].  Additional special\\nissues are planned for future Graph Drawing Symposia.\\n\\nThe author maintains a WWW page (http://www.cs.brown.edu/people/rt/gd.html)\\nwith pointers to graph drawing resources on the Web.\\n\\nThe rest of this paper is organized as follows:\\nSection\\xa03 overviews lower an upper bounds on fundamental\\ndrawing properties,  such as area, and gives tradeoffs between them.\\nBasic graph drawing terminology is reviewed in Section\\xa02.\\nThree-dimensional drawings are discussed in Section\\xa04.\\nSection\\xa05 deals with methods for constraint\\nsatisfaction.  Finally, experimental studies are reported in\\nSection\\xa06.\\n\\nGraph Drawing Glossary\\n\\xa0\\n\\nFirst, we define some terminology on graphs pertinent to graph drawing:\\n\\n\\nn:\\n\\nnumber of vertices of the (di)graph being considered.\\n\\nm:\\n\\nnumber of edges of the (di)graph being considered.\\n\\nd:\\n\\nmaximum vertex degree (i.e., number of incident edges) of the (di)graph being considered.\\n\\ndegree-k graph:\\n\\ngraph with maximum degree   .\\n\\ndigraph:\\n\\ndirected graph, i.e., graph with directed edges (drawn\\nas arrows).\\n\\nacyclic digraph:\\n\\nwithout directed cycles.\\n\\ntransitive edge:\\n\\nedge (u,v) of a digraph is transitive if\\nthere is a directed path from u to v not containing edge (u,v).\\n\\nreduced digraph:\\n\\nwithout transitive edges.\\n\\nsource:\\n\\nvertex of a digraph without incoming edges.\\n\\nsink:\\n\\nvertex of a digraph without outgoing edges.\\n\\nst-digraph:\\n\\nacyclic digraph with exactly one source and one\\nsink, joined by an edge (also called bipolar digraph).\\n\\nconnected graph:\\n\\nany two vertices are joined by a path.\\n\\nbiconnected graph:\\n\\nany two vertices are joined by two\\nvertex-disjoint paths.\\n\\ntriconnected graph:\\n\\nany two vertices are joined by three\\nvertex-disjoint paths.\\n\\ntree:\\n\\nconnected graph without cycles.\\n\\nrooted tree:\\n\\ndirected tree with a distinguished vertex, called\\nthe root, such that each vertex lies\\non a directed path to the root.\\n\\nbinary tree:\\n\\nrooted tree where each vertex has at most two\\nincoming edges.\\n\\nlayered (di)graph:\\n\\nthe vertices are partitioned into sets,\\ncalled layers.  A rooted tree can be viewed as a layered digraph where\\nthe layers are sets of vertices at the same distance from the root.\\n\\nk-layered (di)graph:\\n\\nlayered (di)graph with k layers.\\n\\n\\n\\nIn a drawing of a graph, vertices are represented by points (or by\\ngeometric figures such as circles or rectangles) and edges are\\nrepresented by curves such that any two edges intersect at most in a\\nfinite number of points.  Except for Section\\xa04, which covers\\nthree-dimensional drawings, we consider drawings in the plane.\\nThe following types of drawings are defined:\\n\\n\\npolyline drawing:\\n\\neach edge is a polygonal\\nchain (Fig.\\xa01(a)).\\n\\nstraight-line drawing:\\n\\neach edge is a\\nstraight-line segment (Fig.\\xa01(b)).\\n\\northogonal drawing:\\n\\neach edge is a chain of\\nhorizontal and vertical segments (Fig.\\xa01(c)).\\n\\nbend:\\n\\nin a polyline drawing, point where two segments part of\\nthe same edge meet (Fig.\\xa01(a)).\\n\\ncrossing:\\n\\npoint where two edges intersect (Fig.\\xa01(b)).\\n\\ngrid drawing:\\n\\npolyline drawing such that vertices, crossings\\nand bends have integer coordinates.\\n\\nplanar drawing:\\n\\nno two edges cross (see\\nFig.\\xa01(d)).\\n\\nplanar (di)graph:\\n\\nadmits a planar drawing.\\n\\nembedded (di)graph:\\n\\nplanar (di)graph with a prespecified\\ntopological embedding (i.e., set of faces), which must be preserved in\\nthe drawing.\\n\\nupward drawing:\\n\\ndrawing of a digraph where each edge is\\nmonotonically nondecreasing in the vertical direction (see\\nFig.\\xa01(d)).\\n\\nupward planar digraph:\\n\\nadmits an upward planar drawing.\\n\\nlayered drawing:\\n\\ndrawing of a layered graph such that vertices\\nin the same layer are horizontally aligned  (also called hierarchical\\ndrawing).\\n\\nface:\\n\\na region of the plane  bounded by vertices and edges of a\\nplanar drawing.\\n\\nconvex drawing:\\n\\nplanar straight-line drawing such that the\\nboundary of each face  is a convex polygon.\\n\\nvisibility drawing:\\n\\ndrawing of a graph based on a geometric\\nvisibility relation.   E.g., the vertices might be drawn as horizontal\\nsegments, and the edges associated with vertically visible\\nsegments.\\n\\nproximity drawing:\\n\\ndrawing of a graph based on a geometric\\nproximity relation.   E.g., a tree is drawn as the Euclidean minimum\\nspanning tree of a set of points.\\n\\ndominance drawing:\\n\\nupward drawing of an acyclic digraph such\\nthat there exists a directed path from vertex u to vertex v if and\\nonly if    and   , where    and   \\ndenote the coordinates of a vertex.\\n\\nhv-drawing:\\n\\nupward orthogonal straight-line drawing of a binary\\ntree such that the drawings of the subtrees of each node are separated\\nby a horizontal or vertical line.\\n\\n\\n\\n\\xa0  \\nFigure 1: \\xa0 Types of drawings: (a)\\n\\tpolyline drawing of   ; (b) straight-line drawing of\\n\\t  ; (c) orthogonal drawing of   ; (d) planar\\n\\tupward drawing of an acyclic digraph. \\n\\n\\nStraight-line and orthogonal drawings are special cases of polyline\\ndrawings.  Polyline drawings provide great flexibility since they can\\napproximate drawings with curved edges.  However, edges with more than\\ntwo or three bends may be difficult to ``follow'' for the eye.  Also, a\\nsystem that supports editing of polyline drawings is more complicated\\nthan one limited to straight-line drawings.  Hence, depending on the\\napplication, polyline or straight-line drawings may be preferred.  If\\nvertices are represented by points, orthogonal drawings exist only for\\ngraphs of maximum vertex degree\\xa04.\\n\\nBounds and Tradeoffs on Drawing Properties\\n\\xa0\\n\\nFor various classes of graphs and drawing types, many\\nuniversal/existential upper and lower bounds for specific drawing\\nproperties have been discovered.  Such bounds typically exhibit\\ntrade-offs between drawing properties.  A universal bound applies to\\nall the graphs of a given class.  An existential bound applies to\\ninfinitely many graphs of the class.\\n\\nWhenever we give bounds on the area or edge length, we assume that the\\ndrawing is constrained by some resolution rule that prevents it from\\nbeing arbitrarily scaled down (e.g., requiring a grid drawing, or a\\nminimum unit distance between any two vertices).\\n\\nBounds on the Area\\n\\nTable\\xa01 summarizes selected universal upper\\nbounds and existential lower bounds on the area of drawings of graphs.\\n\\n\\xa0\\n\\n\\n\\n\\n\\n\\nClass of Graphs\\t\\t Drawing Type \\t\\n Area Ref.\\n\\n\\nrooted tree\\t\\t upward planar straight-line grid\\n \\n \\n [12, 79]\\n\\n\\nrooted tree\\t\\t strictly upward planar straight-line grid\\n \\n \\n [12]\\n\\n\\ndegree-    rooted\\ntree\\t\\t\\t upward planar polyline grid\\n \\n O(n)\\t\\t\\n [38]\\n\\n\\nbinary tree\\t\\t upward planar orthogonal grid\\n \\n \\n [38]\\n\\n\\ntree\\t\\t\\t planar straight-line grid\\n \\n \\n [12, 79]\\n\\n\\ndegree-   tree\\t planar polyline grid\\n \\n O(n)\\t\\t\\n [38]\\n\\n\\ndegree-4 tree\\t\\t planar orthogonal grid\\n \\n O(n)\\t\\t\\n [93, 60]\\n\\n\\nplanar graph\\t\\t planar polyline grid\\n \\n \\n [27, 28, 56]\\n\\n\\nplanar graph\\t\\t planar straight-line\\n \\n\\n [40]\\n\\n\\nplanar graph\\t\\t planar straight-line grid\\n \\n \\n [19, 77]\\n\\n\\ntriconnected planar\\ngraph\\t\\t\\t planar straight-line convex grid\\n \\n \\n [56]\\n\\n\\nplanar graph\\t\\t planar orthogonal grid\\n \\n \\n [3, 56, 81, 86]\\n\\n\\nplanar degree-4 graph\\t orthogonal grid\\n \\n \\n [93, 60, 2]\\n\\n\\nupward planar digraph\\t upward planar grid straight-line\\n \\n \\n [1, 28, 39]\\n\\n\\nreduced planar st-digraph  upward planar grid straight-line dominance\\n \\n \\n [28]\\n\\n\\nupward planar digraph\\t upward planar grid polyline\\n \\n \\n [27, 28]\\n\\n\\ngeneral graph\\t\\t polyline grid\\n \\n \\n\\n\\n\\n\\n\\nTable 1:  Universal upper bounds and existential lower bounds on the\\narea of drawings of graphs.  We denote with a an arbitrary constant\\nsuch that    .  We denote with b and c fixed constants\\nsuch that 1 < b < c.\\n\\xa0\\n\\n\\n\\n\\nIn general, the effect of bends on the area requirement is dual.  On\\none hand, bends occupy space and hence negatively affect the area. On\\nthe other hand, bends may help in routing edges without using\\nadditional space.\\n\\nThe following comments apply to Table\\xa01.  Linear or\\nalmost-linear bounds on the area can be achieved for trees.  See\\nTable\\xa04 for trade-offs between area and aspect-ratio in\\ndrawings of trees.  Planar graphs admit planar drawings with quadratic\\narea.  However, the area requirement of planar straight-line drawings\\nmay be exponential if high angular resolution is also desired.  Almost\\nlinear area can be instead achieved in nonplanar drawings of planar\\ngraphs, which have applications to VLSI circuits.  Upward planar\\ndrawings provide an interesting trade-off between area and the total\\nnumber of bends.  Indeed, unless the digraph is reduced, the area can\\nbecome exponential if a straight-line drawing is required.  A quadratic\\narea bound is achieved only at the expense of a linear number of\\nbends.\\n\\nBounds on the Angular Resolution\\n\\nTabe\\xa02 summarizes selected universal lower bounds and\\nexistential upper bounds on the angular resolution of drawings of\\ngraphs.\\n\\n\\xa0\\n\\n\\n\\n\\n\\n\\n\\nClass of Graphs\\t\\t\\t Drawing Type \\n Angular Resolution Ref.\\n\\n\\ngeneral graph\\t\\t\\t\\t\\t straight-line\\n   \\n [35]\\n\\n\\nplanar graph\\t\\t\\t\\t\\t straight-line\\n   \\n [35]\\n\\n\\nplanar graph\\t\\t\\t\\t\\t planar straight-line\\n   \\n [40, 65]\\n\\n\\n\\n\\nTable 2: Universal lower bounds and existential upper bounds on the\\nangular resolution of drawings of graphs.  We denote with c a\\nfixed constant such that c > 1.\\n\\xa0\\n\\n\\nBounds on the Number of Bends\\n\\nTable\\xa03 summarizes selected universal upper\\nbounds and existential lower bounds on the total and maximum number of\\nbends in orthogonal drawings.  Some bounds are stated for    or\\n   because the maximum number of bends is at least 2 for   \\nand at least 3 for the skeleton graph of an octahedron, in any planar\\northogonal drawing\\n\\n\\xa0\\n\\n\\n\\n\\n\\n\\n\\nClass of Graphs\\t\\t\\t Drawing Type \\t\\n Total No. Bends Max No. Bends Ref.\\n\\n\\ndegree-4 graph   orthogonal\\t\\t\\t\\n \\n \\n \\n \\n [3]\\n\\n\\nplanar degree-4 graph   orthogonal planar\\n \\n \\n \\n \\n [3, 89]\\n\\n\\nembedded degree-4 graph \\t orthogonal planar\\t\\n \\n \\n \\n \\n [34, 64, 86, 89]\\n\\n\\nbiconnected embedded degree-4\\ngraph \\t\\t\\t\\t orthogonal planar\\n \\n \\n \\n \\n [34, 64, 86, 89]\\n\\n\\ntriconnected embedded degree-4\\ngraph\\t\\t\\t\\t orthogonal planar\\n \\n \\n \\n \\n [56]\\n\\n\\nembedded degree-3 graph   orthogonal planar\\n \\n \\n \\n \\n [56, 63]\\n\\n\\n\\n\\nTable 3:  Orthogonal drawings: universal upper bounds and existential\\nlower bounds on the total and maximum number of bends.\\nNotes:    ;      .\\n\\xa0\\n\\n\\nTrade-Off Between Area and Aspect-Ratio\\n\\n\\xa0\\n\\nThe ability to construct area-efficient drawings is essential in\\npractical visualization applications, where screen space is at a\\npremium.  However, achieving small area is not enough: e.g., it is easy\\nto see that a drawing with high aspect-ratio may not be conveniently\\nplaced on a workstation screen, even if it has modest area.  Hence, it\\nis important to keep the aspect-ratio small.  Ideally, one would like\\nto obtain small area for any given aspect-ratio in a wide range.  This\\nwould provide graphical user interfaces with the flexibility of fitting\\ndrawings in arbitrarily shaped windows.\\n\\nA variety of trade-offs for the area and aspect-ratio arise even when\\ndrawing graphs with a simple structure, such as trees.\\nTable\\xa04 summarizes selected universal\\nbounds that can be simultaneously achieved on the area and the\\naspect-ratio of various types of drawings of trees.\\n\\n\\xa0\\n\\n\\n\\n\\n\\n\\n\\nClass of Graphs\\t\\t\\t Drawing Type\\n Area \\t\\t\\t\\t Aspect-Ratio\\n Ref.\\n\\n\\nrooted tree\\t\\t\\t upward planar straight-line layered grid\\n   O(1)\\n [72]\\n\\n\\nrooted tree\\t\\t\\t upward planar straight-line grid\\n   \\n [12, 79]\\n\\n\\nrooted degree-O(1) tree\\t upward planar polyline grid\\n O(n)\\t\\t\\t \\n [38]\\n\\n\\nbinary tree\\t\\t \\t upward planar orthogonal grid\\n   \\n [38]\\n\\n\\ndegree-4 tree\\t\\t\\t orthogonal grid\\n O(n)\\t\\t\\t O(1)\\n [93, 60]\\n\\n\\ndegree-4 tree\\t\\t\\t orthogonal grid, leaves on convex hull\\n   O(1)\\n [7]\\n\\n\\nTable 4:  Universal upper bounds that can be simultaneously achieved\\nfor the area and aspect-ratio in drawings of trees.\\nWe denote with a an arbitrary constant such that   .\\n\\xa0\\n\\n\\n\\n\\nWhile upward planar straight-line drawings are the most natural way of\\nvisualizing rooted trees, the existing drawing techniques are\\nunsatisfactory with respect to either the area requirement or the\\naspect ratio.  The situation is similar for orthogonal drawings.\\nRegarding polyline drawings, linear area can be achieved with a\\nprescribed aspect ratio\\xa0[38].  However,\\nexperiments show that this is done at the expense of a somehow\\naesthetically unappealing drawing.\\n\\nFor non-upward drawings of trees, linear area and optimal aspect ratio\\nare possible for planar orthogonal drawings, and a small (logarithmic)\\namount of extra area is needed if the leaves are constrained to be on\\nthe convex hull of the drawing (e.g., pins on the boundary of a\\nVLSI circuit).  However, the non-upward drawing methods do\\nnot seem to yield aesthetically pleasing drawings, and are suited more\\nfor VLSI layout than for visualization applications.\\n\\nTrade-Off Between Area and Angular Resolution\\n\\nTable\\xa05 summarizes selected universal\\nbounds that can be simultaneously achieved on the area and the angular\\nresolution of drawings of graphs.\\n\\n\\xa0\\n\\n\\n\\n\\n\\n\\n\\nClass of Graphs\\t\\t\\t Drawing Type \\n Area \\t\\t\\t\\t Angular Resolution\\n Ref.\\n\\n\\nplanar graph\\t\\t\\t straight-line\\t\\n   \\n [35]\\n\\n\\nplanar graph\\t\\t\\t straight-line\\n   \\n [35]\\n\\n\\nplanar graph\\t\\t\\t planar straight-line grid\\n   \\n [19, 77]\\n\\n\\nplanar graph\\t\\t\\t planar straight-line\\n   \\n [65]\\n\\n\\nplanar graph \\t\\t\\t planar polyline grid\\n   \\n [56]\\n\\n\\n\\n\\nTable 5:  Universal asymptotic upper bounds for the\\narea and lower bounds for the angular resolution that can be\\nsimultaneously achieved in drawings of graphs.   We denote with b and c\\nfixed constants such that b>1 and c>1.\\n\\xa0\\n\\n\\n\\nUniversal lower bounds on the angular resolution exist that depend only\\non the degree of the graph.  Also, substantially better bounds can be\\nachieved by drawing a planar graph with bends or in a nonplanar way.\\n\\nOpen Problems\\n\\n Determine the area requirement of (upward) planar straight-line\\ndrawings of trees.   There is currently an    gap between the\\nknown upper and lower bounds (Table\\xa01). Determine the area requirement of orthogonal (or, more generally,\\npolyline) nonplanar drawings of planar graphs.  There is currently an\\n   gap between the known upper and lower bounds (Table\\xa01). Close the gap between the    universal lower\\nbound and the    existential upper bound on the\\nangular resolution of straight-line drawings of general graphs (Table\\xa02). Close the gap between the    universal lower\\nbound and the    existential upper bound on\\nthe angular resolution of planar straight-line drawings of planar\\ngraphs (Table\\xa02). Determine the best possible aspect-ratio and area that can be\\nsimultaneously achieved for (upward) planar straight-line and\\northogonal drawings of trees (Table\\xa04).\\n\\nThree Dimensional Drawings of Graphs\\n\\xa0\\n\\nRecent advances in hardware and software technology for computer\\ngraphics open the possibility of displaying three-dimensional (3D)\\nvisualizations on a variety of low-cost workstations,\\nand a handful of researchers (and film makers)\\nhave begun to explore the\\npossibilities of displaying graphs using this new technology\\nPrevious research on\\n3D graph drawing has focused on the development of visualization\\nsystems (see, e.g., [73, 76]).\\nMuch work needs to be done on the theoretical foundations of 3D graph\\ndrawing.  Recent progress has been reported\\nin\\xa0[8, 9, 33, 43, 51, 62].\\n\\n3D Convex Drawings\\n\\nA 3D convex drawing of a graph G is a realization of G by the\\nskeleton of a 3D convex polytope (see Fig.\\xa02.  The\\nwell-known Steinitz's theorem says that a graph admits a 3D convex\\ndrawing if and only if it is planar and triconnected\\xa0[80]\\n(see also Grünbaum\\xa0[42]), properties that can be verified\\nin linear time (see, e.g., [48, 49]).\\nInterestingly, it is a simple exercise to derive from the published\\nproofs of Steinitz's theorem a cubic-time method for constructing 3D\\nconvex drawings in the real-RAM model\\xa0[71].  Unfortunately,\\nthis approach seems to require at least exponential volume and an\\nexponential number of bits to implement.  Indeed, Onn and\\nSturmfels\\xa0[68] show how to construct a 3D convex grid\\ndrawing within a cube of side   .\\n\\n\\xa0  \\nFigure 2:  Example of a 3D convex drawing.\\n\\t\\xa0\\n\\n\\n\\nMaxwell\\xa0[67] (see\\nalso\\xa0[10, 11, 94]) describes a mapping that\\ntransforms a 2D convex drawings with a certain ``equilibrium stress\\nproperty'' into a 3D convex drawing.  Further results on this\\ntransformation are given by Hopcroft and Kahn\\xa0[50].  Eades\\nand Garvan\\xa0[31] show how to construct 3D convex drawings\\nby combining the above transformation with the 2D-drawing method of\\nTutte\\xa0[91, 92].  They also show that their drawings\\nhave exponential volume in the worst case.  Smith\\n(see\\xa0[47]) claims a polynomial-time algorithm for\\nconstructing a 3D convex drawing inscribed in a sphere, with vertex\\ncoordinates represented by   -bit numbers, for an n-vertex\\ngraph known to be inscribable (which can be tested in linear time,\\ne.g., for planar triangulations, due to a result of Dillencourt and\\nSmith\\xa0[30]).  Das and Goodrich\\xa0[17] present\\na linear-time algorithm for constructing a 3D convex drawing of a\\nmaximal planar graph such that the vertex coordinates are rational\\nnumbers that can be represented with a polynomial number of bits.\\n\\nChrobak, Goodrich and Tamassia\\xa0[8] have recently shown\\nhow to construct in    time a 3D convex drawing with O(n)\\nvolume such that the vertex coordinates are represented by   -bit rational numbers and any two vertices are at distance at least\\none.\\n\\nConstraint Satisfaction in Graph Drawing\\n\\xa0\\n\\nResearch in graph drawing has traditionally focused on \\nalgorithmic methods, where the drawing of the graph is generated\\naccording to a prespecified set of aesthetic criteria (such as\\nplanarity or area minimization) that are embodied in an algorithm.\\nAlthough the algorithmic approach is computationally efficient, it does\\nnot naturally support constraints, i.e., requirements that the user may\\nwant to impose on the drawing of a specific graph (e.g., clustering or\\naligning a given set of vertices).  Previous work has shown that only a\\nrather limited constraint satisfaction capability can be added to\\nexisting drawing algorithms (see, e.g.,\\n[29, 84]).\\n\\nRecently, several attempts have been made at developing languages for\\nthe specification of constraints and at devising techniques for graph\\ndrawing based on the resolution of systems of constraints (see, e.g.,\\n[20, 55, 66]).  Eades\\nand Lin\\xa0[61] attempt at combining algorithmic and\\ndeclarative methods in drawings of trees.  Brandenburg presents a\\ncomprehensive approach to graph drawing based on graph\\ngrammars\\xa0[4].\\n\\nVisual Graph Drawing\\n\\nA visual approach to graph drawing, where the layout of a graph is\\npictorially specified ``by example,'' is proposed by Cruz, Garg and\\nTamassia\\xa0[15, 16].  Within this approach, a\\ngraph is stored in an object-oriented database, and its drawing is\\ndefined used recursive visual rules of the visual meta-language DOODLE\\xa0[13].  The following types of drawings can be visually\\nexpressed in such a way that the system of constraints obtained from\\nthe application of the visual rules to the input graph can be solved in\\nlinear time:\\n\\n level drawings and box inclusion drawings of binary\\n\\ttrees;  -drawings of series-parallel\\n\\tdigraphs\\xa0[1]; polyline drawings\\xa0[27], visibility\\n\\tdrawings\\xa0[85], and tessellation\\n\\tdrawings\\xa0[87] of upward planar digraphs (see\\n\\tFig.\\xa03).\\n\\n\\n\\n\\xa0  \\nFigure 3:  Drawings of a planar st-digraph:\\n\\t(a) tessellation drawing; (b) visibility drawing;\\n\\t(c) upward polyline drawing.\\n\\t\\xa0\\n\\n\\n\\nIn the rest of this section, we present visual programs for drawing a\\nplanar st-digraph, i.e., an embedded planar acyclic digraph with\\nexactly one source and one sink, joined by an edge.  Such\\ndigraphs play an important role in the theory of ordered sets since\\ntheir transitive reductions are the covering digraphs of planar\\nlattices\\xa0[59].  Such visual programs can be easily modified\\nto construct drawings of upward planar digraphs, which are known to be\\nsubgraphs of planar st-digraphs\\xa0[58, 27].\\n\\nWe show in Figure\\xa04 a complete visual program for\\ntessellation representations.  We assume that the vertices, edges, and\\nfaces of the input planar st-digraph G are database objects,\\nwhere for each object o the following attributes describing the\\nembedding are stored:  left face   , right face   ,\\nbottom vertex   , and top vertex   .  Note that the\\nvalue of each attribute is another database object.\\n\\n\\xa0  \\nFigure 4:  Visual rules for constucting a tessellation\\n\\tdrawing of a planar st-digraph:\\n\\t(a) rule for a face;\\n\\t(b) special rule for the source vertex;\\n\\t(c) rule for a vertex;\\n\\t(d) rule for an edge.\\n\\t\\xa0\\n\\n\\n\\nEach rule defines the visual representation of a database object of a\\ncertain class (vertex, edge, and face).  For tessellation\\nrepresentations, this is a horizontal segment for a vertex, a vertical\\nsegment for a face, and a rectangle for an edge.  The visual notation\\nin the rule for an object o includes:\\n\\n geometric figures that give the visual representation of\\n\\tobject o, such as circles, segments, and rectangles; references to the visual representation of other objects\\n\\tgiven by attributes of o, denoted with dashed boxes labeled\\n\\tby the attribute; landmarks of the visual representations of o and of\\n\\tother referenced objects, shown as small squares with labels\\n\\t(e.g., MS, the ``middle South'' landmark, denotes the middle\\n\\tpoint of the bottom edge of a rectangle); and landmarks of the coordinate system, shown with small circles\\n\\t(e.g., ORIGIN denotes point (0,0)); explicit constraints between landmarks, shown as arrows\\n\\tjoining two landmarks with labels defining the constraint\\n\\timposed on the coordinates of the landmarks (e.g., in rule (d),\\n\\tthe dashed arrow with label max(1,  )[h,v] is an\\n\\texplicit constraint specifying minimum horizontal and vertical\\n\\tdistance 1 from the ``midpoint South'' MS to the\\n\\t``midpoint East'' of the rectangle); implicit constraints between landmarks, given by their\\n\\thorizontal or vertical alignment (e.g., in rule (d), the\\n\\t``midpoint East'' ME of the rectangle representing edge\\n\\te and the ``top endpoint'' TE of the referenced visual\\n\\trepresentation of the right face of e   must have\\n\\tthe same x-coordinate because they are drawn vertically\\n\\taligned).\\n\\n\\n\\nComplete visual programs for visibility representations and upward\\npolyline drawings are shown in Figures\\xa05 and\\n6, respectively.  In these two programs, the visual\\nrepresentation of the faces is a single point associated with landmark\\nF.  This point is invisible but contributes to the definition of\\nthe constraints.  Also, the visual representation of an edge includes a\\nvisible portion (vertical segment for a visibility representation and\\npolygonal chain with three segments for an upward polyline drawing) and\\nan invisible portion drawn with a conventional ``transparent color''\\n(a rectangle or segment with shaded lines in the figures).\\n\\n\\xa0  \\nFigure 5:  Visual rules for constucting a tessellation\\n\\tdrawing of a planar st-digraph:\\n\\t(a) rule for a face;\\n\\t(b) special rule for the source vertex;\\n\\t(c) rule for a vertex;\\n\\t(d) rule for an edge.\\n\\t\\xa0\\n\\n\\n\\n\\xa0  \\nFigure 6:  Visual rules for constucting a tessellation\\n\\tdrawing of a planar st-digraph:\\n\\t(a) rule for a face;\\n\\t(b) special rule for the source vertex;\\n\\t(c) rule for a vertex;\\n\\t(d) rule for an edge.\\n\\t\\xa0\\n\\nExperimental Graph Drawing\\n\\xa0\\n\\nMany graph drawing algorithms have been implemented and used in\\npractical applications.  Most papers show sample outputs, and some also\\nprovide limited experimental results on small test suites (see, e.g.,\\n[18, 36, 37, 53, 55, 57]\\nand the experimental papers in\\xa0[88]).  However, in order to\\nevaluate the practical performance of a graph drawing algorithm in\\nvisualization applications, it is essential to perform extensive\\nexperimentations with input graphs derived from the application\\ndomain.\\n\\nThe performance of four planar straight-line drawing algorithms on\\n10,000 randomly generated maximal planar graphs is compared\\nby Jones et al.\\xa0[52].\\n\\nHimsolt\\xa0[45] presents a comparative study of twelve graph\\ndrawings algorithms based on various approaches.  The experiments are\\nconducted on 100 sample graphs with the graph drawing system \\nGraphEd\\xa0[46].  Many examples of drawings constructed by\\nthe algorithms are shown, and various objective and subjective\\nevaluations on the aesthetic quality of the drawings produced are\\ngiven.\\n\\nBrandenburg and Rohrer\\xa0[6] compare five\\n``force-directed'' methods for constructing straight-line drawings of\\ngeneral undirected graphs.  The algorithms are tested on a a wide\\ncollection of examples and with different settings of the force\\nparameters.  The quality measures evaluated are crossings, edge length,\\nvertex distribution, and running time.  They also identify tradeoffs\\nbetween the running time and the aesthetic quality of the drawings\\nproduced.\\n\\nJünger and Mutzel\\xa0[54] investigate crossing\\nminimization strategies for straight-line drawings of 2-layer graphs,\\nand compare the performance of eight popular heuristics for this\\nproblem.\\n\\nExperiments on Orthogonal Drawings\\n\\nIn\\xa0[22, 23] Di Battista et al. present\\nan extensive experimental study comparing four general-purpose graph\\ndrawing algorithms.   The four algorithms, denoted Bend-Stretch,\\nColumn, Giotto, and Pair, take as input general graphs\\n(with no restrictions whatsoever on the connectivity, planarity, etc.)\\nand construct orthogonal grid drawings, which are widely used in\\nsoftware and database visualization applications.\\n\\nAlgorithms\\xa0Bend-Stretch and Giotto are based on a general approach where\\nthe drawing is incrementally specified in three phases: The first\\nphase, planarization, determines the topology of the drawing. The\\nsecond phase, orthogonalization, computes an orthogonal shape for\\nthe drawing. The third phase, compaction, produces the final\\ndrawing.  This approach allows homogeneous treatment of a wide range of\\ndiagrammatic representations, aesthetics and constraints\\xa0(see, e.g.,\\n[57, 84, 90]) and has been successfully used\\nin industrial tools.  The main difference between the two algorithms is\\nin the orthogonalization phase:  Algorithm Giotto uses a network-flow\\nmethod that guarantees the minimum number of bends but has quadratic\\ntime-complexity\\xa0[81].  Algorithm Bend-Stretch adopts the\\n``bend-stretching'' heuristic\\xa0[86] that only guarantees a\\nconstant number of bends on each edge but runs in linear time.\\n\\nAlgorithm Column is an  extension of the orthogonal drawing algorithm\\nby Biedl and Kant\\xa0[3] to graphs of arbitrary vertex\\ndegree.  The orthogonal grid drawing is incrementally constructed by\\nadding the vertices one at a time.  Namely, at each step a vertex v\\nis added plus the edges connecting v to previously added vertices.\\nSome columns of the grid are ``reserved'' to draw the remaining\\nincident edges of v. Concerning the position of v, since one row is\\nused for each vertex, the y-coordinate is immediately given by the\\norder of visit of v, and the x-coordinate is the one of the\\nreserved column of the incident edge of v that minimizes the number\\nof bends introduced by the new edges.  Algorithm Pair is an\\nextension of the orthogonal drawing algorithm by Papakostas and\\nTollis\\xa0[69, 70] to graphs of arbitrary vertex\\ndegree.\\n\\nExamples of ``typical'' drawings generated by Bend-Stretch, Column, Giotto,\\nand Pair are shown in Figures\\xa07.\\n\\n\\xa0\\n(a)  \\n(b)  \\n(c)  \\n(d)  \\nFigure 7: Drawings of the same 63-vertex graph produced by algorithms\\n\\t\\t(a) Bend-Stretch, (b) Giotto, (c) Column, \\n\\t\\tand (d) Pair, respectively.\\n\\t\\xa0\\n\\n\\nThe test data (available on the Internet) are 11,582 graphs, ranging\\nfrom 10 to 100 vertices, generated from a core set of 112 graphs used\\nin ``real-life'' software engineering and database applications.  The\\nexperiments provide a detailed quantitative evaluation of the\\nperformance of the four algorithms and show that they exhibit\\ntrade-offs between ``aesthetic'' properties (e.g., crossings, bends,\\nedge length) and running time.  For example, Fig.\\xa08 shows\\nthe average area number of crossings, and CPU time.  The observed\\npractical behavior of the algorithms is consistent with their\\ntheoretical properties.  Namely, Giotto outperforms the other\\nalgorithms for most quality measures but is considerably slower than\\nColumn and Pair.\\n\\n\\xa0  \\nFigure 8: (a)\\xa0Average area versus number of vertices.\\n\\t(b)\\xa0Average number of crossings versus number of vertices.\\n\\t(c)\\xa0Average CPU time (seconds) versus number of vertices.\\n\\t\\xa0\\n\\n\\n\\nReferences\\n\\n1\\nP.\\xa0Bertolazzi, R.\\xa0F. Cohen, G.\\xa0Di Battista, R.\\xa0Tamassia, and I.\\xa0G. Tollis.\\nHow to draw a series-parallel digraph.\\nInternat. J. Comput. Geom. Appl., 4:385-402, 1994.\\n\\n2\\nS.\\xa0N. Bhatt and F.\\xa0T. Leighton.\\nA framework for solving VLSI graph layout problems.\\nJ. Comput. Syst. Sci., 28:300-343, 1984.\\n\\n3\\nT.\\xa0Biedl and G.\\xa0Kant.\\nA better heuristic for orthogonal graph drawings.\\nIn Proc. 2nd Annu. European Sympos. Algorithms (ESA '94),\\n  volume 855 of Lecture Notes in Computer Science, pages 24-35.\\n  Springer-Verlag, 1994.\\n\\n4\\nF.\\xa0J. Brandenburg.\\nDesigning graph drawings by layout graph grammars.\\nIn R.\\xa0Tamassia and I.\\xa0G. Tollis, editors, Graph Drawing (Proc.\\n  GD '94), volume 894 of Lecture Notes in Computer Science, pages\\n  416-427. Springer-Verlag, 1995.\\n\\n5\\nF.\\xa0J. Brandenburg, editor.\\nGraph Drawing (Proc. GD '95), volume 1027 of Lecture Notes\\n  in Computer Science.\\nSpringer-Verlag, 1996.\\n\\n6\\nFranz\\xa0J. Brandenburg and Christoph Rohrer.\\nAn experimental comparison of force-directed and randomized graph\\n  drawing algorithms.\\nIn F.\\xa0J. Brandenburg, editor, Graph Drawing (Proc. GD '95),\\n  volume 1027 of Lecture Notes in Computer Science. Springer-Verlag,\\n  1996.\\n\\n7\\nR.\\xa0P. Brent and H.\\xa0T. Kung.\\nOn the area of binary tree layouts.\\nInform. Process. Lett., 11:521-534, 1980.\\n\\n8\\nM.\\xa0Chrobak, M.\\xa0T. Goodrich, and R.\\xa0Tamassia.\\nConvex drawings of graphs in two and three dimensions.\\nIn Proc. 12th Annu. ACM Sympos. Comput. Geom., pages 319-328,\\n  1996.\\n\\n9\\nR.\\xa0F. Cohen, P.\\xa0Eades, T.\\xa0Lin, and F.\\xa0Ruskey.\\nThree-dimensional graph drawing.\\nIn R.\\xa0Tamassia and I.\\xa0G. Tollis, editors, Graph Drawing (Proc.\\n  GD '94), volume 894 of Lecture Notes in Computer Science, pages 1-11.\\n  Springer-Verlag, 1995.\\n\\n10\\nR.\\xa0Connelly.\\nRigidity and energy.\\nInvent. Math., 66:11-33, 1982.\\n\\n11\\nH.\\xa0Crapo and W.\\xa0Whitely.\\nStatics of frameworks and motions of panel structures, a projective\\n  geometric introduction.\\nStructural Topology, 6:42-82, 1982.\\n\\n12\\nP.\\xa0Crescenzi, G.\\xa0Di Battista, and A.\\xa0Piperno.\\nA note on optimal area algorithms for upward drawings of binary\\n  trees.\\nComput. Geom. Theory Appl., 2:187-200, 1992.\\n\\n13\\nI.\\xa0F. Cruz.\\nDOODLE: A visual language for object-oriented databases.\\nIn Proc. ACM SIGMOD, pages 71-80, 1992.\\n\\n14\\nI.\\xa0F. Cruz and P.\\xa0Eades, editors.\\nSpecial Issue on Graph Visualization, volume 6:3 of J.\\n  Visual Languages and Computing.\\n1995.\\n\\n15\\nI.\\xa0F. Cruz and A.\\xa0Garg.\\nDrawing graphs by example efficiently: Trees and planar acyclic\\n  digraphs.\\nIn R.\\xa0Tamassia and I.\\xa0G. Tollis, editors, Graph Drawing (Proc.\\n  GD '94), volume 894 of Lecture Notes in Computer Science, pages\\n  404-415. Springer-Verlag, 1995.\\n\\n16\\nI.\\xa0F. Cruz, A.\\xa0Garg, and R.\\xa0Tamassia.\\nEfficient constraint resolution in visual graph drawing.\\nManuscript, Dept. of Computer Sci., Brown University, 1996.\\n\\n17\\nGautam Das and Michael\\xa0T. Goodrich.\\nOn the complexity of approximating and illuminating three-dimensional\\n  convex polyhedra.\\nIn Proc. 4th Workshop Algorithms Data Struct., volume 955 of\\n  Lecture Notes in Computer Science, pages 74-85. Springer-Verlag, 1995.\\n\\n18\\nR.\\xa0Davidson and D.\\xa0Harel.\\nDrawing graphs nicely using simulated annealing.\\nTechnical report, Department of Applied Mathematics and Computer\\n  Science, The Weizmann Institute of Science, Rehovot, 1989.\\n\\n19\\nH.\\xa0de\\xa0Fraysseix, J.\\xa0Pach, and R.\\xa0Pollack.\\nHow to draw a planar graph on a grid.\\nCombinatorica, 10:41-51, 1990.\\n\\n20\\nE.\\xa0Dengler, M.\\xa0Friedell, and J.\\xa0Marks.\\nConstraint-driven diagram layout.\\nIn Proc. IEEE Sympos. on Visual Languages (VL '93), pages\\n  330-335, 1993.\\n\\n21\\nG.\\xa0Di Battista, P.\\xa0Eades, R.\\xa0Tamassia, and I.\\xa0G. Tollis.\\nAlgorithms for drawing graphs: an annotated bibliography.\\nComput. Geom. Theory Appl., 4:235-282, 1994.\\n\\n22\\nG.\\xa0Di Battista, A.\\xa0Garg, G.\\xa0Liotta, R.\\xa0Tamassia, E.\\xa0Tassinari, and F.\\xa0Vargiu.\\nAn experimental comparison of three graph drawing algorithms.\\nIn Proc. 11th Annu. ACM Sympos. Comput. Geom., pages 306-315,\\n  1995.\\n\\n23\\nG.\\xa0Di Battista, A.\\xa0Garg, G.\\xa0Liotta, R.\\xa0Tamassia, E.\\xa0Tassinari, and F.\\xa0Vargiu.\\nAn experimental comparison of four graph drawing algorithms.\\nSubmitted to Computational Geometry: Theory and Applications,\\n  1995.\\n\\n24\\nG.\\xa0Di Battista, W.\\xa0Lenhart, and G.\\xa0Liotta.\\nProximity drawability: a survey.\\nIn R.\\xa0Tamassia and I.\\xa0G. Tollis, editors, Graph Drawing (Proc.\\n  GD '94), volume 894 of Lecture Notes in Computer Science, pages\\n  328-339. Springer-Verlag, 1995.\\n\\n25\\nG.\\xa0Di Battista and R.\\xa0Tamassia, editors.\\nSpecial Issue on Geometric Representations of Graphs.\\nComput. Geom. Theory Appl.\\nTo appear.\\n\\n26\\nG.\\xa0Di Battista and R.\\xa0Tamassia, editors.\\nSpecial Issue on Graph Drawing.\\nAlgorithmica.\\nTo appear.\\n\\n27\\nG.\\xa0Di Battista and R.\\xa0Tamassia.\\nAlgorithms for plane representations of acyclic digraphs.\\nTheoret. Comput. Sci., 61:175-198, 1988.\\n\\n28\\nG.\\xa0Di Battista, R.\\xa0Tamassia, and I.\\xa0G. Tollis.\\nArea requirement and symmetry display of planar upward drawings.\\nDiscrete Comput. Geom., 7:381-401, 1992.\\n\\n29\\nG.\\xa0Di Battista, R.\\xa0Tamassia, and I.\\xa0G. Tollis.\\nConstrained visibility representations of graphs.\\nInform. Process. Lett., 41:1-7, 1992.\\n\\n30\\nM.\\xa0B. Dillencourt and W.\\xa0D. Smith.\\nA linear-time algorithm for testing the inscribability of trivalent\\n  polyhedra.\\nIn Proc. 8th Annu. ACM Sympos. Comput. Geom., pages 177-185,\\n  1992.\\n\\n31\\nP.\\xa0Eades and P.\\xa0Garvan.\\nDrawing stressed planar graphs in three dimensions.\\nIn F.\\xa0J. Brandenburg, editor, Graph Drawing (Proc. GD '95),\\n  volume 1027 of Lecture Notes in Computer Science. Springer-Verlag,\\n  1996.\\n\\n32\\nP.\\xa0Eades and X.\\xa0Lin.\\nHow to draw a directed graph.\\nIn Proc. IEEE Workshop on Visual Languages (VL'89), pages\\n  13-17, 1989.\\n\\n33\\nP.\\xa0Eades, C.\\xa0Stirk, and S.\\xa0Whitesides.\\nThe techniques of Komolgorov and Bardzin for three dimensional\\n  orthogonal graph drawings.\\nManuscript, Dept. of Computer Sci., Univ. of Newcastle, 1995.\\n\\n34\\nS.\\xa0Even and G.\\xa0Granot.\\nRectilinear planar drawings with few bends in each edge.\\nTechnical Report 797, Computer Science Dept., Technion, 1994.\\n\\n35\\nM.\\xa0Formann, T.\\xa0Hagerup, J.\\xa0Haralambides, M.\\xa0Kaufmann, F.\\xa0T. Leighton,\\n  A.\\xa0Simvonis, E.\\xa0Welzl, and G.\\xa0Woeginger.\\nDrawing graphs in the plane with high resolution.\\nSIAM J. Comput., 22:1035-1052, 1993.\\n\\n36\\nT.\\xa0Fruchterman and E.\\xa0Reingold.\\nGraph drawing by force-directed placement.\\nSoftw. - Pract. Exp., 21(11):1129-1164, 1991.\\n\\n37\\nE.\\xa0R. Gansner, S.\\xa0C. North, and K.\\xa0P. Vo.\\nDAG - A program that draws directed graphs.\\nSoftw. - Pract. Exp., 18(11):1047-1062, 1988.\\n\\n38\\nA.\\xa0Garg, M.\\xa0T. Goodrich, and R.\\xa0Tamassia.\\nArea-efficient upward tree drawings.\\nIn Proc. 9th Annu. ACM Sympos. Comput. Geom., pages 359-368,\\n  1993.\\n\\n39\\nA.\\xa0Garg and R.\\xa0Tamassia.\\nEfficient computation of planar straight-line upward drawings.\\nIn Graph Drawing '93 (Proc. ALCOM Workshop on Graph Drawing),\\n  Paris, France, 1993.\\n\\n40\\nA.\\xa0Garg and R.\\xa0Tamassia.\\nPlanar drawings and angular resolution: Algorithms and bounds.\\nIn Proc. 2nd Annu. European Sympos. Algorithms (ESA '94),\\n  volume 855 of Lecture Notes in Computer Science, pages 12-23.\\n  Springer-Verlag, 1994.\\n\\n41\\nA.\\xa0Garg and R.\\xa0Tamassia.\\nUpward planarity testing.\\nOrder, 12:109-133, 1995.\\n\\n42\\nB.\\xa0Grünbaum.\\nConvex Polytopes.\\nWiley, New York, NY, 1967.\\n\\n43\\nS.\\xa0M. Hashemi and I.\\xa0Rival.\\nUpward drawings to fit surfaces.\\nIn Order, Algorithms, and Applications (Proc. ORDAL '94),\\n  volume 831 of Lecture Notes in Computer Science, pages 53-58.\\n  Springer-Verlag, 1994.\\n\\n44\\nX.\\xa0He and M.-Y. Kao.\\nRegular edge labelings and drawings of planar graphs.\\nIn R.\\xa0Tamassia and I.\\xa0G. Tollis, editors, Graph Drawing (Proc.\\n  GD '94), volume 894 of Lecture Notes in Computer Science, pages\\n  96-103. Springer-Verlag, 1995.\\n\\n45\\nM.\\xa0Himsolt.\\nComparing and evaluating layout algorithms within GraphEd.\\nJ. Visual Languages and Computing, 6(3), 1995.\\n(special issue on Graph Visualization, edited by I. F. Cruz and P.\\n  Eades).\\n\\n46\\nM.\\xa0Himsolt.\\nGraphEd: a graphical platform for the implementation of graph\\n  algorithms.\\nIn R.\\xa0Tamassia and I.\\xa0G. Tollis, editors, Graph Drawing (Proc.\\n  GD '94), volume 894 of Lecture Notes in Computer Science, pages\\n  182-193. Springer-Verlag, 1995.\\n\\n47\\nC.\\xa0D. Hodgson, I.\\xa0Rivin, and W.\\xa0D. Smith.\\nA characterization of convex hyperbolic polyhedra and of convex\\n  polyhedra inscribed in the sphere.\\nBull. (New Series) of the AMS, 27(2):246-251, 1992.\\n\\n48\\nJ.\\xa0Hopcroft and R.\\xa0E. Tarjan.\\nDividing a graph into triconnected components.\\nSIAM J. Comput., 2:135-158, 1973.\\n\\n49\\nJ.\\xa0Hopcroft and R.\\xa0E. Tarjan.\\nEfficient planarity testing.\\nJ. ACM, 21(4):549-568, 1974.\\n\\n50\\nJ.\\xa0E. Hopcroft and P.\\xa0J. Kahn.\\nA paradigm for robust geometric algorithms.\\nAlgorithmica, 7:339-380, 1992.\\n\\n51\\nT.\\xa0Jéron and C.\\xa0Jard.\\n3D layout of reachability graphs of communicating processes.\\nIn R.\\xa0Tamassia and I.\\xa0G. Tollis, editors, Graph Drawing (Proc.\\n  GD '94), volume 894 of Lecture Notes in Computer Science, pages\\n  25-32. Springer-Verlag, 1995.\\n\\n52\\nS.\\xa0Jones, P.\\xa0Eades, A.\\xa0Moran, N.\\xa0Ward, G.\\xa0Delott, and R.\\xa0Tamassia.\\nA note on planar graph drawing algorithms.\\nTechnical Report 216, Department of Computer Science, University of\\n  Queensland, 1991.\\n\\n53\\nM.\\xa0Juenger and P.\\xa0Mutzel.\\nMaximum planar subgraphs and nice embeddings: Practical layout tools.\\nAlgorithmica.\\n(special issue on Graph Drawing, edited by G. Di Battista and R.\\n  Tamassia, to appear).\\n\\n54\\nMichael Jünger and Petra Mutzel.\\nExact and heuristic algorithms for 2-layer straightline crossing\\n  minimization.\\nIn F.\\xa0J. Brandenburg, editor, Graph Drawing (Proc. GD '95),\\n  volume 1027 of Lecture Notes in Computer Science. Springer-Verlag,\\n  1996.\\n\\n55\\nT.\\xa0Kamada.\\nVisualizing Abstract Objects and Relations.\\nWorld Scientific Series in Computer Science, 1989.\\n\\n56\\nG.\\xa0Kant.\\nDrawing planar graphs using the canonical ordering.\\nAlgorithmica.\\n(special issue on Graph Drawing, edited by G. Di Battista and R.\\n  Tamassia, to appear).\\n\\n57\\nG.\\xa0Kant.\\nAlgorithms for Drawing Planar Graphs.\\nPhD thesis, Dept. Comput. Sci., Univ. Utrecht, Utrecht, Netherlands,\\n  1993.\\n\\n58\\nD.\\xa0Kelly.\\nFundamentals of planar ordered sets.\\nDiscrete Math., 63:197-216, 1987.\\n\\n59\\nD.\\xa0Kelly and I.\\xa0Rival.\\nPlanar lattices.\\nCanad. J. Math., 27(3):636-665, 1975.\\n\\n60\\nC.\\xa0E. Leiserson.\\nArea-efficient graph layouts (for VLSI).\\nIn Proc. 21st Annu. IEEE Sympos. Found. Comput. Sci., pages\\n  270-281, 1980.\\n\\n61\\nT.\\xa0Lin and P.\\xa0Eades.\\nIntegration of declarative and algorithmic approaches for layout\\n  creation.\\nIn R.\\xa0Tamassia and I.\\xa0G. Tollis, editors, Graph Drawing (Proc.\\n  GD '94), volume 894 of Lecture Notes in Computer Science, pages\\n  376-387. Springer-Verlag, 1995.\\n\\n62\\nGiuseppe Liotta and Giuseppe Di Battista.\\nComputing proximity drawings of trees in the 3-dimemsional space.\\nIn Proc. 4th Workshop Algorithms Data Struct., volume 955 of\\n  Lecture Notes in Computer Science, pages 239-250. Springer-Verlag,\\n  1995.\\n\\n63\\nY.\\xa0Liu, P.\\xa0Marchioro, R.\\xa0Petreschi, and B.\\xa0Simeone.\\nTheoretical results on at most 1-bend embeddability of graphs.\\nTechnical report, Dipartimento di Statistica, Univ. di Roma ``La\\n  Sapienza'', 1990.\\n\\n64\\nY.\\xa0Liu, A.\\xa0Morgana, and B.\\xa0Simeone.\\nGeneral theoretical results on rectilinear embeddability of graphs.\\nActa Math. Appl. Sinica, 7:187-192, 1991.\\n\\n65\\nS.\\xa0Malitz and A.\\xa0Papakostas.\\nOn the angular resolution of planar graphs.\\nSIAM J. Discrete Math., 7:172-183, 1994.\\n\\n66\\nJ.\\xa0Marks.\\nA formal specification for network diagrams that facilitates\\n  automated design.\\nJournal of Visual Languages and Computing, 2:395-414, 1991.\\n\\n67\\nJ.\\xa0C. Maxwell.\\nOn reciprocal figures and diagrams of forces.\\nPhil. Mag. Ser., 27:250-261, 1864.\\n\\n68\\nS.\\xa0Onn and B.\\xa0Sturmfels.\\nA quantitative Steinitz' theorem.\\nBeiträge zur Algebra und Geometrie / Contributions to\\n  Algebra and Geometry, 35:125-129, 1994.\\n\\n69\\nA.\\xa0Papakostas and I.\\xa0G. Tollis.\\nImproved algorithms and bounds for orthogonal drawings.\\nIn R.\\xa0Tamassia and I.\\xa0G. Tollis, editors, Graph Drawing (Proc.\\n  GD '94), volume 894 of Lecture Notes in Computer Science, pages\\n  40-51. Springer-Verlag, 1995.\\n\\n70\\nA.\\xa0Papakostas and I.\\xa0G. Tollis.\\nImproved algorithms and bounds for orthogonal drawings.\\nTechnical report, 1995.\\n\\n71\\nF.\\xa0P. Preparata and M.\\xa0I. Shamos.\\nComputational Geometry: An Introduction.\\nSpringer-Verlag, New York, NY, 1985.\\n\\n72\\nE.\\xa0Reingold and J.\\xa0Tilford.\\nTidier drawing of trees.\\nIEEE Trans. Softw. Eng., SE-7(2):223-228, 1981.\\n\\n73\\nS.\\xa0P. Reiss.\\nAn engine for the 3D visualization of program information.\\nJ. Visual Languages and Computing, 6(3), 1995.\\n(special issue on Graph Visualization, edited by I. F. Cruz and P.\\n  Eades).\\n\\n74\\nI.\\xa0Rival.\\nGraphical data structures for ordered sets.\\nIn I.\\xa0Rival, editor, Algorithms and Order, pages 3-31. Kluwer\\n  Academic Publishers, 1989.\\n\\n75\\nI.\\xa0Rival.\\nReading, drawing, and order.\\nIn I.\\xa0G. Rosenberg and G.\\xa0Sabidussi, editors, Algebras and\\n  Orders, pages 359-404. Kluwer Academic Publishers, 1993.\\n\\n76\\nG.\\xa0G. Robertson, J.\\xa0D. Mackinlay, and S.\\xa0K. Card.\\nCone trees: Animated 3d visualizations of hierarchical information.\\nIn Proc. CHI, pages 189-193, 1991.\\n\\n77\\nW.\\xa0Schnyder.\\nEmbedding planar graphs on the grid.\\nIn Proc. 1st ACM-SIAM Sympos. Discrete Algorithms, pages\\n  138-148, 1990.\\n\\n78\\nF.\\xa0Shahrokhi, L.\\xa0A. Székely, and I.\\xa0Vrt'o.\\nCrossing numbers of graphs, lower bound techniques and algorithms: a\\n  survey.\\nIn R.\\xa0Tamassia and I.\\xa0G. Tollis, editors, Graph Drawing (Proc.\\n  GD '94), volume 894 of Lecture Notes in Computer Science, pages\\n  131-142. Springer-Verlag, 1995.\\n\\n79\\nY.\\xa0Shiloach.\\nArrangements of Planar Graphs on the Planar Lattice.\\nPhD thesis, Weizmann Institute of Science, 1976.\\n\\n80\\nE.\\xa0Steinitz and H.\\xa0Rademacher.\\nVorlesungen über die Theorie der Polyeder.\\nJulius Springer, Berlin, Germany, 1934.\\n\\n81\\nR.\\xa0Tamassia.\\nOn embedding a graph in the grid with the minimum number of bends.\\nSIAM J. Comput., 16(3):421-444, 1987.\\n\\n82\\nR.\\xa0Tamassia.\\nDrawing algorithms for planar st-graphs.\\nAustralasian Journal of Combinatorics, 2:217-235, 1990.\\n\\n83\\nR.\\xa0Tamassia.\\nPlanar orthogonal drawings of graphs.\\nIn Proc. IEEE Internat. Sympos. on Circuits and Systems, 1990.\\n\\n84\\nR.\\xa0Tamassia, G.\\xa0Di Battista, and C.\\xa0Batini.\\nAutomatic graph drawing and readability of diagrams.\\nIEEE Trans. Syst. Man Cybern., SMC-18(1):61-79, 1988.\\n\\n85\\nR.\\xa0Tamassia and I.\\xa0G. Tollis.\\nA unified approach to visibility representations of planar graphs.\\nDiscrete Comput. Geom., 1(4):321-341, 1986.\\n\\n86\\nR.\\xa0Tamassia and I.\\xa0G. Tollis.\\nPlanar grid embedding in linear time.\\nIEEE Trans. on Circuits and Systems, CAS-36(9):1230-1234,\\n  1989.\\n\\n87\\nR.\\xa0Tamassia and I.\\xa0G. Tollis.\\nTessellation representations of planar graphs.\\nIn Proc. 27th Allerton Conf. Commun. Control Comput., pages\\n  48-57, 1989.\\n\\n88\\nR.\\xa0Tamassia and I.\\xa0G. Tollis, editors.\\nGraph Drawing (Proc. GD '94), volume 894 of Lecture Notes\\n  in Computer Science.\\nSpringer-Verlag, 1995.\\n\\n89\\nR.\\xa0Tamassia, I.\\xa0G. Tollis, and J.\\xa0S. Vitter.\\nLower bounds for planar orthogonal drawings of graphs.\\nInform. Process. Lett., 39:35-40, 1991.\\n\\n90\\nH.\\xa0Trickey.\\nDrag: A graph drawing system.\\nIn Proc. Internat. Conf. on Electronic Publishing, pages\\n  171-182. Cambridge University Press, 1988.\\n\\n91\\nW.\\xa0T. Tutte.\\nConvex representations of graphs.\\nProceedings London Mathematical Society, 10(3):304-320, 1960.\\n\\n92\\nW.\\xa0T. Tutte.\\nHow to draw a graph.\\nProceedings London Mathematical Society, 13(3):743-768, 1963.\\n\\n93\\nL.\\xa0Valiant.\\nUniversality considerations in VLSI circuits.\\nIEEE Trans. Comput., C-30(2):135-140, 1981.\\n\\n94\\nW.\\xa0Whitney.\\nMotions and stresses of projected polyhedra.\\nStructural Topology, 7:13-38, 1982.\\n\\n\\n  About this document ... \\n\\n\\t\\t\\tAdvances in the Theory and \\n Practice of Graph Drawing\\n\\nThis document was generated using the LaTeX2HTML translator Version 96.1 (Feb 5, 1996) Copyright © 1993, 1994, 1995, 1996,  Nikos Drakos, Computer Based Learning Unit, University of Leeds.  The command line arguments were: \\nlatex2html -split 0 ordal96.tex. The translation was initiated by Roberto Tamassia on Thu Aug  1 13:38:15 EDT 1996 ...DrawingResearch supported in part by the National\\n\\t\\t\\tScience Foundation under grant CCR-9423847 and\\n\\t\\t\\tby the U.S. Army Research Office under grant\\n\\t\\t\\tDAAH04-96-1-0013.\\n\\n\\n...makersAn important plot element in\\nthe movie Jurassic Park involves a 3D virtual-reality traversal\\nof a tree representing a Unix file system.\\n\\n \\n \\n\\nRoberto Tamassia \\nThu Aug  1 13:38:15 EDT 1996\\n\\n\\n\\n\",\n",
       " '\\n\\n\\nEarliest Known Uses of Some of the Words of Mathematics (S)\\n\\n\\nEarliest Known Uses of Some of\\nthe Words of Mathematics (S)\\nLast revision: Jan. 24, 2004\\n\\n\\nST. ANDREW\\'S CROSS is the term used by Florian Cajori for the\\nmultiplication symbol X. It appears in 1916 in his \"William\\nOughtred, A Great Seventeenth-Century Teacher of Mathematics.\\n\\nSt. Andrew\\'s cross is found in 1615, although not in a\\nmathematical context, in Crooke, Body of Man: \"[They] doe\\nmutually intersect themselues in the manner of a Saint Andrewes\\ncrosse, or this letter X\" (OED2).\\n\\n\\n\\nThe ST. PETERSBURG PARADOX was formulated by Niklaus Bernoulli in 1713: \\nsee problem 5 in the first letter of\\nCorrespondence of Nicholas Bernoulli concerning the \\nSt Petersburg game with Montmort, Daniel Bernoulli and Cramer (translation by\\nRichard J. Pulskamp.)\\nThe association with St. Petersburg came about because the most prominent discussion was published\\nthere: this was Daniel Bernoulli\\'s \"Specimen Theoriae Novae de Mensara\\nSortis,\" Commentarii Academiae Scientiarum Imperialis Petropolitana,\\n5, 175-192 (1738). The paper has been translated as \"Exposition\\nof a New Theory on the Measurement of Risk,\" Econometrica, 22,\\n(1954), 23-36.\\n\\n\\n\\nIn 1768 D\\'Alembert\\nOpuscules\\nMathématiques vol. IV, p. 78\\n(English translation by\\nRichard J. Pulskamp)\\nused the phrase \"le probléme de petersbourg.\" J. Bertrand\\'s\\nCalcul des probabilités\\n(1889, p. 62) has a section on the \"Paradoxe de Saint-Pétersbourg\" and the \"paradox\" appears\\nin English in J. M. Keynes\\'s A Treatise on Probability (1921).\\xa0 \\n\\n\\n\\n[John Aldrich, based on Jacques Dutka, \"On the St. Petersburg paradox,\" \\nArch. Hist. Exact Sci. 39, No.1, 1988 and David (2001)]\\n\\n\\n\\nSee MORAL EXPECTATION and UTILITY.\\n\\nSADDLE POINT is found in 1922 in A Treatise on the Theory\\nof Bessel Functions by G. N. Watson (OED2).\\n\\n\\nSAGITTA was used in Latin by Fibonacci (1220) to mean the\\nversed sine (Smith, vol. 2). See VERSED SINE.\\n\\n\\nIn 1726 Alberti\\'s Archit. has: \"The .. Line .. from the middle\\nPoint of the Chord up to the Arch, leaving equal Angles on each Side,\\nis call\\'d the Sagitta\" (OED2).\\n\\n\\nWebster\\'s New International Dictionary (1909) has the\\nfollowing definition for sagitta: \"the distance from a point\\nin a curve to the chord; also, the versed sine of an arc; -- so\\ncalled (by Kepler) from its resemblance to an arrow resting on the\\nbow and string; also, Obs., an abscissa.\\n\\n\\n\\nThe 1961 third edition of the same dictionary has the following\\ndefinition: \"the distance from the midpoint of an arc to the\\nmidpoint of its chord.\"\\n\\n\\nSALIENT ANGLE. The OED2 has a 1687 citation for Angle\\nSaliant.\\n\\n\\nIn 1781 Sir John T. Dillon wrote in Travels Through Spain: \"He\\ncould find nothing which seemed to confirm the opinion relating to\\nthe salient and reentrant angles\" (OED2).\\n\\n\\nMathematical Dictionary and Cyclopedia of Mathematical Science\\n(1857) has: \"SALIENT ANGLE of a polygon, is an interior angle, less\\nthan two right angles.\"\\n\\n\\n\\nSee also CONVEX POLYGON.\\n\\nSAMPLE. The juxtaposition of sample and population seems to have originated\\nwith Karl Pearson writing in 1903 in Biometrika 2, 273. The relevant\\npassage appears in OED2: \"If the whole of a population were taken we should have\\ncertain values for its statistical constants, but in actual practice we are only\\nable to take a sample ....\"  Pearson\\'s colleague, the zoologist W. F. R. Weldon,\\nhad been using \"sample\" to refer to collections of observations since 1892.\\n(See also RANDOM SAMPLE.) [John Aldrich]\\n\\n\\nSAMPLE PATH. This term seems to have originated in sequential analysis and\\nthen was transferred to stochastic processes in general. JSTOR gives one pre-1950\\nreference, to Anscombe (1949) \"Large-Sample Theory of Sequential Estimation,\"\\nBiometrika, 36, 455-458 [John Aldrich].\\n\\n\\nSAMPLE SPACE was introduced into statistical theory by J. Neyman and E. S. Pearson, Phil. Trans. Roy.\\nSoc. A (1933), 289-337. It was associated with the representation of a sample \\ncomprising n numbers as a point in n-dimensional space, a representation R. \\nA. Fisher had exploited in articles going back to\\n1915. W. Feller used this notion of sample \\nspace in his \"Note on regions similar to the sample space,\" Statist. \\nRes. Mem., Univ. London 2, 117-125 (1938) but in the Introduction to \\nProbability Theory and its Applications, volume one of 1950 Feller used \\nthe term quite abstractly for the set of outcomes of an experiment. He attributed \\nthis general concept to Richard von Mises (1883-1953) who had referred to the \\nMerkmalraum (label space) in writings on the foundations of probability \\nfrom 1919 onwards [John Aldrich]. \\n\\n\\nSAMPLING DISTRIBUTION. R. A. Fisher seems to have introduced this term. It appears incidentally \\nin a 1922 paper\\n(JRSS, 85, 598)\\nand then in the title of his 1928 paper\\n\"The\\nGeneral Sampling Distribution of the Multiple Correlation Coefficient\",\\nProc. Roy. Soc. A, 213, p. 654.\\n\\n\\nSCALAR. See VECTOR.\\n\\nSCALAR PRODUCT. See VECTOR PRODUCT.\\n\\nSCALENE. In Sir Henry Billingsley\\'s 1570 translation of\\nEuclid\\'s Elements scalenum is used as a noun: \"Scalenum\\nis a triangle, whose three sides are all unequall.\"\\n\\n\\n\\nIn 1642 scalene is found in a rare use as a noun, referring to\\nscalene triangle in Song of Soul by Henry More: \"But if \\'t\\nconsist of points: then a Scalene I\\'ll prove all one with an\\nIsosceles.\"\\n\\n\\nScalenous is found in 1656 in Stanley, Hist. Philos..\\n(1687): \"A Pyramid consisteth of four triangles,..each whereof is\\ndivided..into six scalenous triangles.\"\\n\\n\\nScalene occurs as an adjective is in 1684 in Angular\\nSections by John Wallis: \"The Scalene Cone and Cylinder.\"\\n\\n\\n\\nThe earliest use of scalene as an adjective to describe a\\ntriangle is in 1734 in The Builder\\'s Dictionary. (All\\ncitations are from the OED2.)\\n\\n\\nSCATTER DIAGRAM.  According to H. L. Moore, Laws  of Wages (1911), the term \"scatter diagram\"\\nwas due to Karl Pearson. A JSTOR search finds the term first appearing in a 1906 article in\\nBiometrika (which Pearson edited), \"On the Relation Between the Symmetry of the Egg and\\nthe Symmetry of the Embryo in the Frog (Rana Temporaria)\" by J. W. Jenkinson. However the term only\\ncame into wide use in the 1920s when it began to appear in textbooks, e.g. F. C. Mills,\\nStatistical Methods of 1925. OED2 gives the following quotation from Mills:\\n\"The equation to a straight line, fitted by the method of least squares to the points on the scatter\\ndiagram, will express mathematically the average relationship between these two variables\" (X. 366)\\n[John Aldrich].\\n\\n\\nScattergram is found in 1938 in A. E. Waugh, Elem.\\nStatistical Method: \"This is the method of plotting the data on a\\nscatter diagram, or scattergram, in order that one may see the\\nrelationship\" (OED2).\\n\\n\\nScatterplot is found in 1939 in Statistical Dictionary of\\nTerms and Symbols by Kurtz and Edgerton (David, 1998).\\n\\n\\n\\nThe term SCHUR COMPLEMENT was introduced by Emilie V.\\nHaynsworth (1916-1985) and named for the German mathematician Issai\\nSchur (1875-1941), according to Matrix Analysis and Applied Linear\\nAlgebra by Carl D. Meyer.\\n\\n\\nSCIENTIFIC NOTATION. In 1895 in Computation Rules and\\nLogarithms Silas W. Holman referred to the notation as \"the\\nnotation by powers of ten.\" In the preface, which is dated August\\n1895, he wrote: \"The following pages contain ... an explanation of\\nthe use of the notation by powers of ten ... the notation by powers\\nof 10, as in the explanation here given. It seems unfortunate that\\nthis simple notation, so useful in computation and so great an aid in\\nthe explanation of numerical relations, is not universally\\nincorporated into arithmetical instruction.\" [James A. Landau]\\n\\n\\n\\nIn A Scrap-Book of Elementary Mathematics (1908) by William F.\\nWhite, the notation is called the index notation.\\n\\nScientific notation is found in 1921 in An Introduction to\\nMathematical Analysis by Frank Loxley Griffin: \"To write out\\nin the ordinary way any number given in this \\'Scientific\\nNotation,\\' we simply perform the indicated multiplication -- i.e.,\\nmove the decimal point a number of places equal to the\\nexponent, supplying as many zeros as may be needed.\"\\n\\n\\n\\nAccording to Webster\\'s Second New International Dictionary\\n(1934), numbers in this format are sometimes called condensed\\nnumbers.\\n\\n\\nOther terms are exponential notation and standard\\nnotation.\\n\\nSCORE and METHOD OF SCORING in the theory of statistical estimation. The derivative of the log-likelihood \\nfunction played an important part in R. A. Fisher\\'s theory of maximum likelihood \\nfrom its beginnings in the 1920s but the name score is more recent. The \"score\" \\nwas originally associated with a particular genetic application; a family is \\nassigned a score based on the number of children of each category and there \\nwere different ways scoring associated with different ways of estimating linkage. \\nIn a 1935 paper (\"The Detection of Linkage with Dominant Abnormalities,\" \\nAnnals of Eugenics, 6, 193) Fisher wrote that, because of the efficiency \\nof maximum likelihood, the \"ideal score\" is provided by the derivative \\nof the log-likelihood function. In 1948 C. R. Rao used the phrase efficient \\nscore (Proc. Cambr. Philos. Soc. 44, 50-57) and score by itself \\n(J. Roy. Statist. Soc., B, 10: 159-203) when writing about maximum likelihood \\nin general, i.e. without reference to the linkage application. Today \\n\"score\" is so established in this derivative of the log-likelihood \\nsense that the phrases \"non-ideal score\" or \"inefficient score\" \\nwould convey nothing.\\n\\n\\n\\n\\nIn 1946 - still in the genetic context - Fisher (\"A System \\nof Scoring Linkage Data, with Special Reference to the Pied Factors in Mice. \\nAmer. Nat., 80: 568-578) described an iterative method for obtaining \\nthe maximum likelihood value. Rao\\'s 1948 J. Roy. Statist. Soc. B paper\\ntreats the method in a more general framework and the phrase \"Fisher\\'s \\nmethod of scoring\" appears in a comment by Hartley. Fisher had already \\nused the method in a general context in his 1925\\n\"Theory\\nof Statistical Estimation\" paper\\n(Proc. Cambr. Philos. Soc. 22: 700-725) but it attracted neither attention \\nnor name. [This entry was contributed by John Aldrich, with some information \\ntaken from David (1995).] \\n\\n\\nSECANT (in trigonometry) was introduced by Thomas Fincke\\n(1561-1656) in his Thomae Finkii Flenspurgensis Geometriae rotundi\\nlibri XIIII, Basileae: Per Sebastianum Henricpetri, 1583. (His\\nname is also spelled Finke, Finck, Fink, and Finchius.) Fincke wrote\\nsecans in Latin.\\n\\n\\n\\nVieta (1593) did not approve of the term secant, believing it\\ncould be confused with the geometry term. He used\\nTranssinuosa instead (Smith vol. 2, page 622).\\n\\n\\nSECOND DIFFERENCE is found in 1777 in \"A Method of finding the Value of an\\ninfinite Series of decreasing Quantities of a certain Form,\" by Francis Maseres\\nin the Philosophical Transactions of the Royal Society vol. 67:\\n\"And 2dly, let these numbers be so related to\\neach other, that they not only shall form a decreasing progression\\ntheselves, but that their differences, a-b, b-c,\\nc-d, d-e, e-f, f-g, g-h, &c. shall also form a decreasng\\nprogression, so that b-c shall be less than a-b, and c-d\\nthan b-c, and d-e than c-d, and so on of the following differences;\\nand likewise, that the differences of these differences (which may be\\ncalled the second differences of the original numbers a, b, c, d, e,\\nf, g, h, &c. shall form a decreasing progression; and that the differences\\nof those second differences, or the third differences of the original numbers\\na, b, c, d, e, f, g, h, &c. shall also form a decreasing progression;\\nand in like manner, that the differences of the said third differences, or\\nthe fourth differences, of the original numbers a, b, c, d, e, f, g, h,\\n&c. and the fifth and sixth differences, and all higher differences, of the\\nsame numbers, shall also form decreasing progressions.\"\\n\\n\\nSECULAR EQUATION. See EIGENVALUE.\\n\\nSELF-CONJUGATE. Kramer (p. 388) says Galois used this\\nterm, referring to a normal subgroup.\\n\\n\\n\\nThe term SEMI-CUBICAL PARABOLA was coined by John Wallis\\n(Cajori 1919, page 181).\\n\\n\\n\\nThe term SEMIGROUP apparently was introduced in French as\\nsemi-groupe by J.-A. de Séguier in Élem. de la\\nThéorie des Groupes Abstraits (1904).\\n\\n\\nSEMI-INVARIANT appears in R. Frisch, \"Sur les semi-invariants\\net moments employés dans l\\'étude des distributions\\nstatistiques,\" Oslo, Skrifter af det Norske Videnskaps Academie, II,\\nHist.-Folos. Klasse, no. 3 (1926) [James A. Landau].\\n\\n\\nSENTENTIAL CALCULUS is found in English in 1937 in\\na translation by Amethe Smeaton of The Logical Syntax of Language\\nby Rudolf Carnap: \"Primitive sentences of the sentential calculus\" (OED2).\\n\\n\\nSEPARABLE appears in 1831 in Elements of the Integral\\nCalculus (1839) by J. R. Young: \"We shall first consider the\\ngeneral form X dy + Y dx = 0, which is the simplest for\\nwhich the variables are separable: X being a function of x\\nwithout y, and Y a function of y without x.\\n\\nSEQUENCE. The OED2 shows a use by Sylvester in 1882\\nin the American Journal of Mathematics with the \"rare\"\\ndefinition of a succession of natural numbers in order.\\n\\n\\nSequence is found in 1891 in a translation by George Lambert Cathcart\\nof the German An introduction to the study of the elements of the differential and integral calculus\\nby Axel Harnack:\\n\"What conditions must be fulfilled in order that for continually\\ndiminishing values of [delta]x, the quotient ... may present a continuous sequence of numbers\\ntending to a determinate limiting value: zero, finite or infinitely great?\" [University of\\nMichigan Historical Math Collection; the term may be considerably older.]\\n\\n\\nSERIAL CORRELATION. The term was introduced by G. U. Yule in his 1926 paper \"Why Do We Sometimes\\nGet Nonsense Correlations between Time-series? A Study in Sampling and the Nature of Time-series,\"\\nJournal of the Royal Statistical Society, 89, 1-69 (David 2001).\\n\\n\\nSERIES. According to Smith (vol. 2, page 481), \"The early\\nwriters often used proportio to designate a series, and this\\nusage is found as late as the 18th century.\"\\n\\n\\n\\nJohn Collins (1624-1683) wrote to James Gregory on Feb. 2, 1668/1669,\\n\"...the Lord Brouncker asserts he can turne the square roote into an\\ninfinite Series\" (DSB, article: \"Newton\").\\n\\n\\n\\nJames Gregory wrote to John Collins on Feb. 16, 1671 [apparently O.\\nS.]: \"I do not question that all equations may be formed by tables,\\nbut I doubt exceedingly if all equations can be solved by the help\\nonly of the tables of logarithms and sines without serieses.\"\\n\\n\\n\\nAccording to Smith (vol. 2, page 497), \"The change to the name\\n\\'series\\' seems to have been due to writers of the 17th century. ...\\nEven as late as the 1693 edition of his algebra, however, Wallis used\\nthe expression \\'infinite progression\\' for infinite series.\"\\n\\n\\n\\nIn the English translation of Wallis\\' algebra (translated by him\\nand published in 1685), Wallis wrote:\\n\\nNow (to return where we left off:) Those Approximations\\n(in the Arithmetick of Infinites) above mentioned, (for the Circle or\\nEllipse, and the Hyperbola;) have given occasion to others (as is\\nbefore intimated,) to make further inquiry into that subject; and\\nseek out other the like Approximations, (or continual approaches) in\\nother cases. Which are now wont to be called by the name of\\nInfinite Series, or Converging Series, or other names\\nof like import.\\n\\nThe SERPENTINE curve was named by Isaac Newton (1642-1727) in\\n1701, according to the Encyclopaedia Britannica.\\n\\nSET (earlier sense). In Lectures on Quaternions\\n(London: Whittaker & Co, 1853), Hamilton used the word \"set\" and even\\nonce the term \"theory of sets.\" However, he was not anticipating\\nCantor. Rather Hamilton used \"set\" to mean what we would call an\\n\"n-tuple\" or \"vector,\" that is, a set of numbers which could be used\\nas a coordinate in n-dimensional analytic geometry [James A. Landau].\\n\\n\\n\\nThe term SET first appears in Paradoxien des Unendlichen\\n(Paradoxes of the Infinite), Hrsg. aus dem schriftlichen Nachlasse des\\nVerfassers von Fr. Prihonsky, C. H. Reclam sen., xi, pp. 157, Leipzig,\\n1851. This small tract by Bernhard Bolzano (1781-1848) was published\\nthree years after his death by a student Bolzano had befriended\\n(Burton, page 592).\\n\\n\\nMenge (set) is found in Geometrie der Lage (2nd ed.,\\n1856) by Carl Georg Christian von Staudt: \"Wenn man die Menge aller\\nin einem und demselben reellen einfoermigen Gebilde enthaltenen\\nreellen Elemente durch n + 1 bezeichnet und mit diesem Ausdrucke,\\nwelcher dieselbe Bedeutung auch in den acht folgenden Nummern hat,\\nwie mit einer endlichen Zahl verfaehrt, so ...\" [Ken Pledger].\\n\\n\\n\\nGeorg Cantor (1845-1918) did not define the concept of a set\\nin his early works on set theory, according to Walter\\nPurkert in Cantor\\'s Philosophical Views.\\n\\n\\nCantor\\'s first definition of a set appears in an 1883 paper: \"By a\\nset I understand every multitude which can be conceived as an entity,\\nthat is every embodiment [Inbegriff] of defined elements which\\ncan be joined into an entirety by a rule.\" This quotation is taken\\nfrom Über unendliche lineare Punctmannichfaltigkeiten,\\nMathematische Annalen, 21 (1883).\\n\\n\\n\\nIn 1895 Cantor used the word Menge in Beiträge zur\\nBegründung der Transfiniten Mengenlehre, Mathematische\\nAnnalen, 46 (1895):\\n\\nBy a set we understand every collection\\n[Zusammenfassung] M of defined, well-distinguished\\nobjects m of our intuition [Zusammenfassung] or our\\nthinking (which are called the elements of M brought together\\nto form an entirety.\\n\\nThis translation was taken from Cantor\\'s Philosophical Views\\nby Walter Purkett.\\n\\n\\nSET THEORY appears in Georg Cantor, \"Sur divers\\nthéorèmes de la théorie des ensembles de points\\nsitués dans un espace continu à n dimensions.\\nPremière communication.\" Acta Mathematica 2, pp. 409-414\\n(1883) [James A. Landau].\\n\\n\\n\\nThe term is also found in Ivar Bendixson, \"Quelques\\nthéorèmes de la théorie des ensembles de points,\"\\nActa Mathematica 2, pp. 415-429 (1883) [James A. Landau].\\n\\n\\n\\nIn a letter to Mittag-Leffler, Cantor wrote on May 5, 1883,\\n\"Unfortunately, I am prevented by many circumstances from\\nworking regularly, and I would be fortunate to find, in you and your\\ndistinguished students, coworkers who probably will soon surpass me\\nin \\'set theory.\\'\" This quotation, which is presumably a translation,\\nwas taken from Cantor\\'s Continuum Problem by Gregory H. Moore.\\n\\n\\nTheory of point sets is found in 1912 in volume II of\\nLectures on the Theory of Functions of Real Variables by James\\nPierpont: \"After the epoch-making discoveries inaugurated in 1874 by\\nG. Cantor in the theory of point sets...\" [James A. Landau].\\n\\n\\nSet theory is found in English in 1926 in Annals of\\nMathematics (2d ser.) XXVII. 487: \"An important idea in set\\ntheory is that of relativity\" (OED2 update).\\n\\n\\nSEXAGESIMAL appears in A Proposal About Printing A treatise of\\nAlgebra by John Wallis, which was circulated in 1683: \"The Sexagesimal\\nFractions (introduced it seems by Ptolemy) did but imperfectly supply\\nthe want of such a Method of Numerical Figures.\"\\n\\n\\nSHORT DIVISION is found in 1844 in Introduction to The national arithmetic, on the inductive system\\nby Benjamin Greenleaf (1786-1864):\\n\"The method of operation by Short Division, or when the divisor does not exceed 12\"\\n[University of Michigan Digital Library].\\n\\n\\nSIBLING. The OED2 shows two citations for sibling from\\nthe Middle Ages. In both cases, the word had the obsolete meaning of\\n\"one who is of kin to another; a relative.\"\\n\\n\\nSibling does not appear in the 1890 Funk & Wagnalls\\nunabridged dictionary.\\n\\n\\n\\nThe OED2 shows a use of sib to mean \"brother or sister\" in\\n1901.\\n\\n\\n\\nAfter the two citations from the Middle Ages, the next citation in\\nthe OED2 for sibling is by Karl Pearson in 1903 in\\nBiometrika, where the word is used in its modern sense: \"These\\n[calculations] will enable us .. to predict the probable character in\\nany individual from a knowledge of one or more parents or brethren\\n(\\'siblings\\', = brothers or sisters).\"\\n\\n\\n\\nIn 1931, a translation by E. & C. Paul of Human Heredity\\nby E. Baur et al. has: \"The word \\'sib\\' or \\'sibling\\' is coming into\\nuse in genetics in the English-speaking world, as an equivalent of\\nthe convenient German term \\'Geschwister\\'\" (OED2).\\n\\n\\nSIEVE OF ERATOSTHENES is found in English in 1803 in a\\ntranslation of Bossut\\'s Gen. Hist. Math.: \"The famous sieve of\\nEratosthenes..affords an easy and commodious method of finding prime\\nnumbers\" (OED2).\\n\\n\\nSIGN OF AGGREGATION is found in 1863 in The Normal: or,\\nMethods of Teaching the Common Branches, Orthoepy, Orthography,\\nGrammar, Geography, Arithmetic and Elocution by Alfred Holbrook:\\n\"The signs of aggregation are the bar ___, which signifies that the\\nnumbers over which it is placed are to be taken together as one\\nnumber; also, the parenthesis, (); the brackets, []; and the braces,\\n{}, which signify that the quantities enclosed by them respectively\\nare to be taken together, as one quantity.\"\\n\\n\\n\\nIn 1900 in Teaching of Elementary Mathematics, David Eugene\\nSmith wrote: \"Signs of aggregation often trouble a pupil more than\\nthe value of the subject warrants. The fact is, in mathematics we\\nnever find any such complicated concatenations as often meet the\\nstudent almost on the threshold of algebra.\"\\n\\n\\nSIGN TEST appears in W. MacStewart, \"A note on the power of\\nthe sign test,\" Ann. Math. Statist. 12 (1941) [James A.\\nLandau].\\n\\n\\nSIGNED NUMBER. Signed magnitude appears in 1873 in\\nProc. Lond. Math. Soc.: \"A signed magnitude\" (OED2).\\n\\n\\nSigned number appears in the title \"The [Arithmetic]\\nOperations on Signed Numbers\" by Wilson L. Miser in Mathematics\\nMagazine (1932).\\n\\n\\nSIGNIFICANCE. Significant is found in 1885 in F. Y. Edgeworth, \"Methods \\nof Statistics,\" Jubilee Volume, Royal Statistical Society, pp. 181-217: \\n\"In order to determine whether the observed difference between the mean \\nstature of 2,315 criminals and the mean stature of 8,585 British adult males \\nbelonging to the general population is significant [etc.]\" (OED2).\\n\\n\\nSignificance is found in 1888 in\\nLogic of Chance by John Venn:\\n\"As before, common sense would feel little doubt that such a difference was significant, \\nbut it could give no numerical estimate of the significance\" (p. 486) (OED2). \\n\\n\\n\\nThe terms test of significance and significance \\ntest were used before the 1920s but only rarely. A JSTOR search finds significance \\ntest in Oswald H. Latter \"The Egg of Cuculus Canorus. An Enquiry into \\nthe Dimensions of the Cuckoo\\'s Egg and the Relation of the Variations to the \\nSize of the Eggs of the Foster-Parent, with Notes on Coloration, &c Biometrika, \\n1, (1902), p. 168. \\n\\n\\n\\nThe expression test of significance was very prominent \\nin R. A. Fisher\\'s Statistical \\nMethods for Research Workers (1925). This book introduced the \\nrelated terms level of significance (p. 161), 5 per cent point \\n(p. 198) and statistical significance (p. 218). \\n\\n\\nTesting the significance\\nis found in Student\\x92s \"New tables for testing the significance \\nof observations,\" Metron 5 (3) pp 105-108 (1925). \\n\\n\\nStatistically significant\\nis found in 1931 in L. H. C. Tippett, Methods of Statistics: \\n\"It is conventional to regard all deviations greater than those with probabilities \\nof 0.05 as real, or statistically significant\" (OED2). \\n\\n\\n\\n[This entry was contributed by John Aldrich.] \\n\\n\\nSIGNIFICANT DIGIT. Smith (vol. 2, page 16) indicates\\nLicht used the term in 1500, and shows a use of \"neun bedeutlich\\nfiguren\" by Grammateus in 1518.\\n\\n\\n\\nIn 1544, Michael Stifel wrote, \"Et nouem quidem priores,\\nsignificatiuae uocantur.\"\\n\\n\\nSignifying figures is found in 1542 in Robert Recorde, Gr.\\nArtes (1575): \"Of those ten one doth signifie nothing... The\\nother nyne are called Signifying figures\" (OED2).\\n\\n\\nSignificant figures is found in 1660 in Milton, Free\\nCommw.: \"Only like a great Cypher set to no purpose before a long\\nrow of other significant Figures\" (OED2).\\n\\n\\nSignificant figures is found in the first edition of the\\nEncyclopaedia Britannica (1768-1771) in the article \"Arithmetick\":\\n\"Of these, the first nine, in contradistinction to the cipher, are\\ncalled significant figures.\"\\n\\n\\nMathematical Dictionary and Cyclopedia of Mathematical Science\\n(1857) has this definition:\\n\\nSIGNIFICANT. Figures standing for numbers are called\\nsignificant figures. They are 1, 2, 3, 4, 5, 6, 7, 8, and\\n9.\\nSignificant digit is found in 1871 in Elements of trigonometry, plane and spherical\\nby Lefebure de Fourcy, translated from the last French ed. by Francis H. Smith:\\n\"Thus, the record.5386617, which in reality expresses the logarithm of 3.4567, can be made to express the\\nlogarithms of 345670, 34567, 3456.7, 345.67, 34.567, 3.4567, .34567, .034567, or any number formed by adding\\nciphers to the end of the former, or to beginning of the latter immediately after the decimal point; so that\\nevery logarithm taken out of the Tables for a particular number, becomes, by simply altering its characteristic,\\nthe logarithm of an infinite variety of other numbers, that is, of all that are expressed by the same succession of significant digits\"\\n[University of Michigan Digital Library].\\n\\n\\nNon-significant digit is found in January 1900 in Neal H.\\nEwing, \"The Shakespeare Name,\" Catholic World: \"Naught is the\\nnon-significant digit; though it means nothing, yet it counts for so\\nmuch.\"\\n\\n\\n\\nAn article in The Mathematics Teacher in October 1939\\nexplains that zero is sometimes a \"significant figure.\"\\n\\n\\nSIMILAR. In 1557 Robert Recorde used like in the\\nWhetstone of Witte: \"When the sides of one plat forme, beareth\\nlike proportion together as the sides of any other flatte forme of\\nthe same kinde doeth, then are those formes called like\\nflattes .. and their numbers, that declare their quantities, in\\nlike sorte are named like flattes\" (OED2).\\n\\n\\n\\nIn the manuscript of his Characteristica Geometrica which was\\nnot published by him, Leibniz wrote \"similitudinem ita notabimus:\\na ~ b.\"\\n\\n\\n\\nIn 1660 Isaac Barrow used like in his Euclid: \"If in a\\ntriangle FBE there be drawn AC a parallel to one side\\nFE, the triangle ABC shall be like to the whole\\nFBE (OED2).\\n\\n\\n\\nIn English, similar triangles is found in 1704 in Lexicon\\ntechnicum: \"Similar Triangles are such as have all their\\nthree Angles respectively equal to one another\" (OED2).\\n\\n\\nSIMILAR REGION was introduced \\nby J. Neyman and E. S. Pearson in \"On the Problem of the\\nMost Efficient Tests of Statistical Hypotheses\" Philosophical Transactions \\nof the Royal Society of London. Series A, 231. (1933), pp. 289-337. \\n(David, 1995.)\\n\\n\\nSIMPLE CLOSED CURVE occurs in 1873 in\\n\"On Listing\\'s Theorem\" by Arthur Cayley in the Messenger of Mathematics\\n[University of Michigan Historical Math Collection].\\n\\n\\n\\nSIMPLEX. William Kingdon Clifford (1848-1879) used the\\nterm prime confine in \"Problem in Probability,\"\\nEducational Times, Jan. 1886:\\n\\nNow consider the analogous case in geometry of n\\ndimensions. Corresponding to a closed area and a closed volume we\\nhave something which I shall call a confine. Corresponding to\\na triangle and to a tetrahedron there is a confine with n + 1\\ncorners or vertices which I shall call a prime confine as\\nbeing the simplest form of confine.\\nSIMPLEX METHOD is found in Robert Dorfman, \"Application of the\\nsimplex method to a game theory problem,\" Activity Analysis of\\nProduction and Allocation, Chap. XXII, 348-358 (1951).\\n\\n\\nSimplex approach is found in 1951 by George B. Dantzig (1914-\\n) in T. C. Koopman\\'s Activity Analysis of Production and\\nAllocation xxi. 339: \"The general nature of the \\'simplex\\'\\napproach (as the method discussed here is known)\" (OED2).\\n\\n\\nSIMPLY ORDERED SET was defined by Cantor in Mathematische\\nAnnalen, vol. 46, page 496.\\n\\n\\nSIMPSON\\x92S PARADOX appears in C. R. Blyth\\x92s \"On Simpson\\'s\\nParadox and the Sure-Thing Principle\",\\nJournal of the American Statistical Association, 67, (1972) and refers to a phenomenon\\ndiscussed in E. H. Simpson\\x92s \"The Interpretation of Interaction in Contingency\\nTables\", Journal of the Royal Statistical Society, B, 13, (1951),\\npp. 238-241: the sign of the association (or, in the case of variables,\\ncorrelation) in the population may not match that obtaining in all its\\nsub-populations.\\n\\n\\n\\nThe phenomenon was known to Karl Pearson and to G. Udny Yule from almost\\nthe beginning of their work on correlation and association. \"We are thus forced\\nto the conclusion that a mixture of heterogeneous groups, each of which\\nexhibits no organic correlation, will exhibit a greater or less amount of\\ncorrelation. This correlation may properly be called spurious . . .\" Pearson, Lee;\\n& Bramley-Moore, Philosophical Transactions of the Royal Society A,\\n192, (1899), p. 278. Yule preferred\\nto call the correlation (or association) in the population\\n\"illusory\" as in his Introduction to the Theory of\\nStatistics. [John Aldrich]\\n\\n\\n\\nSee SPURIOUS CORRELATION.\\n\\nSIMPSON\\'S RULE is found in an earlier algebraic sense in 1851 in\\nBonnycastle\\'s introduction to algebra by John Bonnycastle [University of Michigan\\nDigital Library].\\n\\n\\nSimpson\\'s rule is found in 1856 in A treatise on land-surveying\\nby William Mitchell Gillespie:\\n\"When the line determined by the offsets is a curved line, \\'Simpson\\'s\\nrule\\' gives the content more accurately\" [University of Michigan Digital Library].\\n\\n\\n\\nAccording to E. T. Whittaker and G. Robinson The Calculus of Observations (1924, p. 156)\\n\"This formula [generally known as Simpson\\'s or the parabolic rule] was first\\ngiven (in a geometrical form) by Cavalieri [1639], and later by James Gregory\\n[1668] and by Thomas Simpson [1743].\"\\n\\n\\nSIMSON LINE. The theorem was attributed to Robert Simson\\n(1687-1768) by François Joseph Servois (1768-1847) in the\\nGergonne\\'s Journal, according to Jean-Victor Poncelet in\\nTraité des propriétés projectives des\\nfigures. The line does not appear in Simson\\'s work and is\\napparently due to William Wallace. [The University of St. Andrews\\nwebsite]\\n\\n\\nSIMULTANEOUS EQUATIONS occurs in 1842 in Colenso, Elem.\\nAlgebra (ed. 3): \"Equations of this kind, ... to be satisfied by\\nthe same pair or pairs of values of x and y, are called\\nsimultaneous equations\" (OED2).\\n\\n\\nSimultaneous equations also appears in 1842 in G. Peacock,\\nTreat. Algebra: \"Such pairs or sets of equations in which the\\nsame unknown symbols appear, which are assumed to possess the same\\nvalues throughout, are called simultaneous equations\" (OED2).\\n\\n\\nSINE. Aryabhata the Elder (476-550) used the word jya\\nfor sine in Aryabhatiya, which was finished in 499.\\n\\n\\n\\nAccording to Cajori (1906), the Latin term sinus was introduced\\nin a translation of the astronomy of Al Battani by Plato of Tivoli\\n(or Plato Tiburtinus).\\n\\n\\n\\nAccording to some sources, sinus first appears in Latin in a\\ntranslation of the Algebra of al-Khowarizmi by Gherard of Cremona\\n(1114-1187). For example, Eves (page 177) writes:\\n\\nThe origin of the word sine is curious. Aryabhata\\ncalled in ardha-jya (\"half-chord\") and also jya-ardha\\n(\"chord-half\"), and then abbreviated the term by simply using\\njya (\"chord\"). From jya the Arabs phonetically derived\\njiba, which, following Arabian practice of omitting vowels,\\nwas written as jb. Now jiba, aside from its technical\\nsignificance, is a meaningless word in Arabic. Later writers, coming\\nacross jb as an abbreviation for the meaningless jiba,\\nsubstituted jaib instead, which contains the same letters and\\nis a good Arabic word meaning \"cove\" or \"bay.\" Still later, Gherardo\\nof Cremona (ca. 1150), when he made his translations from the Arabic,\\nreplaced the Arabian jaib by its Latin equivalent,\\nsinus, whence came our present word sine.\\n\\nHowever, Boyer (page 278) places the first appearance of sinus\\nin a translation of 1145. He writes:\\n\\nIt was Robert of Chester\\'s translation from the Arabic\\nthat resulted in our word \"sine.\" The Hindus had given the name\\njiva to the half chord in trigonometry, and the Arabs had\\ntaken this over as jiba. In the Arabic language there is also\\na word jaib meaning \"bay\" or \"inlet.\" When Robert of Chester\\n\\ncame to translate the technical word jiba, he seems to have\\nconfused this with the word jaib (perhaps because vowels were\\nomitted); hence he used the word sinus, the Latin word for\\n\"bay\" or \"inlet.\" Sometimes the more specific phrase sinus\\nrectus, or \"vertical sine,\" was used; hence the phrase sinus\\nversus, or our \"versed sine,\" was applied to the \"sagitta,\" or\\nthe \"sine turned on its side.\"\\n\\n\\nSmith (vol. 1, page 202) writes that the Latin sinus \"was\\nprobably first used in Robert of Chester\\'s revision of the tables of\\nal-Khowarizmi.\"\\n\\n\\n\\nFibonacci used the term sinus rectus arcus.\\n\\n\\nRegiomontanus (1436-1476) used sinus, sinus rectus, and\\nsinus versus in De triangulis omnimodis (On triangles\\nof all kinds; Nuremberg, 1533) [James A. Landau].\\n\\n\\n\\nCopernicus and Rheticus did not use the term sine (DSB).\\n\\n\\n\\nThe earliest known use of sine in English is by Thomas Fale in\\n1593:\\n\\n\\nThis Table of Sines may seem obscure and hard to those who\\nare not acquainted with Sinicall computation.\\n\\nThe citation is above is from Horologiographia. The art of\\ndialling: teaching an easie and perfect way to make all kinds of dials\\nvpon any plaine plat howsoeuer placec: With the drawing of the twelue\\nsignes, and houres vnequall in them all... At London, Printed by\\nThomas Orwin, dwelling in Pater noster-Row ouer against the signe of\\nthe Checker, 1593, by Thomas Fale.\\n\\n\\n\\nThe term SINGLE-VALUED FUNCTION (meaning analytic function)\\nwas used by Yulian-Karl Vasilievich Sokhotsky (1842-1927).\\n\\n\\n\\nThe term SINGULAR INTEGRAL is due to Lagrange (Kline, page 532).\\n\\n\\n\\nThe term is found in 1831 in Elements of the Integral Calculus\\n(1839) by J. R. Young:\\n\\nWe see, therefore, that it is possible for a differential\\nequation to have other integrals besides the complete primitive, but\\nderivable from it by substituting in it, for the arbitrary constant\\nc, each of its values given in terms of x and y\\n\\nby the equation (5). Such integrals are called singular\\nintegrals, or singular solutions of the proposed\\ndifferential equation.\\nSINGULAR MATRIX. Singular matrix and non-singular\\nmatrix occur in 1907 in Introduction to Higher Algebra by\\nMaxime Bôcher: \"Definition 2. A square matrix is said to be\\nsingular if its determinant is zero.\"\\n\\n\\nSINGULAR POINT appears in a paper by George Green published in\\n1828. The paper also contains the synonymous phrase \"singular value\"\\n[James A. Landau].\\n\\n\\nSingular point appears in 1836 in the second edition of\\nElements of the Differential Calculus by John Radford Young.\\nAccording to James A. Landau, who supplied this citation, it is not\\nclear what the author meant by the term. Landau writes, \"Judging by\\nthe contents of Chapter IV, to the author \\'singular point\\' was the\\nname of the category to which \\'multiple points,\\' \\'cusps,\\' and \\'points\\nof inflexion\\' belong.\"\\n\\n\\n\\nIn An Elementary Treatise on Curves, Functions and Forces\\n(1846), Benjamin Peirce writes, \"Those points of a curve, which\\npresent any peculiarity as to curvature or discontinuity, are called\\nsingular points.\"\\n\\n\\nSIZE (of a critical region) is found in 1933 in J. Neyman and E. S. Pearson,\\n\"On the Problems of the Most Efficient Tests of Statistical Hypotheses,\"\\nPhilosophical Transactions of the Royal Society of London, Ser. A (1933), 289-337 (David (2001)).\\n\\n\\nSKEW DISTRIBUTION appears in \\n1895 in a paper by Karl Pearson \"Contributions to the Mathematical \\nTheory of Evolution. II. Skew Variation in Homogeneous Material,\"\\nPhilosophical Transactions of the Royal Society \\nof London. A, 186. (1895), pp. 343-414. [James A. Landau]. \\n\\n\\nSKEW SYMMETRIC MATRIX. Skew symmetric determinant\\nappears in 1849 in Arthur Cayley, Jrnl. für die reine und\\nangewandte Math. XXXVIII. 93: \"Ces déterminants peuvent\\nêtre nommés \\x91gauches et symmétriques\\x92\" (OED2).\\n\\n\\nSkew symmetric determinant appears in 1885 in Modern Higher\\nAlgebra by George Salmon: \"A skew symmetric determinant is\\none in which each constituent is equal to its conjugate with its sign\\nchanged.\"\\n\\n\\nSkew symmetric matrix appears in \"Linear Algebras,\" Leonard\\nEugene Dickson, Transactions of the American Mathematical\\nSociety, Vol. 13, No. 1. (Jan., 1912).\\n\\n\\nSKEWES NUMBER appears in 1949 in Kasner & Newman,\\nMathematics and the Imagination: \"A veritable giant is\\nSkewes\\' number, even bigger than a googolplex\" (OED2).\\n\\n\\nSLIDE RULE. In 1630, the terms Grammelogia and\\nmathematical ring were used for a new device which,\\nunlike Gunter\\'s scale, had moving parts.\\n\\n\\n\\nIn 1632, the terms circles of proportion and horizontal\\ninstrument were used to describe Oughtred\\'s device, in a 1632\\npublication, Circles of Proportion.\\n\\nSlide rule appears in the Diary of Samuel Pepys (1633-1703) in\\nApril 1663: \"I walked to Greenwich, studying the slide rule for\\nmeasuring of timber.\" However, the device referred to may not have\\nbeen a slide rule in the modern sense.\\n\\n\\nSlide rule appears in 1838 in Civil Eng. & Arch. Jrnl.:\\n\"To assist in facilitating the use of the slide rule among working\\nmechanics\" (OED2).\\n\\n\\n\\nAmédée Mannheim (1831-1906) designed (c. 1850)\\nthe Mannheim Slide Rule.\\n\\n\\nSliding-rule and sliding-scale appear in 1857 in\\nMathematical Dictionary and Cyclopaedia of Mathematical\\nScience, defined in the modern sense.\\n\\n\\nSlide rule appears in 1876 in Handbk. Scientif. Appar.:\\n\"The slide rule,--an apparatus for effecting multiplications and\\ndivisions by means of a logarithmic scale\" (OED2).\\n\\n\\nSLOPE is found in 1857 in Mathematical Dictionary and\\nCyclopedia of Mathematical Science:\\n\\nSLOPE. Oblique direction. The slope of a plane is its\\ninclination to the horizon. This slope is generally given by its\\ntangent. Thus, the slope, 1/2, is equal to an angle whose tangent is\\n1/2; or, we generally say, the slope is 1 upon 2; that is, we rise,\\nin ascending such a plane, a vertical distance of 1, in passing over\\na horizontal distance of 2. The slope of a curved surface, at any\\npoint, is the slope of a plane, tangent to the surface at that\\n\\npoint.\\n\\nIn 1924 Analytic Geometry by Arthur M. Harding and George W.\\nMullins has: \"If the line is parallel to the y axis, the slope\\nis infinite.\" Modern textbooks say such a line has undefined slope.\\n\\n\\n\\nFor information on the use of m and other symbols for\\nslope, see \\nEarliest Uses of Symbols for Geometry.\\n\\n\\nSLOPE-INTERCEPT FORM is found in 1904 in Elements\\nof the Differential and Integral Calculus by William\\nAnthony Granville [James A. Landau].\\n\\n\\n\\nIn Webster\\'s New International Dictionary (1909), the term is\\nslope form.\\n\\nSMOOTHING. The earliest quotation given by the OED is from Francis Galton\\'s\\n\\nNatural Inheritance chapter vii, p. 100: \\n\"These [curious and apparently very interesting relations] came out distinctly \\nafter I had \\'smoothed\\' the entries.\"\\n\\n\\n\\nThe term SOCIAL MATHEMATICS was used by Condorcet (1743-1794)\\nand may have been coined by him.\\n\\n\\nSOLID GEOMETRY appears in 1733 in the title Elements\\nof Solid Geometry by H. Gore (OED2).\\n\\n\\nSOLID OF REVOLUTION is found in English in 1816 in the\\ntranslation of Lacroix\\'s Differential and Integral Calculus:\\n\"To find the differentials of the volumes and curve surfaces of\\nsolids of revolution\" (OED2).\\n\\n\\n\\nSOLIDUS (the diagonal fraction bar). Arthur Cayley\\n(1821-1895) wrote to Stokes, \"I think the \\'solidus\\' looks very well\\nindeed...; it would give you a strong claim to be President of a\\nSociety for the Prevention of Cruelty to Printers\" (Cajori vol. 2,\\npage 313).\\n\\n\\n\\nThe word solidus appears in this sense in the Century\\nDictionary of 1891.\\n\\n\\nSOLUBLE (referring to groups). Ferdinand Georg Frobenius\\n(1849-1917) wrote in a paper of 1893:\\n\\n Jede Gruppe, deren Ordnung eine Potenz einer Primzahl\\nist, ist nach einem Satze von Sylow die Gruppe einer durch\\nWurzelausdrücke auflösbaren Gleichung oder, wie ich mich\\nkurz ausdrücken will, einer auflösbare Gruppe. [Every\\ngroup of prime-power order is, by a theorem of Sylow, the group of an\\nequation which is soluble by radicals or, as I will allow myself to\\nabbreviate, a soluble group.] \\n\\nPeter Neumann believes this is likely to be the passage that\\nintroduced the term \"auflösbar\" [\"soluble\"] as an adjective\\napplicable to groups into mathematical language.\\n\\n\\nSOLUTION SET appears in 1959 in Fund. Math. by\\nAllendoerfer and Oakley: Given a universal set X and an\\nequation F(x) = G(x) involving x,\\nthe set {x|F(x) = G(x)} is called\\nthe solution set of the given equation\" (OED2).\\n\\n\\n\\nThe term may occur in found in Imsik Hong, \"On the null-set of a\\nsolution for the equation $\\\\Delta u+k^2u=0$,\" Kodai Math. Semin.\\nRep. (1955).\\n\\n\\nSOUSLIN SET is defined in Nicolas Bourbaki, Topologie\\nGenerale [Stacy Langton].\\n\\n\\n\\n\\nThe term SPECIALLY MULTIPLICATIVE FUNCTION was coined by D. H.\\nLehmer (McCarthy, page 65).\\n\\n\\nSPECTRUM (in operator theory). The OED\\'s earliest quotation illustrating the mathematical\\nuse of \"spectrum\" is from P. R. Halmos Finite Dimensional Vector Spaces (1948, ii. 79): \"The set\\nof n proper values [eigenvalues] of A, with multiplicities properly counted, is the spectrum\\nof A.\" However the usage can be traced back to \"Spektrum\" in Hilbert\\'s work on integral\\nequations in 1904-10 and the elaboration of operator theory in the 1920\\'s in works like von Neumann\\'s\\n\"Allgemeine Eigenwerttheorie Hermitische Funktionaloperatoren\" Math. Ann. 102 (1929) 49-131.\\nM. H. Stone\\'s Linear Transformations in Hilbert Space used the English word in 1932. The term SPECTRAL\\nTHEORY came into use in the early 1930\\'s a few years after its German equivalent.\\n(See also EIGENVALUE, STATIONARY STOCHASTIC PROCESS.) [John Aldrich]\\n\\n\\nSPECTRUM and SPECTRAL DENSITY (in generalised harmonic analysis and stochastic processes).\\nThe \"spectrum\" of an irregular motion appears in N. Wiener\\'s \"The Harmonic Analysis of Irregular Motion\\n(Second Paper)\" J. Math. and Phys. 5 (1926) 158-189. One of Wiener\\'s objectives was a theory\\nwhich would include \"an adequate mathematical account of such continuous spectra as that of white light.\"\\n(Wiener Proc. London Math. Soc. 27 (1928))  The term \"power-spectrum\" is also in the 1926\\npaper. The spectrum and spectral density function were important in the probabilistic theory of Khintchine\\n(1934) and Wold (1938) but the functions were not given names. The names appear in J. L. Doob\\'s \"The\\nElementary Gaussian Processes\" Annals of Mathematical Statistics, 15, (1944), 229-282.\\nAround 1940 it became evident that the spectral theory of time series analysis was related to the spectral\\ntheory of operators. (See also the previous entry and STATIONARY STOCHASTIC PROCESS). [John Aldrich]\\n\\n\\nSPHERICAL CONCHOID was coined by Herschel.\\n\\n\\nSPHERICAL GEOMETRY appears in 1728 in Chambers\\'\\nCyclopedia (OED2).\\n\\n\\n\\nThe words spherical geometry and versed sine were used\\nby Edgar Allan Poe in his short story The Unparalleled Adventure\\n\\nOf One Hans Pfaall.\\n\\nSPHERICAL HARMONICS. A. H. Resal used the term fonctions\\nspheriques (Todhunter, 1873) [Chris Linton].\\n\\n\\nSpherical harmonics was used in 1867 by William Thomson\\n(1824-1907) and Peter Guthrie Tait (1831-1901) in  Nat.\\nPhilos.: \"General expressions for complete spherical harmonics of\\nall orders\" (OED2).\\n\\n\\nSPHERICAL TRIANGLE Menelaus of Alexandria (fl. A. D. 100) used\\nthe term tripleuron in his Sphaerica, according to\\nPappus. According to the DSB, \"this is the earliest known mention of\\na spherical triangle.\"\\n\\n\\n\\nThe OED2 shows a use of spherical triangle in English in 1585.\\n\\n\\n\\nIn a letter to L. H. Girardin dated March 18, 1814, Thomas Jefferson\\n(President of the United States) wrote, \"According to your request of\\nthe other day, I send you my formula and explanation of Lord Napier\\'s\\ntheorem, for the solution of right-angled spherical triangles.\"\\n\\n\\nSPHERICAL TRIGONOMETRY is found in the title Trigonometria\\nsphaericorum logarithmica (1651) by Nicolaus Mercator\\n(1620-1687).\\n\\n\\n\\nThe term is found in English in a letter by John Collins to the\\nGovernors of Christ\\'s Hospital written on May 16, 1682, in the phrase\\n\"plaine & spherick Trigonometry, whereby Navigation is performed\"\\n[James A. Landau].\\n\\n\\n\\nIn a letter dated Oct. 8, 1809, Thomas Jefferson wrote, referring to\\nBenjamin Banneker, \"We know he had spherical trigonometry enough to\\nmake almanacs, but not without the suspicion of aid from Ellicot, who\\nwas his neighbor and friend, and never missed an opportunity of\\npuffing him.\"\\n\\n\\nSPINOR appears in 1931 in Physical Review. The citation\\nrefers to spinor analysis developed by B. Van der Waerden (OED2).\\n\\n\\nSPIRAL OF ARCHIMEDES appears in 1836 in the second edition of\\nElements of the Differential Calculus by John Radford Young\\n[James A. Landau].\\n\\n\\nSPLINE CURVE is found in 1946 in I. J. Schoenberg, Q. Appl. Math. IV. 48:\\n\"For k = 4 they represent approximately the curves drawn by means of a spline and for this reason we propose\\nto call them spline curves of order k (OED2).\\n\\n\\n\\nThe term SPORADIC GROUP was coined by William Burnside\\n(1852-1927) in the second edition of his Theory of Groups of\\nFinite Order, published in 1911 [John McKay].\\n\\n\\nSPURIOUS CORRELATION. The term was introduced by Karl Pearson in \"On a Form of Spurious\\nCorrelation Which May Arise When Indices Are Used in the Measurement of Organs,\" Proc. Royal Society,\\n60, (1897), 489-498. Pearson showed that correlation between indices u (= x/z)\\nand v (= y/z) was a misleading guide to correlation between x and y.\\nHis illustration is\\n\\nA quantity of bones are taken from an ossuarium, and are put together in groups which\\nare asserted to be those of individual skeletons. To test this a biologist takes the triplet femur, tibia,\\nhumerus, and seeks the correlation between the indices femur/humerus and tibia/humerus.\\nHe might reasonably conclude that this correlation marked organic relationship, and believe that the bones\\nhad really been put together substantially in their individual grouping. As a matter of fact ... there would be\\n... a correlation of about 0.4 to 0.5 between these indices had the bones been sorted absolutely at random.\\n\\nThe term has been applied to other correlation scenarios with potential for misleading inferences. In Student\\'s\\n\"The Elimination of Spurious Correlation due to Position in Time or Space\" (Biometrika, 10,\\n(1914), 179-180) the source of the spurious correlation is the common trends in the series. In H. A. Simon\\'s\\n\"Spurious Correlation: A Causal Interpretation,\"  Journal of the American Statistical Association,\\n49, (1954), pp. 467-479 the source of the spurious correlation is a common cause acting on the variables.\\nIn the recent spurious regression literature in time series econometrics (Granger & Newbold,\\nJournal of Econometrics, 1974) the misleading inference comes about through applying the regression\\ntheory for stationary series to non-stationary series. The dangers of doing this were pointed out by G. U. Yule\\nin his 1926 \"Why Do We Sometimes Get Nonsense Correlations between Time-series? A Study in Sampling and the Nature\\nof Time-series,\" Journal of the Royal Statistical Society, 89, 1-69. For another popular scenario\\nsee the entry on Simpson\\'s paradox. (Based on Aldrich 1995)\\n\\n\\nSQUARE MATRIX was used by Arthur Cayley in 1858 in\\nCollected Math. Papers (1889): \"The term matrix might be used\\nin a more general sense, but in the present memoir I consider only\\nsquare or rectangular matrices\" (OED2).\\n\\n\\n\\nThe term STANDARD DEVIATION was introduced by Karl Pearson\\n(1857-1936) in 1893, \"although the idea was by then nearly a century\\nold\" (Abbott; Stigler, page 328). According to the DSB:\\n\\nThe term \"standard deviation\" was introduced in a lecture\\nof 31 January, 1893, as a convenient substitute for the cumbersome\\n\"root mean square error\" and the older expressions \"error of mean\\nsquare\" and \"mean error.\"\\n\\nThe OED2 shows a use of standard deviation in 1894 by Pearson\\nin \"Contributions to the Mathematical Theory of Evolution,\\nPhilosophical Transactions of the Royal Society of London,\\nSer. A. 185, 71-110: \"Then σ will be termed its standard-deviation (error of mean\\nsquare).\"\\n\\n\\nSTANDARD ERROR is found in 1897 in G. U. Yule, \"On the Theory\\nof Correlation,\" Journal of the Royal Statistical Society, 60,\\n812-854: \"We see that 1[sqrt](1 - r2) is the standard\\nerror made in estimating x\" (OED2).\\nThere the quantity x was being estimated by a regression residual but Yule\\napplied the term generally in his Introduction to the Theory of Statistics\\n(1911), covering such cases as the standard error of a proportion. [John Aldrich]\\n\\n\\n\\nSee also PROBABLE ERROR.\\n\\nSTANDARD POSITION is found in 1873 in \\nAn elementary course in free-hand geometrical drawing\\nby Samuel Edward Warren:\\n\"a right angle is in its simplest, most natural, or standard position, when its sides are\\nin the fundamental directions of vertical and horizontal\" [University of Michigan Digital Library].\\n\\n\\nStandard position is dated 1950 in MWCD10.\\n\\n\\nSTANDARD SCORE. In 1913\\nElementary school standards : instruction, course of study, supervision, applied to New York City schools\\nby Frank Morton McMurry has:\\n\"The book does not attempt to illustrate accurate measurement of educational results. It is scientific only in\\nso far as it brings to bear organized knowledge and insight on an educational problem. Scientific measurement\\nin education is,  indeed, as yet too little developed to be applied to more than a very limited portion of the work\\nof the elementary schools. Except for arithmetic and penmanship, \\'standard scores\\'\\nor standard achievements are not available for measuring the quality of the results actually attained by the\\nschools; and even for penmanship and arithmetic, the standard measures for each grade are not yet firmly established\"\\n[University of Michigan Digital Library].\\n\\n\\n\\nIn 1921 Univ. Illin. Bur. Educ. Res.\\nBull. has: \"Provision is made for comparing a pupil\\'s achievement\\nscore..with the norm corresponding to his mental age by dividing his\\nachievement age by the standard score for his mental age. This\\nquotient is called the Achievement Quotient\" (OED2).\\n\\n\\nStandard score is dated 1928 in MWCD10.\\n\\n\\nSTANINE is a term first used to describe an examinee\\'s performance on a\\nbattery of tests constructed for the U. S. Army Air Force during World War II.\\n\\n\\n\\nIn a letter dated July 30, 1946, Laurance F. Shaffer, who had been a colonel in charge of Psychological Research Unit No. 1 (PRU #1)\\nat Maxwell Field, Alabama, wrote:\\n\\nThe origin of the word is somewhat hazy. I have complete certainty only with regard to two facts:\\nthat the word was originated at PRU #1 at Maxwell Field, and that the date was in the month\\nof February, 1942.  According to PRU #1 tradition, the word first appeared in the form\\nstand-nine as a shortening of the phrase standard nine-point scale that occurred in area directives.\\nThis was soon shortened to stannine (with a as in stand). Local tradition ascribed\\nthe origin of this term to Sol M. Roshal, who was noncom in charge of computations at that time.\\nFred Wickers has told me that he is very certain that I changed stannine to stanine with a as in\\nstay) when I returned from my expedition to California, which took place in the middle of February, 1942. I do not\\nremember this myself.\\n\\nIn a letter dated February 23, 1946, Frank A. Geldard, formerly a colonel in charge of the whole program and stationed in Texas, wrote:\\n\\nStanine is a portmanteau word deriving from \"standard score on a nine point scale.\" It was a sheer\\n\"shorthand\" invention on the part of an enlisted man in Psychological Research Unit No. 1, AAF Classification Center,\\nMaxwell Field, Ala. The term came to have wide usage in the AAF, not only by psychologists, but by all\\nwho had occasion to refer to aptitude ratings for pilot, bombardier,\\nand navigator training assignments. At first the word was resisted by psychologists, who felt that the term\\nhad little intrinsic logical meaning to recommend it. For a year or so\\nafter its invention official reports might not employ the word; it was regarded\\nas inferior slang. Generality of usage within the AAF\\neventually forced its acceptance, however, and by the end of the war both technical and nontechnical papers\\non aircrew aptitude, standards of qualification, training programs, aircraft accidents, and a host of other topics,\\nemployed it as a \"good\" word. It avoided considerable circumlocution, and its meaning seems\\nrarely to have been misunderstood.\\n\\n\\nBoth of these letters were written to Atcheson L. Hench and appear in an article by him, \"The Coining of \\'Stanine\\'\", in American\\nSpeech, February 1951.\\n\\n\\n\\nThe term STAR PRIME was coined in 1988 by Richard L. Francis (Schwartzman, p. 206).\\n\\n\\nSTATIONARY STOCHASTIC PROCESS appears in the title of A Khintchine\\'s \"Korrelationstheorie der Stationären\\nStochastischen Prozesse,\" Math. Ann. 109, 604.\\n\\n\\n\\nH. Wold translated it as \"stationary random process\" (A Study in the Analysis of Stationary Time Series\\n(1938)).\\n\\n\\n\\nThe phrase \"stationary stochastic process\" appears in J. L. Doob\\'s \"What is a Stochastic Process?\" American\\nMathematical Monthly, 49, (1942), 648-653.\\n\\n\\n\\nAn older term was \"fonction éventuelle homogène,\" which appears in E. Slutsky\\'s \"Sur les Fonctions\\nÉventuelles Continues, Intégrables et Dérivables dans la Sens Stochastique,\"\\n\\n\\nComptes Rendues, 187, (1928), 878 [John Aldrich].\\n\\n\\nSTATISTIC, STATISTICAL and STATISTICS. In the course\\nof the 19th century statistics\\nacquired its modern meaning(s). It is “the department of study that has\\nits object the collection and arrangement of numerical facts or data, whether\\nrelating to human affairs or to natural phenomena” OR they are\\n“numerical facts or data collected and classified.”\\nThe OED1 of the early 20th century also has statistical\\nin the modern sense but its meanings for statistic\\nare archaic. The recasting of statistic came later.\\n\\n\\n\\nThese words all come indirectly from the mediaeval Latin status\\nfor a political state. More directly statistics entered English from the\\nGerman Statistik, as a term comparable to mathematics\\nor ethics. The first citation in OED2 is W. Hooper\\'s translation of Bielfield\\'s Elementary Universal Education:\\n“The science, that is called statistics, teaches us what is the political\\narrangement of all the modern states of the known world.” (1770)\\n\\n\\nWebster\\'s dictionary of 1828 defined statistics as: “A collection of facts respecting the\\nstate of society, the condition of the people in a nation or country, their\\nhealth, longevity, domestic economy, arts, property and political strength, the\\nstate of the country, &c.” Statistical societies, like the London\\nStatistical Society (later Royal Statistical Society) founded in 1835, were\\n\\nestablished to discover such facts.\\n\\n\\n\\nIn the course of the 19th century statistics came to be confined to\\nnumerical facts but the facts did not have to pertain to public administration.\\nThe latter development is illustrated by a quotation from J. C. Maxwell\\nTheory of Heat (1871) xxii. 288: “If however, we adopt a statistical view of the system, and\\ndistribute the molecules into groups . . .”\\n(OED2) This point of view became fixed in the phrase statistical mechanics.\\nFor this the OED2 cites J. W. Gibbs in Proc. Amer. Assoc. Adv.\\nSci. XXXIII, 1885, 57 (heading) “On the fundamental formula of\\nstatistical mechanics, with applications to astronomy and thermodynamics.”\\n\\n\\nStatistic, signifying an individual fact, was rare before the 20th century.\\nThere is an example from 1853 in The United States illustrated\\nedited by Charles Anderson Dana: “An old teamster with a dislodged wheel to his\\n\\'lumbery\\' vehicle, claimed a moment of our strength, and in return for that generosity,\\na la Jupiter, indulged our statistical curiosity with a few minutes of his local knowledge. The\\nsignificant placing of his hand upon his pocket, as he\\nproclaimed the fact that the bridge cost almost a quarter of a million dollars,\\nplainly showed his appreciation of so vast a sum. Nor was the statistic of the\\nbridge, being a mile in length, handed over to the fund of general information,\\nwithout a look which plainly hinted of the many laggard walks it had cost him\\nby the side of his sturdy team.” [University of Michigan Digital Library].\\n\\n\\n\\nIn the 20th century the singular form came to be \\naccepted both in this sense and in another sense. In statistical theory R. A. \\nFisher used statistic to refer to a quantity derived from the observations--before\\nsettling on it he had used \"statistical derivative\"\\n(1915),\\n\"derivate\" (1920)\\nand \"statistical derivate\" (1921).\\nFisher presented the new term in his \\n\"On the\\nMathematical Foundations of Theoretical Statistics\",\\nPhilosophical Transactions of the Royal Society of London, Ser. A., 222, \\n(1922), 309-368: \"These involve the choice of methods of calculating from a \\nsample statistical derivates, or as we shall call them statistics, which are \\ndesigned to estimate the values of the parameters of the hypothetical population.\"\\n(p. 318) The term parameter was also new and with statistic the \\ntwo made a pair. (See the entry on parameter for Fisher\\x92s reasoning.) Fisher \\ncalled the statistics arising in estimation problems estimates. He had \\nno name for statistics arising in testing but since the 1950s they have been \\ncalled \"test statistics.\" \\n\\n\\n\\nFisher\\x92s term was not well-received initially. Arne \\nFisher (no relation) asked him, \"Where ... did you get that atrocity, a statistic?\"\\n(letter (p. 312) in J. H. Bennett\\nStatistical\\nInference and Analysis: Selected Correspondence of R. A. Fisher (1990).) Karl Pearson objected,\\n\"Are we also to introduce the words a mathematic, a physic, an electric etc., for parameters\\nor constants of other branches of science?\" (p. 49n of Biometrika, 28, \\n34-59 1936).\\n\\n\\n\\n[This entry was contributed by John Aldrich, based on G. U. \\nYule Introduction to the Theory of Statistics (1911) and David (2001)] \\n\\n\\nSTEP FUNCTION is dated ca. 1929 in MWCD10.\\n\\n\\nSTEREOGRAPHIC. According to Schwartzman (p. 207), \"the term\\nseems to have been used first by the Belgian Jesuit François\\nAguillon (1566-1617), although the concept was already known to the\\nancient Greeks.\"\\n\\n\\n\\n\\nIn Flattening the Earth: Two Thousand Years of Map\\nProjections, John P. Snyder attributes the term to d\\'Aguillon in\\n1613 [John W. Dawson, Jr.].\\n\\n\\nSTIELTJES INTEGRAL is found in Henri Lebesgue, \"Sur\\nl\\'intégrale de Stieltjes et sur les opérations\\nlinéaires,\" Comptes Rendus Acad. Sci. Paris 150 (1910)\\n[James A. Landau].\\n\\n\\n\\nThe terms STIRLING NUMBERS OF THE FIRST and SECOND KIND\\nwere coined by Niels Nielsen (1865-1931), who wrote in German\\n\"Stirlingschen Zahlen erster Art\" [Stirling numbers of the first kind]\\nand \"Stirlingschen Zahlen zweiter Art\" [Stirling numbers of the second\\nkind]. Nielsen\\'s masterpiece, \"Handbuch der Theorie der\\nGammafunktion\" [B. G. Teubner, Leipzig, 1906], had a great influence,\\nand the terms progressively found their acceptance (Julio\\nGonzález Cabillón).\\n\\n\\n\\nJohn Conway believes the newer terms Stirling cycle and\\nStirling (sub)set numbers were introduced by R. L. Graham, D.\\nE. Knuth, and O. Patshnik in Concrete Mathematics (Addison\\nWesley, 1989 & often reprinted).\\n\\n\\nSTIRLING\\'S FORMULA. Lacroix used Théorème de\\n\\n\\n\\n\\n\\nStirling in Traité élémentaire de calcul\\ndifférentiel et de calcul intégral (1797-1800).\\n\\n\\nStirling\\'s approximation appears in 1938 in Biometrika\\n(OED2).\\n\\n\\nSTOCHASTIC is found in English as early as 1662 with the\\nobsolete meaning \"pertaining to conjecture.\"\\n\\n\\n\\nIn its modern sense, the term was used in 1917 by Ladislaus\\nJosephowitsch Bortkiewicz (1868-1931) in Die Iterationem 3:\\n\"Die an der Wahrscheinlichkeitstheorie orientierte, somit auf \\'das\\nGesetz der Grossen Zahlen\\' sich gründende Betrachtng empirischer\\nVielheiten mö ge als Stochastik ... bezeichnet werden\" (OED2).\\n\\n\\nSTOCHASTIC PROCESS is found in A. N. Kolmogorov, \"Sulla forma\\ngenerale di un prozesso stocastico omogeneo,\" Rend. Accad. Lincei\\nCl. Sci. Fis. Mat. 15 (1) page 805 (1932) [James A. Landau].\\n\\n\\nStochastic process is also found in A. Khintchine\\n\"Korrelationstheorie der stationäre stochastischen Prozesse,\"\\nMath. Ann. 109 (1934) [James A. Landau].\\n\\n\\nStochastic process occurs in English in J. L. Doob, \"Stochastic processes\\nand statistics,\"  Proc. Natl. Acad. Sci. USA 20 (1934).\\n\\n\\n\\nSee AUTORORRELATION, AUTOREGRESSION, BRANCHING PROCESS, ERGODIC, MARKOV, MARTINGALE, MOVING AVERAGE PROCESS,\\nSAMPLE PATH, SPECTRUM, STATIONARY STOCHASTIC PROCESS, WHITE NOISE, WINER PROCESS.\\n\\nSTOKES\\'S THEOREM. According to Finney and Thomas\\n(page 987), Stokes learned of the theorem from Lord Kelvin in 1850 and\\n\"a few years later, thinking it would make a good examination\\nquestion, put it on the Smith Prize examination. It has been known as\\nStokes\\'s theorem ever since.\"\\n\\n\\nStokes\\' theorem is found in 1893 in J. J. Thomsom, Notes\\nRecent Res. Electr. & Magnetism (OED2).\\n\\n\\nSTRAIGHT ANGLE appears in English in 1889 in Dupuis, Elem.\\nSynth. Geom.: \"One-half of a circumangle is a straight angle, and\\none-fourth of a circumangle is a right angle\" (OED2).\\n\\n\\n\\nThere are earlier citations in the OED2 for the term with the\\n\\nobsolete meaning of \"a right angle.\"\\n\\n\\n\\nThe term STRANGE ATTRACTOR was coined by David Ruelle\\nand Floris Takens in their classic paper \"On the Nature of Turbulence\"\\n[Communications in Mathematical Physics, vol. 20, pp. 167-192,\\n1971], in which they describe the complex geometric structure of an\\nattractor during a study of models for turbulence in fluid flow.\\n\\n\\nSTRATIFIED SAMPLING occurs in J. Neyman, \"On the two different\\naspects of the representative method; the method of stratified\\nsampling and the method of purposive selection,\" J. R. Satatist.\\nSoc 97 (1934) [James A. Landau].\\n\\n\\nSTRONG LAW OF LARGE NUMBERS is found in A. N. Kolmogorov, \"Sur\\nla loi forte des grandes nombres,\" Comptes Rendus de l\\'Acade/mie\\ndes Sciences, Paris 191 page 910 (1930) [James A. Landau].\\n\\n\\nSTRONG PSEUDOPRIME. According to Prime Numbers: A\\nComputational Perspective by Carl Pomerance and Richard Crandall (page 124),\\n\"J. Selfridge proposed using Theorem 3.4.1 as a pseudoprime test in the early\\n1970s, and it was he who coined the term \\'strong pseudoprime\\'\" [Paul Pollack].\\n\\n\\nStrong pseudoprime is found in Pomerance, Carl; Selfridge,\\nJ.L.; Wagstaff, Samuel S. Jr. \"The pseudoprimes to 25 x\\n109,\" Math. Comput. 35, 1003-1026 (1980).\\n\\n\\nSTROPHOID appears in 1837 in Enrico Montucci, \"Delle\\nproprietà della strefoide, curva algebrica del terzo grado\\nrecentemente scoperta ed esaminata\" (\"On the property of the\\nstrophoid, an algebraic curve of the third degree recently discovered\\nand examined\"), Memoria letta nell\\'Accademia dei Fisiocratici ... con\\nuna appendice del Venturoli, Siena, G. Mucci, 1837 [Dic Sonneveld].\\n\\n\\nStrophoid was coined by Montucci in 1846, according to Smith\\n(vol. 2, page 330).\\n\\n\\n\\nThe term STRUCTURE for isomorphic relations seems to have first\\nappeared in print in Bertrand Russell\\'s Introduction to Mathematical\\nPhilosophy (1919).\\nRussell probably had the term from Ludwig Wittgenstein, whose\\nTractatus logico-philosophicus (Logisch-philosophische\\nAbhandlung, Vienna 1918, 4.1211 ff) was first published in 1921, and\\nin 1922 in English.\\nThe first Structure in the modern sense -- as a tuple composed of sorts or\\ncarrier sets, relations, operations and distinguished elements -- was first\\nused by David Hilbert in his Grundlagen der Geometrie\\n(Göttingen 1899), there called a „Fachwerk oder Schema von\\nBegriffen“ (p. 163, according to\\nF. Kambartel Erfahrung und Struktur, Münster 1966).\\nThe concept of Structure developed via Rudolf Carnap\\'s Der logische\\nAufbau der Welt (1928), the linguistic and French philosophical\\nStructuralism, the Éléments de\\nmathématique of the N. Bourbaki group (Paris, since 1939), to\\nCategory Theory of Samuel Eilenberg and Saunders Mac Lane (1945).\\n[This entry was contributed by Wolfram Roisch.]\\n\\n\\nSTUDENT\\'S t-DISTRIBUTION. \"Student\" was the pseudonym of William Sealy Gosset (1876-1937). \\nGosset once told R. A. Fisher, \"I am sending you a copy of Student\\'s Tables \\nas you are the only man that\\'s ever likely to use them!\" (Letters from \\nW. S. Gosset to R. A. Fisher, 1915-1936 (1970)). Through Fisher\\x92s writings \\nStudent\\'s tables became very widely used, though not in the form Gosset \\nfirst constructed them. \\n\\n\\n\\nIn his 1908 paper, \"The Probable Error of a Mean\",\\nBiometrika, 6, 1-25 Gosset introduced the statistic, z, for testing hypotheses \\non the mean of the normal distribution. Gosset used the divisor n, not \\nthe modern (n - 1), when he estimated\\n and his z is proportional to the modern t with t = z \\nsqrt (n - 1). Fisher introduced the t form because it fitted in \\nwith his theory of degrees of freedom (q.v.). Fisher used the t symbol \\nand described Student\\'s distribution (and others based on the normal distribution) \\nand the role of degrees of freedom in\\n\"On a Distribution Yielding\\nthe Error Functions of Several well Known Statistics\",\\nProceedings of the International Congress of Mathematics, Toronto, 2, 805-813. Although the paper was presented \\nin 1924, it was not published until 1928 (Tankard, page 103; David, 1995). According \\nto the OED2, the letter t was chosen arbitrarily. A new symbol suited \\nFisher for he was already using z for a statistic of his own (see entry \\nfor F). \\n\\n\\nStudent\\'s distribution\\n(without \"t\") appears in 1925 in R. A. Fisher,\\n\"Applications of \\'Student\\'s\\' Distribution\",\\nMetron 5, 90-104 and in Fisher\\'s\\nStatistical Methods for Research Workers\\n(1925). Fisher\\x92s book made Student\\'s distribution famous; it presented new uses \\nfor the tables and made the tables generally available. \\n\\n\\n\\n\"Student\\'s\" t-distribution appears in 1929 in Nature \\n(OED2). \\n\\n\\n\\nt-distribution appears (without Student) in A. T. McKay, \"Distribution \\nof the coefficient of variation and the extended \\'t\\' distribution,\" \\nJ. Roy. Stat. Soc., n. Ser. 95 (1932). \\n\\n\\nt-test is found in 1932 in \\nthe fourth edition of R. A. Fisher, Statistical Methods for Research Workers: \\n\"The validity of the t-test, as a test of this hypothesis, is therefore \\nabsolute\" (p. 116) (OED2). \\n\\n\\n\\nEisenhart (1979) is the best reference for the evolution of \\nt, although Tankard and Hald also discuss it. \\n\\n\\n\\n[This entry was largely contributed by John Aldrich.] \\n\\n\\nSTUDENTIZATION. According to Hald (p. 669), William Sealy\\nGossett (1876-1937) used the term Studentization in a letter\\nto E. S. Pearson of Jan. 29, 1932.\\n\\n\\nStudentized D2 statistic is found in R. C. Bose and\\nS. N. Roy, \"The exact distribution of the Studentized D2\\nstatistic,\" Sankhya 3 pt. 4 (1935) [James A. Landau].\\n\\n\\nSTURM\\'S THEOREM appears in 1836 in the title Du Theoreme de\\nM. Sturm, et de ses Applications Numeriques by M. E. Midy [James\\nA. Landau].\\n\\n\\nSturm\\'s theorem appears in English in 1841 in the title\\nMathematical Dissertations, for the use of students in the modern\\nanalysis; with improvements in the practice of Sturm\\'s Theorem, in\\nthe theory of curvature, and in the summation of infinite series\\nby J. R. Young [James A. Landau].\\n\\n\\nSUBFACTORIAL was introduced in 1878 by W. Allen Whitworth in\\nMessenger of Mathematics (Cajori vol. 2, page 77).\\n\\n\\nSUBFIELD is found in \"On the Base of a Relative Number-Field,\\nwith an Application to the Composition of Fields,\" G. E. Wahlin,\\nTransactions of the American Mathematical Society, Vol. 11,\\nNo. 4. (Oct., 1910).\\n\\n\\nSUBGROUP. Felix Klein used the term untergruppe.\\n\\nSubgroup appears in 1881 in Arthur Cayley,\\n\"On the Schwarzian Derivative, and the Polyhedral Functions,\"\\nTransactions of the Cambridge Philosophical Society:\\n\"But there is no sub-group of an order divisible by 5; and hence, these\\ntwo transformations being identified with the two substitutions, the other\\ntransformations correspond each of them to a determinate substitution\"\\n[University of Michigan Historical Math Collection].\\n\\n\\nSUBRING is found in English in 1937 in the phrase invariant\\nsubring in Modern Higher Algebra (1938) by A. A. Albert\\n(OED2).\\n\\n\\nSUBSET. Cantor used the word subset (in the sense that\\n\"proper subset\" is now used) in \"Ein Beitrag zur\\n\\n\\nMannigfaltigkeitslehre,\" Journal für die reine und angewandte\\nMathematik 84 (1878).\\n\\n\\nSubset occurs in English in \"A Simple Proof of the Fundamental\\nCauchy-Goursat Theorem,\" Eliakim Hastings Moore, Transactions of\\nthe American Mathematical Society, Vol. 1, No. 4. (Oct., 1900).\\n\\n\\nSUBTRACT. When Fibonacci (1201) wishes to say \"I subtract,\" he\\nuses some of the various words meaning \"I take\": tollo,\\naufero, or accipio. Instead of saying \"to subtract\" he\\nsays \"to extract.\"\\n\\n\\n\\n\\n\\nIn English, Chaucer used abate around 1391 in Treatise on\\nthe Astrolabe: \"Abate thanne thees degrees And minutes owt of 90\"\\n(OED2).\\n\\n\\n\\nIn a manuscript written by Christian of Prag (c. 1400), the word\\n\"subtraction\" is at first limited to cases in which there is no\\n\"borrowing.\" Cases in which \"borrowing\" occurs he puts under the\\ntitle cautela (caution), and gives this caption the same\\nprominence as subtractio.\\n\\n\\nIn Practica (1539) Cardano used detrahere (to draw or\\ntake from).\\n\\n\\n\\nIn 1542 in the Ground of Artes Robert Recorde used\\nrebate: \"Than do I rebate 6 out of 8, & there resteth 2.\"\\n\\n\\n\\nIn 1551 in Pathway to Knowledge Recorde used abate:\\n\"Introd., And if you abate euen portions from things that are equal,\\nthose partes that remain shall be equall also\" (OED2).\\n\\n\\n\\nDigges (1572) writes \"to subduce or substray any sume, is wittily to\\npull a lesse fro a bigger number.\"\\n\\n\\n\\nSchoner, in his notes on Ramus (1586 ed., p. 8), uses both\\nsubduco and tollo for \"I subtract.\"\\n\\n\\n\\nIn his arithmetic, Boethius uses subtrahere, but in geometry\\nattributed to him he prefers subducere.\\n\\n\\nThe first citation for subtract in the OED2 is in 1557 by\\nRobert Recorde in The whetstone of witte: \"Wherfore I subtract\\n16. out of 18.\"\\n\\n\\n\\nHylles (1592) used \"abate,\" \"subtact,\" \"deduct,\" and \"take away\"\\n(Smith vol. 2, pages 94-95).\\n\\n\\n\\nFrom Smith (vol. 2, page 95):\\n\\nThe word \"subtract\" has itself had an interesting\\nhistory. The Latin sub appears in French as sub, soub,\\nsou, and sous, subtrahere becoming soustraire and\\nsubtractio becoming soustraction. Partly because of\\nthis French usage, and partly no doubt for euphony, as in the case of\\n\"abstract,\" there crept into the Latin works of the Middle Ages, and\\nparticularly into the books printed in Paris early in the 16th\\ncentury, the form substractio. From France the usage spread to\\nHolland and England, and form each of these countries it came to\\nAmerica. Until the beginning of the 19th century \"substract\" was a\\ncommon form in England and America, and among those brought up in\\nsomewhat illiterate surroundings it is still to be found. The\\nincorrect form was never popular in Germany, probably because of the\\nTeutonic exclusion of international terms.\\nSUBTRACTION. Fibonacci (1201) used extractio.\\n\\n\\nTonstall (1522) devoted 15 pages to Subductio. He wrote, \"Hanc\\nautem eandem, uel deductionem uel subtractionem appellare Latine\\nlicet\" (1538 ed., p. 23; 1522 ed., fol. E 2, r).\\n\\n\\n\\nGemma Frisius (1540) has a chapter De Subductione siue\\nSubtractione.\\n\\n\\nClavius (1585 ed., p. 26) says \"Subtractio est ... subductio.\"\\n\\n\\n\\nSee also ADDITION.\\n\\nSUBTRAHEND is an abbreviation of the Latin numerus\\nsubtrahendus (number to be subtracted).\\n\\n\\nSUCCESSIVE INDUCTION. This term was suggested by Augustus De\\nMorgan in his article \"Induction (Mathematics)\" in the Penny\\nCyclopedia of 1838. See also MATHEMATICAL INDUCTION,\\nINDUCTION, COMPLETE INDUCTION.\\n\\nSUFFICIENCY, SUFFICIENT STATISTIC and CRITERON OF SUFFICIENCY all appear in 1922 in R. A. Fisher\\'s\\n\"On the Mathematical Foundations of Theoretical Statistics\",\\nPhilosophical Transactions of the Royal Society of London, Ser. A, 222, \\n309-368: \\n\\nThe statistic chosen should summarise the whole of the relevant \\ninformation supplied by the sample. This may be called the Criterion of Sufficiency. \\n(p. 316) \\n\\n\\n\\nIn the case of the normal curve of distribution it is evident \\nthat the second moment is a sufficient statistic for estimating the standard \\ndeviation. (p. 359)\\n\\nThe term sufficient statistic is more prominent in section 3 of Fisher\\'s\\nStatistical Methods for Research Workers\\nand section 9 of\\n\"Theory of Statistical Estimation\"\\nboth published in 1925.\\n\\n\\n\\nThe concept of sufficiency was already emerging in 1920\\n(p. 769) when Fisher wrote of the sample variance that \"The whole of the information respecting σ,\\nwhich a sample provides is summed up [in its value].\"\\n\\n\\n\\n[John Aldrich]\\n\\n\\nSUM. Nicolas Chuquet used some in his Triparty en la\\nScience des Nombres in 1484.\\n\\n\\n\\nThe term SUMMABLE (referring to a function that is Lebesgue\\nintegrable such that the value of the integral is finite) was\\nintroduced by Lebesgue (Klein, page 1045).\\n\\n\\n\\nSUPPLEMENT. \"Supplement of a parallelogram\" appears in English\\nin 1570 in Sir Henry Billingsley\\'s translation of Euclid\\'s\\nElements.\\n\\n\\nIn 1704 Lexicon Technicum by John Harris has \"supplement of an\\nArk.\"\\n\\n\\n\\nIn 1796 Hutton Math. Dict. has \"The complement to 180° is\\nusually called the supplement.\\n\\n\\n\\nIn 1798 Hutton in Course Math. has \"supplemental arc\" (one of\\ntwo arcs which add to a semicircle) (OED2).\\n\\n\\n\\nSupplement II to the 1801 Encyclopaedia Britannica has, \"The\\nsupplement of 50° is 130°; as the complement of it is 40\\n°\" (OED2).\\n\\n\\n\\nIn 1840, Lardner in Geometry vii writes, \"If a quadrilateral\\nfigure be inscribed in a circle, its opposite angles will be\\nsupplemental\" (OED2).\\n\\n\\nSupplementary angle is dated ca. 1924 in MWCD10.\\n\\n\\nSURD. According to Smith (vol. 2, page 252), al-Khowarizmi\\n(c. 825) referred to rational and irrational numbers as \\'audible\\' and\\n\\'inaudible\\', respectively.\\n\\n\\n\\nThe Arabic translators in the ninth century translated the Greek\\nrhetos (rational) by the Arabic muntaq (made to speak)\\nand the Greek alogos (irrational) by the Arabic asamm\\n(deaf, dumb). See e. g. W. Thomson, G. Junge, The Commentary of\\nPappus on Book X of Euclid\\'s Elements, Cambridge: Harvard\\nUniversity Press, 1930 [Jan Hogendijk].\\n\\n\\n\\nThis was translated as surdus (\"deaf\" or \"mute\") in Latin.\\n\\n\\n\\nAs far as is known, the first known European to adopt this\\nterminology was Gherardo of Cremona (c. 1150).\\n\\n\\n\\nFibonacci (1202) adopted the same term to refer to a number that has\\nno root, according to Smith.\\n\\n\\n\\nSurd is found in English in Robert Recorde\\'s The Pathwaie\\nto Knowledge (1551): \"Quantitees partly rationall, and partly\\nsurde\" (OED2).\\n\\n\\n\\nAccording to Smith (vol. 2, page 252), there has never been a general\\nagreement on what constitutes a surd. It is admitted that a number\\nlike sqrt 2 is a surd, but there have been prominent writers who have\\nnot included sqrt 6, since it is equal to sqrt 2 X sqrt 3. Smith\\nalso called the word surd \"unnecessary and ill-defined\" in his\\nTeaching of Elementary Mathematics (1900).\\n\\n\\n\\nG. Chrystal in Algebra, 2nd ed. (1889) says that \"...a surd\\nnumber is the incommensurable root of a commensurable number,\" and\\nsays that sqrt e is not a surd, nor is sqrt (1 + sqrt 2).\\n\\n\\nSURJECTION appears in 1964 in Foundations of\\nAlgebraic Topology by W. J. Pervin (OED2).\\n\\n\\nSURJECTIVE appears in 1956 in C. Chevalley, Fund. Concepts\\nAlgebra: \"A homomorphism which is injective is called a\\nmonomorphism; a homomorphism which is surjective is called an\\n\\nepimorphism\" (OED2).\\n\\n\\n\\nThe term SURREAL NUMBER was introduced by Donald Ervin Knuth\\n(1938-  ) in 1972 or 1973, although the notion was previously\\ninvented by John Horton Conway (1937-  ) in 1969.\\n\\n\\n\\nThe term SYLOW\\'S THEOREM is found in German in G. Frobenius,\\n\\n\\n\"Neuer Beweis des Sylowschen Satzes,\" Journ. Crelle, 100,\\n(1887), p. 179-181 [Dirk Schlimm].\\n\\n\\nSylow\\'s Theorem is found in English in 1893 in Proceedings\\nof the London Mathematical Society XXV 14 (OED2).\\n\\n\\n\\nThe term SYMMEDIAN was introduced in 1883 by Philbert Maurice\\nd\\'Ocagne (1862-1938) [Clark Kimberling].\\n\\n\\nSYMMEDIAN POINT. Emil Lemoine (1840-1912) used the term\\ncenter of antiparallel medians.\\n\\n\\nThe proposal to name the point after Ernst Wilhelm Grebe (1804-1874)\\ncame from E. Hain (\"Ueber den Grebeschen Punkt,\" Archiv der\\nMathematik und Physik 58 (1876), 84-89). Afterwards, the term\\nGrebe\\'schen Punkt appeared many times in the Jahrbuch ueber\\ndie Fortschritte der Mathematik by reviewers such as Dr. Schemmel\\n(Berlin, 1875), Prof. Mansion (Gent, 1881), Prof. Lampe (Berlin,\\n1881), and Dr. Lange (Berlin, 1885) [Peter Schreiber, Julio\\nGonzález Cabillón].\\n\\n\\n\\nIn 1884, Joseph Jean Baptiste Neuberg (1840-1926) gave it the name\\nLemoine point, for Emile Michel Hyacinthe Lemoine (1840-1912).\\n\\n\\n\\nThe point was thus called the Lemoine point in France and the Grebe\\npoint in Germany [DSB].\\n\\n\\nSymmedian point was coined by Robert Tucker (1832-1905) in the\\ninterest of uniformity and amity.\\n\\n\\n\\nThe term SYMPLECTIC GROUP was proposed in 1939 by\\nHerman Weyl in The Classical Groups. He wrote on page 165:\\n\\nThe name \"complex group\" formerly advocated by me in\\nallusion to line complexes, as these are defined by the vanishing of\\nantisymmetric bilinear forms, has become more and more embarrassing\\nthrough collision with the word \"complex\" in the connotation of\\ncomplex number. I therefore propose to replace it by the corresponding\\nGreek adjective \"symplectic.\" Dickson calls the group the \"Abelian\\nlinear group\" in homage to Abel who first studied it.\\n\\n[This information was provided by William C. Waterhouse.]\\n\\n\\n\\nAccording to Lectures on Symplectic Geometry by Ana Cannas da\\nSilva, \"the word symplectic in mathematics was coined by Weyl who\\nsubstituted the Greek root in complex by the corresponding Latin\\nroot, in order to label the symplectic group. Weyl thus avoided that\\nthis group connoted the complex numbers, and also spared us from much\\nconfusion had the name remained the former one in honor of Abel:\\nabelian linear group.\"\\n\\n\\nSYNTHETIC DIVISION is found in 1857 in Mathematical\\nDictionary and Cyclopedia of Mathematical Science.\\n\\nSYNTHETIC GEOMETRY appears in Gigon, \"Bericht über: Jacob\\nSteiner\\'s Vorlesungen über synthetische Geometrie, bearbeitet\\nvon Geiser und Schröter,\" Nouv. Ann. (1868).\\n\\n\\nSynthetic geometry appears in English in 1870 in Report on education\\nby John Wesley Hoyt, published by the U. S. Government Printing Office:\\n\"First year\\'s course in mathematical section. Theory of numbers; differential and integral calculus; theory of functions, with repetitions;\\nanalytical geometry of the plane; experimental physics, with repetitions; experimental chemistry, with repetitions; descriptive geometry,\\nwith exercises and repetitions; synthetic geometry; machine-drawing\"\\n[University of Michigan Digital Library].\\n\\n\\n\\nThe term SYSTEM OF EQUATIONS is found in 1843 in\\n\"Chapters in the Analytical Geometry of (n) Dimensions\" by Arthur Cayley in the Cambridge Mathematical Journal,\\nvol. IV: \"On the determination of linear equations in\\nx1,\\nx2,...,\\nxn\\nwhich are satisfied by the values of these quantities derived from given systems of linear equations.\"\\n[This citation, from the University of Michigan Digital Library, is a chapter title and thus appears in italics in the original.]\\n\\n\\n\\n\\n\\nFront -\\nA -\\nB -\\nC -\\nD -\\nE -\\nF -\\nG -\\nH -\\nI -\\nJ -\\nK -\\nL -\\nM -\\nN -\\nO -\\nP -\\nQ -\\nR -\\nS -\\nT -\\nU -\\nV -\\nW -\\nX -\\nY -\\nZ -\\nSources\\n\\n\\n\\n\\n',\n",
       " '\\n\\n\\n\\nTelenor - Forskning og Utvikling\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTelenor \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\nAbout Telenor R&D\\n\\n\\n\\xa0\\nProjects\\n\\n\\n\\xa0\\nExternal Relations\\n\\n\\n\\xa0\\nEntrepreneurship\\n\\n\\n\\xa0\\nPublications\\n\\n\\n\\xa0\\nIn Norwegian\\n\\xa0<< Til forsiden\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMain page |\\nTelektronikk |\\nR&D publications |\\nAnnual Results |\\n\\n\\n\\n\\n\\n\\n\\n\\nGuest \\n        editorial\\nRolv \\n        Bræk \\nWhen the CCITT, now ITU-T, \\n        initiated work on specification and programming languages back in 1972, \\n        it was a bold step. At that time software engineering was in its infancy \\n        and the development of communication software very much a pioneering thing. \\n        Every system development involved breaking some new technological ground. \\n        At the same time it was clear that software offered far more possibilities \\n        than mere replacement of electromechanical and electronic solutions. Entirely \\n        new functionality was possible and was therefore gradually introduced \\n        into the systems. This is a well-known pattern from all areas of computing. \\n        But communication systems were not allowed to trade functionality for \\n        quality, as has been so common in other strands of computing. Even as \\n        the complexity was growing beyond bounds, the systems had to satisfy outstanding \\n        requirements to high-performance, reliability and no-break operation. \\n        Therefore, it became essential at an early stage to find ways to master \\n        the quality in the face of growing complexity.\\nThe \\n        combination of high complexity with high reliability forced the communication \\n        software industry to take a pro-active approach to software quality from \\n        the very beginning. Since communication software always has been embedded \\n        real-time software with a high degree of concurrency, distribution and \\n        heterogeneity, the solutions that were developed attacked these problems \\n        from the very beginning, while they were not yet considered important \\n        in mainstream software engineering.\\nThe \\n        early techniques developed for software engineering in general, such as \\n        SADT and Structured Analysis/Structured Design, focused on activities \\n        and data-flow. Quite deliberately they did not deal with sequential behaviour, \\n        concurrency and distribution. They emphasised abstraction and human understanding \\n        more than formality. They provided no formal semantics, and therefore \\n        it was not possible to simulate and analyse the system behaviour before \\n        it was implemented. Moreover, the mapping from abstract model to concrete \\n        design was unclear, and the value of abstract models was therefore limited \\n        to the early phases. They had little documentation value for the final \\n        product and were in many cases just thrown away. Apparently, the activity-oriented \\n        approach of those techniques did not deliver all the benefits promised, \\n        not even outside the communication domain.\\nLater \\n        developments have focused more on data modelling, and these have been \\n        considerably more successful, especially for data-intensive applications. \\n        In recent years, the trend has been towards object-orientation and more \\n        formality. The Unified Modelling Language, UML, is the latest and most \\n        notable development in this direction. It combines a set of graphical \\n        notations with a partial semantics that makes its meaning more precise. \\n        It has notation for sequential and concurrent behaviours based on StateCharts \\n        that enable a partial simulation of behaviour before it is implemented, \\n        but still it lacks a complete semantics.\\nThe \\n        techniques developed for communication systems on the other hand, emphasised \\n        formality and dealt explicitly with sequential behaviour and concurrency \\n        from the beginning. All the formal description techniques (FDTs) ESTELLE, \\n        LOTOS and SDL had state transition based semantics that enabled simulation \\n        and analysis to take place before implementation. SDL had the additional \\n        benefit of a graphical notation that supported human comprehension combined \\n        with an underlying finite state machine semantics that could be implemented \\n        effectively. For this reason SDL has been the most successful of the FDTs, \\n        with a good track record from numerous industrial development projects.\\nSDL \\n        as a language was object-based already when first recommended in 1976, \\n        and since 1992 it has been a full-fledged object-oriented language. It \\n        has a semantics that supports formal validation and enables complete simulation \\n        to take place before implementation, and also to generate complete and \\n        efficient implementation code automatically. These properties enable development \\n        organisations to move from an implementation oriented development paradigm \\n        to a design oriented development paradigm. In the latter, a system is \\n        documented and maintained primarily using design descriptions and not \\n        by implementation code.\\nContrary \\n        to the popular belief that techniques coming from the communication world \\n        are \\x93old fashioned\\x94 they are still leading edge in the areas of object \\n        and behaviour modelling. When communication systems and information systems \\n        now merge into ICT systems, a corresponding merge of techniques from the \\n        \\x93I\\x94 world and the \\x93C\\x94 worlds is bound to take place. As the software industry \\n        in general moves towards distributed heterogeneous solutions we now see \\n        a convergence towards a similar merge for the software industry at large. \\n        This convergence leads to considerable cross-fertilisation and integration \\n        of previously different disciplines such as control systems, user interfaces \\n        and databases.\\nUML \\n        \\x96 now emerging as a family of languages that is competing with the ITU-T \\n        languages \\x96 is developing fast and attracting far more attention than \\n        the ITU-T languages ever did. From a technical point of view, the ITU-T \\n        languages and UML partly overlap and partly complement each other. The \\n        overlap area has been greatly extended by introducing into SDL-2000 notation \\n        from UML Class Diagrams and by introducing the notion of composite states \\n        from UML State Machines/State-Charts. It is now possible to define associations \\n        between types and also partially to define types using (parts of) the \\n        UML Class diagram notation within SDL. On the other hand, SDL complements \\n        UML by providing a complete operational semantics and the possibility \\n        to precisely define the component structure of aggregate types. MSC complements \\n        UML by providing structuring mechanisms entirely missing in the UML sequence \\n        diagrams and collaboration diagrams.\\nThis \\n        issue of Telektronikk is about the family of languages currently standardised \\n        by ITU-T, and related methods, tools and middleware. The ITU-T language \\n        family presently consists of:\\n\\n\\nThe \\n            Specification and Description Language, SDL. The new version of SDL, \\n            called SDL-2000, is a major revision and is presented for the first \\n            time in a popular form in the article by Rick Reed. Rick Reed also \\n            presents the history of SDL in an accompanying article.\\n\\n\\nMessage \\n            Sequence Charts, MSC, which are used to describe external behaviour \\n            properties by means of interaction cases. MSC provide a useful complement \\n            to SDL and is used both as input when making SDL descriptions and \\n            as specification when performing verification and testing. Øystein \\n            Haugen presents the latest developments of MSC in the article MSC-2000: \\n            interacting with the future.\\n\\n\\nThe \\n            Abstract Syntax Notation One (ASN.1) is used to describe data structures, \\n            especially in connection with protocols. In combination with encoding \\n            rules for the physical transfer of data, ASN.1 is much used in protocol \\n            development, and may also be combined with SDL. Colin Willcock describes \\n            ASN.1 in his article.\\n\\n\\nThe \\n            Tree and Tabular Combined Notation, TTCN, which is used to describe \\n            test cases. TTCN may be generated from SDL and MSC. Ina Schieferdecker \\n            and Jens Grabowski describe TTCN in their article.\\n\\n\\nCHILL \\n            \\x96 the CCITT HIgh Level (programming) Language. CHILL is an advanced \\n            programming language that supports concurrent processes. It has been \\n            adopted by many major telecom manufacturers and used successfully \\n            to develop a wide range of complex systems. CHILL is described in \\n            the article by Jürgen Winkler.\\n\\n\\nThe \\n            Object Definition Language, ODL, which is an extension of the Interface \\n            Definition Language, IDL, known from CORBA. ODL is introduced in the \\n            paper by Joachim Fischer and Marc Born.\\n\\n\\nA mapping \\n        between SDL and UML has been defined in the ITU-T recommendation Z.109, \\n        SDL combined with UML, which is elaborated in the article by Birger Møller-Pedersen. \\n        This mapping allows developers and tools to put leverage on the strengths \\n        of both languages by facilitating a combined use.\\nThe \\n        article by Rolv Bræk and Arve Meisingset presents an overview of the language \\n        features of SDL-2000, MSC-2000 and UML. The purpose is to give readers \\n        that are unfamiliar with these languages a first introduction, and also \\n        a feeling for their main content as a background for the more detailed \\n        articles that follow. Readers with a basic knowledge of the languages, \\n        who are more interested in the new features, should move directly to the \\n        specialist articles.\\nOne \\n        important asset of the ITU-T languages, especially SDL, has been its formally \\n        defined semantics. Principles for defining formal semantics are presented \\n        in the paper by Andreas Prinz, using examples from SDL and MSC as illustration.\\nIt is \\n        a common misunderstanding that formal language and formal method is the \\n        same thing, but it is not. Methods are concerned with how to use the languages \\n        to achieve better results. Several methods have been introduced that are \\n        based on the ITU-T languages and UML. The article by Rolv Bræk presents \\n        some general methodology issues for using the ITU-T languages and UML. \\n        The article by Steve Randall presents specific guidelines for formal use \\n        of SDL in development, e.g. of ETSI standards, and Anthony Wiles and Milan \\n        Zoric report on their experiences from the application of these guidelines \\n        in the development of the Hiperlan standards.\\nOne \\n        strongpoint of the ITU-T languages is that abstraction, using concepts \\n        suitable for human comprehension, is combined with semantics suitable \\n        both for extensive tool support and efficient implementation. Extensive \\n        tool support for validation and testing is one of the benefits that result \\n        from this. The article by Dieter Hogrefe, Beat Koch and Helmuth Neukirchen \\n        introduces the general principles of validation and testing. The possibility \\n        to derive efficient implementations is another benefit elaborated in the \\n        article by Richard Sanders.\\nTwo \\n        commercial sets of tools that support the ITU-T languages in combination \\n        with UML are presented in two separate articles. Anders Olsen and Finn \\n        Kristoffersen present the Cinderella tools, while Philippe Leblanc, Thomas \\n        Hjelm and Anders Ek present the Telelogic tools.\\nMiddleware \\n        is an important area where communication and general computing converge. \\n        The article by Anastasius Gavras outlines the needs for distributed platforms \\n        for telecommunication applications.\\nAmardeo \\n        Sarma presents perspectives on future standardisation in the areas covered \\n        by this issue of Telektronikk in his article.\\nThe \\n        idea behind the feature section was very ambitious. A lot of internationally \\n        acknowledged specialists have been involved. I would like to express special \\n        thanks to Arve Meisingset for all his co-editing work throughout this \\n        process. To my knowledge, this is the first time that all the ITU-T languages \\n        with associated topics have been presented in one place. Enjoy!\\n\\nBack \\n        to the main page\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\xa0\\n\\n\\n\\nGå direkte til:\\nMobiltelefoni \\n|\\nMobile Bedr.løsninger \\n|\\n\\nSmartphone \\n|\\nVIP Nett \\n|\\nEpostleser \\n\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\nSiden sist oppdatert: 02.08.2001.\\nAnsvarlig: Telenor.\\nGi oss tilbakemelding!\\n\\n\\n\\n\\n\\n\\n\\n^ Til toppen av siden\\n\\n\\nProdusert av Icon Medialab / \\n© Telenor 2000 \\n\\xa0\\xa0\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " \"\\n\\nPeople and RolesSoftware Release Tools ManualPrevChapter 1 IntroductionNext1.2 People and RolesIt is assumed that each software project managed with these\\ntools has one software\\nlibrarian that is responsible for the software repository as a whole.\\nIt is the job of the librarian to manage the lifetime of\\nreleases: coherent and stamped snapshots of the software.\\nThe librarian need not be responsible for all aspects of the software.\\nIn fact, there is no need for the librarian to have anything to do\\nwith the architecture, design or even the code of the software\\nproject.  It does help a lot if he or she has a good understanding of\\nthe system's structure and is experienced with software development\\ntools.\\rIn addition to the librarian each project has one or more\\ndevelopers who produce and modify the code.  It is assumed that not\\nall developers work with all of the software.  Rather, the system is\\nexpected to be partitioned into smaller parts which we shall call\\npackages.  Each package has one main author, or coordinator, responsible for its\\ndevelopment.  Other developers may join the coordinator as peers, but\\nthey do not share the responsibilities.  This subdivision of\\nresponsibilities may apply recursively so that a package may be\\ncomposed of smaller packages.[1] Thus, the concept of\\ndistributed responsibility is very germane to this tool suite.\\rFinally, there is one more category of people involved: the end\\nusers who do not develop the software but do something with\\nit.[2] Since\\nthe Software Release Tools are directed at developers, there is little\\nmaterial in this manual that is of benefit to end users.  Each\\nsoftware project must describe how its software is to be used; this\\nshould be a natural part of the documentation.\\rOne issue worth noting is that the above classifications are\\nonly roles.  The coordinator of one package may be a peer developer\\nfor another package.  The person wearing the software librarian's hat\\nat one moment may take the hat of a regular developer an instant\\nafter, and the role of an end user later.  The responsibilities may\\nalso shift from one person to another over the time.  A package may\\nfor instance have different coordinators at different times.  What\\nreally matters is that there is one coordinator for each package at\\nany one time.\\rNotes[1]Typically the\\npartitioning into packages follows naturally the domain decomposition\\nof the software project itself.[2]In the context for which these tools are developed,\\nthe users almost always run the software directly from the releases.\\nHence the lack of support for creating distributions.PrevHomeNextIntroductionUpWhat The Release Tools Provide\\n\",\n",
       " ' Stochastic program A point x is feasible if it is in X and satisfies the constraints: g(x) <= 0 and h(x) = 0. A point x* is optimal if it is feasible and if the value of the objective function is not less than that of any other feasible solution: f(x*) >= f(x) for all feasible x. The sense of optimization is presented here as maximization , but it could just as well be minimization , with the appropriate change in the meaning of optimal solution: f(x*) <= f(x) for all feasible x. A mathematical program is often extended to indicate a family of mathematical programs over a parameter space , say P. This merely involves extending the domain of the functions, f, g and h, and we use the semi-colon to separate the decision variables from the parameters. Maximize f(x; p): x in X, g(x; p) <= 0, h(x; p) = 0. (We could also have X depend on p, but this form generally suffices.) Mathematical programming is the study or use of the mathematical program. It includes any or all of the following: Theorems about the form of a solution, including whether one exists; Algorithms to seek a solution or ascertain that none exists; Formulation of problems into mathematical programs, including understanding the quality of one formulation in comparison with another; Analysis of results, including debugging situations, such as infeasible or anomalous values; Theorems about the model structure, including properties pertaining to feasibility, redundancy and/or implied relations (such theorems could be to support analysis of results or design of algorithms); Theorems about approximation arising from imperfections of model forms, levels of aggregation, computational error, and other deviations; Developments in connection with other disciplines, such as a computing environment. One taxonomy for mathematical programming is by its defining ingredients: Abstract Biconvex Bilinear Composite concave Convex Disjunctive Dynamic Factorable Fractional Geometric Integer Infinite Lattice Linear Mixed-Integer Multilevel Nonlinear Pseudo-boolean Reverse convex Semi-definite Semi-infinite Separable This glossary contains some mathematical programming problems , which you might examine as examples. Other web sources that explain the nature of mathematical programming are given in the NEOS Guide . You might also consult my Myths and Counterexamples in Mathematical Programming . Notation Last updated: Sunday, January 25, 2004 Send questions and comments to Harvey.Greenberg@cudenver.edu . Copyright © 1996 – 2003, Mathematical Programming Glossary, by Harvey Greenberg \\n\\t\\n\\n-->\\n\\n\\nMathematical Programming Glossary\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n \\n\\n \\n\\n\\n \\n \\n\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n\\n \\n\\n\\n \\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\n \\xa0 \\n\\n\\nNature of Mathematical Programming\\n\\n The Nature of Mathematical Programming\\n\\n\\n\\n\\n\\n   George B. Dantzig\\n Father of Mathematical Programming\\n\\n\\n\\nHere is a biography.\\n             \\n     \\n\\n\\n\\n\\n \\n  A mathematical program is an optimization problem of the form: \\n\\n\\n               Maximize f(x): x in X, g(x)  <=  0, h(x) = 0,\\n \\n\\n  where X is a subset of R^n and is in the domain of the real-valued\\n  functions, f, g and h.  The relations, g(x) <= 0 and h(x) = 0 are\\n  called constraints, and f is called the objective function.\\n \\n\\n  There are, however, forms that deviate from this paradigm, and it is \\n  typically a modeling issue to find an equivalent standard form.\\n  Important examples are as follows:\\n\\n \\n  Fuzzy mathematical program\\n        Goal program\\n Multiple objectives\\n  Randomized program\\n\\n  Stochastic program\\n\\n\\n\\n  A point x is feasible if it is in X and satisfies the constraints:\\n  g(x) <= 0 and h(x) = 0.  A point x* is\\n  optimal if it is feasible and if the value of the objective\\n  function is not less than that of any other feasible solution:  f(x*) >=\\n  f(x) for all feasible x.  The sense of optimization is presented\\n  here as maximization, but it could just as well be \\n  minimization, with the appropriate change in the meaning of optimal \\n  solution:  f(x*) <= f(x) for all feasible x.\\n \\n\\n  A mathematical program is often extended to indicate a family of\\n  mathematical programs over a parameter space, say P.  This merely\\n  involves extending the domain of the functions, f, g and h, and we use\\n  the semi-colon to separate the decision variables from the parameters.\\n\\n \\n        Maximize f(x; p): x in X, g(x; p)  <=  0, h(x; p) = 0.\\n \\n (We could also have X depend on p, but this form generally suffices.)\\n \\nMathematical programming is the study or use of the mathematical\\n  program.  It includes any or all of the following:\\n\\n \\nTheorems about the form of a solution, including whether one exists;\\n\\nAlgorithms to seek a solution or ascertain that none exists;\\n\\nFormulation of problems into mathematical programs, including \\n      understanding the quality of one formulation in comparison with another;\\n\\nAnalysis of results, including debugging situations, such as\\n      infeasible or anomalous values;\\n\\nTheorems about the model structure, including properties pertaining\\n      to feasibility, redundancy and/or implied relations (such theorems\\n      could be to support analysis of results or design of algorithms);\\n\\nTheorems about approximation arising from imperfections of model\\n      forms, levels of aggregation, computational error, and other deviations;\\n\\nDevelopments in connection with other disciplines, such as a\\n      computing environment.\\n\\n\\nOne taxonomy for mathematical programming is by its defining ingredients:\\n     \\n\\n          Abstract\\n\\n          Biconvex\\n\\n          Bilinear\\n\\n Composite concave\\n\\n            Convex\\n\\n       Disjunctive\\n\\n                        Dynamic\\n\\n        Factorable\\n\\n        Fractional\\n\\n         Geometric\\n\\n                        Integer\\n\\n          Infinite\\n\\n           Lattice\\n\\n                        Linear\\n\\n                       Mixed-Integer\\n\\n        Multilevel\\n\\n                       Nonlinear\\n\\n    Pseudo-boolean\\n\\n    Reverse convex\\n\\n     Semi-definite\\n\\n     Semi-infinite\\n\\n         Separable\\n\\n\\n\\n  This glossary contains some mathematical programming \\n  problems, which you might examine as examples.  \\n\\nOther web sources that explain the nature of mathematical programming are given \\nin the \\nNEOS Guide. \\nYou might also consult my \\n\\nMyths and Counterexamples in Mathematical Programming. \\n\\n\\xa0 \\n\\t  \\n\\t  \\n      \\n\\n\\nNotation\\n\\n\\n\\n\\n        Last updated:  Sunday, January 25, 2004         Send questions and comments to \\nHarvey.Greenberg@cudenver.edu. \\n        \\n        Copyright© 1996 – 2003, \\n   Mathematical Programming Glossary, by \\n     Harvey Greenberg \\n\\xa0\\n\\n\\n\\n\\n',\n",
       " '\\n\\n\\n\\nProgramming Languges\\n\\n\\nProgramming Languages --\\nThe LOOPS Project (1982-1986)\\nMain Participants: Daniel Bobrow, Sanjay\\nMittal, Stanley Lanning, and Mark Stefik.\\nThe LOOPS (Lisp Object-Oriented Language)\\nproject was started to support development of expert systems project at PARC. We wanted a\\nlanguage that had many of the features of frame languages, such as objects, annotated\\nvalues, inheritance, and attached procedures. We drew heavily on Smalltalk-80, which was\\nbeing developed next door.\\nBobrow and Stefik had done frame\\nlanguages before (KRL and UNITS, respectively). KRL was one of the first frame languges\\never and established the paradigm. Units was part of a doctoral dissertation, was heavily\\nused at Stanford, and was subsequently developed by Intellicorp to become KEE. \\nLoops was a multiple-paradigm extension to\\nInterlisp-D. It had\\n\\nObject-oriented\\n    programming. (Classes and objects, class variables, instance variables, methods,\\n    multiple-inheritance, interactive class browsers)\\n\\nAccess-oriented\\n    programming. Nestable active values that can be attached to instance variables.\\n    Whenever a program puts a value to an instance variable or gets a value from an instance\\n    variable to which active values are attached, procedures specified in the active value are\\n    triggered. Active values also enabled attaching lists of property values to instance\\n    variables. This was\\xa0 used for creating audit trails and other things.\\n\\n\\n\\nRule-oriented\\n    programming. A simple forward-chaining rule language with facilities for leaving an\\n    audit trail via active values.\\n\\n\\n\\n\\n\\n\\nLoops provided \"gauges\" that could be\\n    used for debugging and for monitoring in simulation programs. Gauges were LOOPS objects.\\n    They could be attached\\xa0 to any value of any object in the system and provide a visual\\n    indication of program activity.\\n\\n\\n\\nAs part of developing LOOPS, we developed a one week course\\nabout the language and offered it to research colleagues and Xerox\\xa0 customers. As\\npart of the pedagogy of that course, students developed a program that participated in a\\nsimulation game. The program was called Truckin\\'. On the last day of the course, programs\\nfrom all of the students were loaded into a single simulation environment (sometimes\\nacross multiple machines). The programs competed with each other in the Truckin\\'\\nsimulation world. The event was called a \"knowledge competition.\" For more about\\nthis, click on Truckin\\' under Past\\nActivities.\\xa0 \\n\\n\\n\\n\\n\\n\\n\\n\\nLoops became part of the XSIS software product for the Lisp\\nmachines it was selling. The photograph above shows Larry Masinter and Danny Bobrow\\nmanning the booth at one of the AI conferences.\\n\\n\\nIn the 1980s, the Lisp community was fragmented\\n    into many subcommunities using language variants -- including MIT (Symbolics) Lisp,\\n    Interlisp, T, NIL, and a few others. MIT Lisp had its own object-oriented sublanguage\\n    known as Flavors. There was a movement to unify the various lisps with a language known as\\n    Common Lisp. With Gregor Kiczales, Ken Kahn and others, we redesigned Loops to create\\n    Common Loops for Common Lisp. CommonLoops added polymorphism to method invocation, and\\n    unified the \"message sending\" syntax of LOOPS for method invocation with\\n    procedure invocation. It also shed the rule language (good idea) and active values (too\\n    bad).\\xa0 During this process we joined with others from the MIT Lisp community and\\n    created a unified object language that was known as CLOS (Common Lisp Object System). This\\n    language eventually became part of Common Lisp.\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\nShown (left to right) are Sanjay Mittal, Mark\\n    Stefik, and Daniel Bobrow in around 1983. Mittal focused\\xa0 on the Loops automatic\\n    testing suite and coded much of the Truckin\\' game. Stefik developed the rule language,\\n    composite objects, and much of the system documentation. Bobrow did much of work on active\\n    values and the heavy lifting in the system underpinnings. Later, Stan Lanning (not shown)\\n    rewrote many portions of the system and developed the official documentation used by XSIS\\n    in its product.\\n\\n\\n\\nMain Papers \\n\\nBobrow, D.G., and Stefik, M. J. Perspectives on Artificial\\n    Intelligence Programming. Science 231:4741, pp. 951-956, 28 February 1986.\\n    (Reprinted in Rich, C. & Waters R.C. (Eds.) Readings in Artificial Intelligence and\\n    Software Engineering, pp. 581-587, Los Altos: Morgan Kaufman Publishers, 1986.)\\n\\n\\nStefik, M. Bobrow, D.G., and Kahn, K. Integrating\\n    access-oriented programming into a multiparadigm environment. IEEE Software, 3:1,\\n    pp. 10-18, January 1986. (Reprinted in Peterson, G.E. (ed), Object-Oriented Computing,\\n    Volume 2: Implementations, IEEE Computer Society Press, pp. 170-179, 1987. Also\\n    reprinted in Richer, M.H. (ed.) AI Tools and Techniques, pp. 47-63, Ablex\\n    Publishing Corporation, Norwood, New Jersey.)\\n\\n\\nBobrow, D.G., Kahn, K., Kiczales, G., Masinter, L., Stefik,\\n    M., and Zdybel, F. CommonLoops: Merging Lisp and Object-Oriented Programming. OOPSLA\\n    \\'86: Proceedings of the ACM Conference on Object-Oriented Programming Systems, Languages,\\n    and Applications, pp. 17-29, Portland, Oregon, September 29 - October 2, 1986, Edited\\n    by Normal Meyrowitz, Special Issue of SIGPLAN Notices 21:11, November 1986. (Reprinted in\\n    Peterson, G.E. (ed), Object-Oriented Computing, Volume 1: Concepts, IEEE Computer\\n    Society Press, pp. 169-181, 1987. Also reprinted in Russian in Agafonov, V. (Ed.), Object-Oriented\\n    Programming, MIR publishers, 1990.)\\n\\n\\nStefik, M. Bobrow, D.G., and Kahn, K. Access-oriented\\n    programming for a multiparadigm environment. Proceedings of the Hawaii International\\n    Conference on System Sciences, January 1986. (Note: This paper won the best paper\\n    award out of 80 papers for the conference. An expanded version of this paper appeared by\\n    invitation in IEEE Software. This paper has also been reprinted in various books.)\\n\\n\\nBobrow, D. G. & Stefik, M. J. LOOPS: Data and Object\\n    Oriented Programming for Interlisp. European AI Conference, Orsay, France. 1982.\\n\\n\\nStefik, M. An examination of a frame-structured\\n    representation system.Proceedings of the International Joint Conference on Artificial\\n    Intelligence, Tokyo, Japan, pp. 845-852, August 1979.\\n\\n\\nStefik, M. Bobrow, D.G. Object-oriented programming: Themes\\n    and Variations. AI Magazine 6:4, pp. 40-62, Winter 1986. (Reprinted in Peterson,\\n    G.E. (ed), Object-Oriented Computing, Volume 1: Concepts, IEEE Computer Society\\n    Press, pp. 182-204, 1987. Also reprinted in Richer, M.H. (ed.) AI Tools and Techniques,\\n    pp. 3-45, Ablex Publishing Corporation, Norwood, New Jersey.)\\n\\n\\nBobrow, D. G., Stefik, M. The Loops Manual. Knowledge-Based\\n    VLSI Design Group Memo KB-VLSI-81-13. January 1983.\\n\\n\\n\\n',\n",
       " '\\n\\nUK Memes Central: Susan Blackmore: The Meme Machine: Strange Creatures\\n\\n\\n\\nHere we present the first chapter from Dr Susan Blackmore\\'s new book\\nThe Meme Machine, ISBN 019-850365-2, published by Oxford\\nUniversity Press March 1999. See the synopsis below.\\n(Buy it from Amazon.)\\n\\n\\n\\n\\nExtract from the foreword by Richard Dawkins\\n(Dawkins invented the term `meme\\' in 1976)\\n\\nI was always open to the possibility that the meme might one day be\\ndeveloped into a proper hypothesis of the human mind, and I did not\\nknow how ambitious such a thesis might turn out to be. Any theory\\ndeserves to be given its best shot, and that is what Susan Blackmore\\nhas given the theory of the meme. I do not know whether she will be\\njudged too ambitious in this enterprise, and I would even fear for her\\nif I did not know her redoubtable qualities as a fighter. Redoubtable\\nshe is, and hard nosed too, but at the same time her style is light and\\npersonable. Her thesis undermines our most cherished illusions (as she\\nwould see them) of individual identity and personhood, yet she comes\\nacross as the kind of individual person you would wish to know. As one\\nreader I am grateful for the courage, dedication and skill she has put\\ninto her difficult task of memetic engineering, and I am delighted to\\nrecommend her book.\\n\\n\\n\\nStrange Creatures\\nWe humans are strange creatures. There is no doubt that our bodies\\nevolved by natural selection just as other animals\\' did. Yet we differ\\nfrom all other creatures in many ways. For a start we speak. We believe\\nourselves to be the most intelligent species on the planet. We are\\nextraordinarily widespread and extremely versatile in our ways of\\nmaking a living. We wage wars, believe in religions, bury our dead, and\\nget embarrassed about sex. We watch television, drive cars and eat ice\\ncream. We have had such a devastating impact upon the ecosystems of our\\nplanet that we appear to be in danger of destroying everything on which\\nour lives depend. One of the problems of being a human is that it is\\nrather hard to look at humans with an unprejudiced eye.\\n\\nOn the one hand, we are obviously animals comparable with any others.\\nWe have lungs, hearts and brains made of living cells; we eat and\\nbreathe and reproduce. Darwin\\'s theory of evolution by natural\\nselection can successfully explain how we, along with the rest of life\\non this planet, came to be here and why we all share so many\\ncharacteristics. On the other hand we behave quite differently from\\nother animals. Now that biology has so successfully explained much of\\nour similarity with other creatures we need to ask the opposite\\nquestion. What makes us so different? Could it be our superior\\nintelligence, our consciousness, our language, or what?\\n\\nA common answer is that we are simply more intelligent than any other\\nspecies. Yet the notion of intelligence is extremely slippery, with\\ninterminable arguments about how to define it, how to measure it and to\\nwhat extent it is inherited. Research in artificial intelligence (AI)\\nhas provided some nice surprises for those who thought they knew what\\nmakes human intelligence so special.\\n\\nIn the early days of AI, researchers thought that if they could teach a\\ncomputer to play chess they would have reproduced one of the highest\\nforms of human intelligence. In those days the idea that a computer\\ncould ever play well, let alone beat a Grand Master, was unthinkable.\\nYet now most home computers come with passable chess programs already\\ninstalled, and in 1997 the program \"Deep Blue\" beat World Champion\\nGarry Kasparov ending human supremacy at the game. Computers may not\\nplay chess in the same way as humans, but their success shows how wrong\\nwe can be about intelligence. Clearly what we thought were human\\nbeings\\' most special capabilities may not be.\\n\\nQuite the opposite goes for some apparently quite unintelligent things\\nlike cleaning the house, digging the garden or making a cup of tea.\\nTime and again AI researchers have tried to build robots to carry out\\nsuch tasks and been defeated. The first problem is that the tasks all\\nrequire vision. There is a popular (though possibly apocryphal) story\\nabout Marvin Minsky at MIT (the Massachusetts Institute of Technology);\\nthat he once gave his graduate students the problem of vision as a\\nsummer project. Decades later the problem of computer vision is still\\njust that - a problem. We humans can see so effortlessly that we cannot\\nbegin to imagine how complex the process has to be. And in any case\\nthis kind of intelligence cannot distinguish us from other animals\\nbecause they can see too.\\n\\nIf intelligence does not provide simple answers perhaps consciousness\\nmight. Many people believe that human consciousness is unique and is\\nresponsible for making us human. Yet scientists cannot even define the\\nterm `consciousness\\'. Everyone knows what their own consciousness is\\nlike but they cannot share that knowledge with anyone else. This\\ntroublesome fact - the subjectivity of consciousness - may explain why\\nfor most of this century the whole topic of consciousness was more or\\nless banned from scientific discussion. Now at last it has become\\nfashionable again, but scientists and philosophers cannot even agree on\\nwhat an explanation of consciousness would look like. Some say that the\\n`Hard Problem\\' of subjectivity is quite different from any other\\nscientific problem and needs a totally new kind of solution, while\\nothers are sure that when we fully understand brain function and\\nbehaviour the problem of consciousness will have disappeared.\\n\\nSome people believe in the existence of a human soul or spirit that\\ntranscends the physical brain and explains human uniqueness. With the\\ndecline in religious belief fewer and fewer people intellectually\\naccept that view, yet most of us continue to think of ourselves as a\\nlittle conscious \"me\" inside our brain; a \"me\" who sees the world,\\nmakes the decisions, directs the actions and has responsibility for\\nthem.\\n\\nAs we shall see later on, this view has to be wrong. Whatever the\\nbrain is doing it does not seem to need help from an extra, magical\\nself. Different parts of the brain carry on their tasks independently\\nof each other and countless different things are always going on at\\nonce. We may feel as though there is a central place inside our heads\\nin to which the sensations come and from which we consciously make the\\ndecisions. Yet this place simply does not exist. Clearly something is\\nvery wrong with our ordinary view of our conscious selves. From this\\nconfused viewpoint we cannot say with certainty that other animals are\\nnot conscious or that consciousness is what makes us unique. So what\\ndoes?\\nWhat makes us different?\\nThe thesis of this book is that what makes us different is our ability\\nto imitate. Imitation comes naturally to us humans. Have you ever sat\\nand blinked, or waved, or \"goo gooed\", or even just smiled, at a baby?\\nWhat happens? Very often they blink too, or wave or smile back at you.\\nWe do it so easily, even as an infant. We copy each other all the time.\\nLike seeing, it comes so effortlessly that we hardly think about it. We\\ncertainly don\\'t think of it as being something very clever. Yet, as we\\nshall see, it is fantastically clever.\\n\\nCertainly other animals don\\'t take naturally to it. Blink, or wave, or\\nsmile at your dog or cat and what happens? She might purr, wag her\\ntail, twitch, or walk away, but you can be pretty sure she will not\\nimitate you. You can teach a cat, or rat, to beg neatly for its food by\\nprogressively rewarding it but you cannot teach it by demonstrating the\\ntrick yourself - nor can another cat or rat. Years of detailed research\\non animal imitation has lead to the conclusion that it is extremely\\nrare (I shall return to this in Chapter 4). Though we may think of\\nmother cats as teaching their kittens to hunt, or groom or use the cat\\ndoor, they do not do it by demonstration or imitation. Parent birds\\n\"teach\" their babies to fly more by pushing them out of the nest and\\ngiving them the chance to try it than by demonstrating the required\\nskills for them to copy.\\n\\nThere is a special appeal to stories of animals copying human\\nbehaviour, and pet owners are fond of such tales. I read on the\\ninternet about a cat who learned to flush the toilet and soon taught a\\nsecond cat the same trick. Now the two of them sit together on the\\ncistern flushing away. A more reliable anecdote was told by Diana\\nReiss, a psychologist at Rutgers University. She works with bottlenose\\ndolphins, who are known to be able to copy vocal sounds and artificial\\nwhistles, as well as simple actions (Bauer & Johnson, 1994; Reiss &\\nMcCowan, 1993). She trained the dolphins by giving them fish as a\\nreward and also by a \"time out\" procedure for punishment. If they did\\nthe wrong thing she would walk away from the water\\'s edge and wait for\\none minute before returning to the pool. One day she threw a fish to\\none of the dolphins but had accidentally left on some spiky bits of\\nfin. Immediately the dolphin turned, swam away, and waited for a minute\\nat the other side of the pool.\\n\\nThat story touched me because I couldn\\'t help thinking of the dolphin\\nas understanding the action, as having intelligence and consciousness\\nand intentionality like ours. But we cannot even define these things,\\nlet alone be sure that the dolphin was using them in this apparent act\\nof reciprocation. What we can see is that it imitated Dr Reiss in an\\nappropriate way. We are so oblivious to the cleverness of imitation\\nthat we don\\'t even notice how rare it is in other animals and how often\\nwe do it ourselves.\\n\\nPerhaps more telling is that we do not have separate words for\\nradically different kinds of learning. We use the same word \"learning\"\\nfor simple association or `classical conditioning\\' (which almost all\\nanimals can do), for learning by trial and error or `operant\\nconditioning\\' (which many animals can do), and for learning by\\nimitation (which almost none can do). I want to argue that the supreme\\nease with which we are capable of imitation, has blinded us to this\\nsimple fact - that imitation is what makes us special.\\nImitation and the meme\\nWhen you imitate someone else, something is passed on. This \"something\"\\ncan then be passed on again, and again, and so take on a life of its\\nown. We might call this thing an idea, an instruction, a behaviour, a\\npiece of information ... but if we are going to study it we shall need\\nto give it a name.\\n\\nFortunately there is a name. It is the \"meme\".\\n\\nThe term meme first appeared in 1976, in Richard Dawkins\\'s best-selling\\nbook The Selfish Gene. In that book Dawkins, an Oxford zoologist,\\npopularised the increasingly influential view that evolution is best\\nunderstood in terms of the competition between genes. Earlier in the\\ntwentieth century biologists had blithely talked about evolution\\noccurring for the \"good of the species\" without worrying about the\\nexact mechanisms involved but in the 1960s serious problems with this\\nview began to be recognised (Williams, 1966). For example, if a group\\nof organisms all act for the good of the group then one individual who\\ndoes not can easily exploit the rest. He will then leave more\\ndescendants who in turn do not act for the group. On the more modern\\n\"gene\\'s eye view\", evolution may appear to proceed in the interests of\\nthe individual, or for the good of the species, but in fact it is all\\ndriven by the competition between genes. This new viewpoint provided a\\nmuch more powerful understanding of evolution and has come to\\nbe known as `selfish gene theory\\'.\\n\\nWe must be absolutely clear about what `selfish\\' means in this context.\\nIt does not mean genes for selfishness. Such genes would incline their\\ncarriers to act selfishly and that is something quite different. The\\nterm `selfish\\' here means that the genes act only for themselves; their\\nonly interest is their own replication; all they want is to be passed\\non to the next generation. Of course genes do not `want\\' or have aims\\nor intentions in the same way as people do; they are only chemical\\ninstructions that can be copied. So when I say they `want\\', or are\\n`selfish\\' I am using a short-hand, but this short-hand is necessary to\\navoid lengthy explanations. It will not lead us astray if we remember\\nthat genes either are or are not successful at getting passed on into\\nthe next generation. So the short-hand \"genes want x\" can always be\\nspelled out as \"genes that do x are more likely to be passed on\". This\\nis the only power they have - replicator power. And it is in this sense\\nthat they are selfish.\\n\\nDawkins also introduced the important distinction between `replicators\\'\\nand their `vehicles\\'. A replicator is anything of which copies are\\nmade, including `active replicators\\' whose nature affects the chances\\nof their being copied again. A vehicle is the entity that interacts\\nwith the environment, which is why Hull (1988a) prefers the term\\n`interactors\\' for a similar idea. Vehicles or interactors carry the\\nreplicators around inside them and protect them. The original\\nreplicator was presumably a simple self-copying molecule in the\\nprimeval soup but our most familiar replicator now is DNA. Its vehicles\\nare organisms and groups of organisms that interact with each other as\\nthey live out their lives in the seas or the air, the forests or\\nfields. Genes are the selfish replicators which drive the evolution of\\nthe biological world here on earth but Dawkins believes there is a more\\nfundamental principle at work. He suggested that wherever it arises,\\nanywhere in the universe, \"all life evolves by the differential\\nsurvival of replicating entities\" (1976, p 192). This is the foundation\\nfor the idea of Universal Darwinism; the application of Darwinian\\nthinking way beyond the confines of biological evolution.\\n\\nAt the very end of the book he asked an obvious, if provocative,\\nquestion. Are there any other replicators on our planet? The answer, he\\nclaimed, is \"Yes\". Staring us in the face, though still drifting\\nclumsily about in its primeval soup of culture, is another replicator -\\na unit of imitation.\\n\\n\"We need a name for the new replicator, a noun that conveys the idea of\\na unit of cultural transmission, or a unit of imitation. `Mimeme\\' comes\\nfrom a suitable Greek root, but I want a monosyllable that sounds a bit\\nlike `gene\\'. I hope my classicist friends will forgive me if I\\nabbreviate mimeme to meme.\"\\n\\nAs examples he suggested \"tunes, ideas, catch-phrases, clothes\\nfashions, ways of making pots or of building arches.\" He mentioned\\nscientific ideas that catch on and propagate themselves around the\\nworld by jumping from brain to brain. He wrote about religions as\\ngroups of memes with a high survival value, infecting whole societies\\nwith belief in a God or an afterlife. He talked about fashions in dress\\nor diet, and about ceremonies, customs and technologies - all of which\\nare spread by one person copying another. Memes are stored in human\\nbrains (or books or inventions) and passed on by imitation.\\n\\nIn a few pages he laid the foundations for understanding the evolution\\nof memes. He discussed their propagation by jumping from brain to\\nbrain, likened them to parasites infecting a host, treated them as\\nphysically realised living structures, and showed how mutually\\nassisting memes will gang together in groups just as genes do. Most\\nimportant, he treated the meme as a replicator in its own right. He\\ncomplained that many of his colleagues seemed unable to accept the idea\\nthat memes would spread for their own benefit, independently of any\\nbenefit to the genes. \"In the last analysis they wish always to go back\\nto `biological advantage\\'\" to answer questions about human behaviour.\\nYes, he agreed, we got our brains for biological (genetic) reasons but\\nnow we have them a new replicator has been unleashed. \"Once this new\\nevolution begins, it will in no necessary sense be subservient to the\\nold\" (Dawkins, 1976, 193-4). In other words, memetic evolution can now\\ntake off without regard to its effects on the genes.\\n\\nIf Dawkins is right then human life is permeated through and through\\nwith memes and their consequences. Everything you have learned by\\nimitation from someone else is a meme. But we must be clear what is\\nmeant by the word `imitation\\' because our whole understanding of\\nmemetics depends on it. Dawkins said that memes jump from \"brain to\\nbrain via a process which, in the broad sense, can be called\\nimitation.\" (1976, p 192). I will also use `imitation\\' in the broad\\nsense. So if, for example, a friend tells you a story and you remember\\nthe gist and pass it on to someone else then that counts as imitation.\\nYou have not precisely imitated your friend\\'s every action and word,\\nbut something (the gist of the story) has been copied from her to you\\nand then on to someone else. This is the `broad sense\\' in which we must\\nunderstand the term `imitation\\'. If in doubt, remember that something\\nmust have been copied.\\n\\nEverything that is passed from person to person in this way is a meme.\\nThis includes all the words in your vocabulary, the stories you know,\\nthe skills and habits you have picked up from others and the games you\\nlike to play. It includes the songs you sing and the rules you obey.\\nSo, for example, whenever you drive on the left (or the right!), eat\\ncurry with lager or pizza and coke, whistle the theme tune from\\n\"Neighbours\" or even shake hands, you are dealing in memes. Each of\\nthese memes has evolved in its own unique way with its own history, but\\neach of them is using your behaviour to get itself copied.\\n\\nTake the song \"Happy Birthday to You\". Millions of people - probably\\nthousands of millions of people the world over - know this tune. Indeed\\nI only have to write down those four words to have a pretty good idea\\nthat you may soon start humming it to yourself. Those words affect you,\\nprobably quite without any conscious intention on your part, by\\nstirring up a memory you already possess. And where did that come from?\\nLike millions of other people you have acquired it by imitation.\\nSomething; some kind of information, some kind of instruction, has\\nbecome lodged in all those brains so that now we all do the same thing\\nat birthday parties. That something is what we call the meme.\\n\\nMemes spread themselves around indiscriminately without regard to\\nwhether they are useful, neutral or positively harmful to us. A\\nbrilliant new scientific idea, or a technological invention may spread\\nbecause of its usefulness. A song like \"Jingle Bells\" may spread\\nbecause it sounds OK, though it is not seriously useful and can\\ndefinitely get on your nerves. But some memes are positively harmful -\\nlike chain letters and pyramid selling, new methods of fraud and false\\ndoctrines, ineffective slimming diets and dangerous medical `cures\\'. Of\\ncourse the memes don\\'t care; they are selfish like genes and will\\nsimply spread if they can.\\n\\nRemember that the same shorthand applies to memes as to genes. We can\\nsay that memes are `selfish\\', that they `don\\'t care\\', that they `want\\'\\nto propagate themselves and so on when all we mean is that successful\\nmemes are the ones that get copied and spread, while unsuccessful ones\\ndo not. This is the sense in which memes `want\\' to get copied, `want\\'\\nyou to pass them on and `don\\'t care\\' what that means to you or your\\ngenes.\\n\\nThis is the power behind the idea of memes. To start to think\\nmemetically we have to make a giant flip in our minds just as\\nbiologists had to do when taking on the idea of the selfish gene.\\nInstead of thinking of our ideas as our own creations, and as working\\nfor us, we have to think of them as autonomous selfish memes, working\\nonly to get themselves copied. We humans, because of our powers of\\nimitation, have become just the physical \"hosts\" needed for the memes\\nto get around. This is how the world looks from a \"meme\\'s eye view\".\\nMeme fear\\nThis is a scary idea indeed. And perhaps that is why the word \"meme\" is\\nso often written with inverted commas around it, as though to apologise\\nfor using it. I have even seen eminent lecturers raise both hands and\\ntweak them above their ears when forced to say \"meme\" out loud.\\nGradually the word has become more generally known, and has even been\\nadded to the Oxford English Dictionary. There are discussion groups and\\na Journal of Memetics on the internet, and the idea almost seems to\\nhave acquired a cult following in cyber-space. But in academia it has\\nnot yet been so successful. A perusal of some of the best recent books\\non human origins, the evolution of language and evolutionary psychology\\nshows that the word does not appear at all in most of them (\"meme\" is\\nnot in the index of Barkow, Cosmides and Tooby, 1992; Diamond, 1997;\\nDunbar, 1996; Mithen, 1996; Pinker, 1994; Ridley, 1996; Tudge, 1995;\\nWills, 1993; Wright, 1994). The idea of memes seems extremely relevant\\nto these disciplines and I want to argue that it is time for us to take\\non board the notion of a second replicator at work in human life and\\nevolution.\\n\\nOne of the problems with the idea of memes is that it strikes at our\\ndeepest assumptions about who we are and why we are here. This is\\nalways happening in science. Before Copernicus and Galileo people\\nbelieved they lived at the centre of the universe in a world created\\nespecially for them by God. Gradually we had to accept, not only that\\nthe sun does not revolve around the earth, but that we live on some\\nminor little planet in an ordinary galaxy in a vast universe of other\\ngalaxies.\\n\\nA hundred and forty years ago Darwin\\'s theory of evolution by natural\\nselection provided the first plausible mechanism for evolution without\\na designer. People\\'s view of their own origin changed from the Biblical\\nstory of special creation in the image of God, to an animal descended\\nfrom an ape-like ancestor - a vast leap indeed, and one that lead to\\nmuch ridicule and fanatical opposition to Darwin. Still - we have all\\ncoped with that leap and come to accept that we are animals created by\\nevolution. However, if memetics is valid, we will have to make another\\nvast leap in accepting a similar evolutionary mechanism for the origin\\nof our minds and our selves.\\n\\nWhat will determine whether the theory of memes is worth having or not?\\nAlthough philosophers of science argue over what makes a scientific\\ntheory valid, there are at least two commonly agreed criteria and I\\nwill use these in judging memetics. First, a theory must be able to\\nexplain things better than its rival theories; more economically or\\nmore comprehensively. And second, it must lead to testable predictions\\nthat turn out to be correct. Ideally those predictions should be\\nunexpected ones - things that no one would have looked for if they\\nweren\\'t starting from a theory of memetics.\\n\\nMy aim in this book is to show that many aspects of human nature are\\nexplained far better by a theory of memetics than by any rival theory\\nyet available. The theory starts only with one simple mechanism - the\\ncompetition between memes to get into human brains and be passed on\\nagain. From this it gives rise to explanations for such diverse\\nphenomena as the evolution of the enormous human brain, the origins of\\nlanguage, our tendency to talk and think too much, human altruism, and\\nthe evolution of the internet. Looked at through the new lens of the\\nmemes, human beings look quite different.\\n\\nIs the new way better? It seems obviously so to me, but I expect that\\nmany people will disagree. This is where the predictions come in. I\\nshall try to be as clear as I can in deriving predictions and showing\\nhow they follow from memetic theory. I may speculate and even, at\\ntimes, leap wildly beyond the evidence, but as long as the speculations\\ncan be tested then they can be helpful. In the end the success or\\nfailure of these predictions will decide whether memes are just a\\nmeaningless metaphor or the grand new unifying theory we need to\\nunderstand human nature.\\n\\n\\nBook Synopsis\\nForeword by Richard Dawkins\\n\\nStrange creatures What makes us different, this book argues, is our\\ncapacity to imitate. We humans can pass on ideas, stories, tunes, and\\ntheories from one person to another. All these are memes - and memes,\\nlike genes, are replicators that undergo evolution. A brief history of\\nthe meme is given, and a few implications sketched out.\\n\\nUniversal Darwinism The evolutionary algorithm requires replication,\\nheredity, and selection to run - and when it runs it produces design\\nout of nowhere. If memes are replicators then the design of societies\\nand minds is an evolutionary process. We must remember, though, that\\nmemes and genes are different in many ways. Their similarity is that\\nboth effectively say \"Copy me!\". Examples of self-replicating \"copy me\"\\nmemes are provided from chain letters to political beliefs.\\n\\nThe evolution of culture Inventions are memes, from the origins of\\nfarming, to engines, books and coca cola cans. But who benefits? We\\nmay think we do but according to memetic theory it is the memes\\nthemselves who are the beneficiaries, not the genes, and certainly not\\nus - their creatures.\\n\\nTaking the meme\\'s eye view Why can\\'t we stop thinking ? The\\nsurprising answer from memetics is that it is because the memes force\\nus to keep thinking - and talking - to spread more memes. Some words\\nof caution - Not everything is a meme, only those things that are\\npassed on by imitation are memes. Distinctions between imitation,\\ncontagion, and social learning are made. A lot of what we experience\\nthrough perception and learning has nothing to do with memes.\\n\\nThree problems with memes Three important problems are discussed. We\\ncannot specify the unit of a meme, we do not know the mechanism for\\ncopying and storing memes, and memetic evolution appears to be\\n\"Lamarckian\". This last has caused enormous controversy but rests upon\\na false comparison between genetic and memetic evolution.\\n\\nThe big brain No one knows why the human brain is so relatively\\nenormous. The origins of the human brain are discussed along with\\nvarious theories of its origins. A new memetic theory is proposed -\\nthat memes designed the human brain for their own replication.\\n\\nThe origins of language The evolution of language has been hotly\\ndebated for more than a century. The major theories and their\\nstrengths and weaknesses are reviewed.\\n\\nMeme-gene coevolution A new theory of meme-gene coevolution is\\nproposed and applied to the origins of language. The theory suggests\\nthat language was created by the memes as a way of improving the\\nreplication of memes by increasing fidelity, fecundity and longevity.\\nIn other words, the purpose of language is to spread memes. Both our\\nbig brains and our language have been meme driven.\\n\\nThe limits of sociobiology Sociobiology has made great progress, for\\nexample in overthrowing the Standard Social Science Model of human\\nbehaviour. However, sociobiologists believe that the genes hold\\nculture on a \"leash\". According to memetics this is wrong - the memes\\nhave leapt off the leash and are driving the genes. The concept of\\nmemetic drive takes us far beyond the interests of the genes.\\n\\nAn orgasm saved my life Sex spreads memes. The sociobiology of sex\\nis reviewed and the importance of love, beauty, and parental\\ninvestment considered. Biological approaches can explain a lot about\\nsex but mysteries remain.\\n\\nSex in the modern world\\nFrom the genes\\' point of view the major mysteries of modern human\\nsexual behaviour are celibacy, birth control, and adoption. Each of\\nthese can be easily explained in terms of an advantage to memes - not\\ngenes.\\n\\nA memetic theory of altruism Altruism has long been a problem for\\ngenetic explanations of behaviour. The varieties of human altruism and\\ncooperation are reviewed. Conflict between genes and memes appear and\\nagain can be resolved by seeing that the meme is a replicator in its\\nown right.\\n\\nThe altruism trick A new theory of memetic altruism is proposed.\\nAltruism spreads memes and therefore thrives even at the expense of\\nthe genes. Some memes just look like altruism, but whole memeplexes\\n(co-adapted meme-complexes) can use the \"altruism trick\". Debts,\\nobligations, and bartering are all affected by memes.\\n\\nMemes of the New Age Alien abduction is a memeplex, as are many\\nother popular new age ideas. Strong emotions and inexplicable\\nexperiences provide specially ripe conditions for spreading false\\nmemes. Near-death experiences are another, as are the memeplexes of\\ndivination and fortune telling. These may all be relatively harmless\\nbut big money is involved in peddling the memes of ineffective\\nalternative therapies.\\n\\n Religions as memeplexes Religions have been used as a prime\\nexample of powerful, and usually false, memes. Their power is\\nexplained in memetic terms. Religions and genes have coevolved,\\nproviding us with brains that are especially likely to pick up and\\nenjoy religious ideas - even when they are false. The true insights at\\nthe heart of some religions can be swamped by other more powerful\\nmemes. Group selection (so controversial in biology) may also play a\\nrole. Finally, what is the difference between science and religion?\\n\\nInto the internet The memes took a great step forward when they\\ninvented writing- and then printing, and then other forms of\\ncommunication, from railways and ships to fax machines. The important\\nconcepts of copy-the-product versus copy-the-instruction are\\nexplained. We can now understand how and why the internet has evolved\\nand guess at the direction the memes will push it in.\\n\\nThe ultimate memeplex This is not some super-invention of the web,\\nbut our familiar and ordinary \"self\". What am I? A conglomeration of\\nmemes - a massive memeplex living in a brain. Many illusions are\\ncreated by the memes and, if this view of memetics is true, we are not\\nreally in charge of our lives at all - the replicators are. Our \"self\"\\nwas created by and for the memes.\\n\\nOut of the meme race Our place in the universe has to be\\nreconsidered in the light of the power of the memes. We have no free\\nwill, and our consciousness is not the driving force of our behaviour.\\nCreativity and foresight owe more to memetic evolution than to\\nindividual brilliance. In other words, we are meme machines through\\nand through, and we need to learn to live with it. Dawkins claims that\\nwe alone can rebel against the tyranny of the selfish replicators but\\nreally, the book concludes, there is no one to rebel.\\n\\n\\n\\n Memes Central home page\\nCopyright (c) Susan Blackmore 1998.\\nImage from DHD Photo Gallery.\\n\\n',\n",
       " '\\n\\n\\n\\nUNIL / Linguistique - phonetic\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0 \\n\\n\\n\\n\\n\\n\\n\\n\\n3.1 “Occlusives”\\nThe “occlusives” require a complete \\n                closure of the speech canal, not just a restriction. This distinguishes \\n                them from the continuants.\\nThe “occlusion” is twofold:\\n\\n\\nthe airstream is halted by a sudden closure in the speech \\n                    canal; \\n                  the trapped air is freed by abruptly releasing the closure. \\n                \\n\\n\\n\\n3.1.1 Oral Stops (Oral Plosives)\\nYou can hear each sound by clicking on the appropriate \\n                paragraph title.\\n\\nVoiceless \\n                bilabial stop. The lips are pressed tightly together (see \\n                figure\\xa03.1 below). There exists a corresponding lax \\n                articulation that is one of the spirants.\\n\\n\\n\\n\\n\\n\\n\\nFigure 3.1: bilabial stop\\n\\n\\n\\nVoiced \\n                bilabial stop. Same as above, but with vibration of the vocal \\n                cords. The corresponding bilabial \\n                nasal is usually voiced as well. There is also a corresponding \\n                lax articulation that \\n                is one of the spirants.\\n\\nVoiceless \\n                dental or alveolar stop. The tongue makes contact with the \\n                front teeth or with the alveolar \\n                ridge directly above them (see figure\\xa03.2 below). There \\n                exists a corresponding lax \\n                articulation that is one of the spirants.\\n\\n\\n\\n\\n\\n\\n\\nFigure 3.2: dental or alveolar stop\\n\\n\\n\\nVoiced \\n                dental or alveolar stop. Same as above, but with vibration \\n                of the vocal cords. The corresponding dental \\n                or alveolar nasal is usually voiced as well. There is also \\n                a corresponding lax articulation \\n                that is one of the spirants.\\n Voiceless \\n                retroflex stop. The tongue curves up and back so that its \\n                tip or its underside makes contact with the roof of the mouth \\n                (see figure\\xa03.3 below). (Warning! Do not confuse this \\n                symbol with that of the ordinary voiceless dental \\n                or alveolar stop!)\\n\\n\\n\\n\\n\\n\\n\\nFigure 3.3: rétroflex stop\\n\\n\\n Voiced \\n                retroflex stop. Same as above, but with vibration of the vocal \\n                cords. The corresponding retroflex \\n                nasal is usually voiced as well.\\n\\nVoiceless \\n                palatal stop. The tongue tip is directed down towards the \\n                lower teeth, while the tongue body makes contact with the hard \\n                palate (see figure 3.4 below). (It is important to distinguish \\n                between the true palatal articulation and that of a dental + [j].)\\n\\n\\n\\n\\n\\n\\n\\nFigure 3.4: palatal stop\\n\\n\\n Voiced \\n                palatal stop. Same as above, but with vibration of the vocal \\n                cords. The corresponding palatal \\n                nasal palatale is usually voiced as well.\\n\\nVoiceless \\n                velar stop. With the tongue tip resting against the lower \\n                teeth, the back of the tongue makes contact with the soft palate \\n                (see figure\\xa03.5 below).\\n\\n\\n\\n\\n\\n\\n\\nFigure 3.5: velar stop\\n\\n\\n\\nVoiced \\n                velar stop. Same as above, but with vibration of the vocal \\n                cords. The corresponding velar \\n                nasal is usually voiced as well.\\n\\nVoiceless \\n                uvular stop. The tongue tip remains placed against the lower \\n                teeth, and the tongue body is raised far enough back to make contact \\n                with the soft palate near the uvula (see figure\\xa03.6 below).\\n\\n\\n\\n\\n\\n\\n\\nFigure 3.6 : uvular stop\\n\\n\\n Voiced \\n                uvular stop. Same as above, but with vibration of the vocal \\n                cords. The corresponding uvular \\n                nasal is usuall voiced as well.\\n Glottal \\n                stop. The glottal stop is produced either by the suddent opening \\n                of the glottis under pressure from the air below, or by the abrupt \\n                closure of the glottis to block the airstream. The glottal stop \\n                is always voiceless, as the complete closure of the vocal \\n                cords precludes their vibration.\\n 3.1.2 Nasals\\nBack \\n                to the table\\n\\n\\n\\n \\n\\n\\xa0\\n\\n\\nDernière \\n                      mise à jour: 03..11.02. Page maintenue par Benoit \\n                      Curdy\\n\\n\\n \\n\\xa0 \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " \"\\n\\n\\n\\n Local and Global Variables\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n \\nContents\\n Next:\\n Using an Editor\\nUp:\\n Defining LISP functions\\n Previous:\\n Defining Functions: Defun\\n  \\n Local and Global Variables\\n\\nAn important question that might occur to you is what would happen if you had set x to have some value, before using the function square?  Try it out:\\n\\n>(setf x 3)\\n3\\n\\n>(square 345)\\n119025\\n\\n>x\\n3\\n\\n\\nSetting x to 3 has no effect on the operation of square -- neither does the function square have a (lasting) effect on the value of x.  This is because the interpreter makes a distinction between local variables and global variables.\\n\\nGlobal variables have values that can be accessed by any function.  The values of local variables are defined only relative to a certain ``block'' of code.  The body of a function definition implicitly constitutes a code block.\\n\\nIn the definition of square, the variable list (x) tells the interpreter what variables are local to the body of the function, i.e. in this case x is a local variable while the block (* x x) is evaluated.\\n\\nWhen you make a call to a square, e.g., (square 345), the interpreter assigns 345 as the value of x that is local to the function square.  ``Local'' means that functions other than square do not know about this value for x.  Inside the body of square the local value of x (e.g., 345) is preferred to the global value (e.g., 3) that you gave it at the top level.  As soon as (square 345) returns 119025, the local value of x no longer is stored, and the only value of x the interpreter knows about is its global value, 3.\\n\\nThe rule the interpreter follows for evaluating symbols is that inside code blocks local values are always looked for first.  If a local value for the variable does not exist, then a global value is sought.  If no global value is found then the result is an error.  This precedence order can be seen with the following example.  First define the following function:\\n\\n>(defun y-plus (x)\\n   (+ x y))\\nY-PLUS\\n\\n\\nIf you have not assigned a value to y, then typing (y-plus 2) will give an error (unbound variable y).  Now do the following:\\n\\n>(setq y 2)\\n2\\n\\n>(y-plus 4)\\n6\\n\\n>(setq x 5)\\n5\\n\\n>(y-plus 23)\\n25\\n\\n>x\\n5\\n\\n>(setq y 27)\\n27\\n\\n>(y-plus 43)\\n70\\n\\n\\nGo through these examples (and try out others) to make sure you understand why they behave as shown.\\n\\nThe distinction between local and global variables is very important, and we will come back to it several times.  If LISP is not your first programming language one of the most likely signs of an ``accent'' from the other language(s) is the overuse of global variables.  Good LISP programmers use local variables as much as they can, rather than using global ones.  Local variables disappear when the function that uses them is done.  Global variables hang around for ever, so they take up more memory.\\n\\n \\n\\n\\n \\nContents\\n Next:\\n Using an Editor\\nUp:\\n Defining LISP functions\\n Previous:\\n Defining Functions: Defun\\n  \\n \\n\\n\\n© Colin Allen & Maneesh Dhagat \\nNovember 1999\\n\\n\\n\",\n",
       " '\\n\\n\\n\\nPL Concepts: Lambda Calculus\\n\\n\\n\\n\\nPL Concepts: Lambda Calculus\\nLambda calculus is a notation that allows the definition of\\n\"unnamed\" functions.  In particular, the notation permits treating\\nfunctions mathematically as first-class objects (well, math doesn\\'t\\nreally deal with objects...).  The notation that defines a function\\nconsists of a lambda, a variable to be used as the input of the\\nfunction, a period, and an expression (in terms of the variable) that\\ndefines the output of the function.\\nIn the examples below, L is used in place of a lambda\\nsince HTML doesn\\'t support Greek letters or equations (yet).\\nA simple function is the identity function:\\nId = Lx.x\\nId is defined to be a function of a variable x that\\nsimply returns x.  Note that this notation is essentially\\nequivalent to\\nId(x) = x\\nLambda notation is particularly useful in dealing with\\ncurried functions, in that one can define\\nfunctions without having to give all the arguments immediately.  For\\nexample,\\na = Lx.Ly.x+y\\ndefines a function a which, depending on how you look at\\nit,\\n\\ntakes two arguments (and adds them),\\ntakes one argument and returns a function of one argument,\\nor\\ntakes no arguments and returns a function of two\\narguments.\\n\\nThe utility of the lambda notation, particularly with respect to\\ncurrying, is thus clear: it allows the\\nmanipulation of functions without excessive concern for naming the\\nfunction or defining its arguments immediately.\\nLambda calculus is used extensively in denotational semantics; it\\nalso motivated various language constructs, particularly those in\\nLisp and Scheme.\\nLambda Capture\\nLambda capture is the process of \"capturing\" the meaning\\nof a function\\'s arguments in the environment in which they belong.\\nThat is, when a function is applied to its arguments, the function\\'s\\nparameters are bound to the meaning of the arguments outside the\\nfunction\\'s environment.  Hence, if the argument contains free\\nvariables, then the parameter is bound to the argument with its free\\nvariables.\\nThe importance of lambda capture is visible primarily when a\\nfunction\\'s parameters have the same name as one or more free\\nvariables--for example,\\n(Lm.(Ln.n+1)(m))(2n)\\nHere, the n in the inner lambda expression is a bound\\nvariable, while the n in the function\\'s argument is a free\\nvariable.  When the outer function (Lm...) is applied to its\\nargument, the free variable n is captured and is therefore\\nnot confused with the bound variable n.\\nContributors: John Doppke <doppke@cs.colorado.edu>\\nPL Prelim Home Page\\n\\n\\n\\n\\n',\n",
       " '\\n\\n\\n\\nComputational Morphology\\n\\n\\n\\n\\n\\n\\nComputational Morphology\\n\\n\\n \\n\\n\\nAuthor: Harald Trost\\nAbstract\\nComputational morphology deals with the processing of words and word forms, in both their graphemic, i.e., written form, and their phonemic, i.e., spoken form. It has a wide range of practical applications. Probably every one of you has already come across some of them. Ever used spelling correction? Or wondered about some strange hyphenation in a newspaper article? This is computational morphology at work. To solve such seemingly simple tasks often poses hard problems for a computer program. This section shall provide you with some insights into why this is so and what techniques are available to tackle these tasks.\\n\\n1 Introduction\\nNatural languages have intricate systems to create words and word forms from smaller units in a systematic way. The part of linguistics dealing with these phenomena is morphology. This chapter starts with a quick overview over this fascinating field. It continues with applications of computational morphology. The rest is devoted to processing techniques. Computational morphology has evolved from very modest beginnings using full form lexica or some ad-hoc concatenation techniques to the much more powerful tools available today. The chapter concludes with a number of examples for encoding morphological phenomena from different languages using these tools.\\n\\n2 Linguistic fundamentals\\nWhat is morphology all about? A simple answer is that morphology deals with words. In formal language words are just arbitrary strings denoting constants or variables. Nobody would care about a morphology of formal languages. In natural languages the picture is very different. Every human language contains some hundred thousands of words. And continuously new words are integrated while others are drifting out of use. This infinity of words is produced from a finite collection of smaller units. The task of morphology is to find and describe the mechanisms behind this process.\\n\\nThe basic building blocks in morphology are MORPHEMES. They are defined as the smallest unit in language to which a meaning may be assigned or, alternatively, as the minimal unit of grammatical analysis. Morphemes are abstract entities that express basic features. Either semantic concepts denoting entities or relationships in our world like door, blue or take. Such morphemes are called roots. Or syntactic features like past or plural. \\n\\nTheir realisation as part of a word is called MORPH. Often, there is a one-to-one relation, e.g., the morpheme door is realized as the morph door. With take, on the other hand, we find the two possibilities take and took. In such a case we speak of allomorphs. Plural in English is usually expressed by the morph —s. There are exceptions though: in oxen plural is expressed through the morph —en, in men by stem vowel alteration. All these different forms are allomorphs of the plural morpheme.\\n\\nA basic distinction is the one between bound and free morphs. A free morph may form a word on its own, e.g., the morph door. We call such words monomorphemic because they consist of a single morph. Bound morphs, on the other hand, occur only in combination with other forms. All affixes are bound morphs. For example, the word doors consists of the free morph door and the bound morph -s. Words may also consist of free morphs only, e.g., tearoom, or bound morphs only, e.g., aggression.\\n\\nEvery language typically contains some ten thousand morphs. This is a magnitude below the number of words. Strict rules govern the  combination of these morphs to words (cf. 2.4). This way of structuring the lexicon makes the cognitive load of remembering so many words much easier.\\n\\n2.1 What is a word?\\nSurprisingly, there is no easy answer to this question. One can easily spot „words\" in a text because they are separated from each other by blanks or punctuation. However, if you record ordinary speech you will find out that there are no breaks between words. But, we could isolate units which occur over and over again in speech, but in different combinations. So the notion of „word\" makes sense. But how do we define it?\\n\\nWe may look at „words\" from different perspectives. To syntax „words\" are the units that make up sentences. Words are grouped according to their function in the sentential structure. Each groups gets a tag–usually called part-of-speech or word category–and grammar deals with these tags only, omitting the details of  specific words. \\n\\nMorphology, on the other hand, is concerned with the inner structure of „words\". It tries to uncover the rules that govern the formation of words from smaller units. We notice that words that convey the same meaning look differently depending on their syntactic context. Take, e.g., the words degrade, degrades, degrading, and degraded. We can think of those as different forms of the same „word\". We call the part that carries the meaning of those forms a base form. In our example this is the form degrade. All other forms in this example are produced by adding a suffix. A wide range of other possibilities will be shown in section 2.3. All the different forms of a word together are called its paradigm. The part of morphology governing the production of these forms is called \\ninflection .\\n\\nBase forms in English are at the same time always word forms in their own right, e.g., the base form degrade is also present tense, active voice, non 3rd person singular. In other languages we find a slightly different situation. In Italian nouns are marked for gender and number. Different affixes are used to signal masculine and feminine on the one hand and singular an plural on the other hand.\\n\\n\\nSINGULARPLURAL\\nMASCULINE\\npomodoro\\npomodori\\n‘tomato’\\n\\nFEMININE\\ncipolla\\ncipolle‘onion’\\n\\n\\nNeither of the two forms of a noun can function as the base form. Instead, we must assume that the base form is what is left over after removing the respective suffixes, i.e.,  pomodor- and cipoll-. Such base forms that cannot occur as word forms in their pure form are called stems.\\nBase forms themselves are not necessarily atomic. By comparing degrade to downgrade, retrograde and upgrade on the one hand and decompose, decrease and deport on the other hand we can see that it is composed of the morphs de- and grade. The morpheme carrying the central meaning is often called the root of the word. A root may combine with suffixes (cf. 2.2.2.2) or other roots (cf. 2.2.2.3) to form new base forms.\\nFinally, we can describe „word\" from a phonological perspective. Important for morphology is that phonological units define the range for phonological processes. Often, the phonological word is identical to the morphological word but sometimes boundaries differ. For example, the morphophonological process of final devoicing in German (cf. 2.3.2.2) works on syllable structure. Let’s look at two words derived from the root lieb. The word be+lieb+ig (arbitrary) is realized as /b´li\\x98bik/ because it is a single phonological word. On the other hand, lieb+lich (lovely) is realized as /li\\x98pliC/. Here, the last consonant of the root is devoiced because the two morphs are separated by a phonological word boundary.\\n\\n2.2 Functions of morphology\\nHow much and what sort of information is expressed by morphology differs widely between languages. Information that in some languages is expressed by syntax is expressed morphologically in others. Take, e.g., the expression of future tense: English uses an auxiliary verb construction, Spanish a suffix.\\n\\n\\nI speak-hablo\\nI will speak-hablaré\\n\\nAlso, some type of information may be present in one language while and missing in another. In many languages plural marking for nouns is mandatory. In Japanese it is absent.\\n\\n\\nbook-hon\\nbooks-hon\\n\\nThe means for encoding information also vary widely. Most common is the use of different types of affixes. Traditionally, linguists discriminate between the following types of languages with regard to morphology:\\n\\n\\nIsolating languages (e.g. Mandarin Chinese): there are no bound forms, e.g., no affixes  that can be attached to a word. The only morphological operation is composition.\\nAgglutinative languages (e.g. Ugro-Finnic and Turkic languages): all bound forms are either prefixes or suffixes, i.e., they are added to a stem like beads on a string. Every affix represents a distinct morphological feature. Every feature is expressed by exactly one affix.\\nInflectional languages (e.g. Indo-European languages): distinct features are merged into a single bound form (a so-called portmanteau morph). The same underlying feature may be expressed differently, depending on the paradigm\\nPolysynthetic languages (e.g. Inuit languages): these languages express more of syntax in morphology than other languages, e.g., verb arguments are incorporated into the verb.\\n\\nThis classification is quite artificial. Real languages rarely fall cleanly into one of the above classes, e.g., even Mandarin has a few suffixes. Moreover, this classification mixes the aspect of what is expressed morphologically and the means for expressing it.\\n2.2.1 Inflection\\nInflection is required in particular syntactic contexts. It does not change the part-of-speech category but the grammatical function. The different forms of a word produced by inflection form its paradigm. Inflection is complete, i.e., with rare exceptions all the forms of its paradigm exist for a specific word. Regarding inflection, words can be categorized in three classes: \\n\\n\\nParticles or not-inflecting words: they occur in just one form. In English, prepositions, adverbs, conjunctions and articles are particles;\\nVerbs or words following conjugation;\\nNominals or words following declination, i.e., nouns, adjectives, and pronouns.\\n\\nConjugation is mainly concerned with defining tense and aspect and agreement features like person and number. Take for example the German verb ‘lesen’  (to read). German verb forms come in present and past tense, indicative or subjunctive. \\n\\n\\n\\nPRESENT\\nPAST\\n\\nINDICATIVEINDICATIVE\\nSUBJUNCTIVESUBJUNCTIVE\\n\\nSINGULARPLURALSINGULARPLURAL\\nSINGULARPLURALSINGULARPLURAL\\n1st PERSON\\nleselesenleselesen\\nlaslasenläseläsen\\n2nd PERSON\\nliestlestlesestleset\\nlastlastläsestläset\\n3rd PERSON\\nliestlesenleselesen\\nlaslasenläseläsen\\nPARTICIPLE\\nlesend\\ngelesen\\nIMPERATIVE\\nlieslest\\n\\nINFINITIVE\\nlesen\\n\\n\\nDeclination marks various agreement features like number (singular, plural, dual, etc.), case (as governed by verbs and prepositions, or to mark various kinds of semantic relations), gender (male, female, neuter), and comparison.\\n2.2.2 Derivation and Compounding\\nIn contrast to inflection which produces different forms of the same word derivation and compounding are processes that create new words. Thus, derivation and compounding have nothing to do with morphosyntax. They are a means to extend our lexicon in an economic and principled way.\\n\\nIn derivation, a different word--often of a different part-of-speech category--is produced by adding a bound morph to a stem. Derivation is incomplete, i.e., a derivational morph cannot be applied to all words of the appropriate class. For example, in German the very productive derivational suffix -bar can be applied to many but not all verbs to produce adjectives:\\n\\n\\nessen‘to drink’-essbar‘eatable’\\nhören‘to hear’-hörbar‘audible’\\nabsehen‘to conceive’-absehbar‘conceivable’\\nsehen‘to see’-*sehbar‘visible’\\n\\nApplication of a derivational morpheme may be restricted to a certain subclass. For example, application of the English derivational suffix -ity is restricted to stems of Latin origin, while the suffix -ness can apply to a wider range:\\n\\n\\nrare-rarity-?rareness\\nred-*reddity-redness\\ngrave-gravity-graveness\\nweird-*weirdity-weirdness\\n\\nDerivation can be applied recursively, i.e., words that are already the product of derivation can undergo the process again. That way a potentially infinite number of words can be produced. Take, for example, the following chain of derivations:\\n\\n\\nhospital — hospitalize — hospitalization — pseudohospitalization\\n\\nSemantic interpretation of the derived word is often difficult. While a derivational suffix can usually be given a unique semantic meaning many of the derived words may still resist compositional interpretation. This may be due to lexicalization, i.e. a form is no more transparent because, or ambiguity of the underlying base form. For a more detailed discussion see Trost (1993).\\n\\nWhile inflectional and derivational morphology are mediated by the attachment of a bound morph to a base form, compounding is the joining of two or more base forms to form a new word. Most common is just setting two words one after the other, as in state monopoly, bedtime or red wine. In some cases parts are joined by a linking morphem (usually the remnant of case marking) as in bull’s eye or German Liebeslied (love-song).\\n\\nThe last part of a compound usually defines its morphosyntactic properties. Semantic interpretation is even more difficult than with derivation. Almost any semantic relationship may hold between the components of a compound:\\n\\n\\nWienerschnitzel\\t‘cutlet made in Viennese style’\\nSchweineschnitzel\\t‘cutlet made of pork’\\nKinderschnitzel\\t‘cutlet made for children’\\n\\nThe boundary between derivation and compounding is fuzzy. Historically, most derivational suffixes developed from words frequently used in compounding. An obvious example is the —ful suffix as in hopeful, wishful, thankful.\\nPhrases and compounds cannot always be distinguished. The English expression red wine in its written form could be both.\\tIn spoken language the stress pattern differs: red wíne vs. réd wine. In German phrases are morphologically marked, while compounds are not: roter Wein  vs. Rotwein. But for verb compounds the situation is similar to English: zu Hause bleiben  vs. zuhausebleiben.\\n\\n2.3 What constitutes a morph?\\nEvery word form must at the core contain some root form. This root can (must) then be complemented with additional morphs. How are morphs realized? Obviously, a morph must somehow be recognizable in the phonetic or orthographic pattern constituing the word. The most common type of morph is a continuous sequence of phonemes. All roots and affixes are of this form. A complex word can then be analyzed as a series of morphs concatenated together. Agglutinative languages function almost exclusively this way. But there are surprisingly many other possibilities. \\n2.3.1 Affixation\\nAn affix is a bound morph that is realised as a sequence of phonemes (or graphemes). The by far most common types of affixes are prefixes and suffixes. Many languages have only these two types of affixes. Among them is English (at least under standard morphological analyses).\\n\\nA prefix is an affix that is attached in front of a stem. An example is the English negative marker un- attached to adjectives:\\ncommon\\tuncommon\\nA suffix is an affix that is attached after a stem. Take, e.g., the English plural marker —s:\\nshoe\\t\\tshoes\\nAcross languages suffixation is far more frequent than prefixation. Also, certain kinds of morphological information are never expressed via prefixes, e.g., nominal case marking. Many computational systems for morphological analysis and generation assume a model of morphology based on prefixation and suffixation only. \\n\\nA circumfix is the combination of a prefix and a suffix which together express some feature. Both theoretically and from a computational point of view a circumfix can be viewed as really two affixes applied one after the other.\\n\\nIn German,  the circumfixes ge--t and ge--n form the past participle of verbs:\\n\\n\\nsagen\\t\\t‘\\nto say’\\t\\t\\ngesagt\\t\\t‘\\nsaid’\\nlaufen\\t‘\\nto run’\\t\\t\\ngelaufen\\t\\t‘\\nrun’\\n\\nAn infix is an affix where the placement is defined in terms of some phonological condition(s). These might result in the infix appearing within the root to which it is affixed. In Bontoc, a Philippine language, the infix -um- turns adjectives and nouns into verbs (Fromkin and Rodman 1983). The infix attaches after the initial consonant:\\n\\nReduplication is a border case of affixation. The form of the affix is a function of the stem to which it is attached, i.e., it copies (some portion of) the stem. Reduplication may be complete or partial. In the latter case it may be prefixal, infixal or suffixal. Reduplication can include phonological alteration on the copy or the original.\\nIn Javanese complete reduplication is used to express the habitual-repetitive. In case the second vowel is non-/a/, the first vowel in the copy is made nonlow (changing /a/ to /o/ and /E/ to /e/) and the second becomes /a/. When the second vowel is /a/, the copy remains unchanged while in the original the /a/ is changed to /E/ (Kiparsky 1987):\\n\\nPartial reduplication is more common. In Yidiny, an Australian language, prefixal reduplication is used for  plural marking. Reduplication involves copying the ‘minimal word’ (Nash 1980).\\n\\n\\nAn example for infixal reduplication is the frequentative in Amharic, a semitic language spoken in Ethiopia  (Rose 2000). \\n\\nFrom a computational point of view one property of reduplication is especially important: Since reduplication involves copying it cannot–at least in the general case–completely be described with the use of finite-state methods.\\n2.3.2 Root-and-template morphology\\nSemitic languages (at least according to standard analyses) exhibit a very peculiar type of morphology: A so-called root, consisting of two to four consonants, conveys the basic semantic meaning. A vowel pattern marks information about voice and aspect. A derivational template gives the class of the word (traditionally called binyan).\\n\\nIn Arabic verb stems are constructed this way. The root ktb (write) produces--among others--the following stems:\\n\\n\\nTemplateVovel pattern\\na (active)ui (passive)\\nCVCVCkatabkutib‘write’\\nCVCCVCkattabkuttib‘cause to write’\\nCVVCVCka:tabku:tib‘correspond’\\ntVCVVCVCtaka:tabtuku:tib‘write each other’\\nnCVVCVCnka:tabnku:tib‘subscribe’\\nCtVCVCktatabktutib‘write’\\nstVCCVCstaktabstuktib‘dictate’\\n\\n2.3.3 Modification in phonetic substance\\nThis term subsumes processes which do neither introduce new nor remove existing segments. Morphs are not realized as any string of phonemes, but as a change of phonetic properties or an alteration of the prosodic shape.\\n\\nAblaut refers to vowel alternations inherited from Indo-European. It is a pure example of vowel modification as a morphological process. Examples are strong verbs in Germanic languages like English (e.g., swim — swam — swum). In Icelandic this process is still more common and more regular than in most other Germanic languages. The following example is from Sproat (1992, p.62):\\n\\n\\nUmlaut has its origin in a phonological process, whereby root vowels were assimilated to a high-front suffix vowel. When this suffix vowel was lost later on, the change in the root vowel became the sole remaining mark of the morphological feature originally signalled by the suffix.\\nIn German  the plural of nouns may be marked by umlaut (sometimes in combination with a suffix), whereby in the stem vowel the feature back is changed to front:\\n\\nAnother possibility to realize a morpheme is to alter the prosodic shape. Tone modification can be used to signal certain morphological features.\\nIn Ngbaka, spoken in the Democratic Republic of Congo, tense-aspect contrasts are expressed by four different tonal variants (Nida 1949):\\n\\nA morpheme may be realised by a stress shift. English noun-verb derivation sometimes uses a pattern where the stress is shifted from the first to the second syllable:\\n\\n\\nNOUNVERB\\néxportexpórt\\nrécordrecórd\\ncónvictconvíct\\t\\n\\n2.3.4 Suppletion \\nTotal modification is a process occurring sporadically and idiosyncratically within inflectional paradigms. It is usually associated with forms that are used very frequently. Examples in English are went, the past tense of go, and the forms of to be: am, are, is, was and were.\\n2.3.5 Zero Morphology\\nSometimes a morphological operation has no phonological expression whatsoever. Examples are found in many languages.\\nEnglish noun-to-verb derivation is often not explicitly marked: \\nman\\t\\tThe man smiled.\\tMan the boats.\\nhouse  \\tHe buys a house.\\tThey house  in a cave.\\nA possible analysis is to assume a zero morph which attaches to the noun to form a verb: book+ØV. Another possibility is to assume two independent lexical items disregarding any morphological relationship. \\n2.4 The structure of words: Morphotactics\\nSomehow morphs must be put together to form words. A word grammar is determining the way this has to be done. This part of morphology is called morphotactics. As we have seen, the most usual way is simple concatenation. Let´s have a look at the constraints involved. What are the conditions governing the ordering of morphemes in pseudohospitalization?\\n(1)\\t*hospitalationizepseudo, *pseudoizehospitalation\\n(2)\\t*pseudohospitalationize\\nIn (1) an obvious restriction is violated: pseudo- is a prefix and must appear ahead of the stem, -ize and —ation are suffixes and must appear after the stem. The violation in (2) is less obvious. In addition to the pure ordering requirements there are also rules governing to which types of stems an affix may attach:  —ize attaches to nouns and produces verbs, —ation attaches to verbs and produces nouns.\\nOne possibility to describe the word formation process is to assume a functor-argument structure. Affixes are functors that pose restrictions on their (single) argument. That way a binary tree is constructed. Prefixes induce right branching and suffixes left branching.\\n\\nFig. 1: The internal structure of the word pseudohospitalization\\nIn figure 1 the functor pseudo- takes a nominal argument to form a noun, —ize a  nominal argument to form a verb, and —ation a verbal argument to form a noun. This description renders two different possible structures for pseudohospitalization. The one given in figure 1 and a second one where pseudo- combines first directly with hospital. We may or may not accept this ambiguity. To avoid the second reading we could state a lexical constraint that a word with the head pseudo- cannot serve as an argument anymore.\\n2.4.1 Constraints on affixes\\nAffixes is that they attach to specific categories only. This is an example for a syntactic restriction. Restrictions may also be of a phonological, semantic or purely lexical nature. A semantic restriction on the English adjectival prefix un- prevents its attachment to an adjective that already has a negative meaning:\\n\\nunhappy\\t*unsad\\nunhealthy\\t*unill\\nunclean\\t*undirty\\n\\nThe fact that in English some suffixes may only attach to words of Latin origin (cf. 2.2.2) is an example for a lexical restriction.\\n2.4.2 Morphological vs. phonological structure\\nIn some cases there is a mismatch between the phonological and the morphological structure of a word. One example is comparative formation with the suffix —er in English. Roughly, there is a phonological rule that prevents attaching this suffix to words that consist of more than two syllables:\\n\\ngreat\\t\\tgreater\\ntall\\t\\ttaller\\nhappy \\thappier\\ncompetent\\t*competenter\\nelegant\\t*eleganter\\n\\nIf we want to stick to the above rule unrulier has to be explained with a structure where the prefix un- is attached to rulier. But, from a morphological point of view, the adjective ruly does not exist, only the negative form unruly. This implies that the suffix —er is attached to unruly. We end up with an obvious mismatch!\\nAnother potential problem is cliticization. A clitic is a syntactically separate word phonologically realized as an affix. The phenomenon is quite common across languages.\\n\\n\\nIn English auxiliaries have contracted forms that function as affixes:\\n\\nhe shall return -> he’ll return\\n\\nIn German prepositions can combine with the definite article\\n\\nan dem Tisch -> am Tisch\\n\\nin das Haus -> ins Haus\\nIn Italian personal pronouns can be attached to the verb. In this process the ordering of constituents is also altered.\\n\\nce ne facciamo -> facciamocene\\n\\n\\n2.5 The Influence of Phonology\\nMorphotactics is responsible to govern the rules for the combination of morphs into larger entities. One could assume that this is all a system needs to know to break down words into their component morphemes. But there is another aspect that makes things more complicated: Phonological rules may apply and change the shape of morphs. To deal with these changes and their underlying reasons is the area of morphophonology.\\n2.5.1 Phonology vs. orthography\\nMost applications of computational morphology deal with text rather than speech. But, written language is rarely a true phonemic description. For some languages, e.g., Finnish, Spanish or Turkish orthography is a good approximation for a phonetic transcription. English, on the other hand, has very poor correspondence between writing and pronounciation. As a result, we often have to deal with orthography rather than phonology. A good example are English plural rules (cf. 2.4.1).\\n2.5.2 Local phenomena\\nWe have shown that, by and large, words are composed by concatenating morphs. In many cases this concatenation process will induce some phonological change in the vicinity of the morph boundary.\\nAssimilation is a process where the two segments at a morph boundary influence each other, resulting in some feature change that makes them more similar. Take, for example, the English in- prefix where the n changes to m before labials:\\n\\n<in+feasible>->infeasible\\n<in+mature>->immature\\n<in+probable>->improbable\\n<in+secure>\\t->insecure\\n\\nAnother possibility is epenthesis (insertion) or elision (deletion) of a segment under certain (phonological) conditions. Take for example the English plural formation:\\n\\n<cat+s>->cats\\n<door+s>->doors\\n<dish+s>->dishes\\n<bliss+s>->blisses\\n<match+s>->matches\\n<fox+s>->foxes\\n\\nIn this case the rule requires the insertion of an /´/ between /s/, /z/, /S/, or /Z/ and another /s/. On the other hand, in German the suffix —st attached to stems ending in /s/ looses its starting segment /s/:\\n\\n<leb+st>->lebst\\n<sag+st>->sagst\\n<ras+st>->rast\\n<trotz+st>->trotzt\\n<hex+st>->hext\\n\\nWe see that the change is not purely phonologically motivated. The same condition, namely two adjoining /s/ phonemes leads to different results: Either the epenthesis of an /´/ between the two, or the elision of the second /s/. Moreover, the notion of insertion or deletion is purely descriptive. Phonological theory may explain the underlying processes completely different. Nonetheless, this is the view most often taken by work in computational morphology.\\n2.5.3 Long-distance effects\\nMost common is vowel harmony but there are olso examples of consonant harmony. Vowel harmony is a phonological process where the leftmost (in rare cases the rightmost) vowel in a word influences all the following (preceding) vowels. Among the languages exhibiting vowel harmony are Finnish, Hungarian, Turkic and many African languages.\\nLet’s have a look at vowel harmony in Turkish. The nine vowels of the Turkish language can be specified the following way:\\n\\n\\niIüueaöo\\nHIGH++++\\nBACK++++\\nROUND++++\\n\\nTurkish has two different harmony rules, called small and large respectively: \\n\\n\\n\\ne i ö ü -> e\\xa0\\xa0\\xa0\\xa0\\ne i -> i\\nö ü -> ü\\na Io u -> a\\na I -> I\\no u -> u\\n\\n\\nOnly the stem vowel in a word is lexically determined.  All the following vowels are realized in accordance to the harmony rules. For example, the root ev (house) induces either an e or an i in the attached suffixes. Which one of the two is realized is a property of the respective suffix.\\n\\nIn this example the plural suffix follows the „small\" harmony and the genitive suffix the „large\" harmony rule.\\t\\n3 Applications of Computational Morphology\\nComputational morphology has many practical applications. Besides low-level applications, computational morphology contributes to many speech and language processing systems.\\n3.1 Low-level applications\\nHyphenation is almost exclusively done automatically. Although the task seems at first glance extremely simple only a human expert can achieve a 100% success rate. Segmenting words correctly into their morphs helps to solve the task. The major problem are spurious segmentations.\\nSpelling correction is another low-level application. Just comparing input against a list of word forms has a number of drawbacks. Such a list will never contain all the words occurring in a text and enlarging the list has the negative side effect of including more and more obscure words that will match with typos thus preventing their detection. Most systems use a root lexicon, plus a relatively small set of affixes and simple rules to cover morphotactics.\\nStemmers are used in information retrieval to reduce as many related words and word forms as possible to a common canonical form which can then be used in the retrieval process. One should note that this canonical form is not necessarily the base form. The main requirement is–like in all the above tasks–robustness.\\nAnother application is to segment text in Chinese, Japanes or Korean. In these languages words in a sentence are not separated by blanks or punctuation marks. Morphological analysis can be used to perform the task of word separation. \\nA related problem is the inputting of Japanese text. Japanese is written with a combination of two independent character sets. Kanji, the morphemic Chinese characters are used for open-class morphemes (verbs, nouns and adjectives). Kana has (about 50) syllabic characters and is mainly used for closed-class morphemes although in principle all Japanese words can be written exclusively in kana.\\nSince there are several thousand kanji characters, many Japanese text input systems use kana-kanji conversion. The text is typed in kana and the relevant portions are subsequently converted to kanji. The mapping from kana to kanji is quite ambigous. A combination of statistical and morphological methods is applied to solve that task.\\n3.2 Natural language applications\\nAn obvious application area for morphological components are more general natural language processing systems involving parsing and/or generating natural language utterances in written or spoken form. There is a wide range of such applications from message and information extraction to dialog systems and machine translation. For many current applications, only inflectional morphology is considered.\\nIn a parser, morphological analysis of words is an important prerequisite for syntactic analysis. Properties of a word the parser needs to know are its part-of-speech category and the morphosyntactic information encoded in the particular word form. Another important task is lemmatization, i.e., finding the corresponding dictionary form for a given input word, because for many applications a lemma lexicon is used to provide more detailed syntactic (e.g, valency) and semantic information for a deep analysis.\\nIn generation, on the other hand, the task is to produce the correct word form from the base form plus the relevant set of morphosyntactic features.\\n3.3 Speech applications\\nA text-to-speech system takes (electronically stored) text as input and produces speech from it. Morphological analysis helps to solve two different tasks in such systems. One is to guide the grapheme-to-phoneme conversion. Characters are often ambiguous with respect to their translation into phonemes. Finding out the underlying morphological structure is necessary for solving the task correctly. The sequence th, is usually pronounced as /D/ or /T/ in English. In the word hothouse we need to know the morph structure <hot+house> to correctly pronounce the th sequence as /th/.\\nA less obvious application is the use of morphological analysis to help in determining the part-of-speech category of words. This is an important prerequisite of syntactic analysis which is the basis for coming up with a correct prosody.\\nSpeech recognition is a field where morphological analysis will become ever more important. At the moment most available systems make use of full form lexicons and perform their analysis on a word basis. Increasing demands on the lexicon size on the one hand and the need to limit the necessary training time on the other hand will make morph-based recognition systems more attractive.\\n4 Computational Morphology\\nThe most basic task in computational morphology is to take a string of characters or phonemes as input and deliver an analysis as output. The input could, for example be the English word form in (1). One possible output could be the string of underlying morphemes as in (2), another one a morphosyntactic interpretation as in (3).\\n\\nincompatibilities\\nin+con+patible+ity+s\\nincompatibility+NounPlural\\nLet’s start with the task of mapping (1) to (3). The easiest way to achieve a result is to have a long list of pairs where the left side represents some word form and the right side its interpretation. This is basically the notion of full form lexicon. Its advantages are simplicity and applicability to all possible phenomena. The main disadvantages are redundancy and inability to cope with forms not contained in the lexicon.\\nLess redundant are so-called lemma lexica. A lemma is a canonical form taken as the representative for all the different forms of a paradigm. Usually, the base form is selected as this canonical form. An interpretation algorithm relates every form to its lemma plus delivering a morphosyntactic interpretation. As a default, forms are expected to be string concatenations of base form (= lemma) and affixes. Affixes must be stored in a separate repository together with the relevant morphotactic information about how they may combine with other forms. Interpretation then simply means finding a sequence of affixes and a base form that conforms to morphotactics. For different reasons a given word form may not conform to this simple picture:\\n\\nWith very frequently used words we often find suppletion, e.g., to go has the completely unrelated form went.\\n\\nOne clearly needs some exception handling mechanism to cope with suppletion. A possible solution is to have secondary entries where you store suppleted forms together with their morphosyntactic information. These secondary forms are then linked to the corresponding primary form, i.e., the lemma.\\n\\nMorphs are realised in a non-concatenative way, e.g., tense of strong verbs in English: give — gave - given, find - found — found\\n\\nIn languages like English, where these phenomena affect only a fairly small and closed set of words these forms can be treated like suppletion. Alternatively, some exception handling mechanism (usually developed ad-hoc and language-specific) is applied.\\n\\nDue to phonological rules a word form may exhibit some change in shape, e.g., in English suffixes starting with s (plural of nouns, 3rd person marker, superlative marker) may not directly follow stems ending in a sybillant (e.g., dish — dishes)\\n\\nIf morphophonological processes in a language are few and local the lemma lexicon approach can still be successful. In our example it suffices to assume two plural  endings: -s and —es. For all base forms it must be specified whether the former or the latter of the two endings may be attached.\\nApart from the obvious limitations with regard to the treatment of morphophonological rules on a more general scale the approach has some other inherent restrictions.\\n\\n\\nThe algorithm is geared towards analysis. For generation purposes, one needs a completely different algorithm and data.\\nInterpretation algorithms are language-specific because they encode both the basic concatenation algorithm and the specific exception-handling mechanism.\\nThe approach was developed for morphosyntactic analysis. An extension to handle more generally the segmenting of word forms into morphs is difficult to achieve.\\n\\n4.1 Finite-state Morphology\\nBecause most morphological phenomena can be described with regular expressions the use of finite-state techniques for morphological components is common. In particular, when morphotactics is seen as a simple concatenation of morphs it can straightforwardly be described by a finite automata.\\nIt was not so obvious though how to describe non-concatenative phenomena like vowel harmony, root-and-template morphology or infixation in such a framework.\\n4.1.1 Two-level morphology\\nIn this section we describe a system where morphophonology is taken care of by a separate mechanism that is well integrated with the morphotactical component. It has the further advantages of being non-directional (applicable to analysis and generation) and language-independent (because of its purely declarative specification of language-specific data).\\nRules for the description of morphophonological phenomena are standard in generative phonology.  There, the derivation of a word form from its lexical structure is performed by the successive application of phonological rules creating a multi-step process involving several intermediate levels of representation. Such an approach may be suited for generation but leads to problems if applied to analysis. Since the ordering of rule application influences the result it is difficult to reverse the process.\\nSeveral proposals were made on how to restrict rules and their application to overcome these problems. Two-level morphology is a an attempt to overcome these problems. Originally proposed by Kimmo Koskenniemi (1984) it has since been implemented in a number of different systems and applied to a wide range of natural languages.\\n4.1.1.1 Two-level rules \\nAs the name suggests two levels--called lexical level and surface level--suffice to describe the phonology (or orthography) of a natural language. On the surface level words appear just as they are pronounced (or written) in ordinary language, with the important exception of the null character which will be described later on.  On the lexical level, the alphabet includes special symbols--so-called diacritics--which are mainly used to represent features that are no phonemes (or graphemes) but nevertheless constitute necessary phonological information.  The diacritics \\'+\\' and \\'#‘ are used to indicate morph and word boundary respectively.\\nThe two levels are linked by a set of pairs of lexical and surface characters constituting possible mappings between lexical and surface characters.  Pairs are written as lexical character - colon - surface character (e.g. a:a or +:0).  To any of these pairs rules may be attached to restrict their applicability.  Pairs with no attached rules are applied by default.  Rules serve to licence the application of a pair in a certain phonological context. They are viewed as constraints on the mapping between the surface and the lexical form of morphs.  Accordingly, they are applied in parallel and not one after the other like in generative phonology. Since no ordering of the rules is involved this is a completely declarative way of description.\\nA rule consists of the following parts:\\n\\n\\nA substitution that indicates the affected character pair. \\nleft and right context define the phonological conditions for the substitution. \\nOne of four available operators defines the status of the rule:  The context restriction operator <= makes the substitution of the lexical character obligatory in the context defined by that rule (other phonological contexts are not affected). The surface coercion operator => restricts the substitution of the lexical character to exactly this context (it may not occur anywhere else).  The <=> is a combination of the former two, i.e., the substitution must take place in exactly this context and nowhere else. The fourth operator /<= states prohibitions, i.e., the substitution may not take place in this context.\\n\\nLet\\'s look at a simple epenthesis rule:\\n (1a)\\t+:e <= s x z [ { s c } h ] :  _ s ;\\nIt specifies that a lexical morph boundary (indicated by  \\'+\\') between s, x, z, sh, or ch on the left side and an s on the right side must correspond to surface level e. By convention a pair with identical lexical and surface character may be denoted by just a single character. Curly brackets indicate a set of alternatives, square brackets a sequence.\\nRule (1a) makes no statements about other contexts where \\'+\\' may map to an \\'e\\'.  The rule covers some of the cases where an \\'e\\' is inserted between stem and an inflectional morph starting with \\'s\\' (plural morpheme, 3rd person marker, superlative) in English. By default a morph boundary will map to the null character, but in the given specific context it maps to \\'e\\'. The following example shall demonstrate the application of this rule (Vertical bars denote a default pairing, numbers the application of the corresponding rule): \\n\\n\\n\\n#bliss+s#\\t\\t#fox+s#\\t#dish+s#\\t#watch+s#\\n||||||1||\\t\\t||||1||\\t|||||1||\\t||||||1||\\t\\n0blisses0\\t\\t0foxes0\\t0dishes0\\t0watches0\\nObviously, (1a) does not capture all the cases where epenthesis of \\'e\\' occurs. For example, the forms spies, shelves or potatoes are not covered.  A more complete rule is:\\n\\n(1b)\\t+:e <=> {s x z [ { s c} h:h ] :v [ C y: ] [ C o ] } _ s ;\\n\\nFormally, rule (1b) defines exactly all the contexts where \\'+\\' maps to an \\'e\\' (because of the use of the ¤ operator). It also makes use of some additional writing conventions. A colon followed by a character denotes the set of all pairs with that surface character. Accordingly, a character followed by a colon means the set of all pairs with that lexical character.  Sets of characters can be globally defined and given names.  The C stands for the set of English consonants (i.e., b:b, c:c, d:d,...).  To cope with the spies example we need another rule which licences the mapping from \\'y\\' to \\'i\\'.\\n\\n\\n\\n(2)y:i <=>  C _  { +:e  [ +:  e ]  } ;\\nV C+ _  +:  C ;\\n\\n\\nRule (2)  specifies two distinct contexts. If either of them is satisfied the substitution must occur, i.e., contexts are OR-connected. The \\'+\\' operator in the second context indicates at least one occurrence of the preceding sign (accordingly, the operator \\'*\\' has the reading arbitrarily many occurrences).  V stands for the set of vowels. Rules (1) and (2) in combination now correctly map spies with spy+s. Jointly with rule (3) for the mapping from \\'f\\' to \\'v\\' (1) takes also care of forms like shelves and  potatoes: \\n\\n(3) f:v <=  { e l } _ +:  s ;\\nV _ e +: s;\\n\\nLet’s see how the three rules interact to produce the expected results:\\n\\n\\n\\n#spy+s#\\t#toy+s#\\t#shelf+s#\\t#wife+s#\\t#potato+s#\\n|||21||\\t|||||||\\t|||||31||\\t|||3||||\\t|||||||1||\\n0spies0\\t0toy0s0\\t0shelves0\\t0wive0s0\\t0potatoes0\\nA given pair of lexical and surface strings can only map if they are of equal length.  There is no possibility of omitting or inserting a character in one of the levels.  On the other hand, elision and epenthesis are common phonological phenomena.  To cope with these, the null character (written as 0) is included in both the surface and the lexical alphabet.  The null character is taken to be contained in the surface string for the purpose of mapping lexical to surface string and vice versa but it does not show up in the output or input of the system. Diacritics are mapped to the null character by default.  Any other mapping of a diacritic has to be licensed by a rule.\\nAssumption of the explicit null character is essential for processing. A mapping between a lexical and a surface string presupposes that for every position a character pair exists.  This implies that both strings are of equal length (nulls are considered as characters in this respect). Rules can either be directly interpreted or compiled into finite state transducers. The use of finite state machinery allows for very efficient implementation. For a more in-depth discussion of implementational aspects consult chapter 37 and Beesley and Karttunen (2000).\\nOne subtle difference between direct rule interpretation and transducers occurs in the repeated application of the same rule to one string.  The transducer implicitly extends the phonological context to the whole string.  It must therefore explicitly take care of overlapping right and left contexts (e.g., in (1) the pair s:s constitutes both a left and right context).  With direct interpretation a new instance of the rule is activated every time the left context is found in the string and overlapping must not be treated explicitly.\\n4.1.1.2 The continuation lexicon\\nUp to now we have only described the rule part of two-level morphology which is responsible for taking care of morphonological phenomena.  It is complemented by a partitioned lexicon of morphs (or words) that takes care of word formation by affixation.  The lexicon consists of (non-disjunctive) sublexica, so-called continuation classes. For every morph, a set of legal continuation classes is specified.  This set defines which sublexicon must be searched for continuations.  The class of morphs which can start a word is stored in the so-called \"init lexicon\".\\nThe whole process is equivalent to stepping through a finite automaton.  A successful match can be taken as a move from some state x of the automaton to some other state y. Lexical entries can be thought of as arcs of the automaton: a sublexicon is a collection of arcs having a common from state.\\nThe lexicon in two-level morphology is used for two purposes: one is to describe which combinations of morphs are legal words of the language, the other one is to act as a filter whenever a surface word form shall be mapped to a lexical form.  Its use for the second task is crucial because otherwise there would be no way to limit the insertion of the null character.\\nTo enable fast access, lexicons are organized in the form of a letter trie (Fredkin, 1960).  Such a structure is well suited for an incremental (letter-by-letter) search because at every point in the trie exactly those continuations leading to legal morphs are available.  With every node which represents a legal morph its continuation classes are stored.  In recognition we can now make use of that structure.  Search starts at the root of the trie.  Each character which is proposed must be matched against the lexicon.  Only if that character is a legal continuation at that node in the trie it may be considered as a possible mapping.\\nIn recent implementations the lexicon and the two-level rules are collapsed into a single, large transducer, resulting in a very compact and efficient system\\n4.1.2 Related Formalisms\\nBlack et al. (1987) note the inelegance of Koskenniemi\\'s formalism when describing a phonological (or orthographic) change affecting sequences of characters. They propose a rule format consisting of a surface string (called LHS for left hand side), an operator (‹ or Þ) and a lexical string (called RHS for right hand side).  LHS and RHS must be of equal length.  Surface-to-lexical rules (Þ) request that there exists a partition of the surface string where each part is the LHS of a rule and the lexical string the concatenation of the corresponding RHSs.  Lexical-to-surface rules (‹) request that any substring of a lexical string which equals a RHS of a rule must correspond to the surface string of the LHS of the same rule. The rules in (4) are equivalent to rule (1a).\\n\\n\\n(4)ses => s+sses <= s+sshes => sh+sshes <= sh+sxes => x+s xes <= x+s\\nzes => z+szes <= z+sches => ch+sches <= ch+s\\n\\nThese rules collapse context and substitution into one undistinguishable unit.  Instead of regular expressions only strings are allowed. One drawback is that surface-to-lexical rules may not overlap.  If two different changes happen to occur close to each other they must be captured in a single rule.  Also, long-distance phenomena like vowel harmony cannot be described in this scheme. Ruessink (1989) removes this problem by introducing contexts again.  Both LHS and RHS may come with a left and right context. LHS and RHS may also be of different length, doing away with the null character. Though he gives no account of the complexity of his algorithm one can suspect that it is in general less constrained than the Koskenniemi system. \\nAn inherently difficult problem for two-level morphology is the root-and-template morphology of Semitic languages. One solution is the introduction of multi-tape formalisms as first described in the seminal paper by Kay (1987). The best-documented current system is SEMHE described in Kiraz (1996, 1997). SEMHE is based on Ruessink’s formalism with the extension of using three input tapes: one each for the root, the vowel pattern and the template.\\nAnother extension to the formalism is realized in X2MorF  (Trost 1992). In the standard system, morphologically motivated phenomena like umlaut must be described by introducing some pseudosegmental material in the lexical level (see, e.g., 2.4.3.3). In X2MorF an additional morphological context is available to describe such phenomena more naturally.\\n4.2 Alternative formalisms\\nAlternative proposals for morphological systems have been made in computational linguistics. They include so-called paradigmatic morphology described in Calder (1989) and the DATR system (Evans and Gazdar 1996).  Common to both is the idea to introduce some default mechanism which makes it possible to define a hierarchically structured lexicon where general information is stored at a very high level.  Lower in the hierarchy this information can be overwritten.  Both systems seem to be more concerned with morphosyntax than with morphonology.  It is an open question if these approaches could somehow be combined with two-level rules.  \\n4.3 Examples\\n4.3.1 Vowel harmony in Finnish\\n\\nFinnish has eight vowels. They are classified into back+ (a, o, u), back- (ä, ö, y) and neutral (e, i). In a Finnish word vowels must be either all back+ or all back- (disregarding neutral vowels).\\n\\n\\nV = {a, o, u, ä, ö, y, e, i}\\nVb = {a, o, u}\\tVf = {ä, ö, y}\\n[1] {A:a|O:o|U:u} Þ =:Vb =:(-Vf)* _;\\n[2]\\t{A:ä|O:ö|U:y} Þ {#|=:Vf} =:(-Vb)* _;\\n #taivas+tA#\\t\\t#puhelin+tA#\\t\\t#syy+tA#\\n|||||||||1|\\t\\t||||||||||1|\\t\\t||||||2|\\n0taivas0ta0\\t\\t0puhelin0ta0\\t\\t0syy0tä0\\n\\n\\n4.3.2 Final devoicing in (spoken) German\\nFinal devoicing is a morphophonological process where a voiced consonant is devoiced when it occurs in final position in the syllable. Take for example the root /ra\\x98:d/ (wheel). The singular form is realized as /ra:t/, while in the plural form /re\\x98:då/ the consonant stays voiced. This phenomenon is not reflected in the orthography where always the voiced consonant is kept.\\n\\n\\n[1] Cx:Cy ¤ _ #:0 ; \\n\\n\\t\\t        where Cx in (b d g) \\n\\t\\t\\t         Cy in (p t k)  matched;\\n\\n#lo\\x98b#\\t\\t#ra\\x98d#\\t\\t#we:g#\\t\\t#we:g+e#\\n||| 1|\\t\\t||| 1|\\t\\t||| 1|\\t\\t||| ||||\\n0lo\\x98p0\\t\\t0ra\\x98t0\\t\\t0we:k0\\t\\t0we:g0e0\\t\\n\\nThe two-level rule realises b, d and g as their voiceless counterparts p, t, and k respectively whenever directly followed by a boundary. \\nWhile the original linguistic motivation behind two-level morphology was SPE and two-level rules were designed to describe morphophonology the mechanism can deal with a much wider range of phenomena.\\n4.3.3 Umlaut in German\\nGerman umlaut is used to mark--among other morphosyntactic features–plural. \\n\\n\\nV = {a, ä, e, i, o, ö, u, ü, A:a, A:ä, O:o, O:ö, U:u, U:ü}\\n\\n[1] {A:ä|O:ö|U:ü} Þ _ ?* $:0;\\n\\n\\nAll stem vowels eligible for umlaut are realized by a vowel underspecified for the back/front distinction at the lexical level. A pseudo-ending $ is used to trigger the rule application, thus realizing the umlaut. In all other cases the default pairing is used. This way a morphological property is described as a morphophonological process. The ?* signifies zero or more occurrences of anything.\\n\\n\\nMutter Æ Mütter\\t\\tGarten Æ Gärten\\t\\tHof Æ Höfe\\n\\n#mUtter+$#\\t\\t#gArten+$#\\t\\t\\t#hOf+$e#\\n||1|||||||\\t\\t||1|||||||\\t\\t\\t||1|||||\\n0mütter000\\t\\t0gärten000\\t\\t\\t0höf00e0\\n\\n\\n4.3.4 Reduplication and Infixation in Tagalog\\n\\nIn this (simplified) example from Tagalog we shall see how two-level rules can be used to describe reduplication and infixation.\\n\\n\\nV = {a, i, u, E}\\nC = {p t k b d g m n N s l r w y R}\\n\\n\\nThe rule for infix  insertion. On the lexical level, the prefix X is assumed. While the X is not realized on the surface it triggers the insertion of —In- between initial consonant and following vowel.\\n\\n[1] X:0 Þ _ +:0 C 0:i 0:n V:V;\\n\\npili Æ pinili \\t\\ttahi Æ tinahi\\t\\t \\n\\n#X+p00ili#\\t\\t#X+t00ahi#\\t\\n|1||||||||\\t\\t|1||||||||\\t\\n000pinili0\\t\\t000tinahi0\\t\\n\\nThe rules for reduplication of the first (open) syllable. The R copies the initial consonant, the E the following consonant. The rule also takes care of the case where the infix is inserted as well:\\n\\n\\n\\n[2] R:Cx Þ _ (0:i 0:n) E:V +:0 :Cx;\\n\\t\\twhere Cx in (p p:m t t:n k k:N);\\n[3] E:Vx Þ R:C (0:i 0:n) _ +:0 C Vx;\\n\\t\\twhere Vx in (a i u);\\npili Æ pipili \\t\\ttahi Æ tatahi\\t\\t\\n\\n#RE+pili#\\t\\t#RE+tahi#\\t\\t\\n|23||||||\\t\\t|23||||||\\t\\t\\n0pi0pili0\\t\\t0ta0tahi0\\t\\n\\n pili Æ pinipili\\t\\ttahi Æ tinatahi\\n\\n\\n#X+R00E+pili#\\t\\t#X+R00E+tahi#\\n|1|2||3||||||\\t\\t|1|2||3||||||\\n000pini0pili0\\t\\t000tina0tahi0\\n\\n\\n5 Further reading and relevant resources\\nThe most comprehensive book about computational morphology is Richard Sproat’s  book Morphology and Computation (Sproat 1992). It gives a concise introduction into morphology with examples from various languages and a good overview of applications of computational linguistics.  On the methodological side it concentrates on finite-state morphology omitting other paradigms. Computational Morphology (Black et al. 1992) gives a more in-depth description of finite-state morphology but concentrates exclusively on English. An excellent overview of morphology with examples from diverse languages is found in the Handbook of Morphology (Spencer and Zwicky 1998). \\nTo get some hands-on experience with morpological processing connect to \\nRXRC Europe \\nand Lingsoft. A free downloadable \\nversion of a two-level morphology is available from \\nSIL.\\n\\nReferences\\n\\nBeesley K.R. and Karttunen L. 2000. Finite-State Morphology: Xerox Tools and Techniques. Cambridge University Press, Cambridge.\\nBlack A.W., Ritchie G.D., Pulman S.G., Russell G.J. 1987. Formalisms for Morphographemic  Description,  Proc. 3rd European ACL, pp11-18, Kopenhagen.\\nCalder J. 1989. Paradigmatic Morphology, Proc. 4th European ACL, pp58-65, Manchester.\\nChomsky N. and Halle M.: The Sound Pattern of English, Harper & Row, Hagerstown/London/New York, 1968.\\nEvans R., Gazdar G. 1996 DATR,: A Language for Lexical Knowledge Representation, Computational Linguistics 22(2)167-216.\\nFredkin E. 1960. Trie Memory, Communications ACM 3, pp490-499.\\nFromkin V., Rodman R. 1983. An Introduction to Language. Holt, Rinehart & Winston, New York.\\nKay M. 1987. Noncatenative finite-state morphology, in Proc. of the 3rd Conference of the European Chapter of the ACL, Copenhagen, Denmark, pp2-10.\\nKiparsky P. 1987. The Phonology of Reduplication. Manuscript. Stanford University.\\nKiraz G.A. 1996. SEMHE: A Generalized Two-Level System, in Proc. of 34th Annual Meeting of the Association for Computational Linguistics, Morgan Kaufmann, Los Altos, pp159-166. \\nKiraz G.A. 1997. Compiling Regular Formalisms with Rule Features into Finite-State Automata, in Cohen P.R., Wahlster W.(eds.), Proc. 35th Annual Meeting of the Association for Computational Linguistics, Morgan Kaufmann, Los Altos, pp329-336. \\nKoskenniemi  K. 1984.  A  General  Computational  Model  for   Word-Form Recognition   and   Production,   Proceedings 10th International Conference on Computational Linguistics,  Stanford, CA.\\nNash D. 1980. Topics in Warlpiri Grammar, PhD Thesis, MIT, Cambridge, MA.\\nNida E. 1949. Morphology: The Descriptive Analysis of Words. University of Michigan Press.\\nRitchie G.D., Russel G.J., Black A.W., Pulman S.G. 1991. Computational Morphology, MIT Press, Cambridge.\\nRose S. 2000. Triple Take: Tigre and the case of internal reduplication. Studies in Afroasiatic Grammar.\\nRuessink H. 1989. Two-Level Formalisms, Working Papers in Natural Language Processing 5, Rijksuniversiteit Utrecht.\\nSpencer A., Zwicky A. (eds.) 1998. The Handbook of Morphology, Basil Blackwell, Oxford.\\nSproat R.W., 1992. Morphology and Computation, MIT Press, Cambridge, MA.\\nTrost H. 1992. X2MORPH: A Morphological Component Based on Augmented Two-Level Morphology, in Proc. 12th Intenational Joint Conference on Artificial Intelligence, Sydney, Morgan Kaufmann, San Mateo, pp.1024-1030.\\nTrost H. 1993. Coping With Derivation in a Morphological Component, in Proc. 6th Conference of the European Chapter of the Association for Computational Linguistics, Utrecht, pp368-376.\\n\\n\\n\\n',\n",
       " '\\n\\n\\n\\nX-Ray Trans. Energies Database - Experi. Transition Energies\\n\\n\\n\\n\\n\\nExperimental Transition Energies\\n\\nThe experimental portion of this data set was generated to provide the user \\nwith a set of up-to-date accurate experimental x-ray energies. The reported \\nvalues have been corrected for all known errors and are determined using the \\nmost recent Recommended Values for the Fundamental Physical Constants: 1998 \\n[115] and appropriate conversion factors. The \\nvalues are consistent with the International System of measurement.\\n\\n\\nThe experimental energies included in this database were obtained using the \\nfollowing procedures.\\n\\nDirectly measured transitions\\n\\nFirst, a rather sparse network of x-ray transitions that are directly linked to \\nthe International System of measurement was generated. All of these x-ray \\ntransitions were measured using diffraction spectroscopy and crystals of known \\nlattice spacing. The crystal lattice spacings were determined using combined \\nx-ray and optical interferometry. These transitions are commonly referred to as \\nthe optically based data.\\n\\n\\nTransitions linked to the directly measured transitions\\n\\nIn several cases it has been possible to expand the directly measured \\ntransitions by considering earlier work in which several elemental spectra were \\nobtained using a common instrument platform and protocol, and at least one \\nmember of the collection of lines reported belongs to our directly measured set.\\nIn such cases it is relatively easy to re-scale the data reported so that it is \\nconsistent with the directly measured transitions.\\n\\nTransitions linked to directly measured gamma-ray lines\\n\\nSome x-ray transitions (primarily in the high-Z region) have been \\nmeasured relative to gamma-ray transitions that have been determined using \\ncalibrated crystals and diffraction spectroscopy (procedures identical to those \\nused to generate the optically based x-ray data). X-ray transitions obtained in \\nthis fashion are also consistent with the directly measured transitions.\\n\\nRe-scaling of existing earlier databases\\n\\nIn order to generate a more complete all-Z data set, we have used the \\nBearden database for transitions not included in the above three categories. \\nBecause the Bearden database [1] uses the Å* unit \\n(based on the WK1\\ntransition), it is relatively easy to re-scale these data so that they are \\nconsistent with the optically based data.\\n\\nReference citations\\n\\nThe numbers in the reference column with the subscript \"d\" refer to \\nthe experimental transition energies.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " ' Browse and Search the Library Home : Math Topics : Geometry : Algebraic Geom. Library Home || Search || Full Table of Contents || Suggest a Link || Library Help Selected Sites (see also All Sites in this category ) Algebraic Geometry - Dave Rusin; The Mathematical Atlas A short article designed to provide an introduction to algebraic geometry, which combines the algebraic with the geometric for the benefit of both. Thus the recent proof of \"Fermat\\'s Last Theorem\" - ostensibly a statement in number theory - was proved with geometric tools. Conversely, the geometry of sets defined by equations is studied using quite sophisticated algebraic machinery. This is an enticing area but the important topics are quite deep. This area includes elliptic curves. Applications and related fields and subfields; textbooks, reference works, and tutorials; software and tables; other web sites with this focus. more>> The Algebraic Geometry Notebooks - Aksel Sogstad For non-experts. Contents: What is algebraic geometry?; What can algebraic geometry be used for?; Classification of curves in the Weierstrass familiy; Bezout\\'s Theorem; Varieties, orbits and orbit-spaces; Automatic theorem proving using Singular. more>> All Sites - 97 items found, showing 1 to 50 A 27-Vertex Graph That Is Vertex-Transitive and Edge-Transitive But Not 1-Transitive - Peter Doyle Hypertext and Postscript versions of a paper describing a 27-vertex graph that is vertex-transitive and edge-transitive but not 1-transitive. While all vertices and edges of the graph are similar, there are no edge-reversing automorphisms. ...more>> Advanced Geometry - Math Forum Links to some of the best Internet resources for advanced geometry: Web sites, software, Internet projects, publications, and public forums for discussion. ...more>> AG Algebraic Geometry (Front for the Mathematics ArXiv) - Univ. of California, Davis Algebraic Geometry preprints, from the U.C. Davis front end for the xxx.lanl.gov e-Print archive, a major site for mathematics preprints that has incorporated many formerly independent specialist archives. Search by keyword or browse by topic. ...more>> AGATHOS - Algebraic Geometry: A Total Hypertext Online System - Kevin R. Coombes; Department of Biostatistics, University of Texas M.D.Anderson Cancer Center This course assumes that the reader has completed a year-long graduate course in abstract algebra. It can be approached as a linear text, a list of interesting examples, a list of important theorems in algebraic geometry, key concepts, or exercises that ...more>> Algebraic Curves - Silvio Levy; The Geometry Center Curves that can be given in implicit form as f(x,y)=0, where f is a polynomial, are called algebraic. This site provides an overview, with illustrations. ...more>> Algebraic Geometry - Peter Stiller; Dept. of Mathematics & Computer Science, Texas A&M Univ. This page answers the question, \"What is Algebraic Geometry?\" - a subject with historical roots in analytic geometry, concerned with the geometry of the solutions of a system of polynomial equations. With a link to a bibliography of related publications. ...more>> Algebraic Geometry Seminar - Michael Harrison; Univ. of Washington Online notes for a seminar, including handouts in dvi and postscript formats, solutions to the problems in Chapter 1 of Hartshorne\\'s Algebraic Geometry, and notes on algebraic geometry by Professor J. S. Milne, Univ. of Michigan. ...more>> Algebraic Geometry - Tayler Jarvis A page of links to general algebraic geometry sites, sites on algebraic curves, algebraic surfaces, algebraic geometers and the like, and related sites. ...more>> Algebraic Number Theory and Elliptic Curves - Ghitza, Osserman; Massachusetts Institute of Technology A semester-long seminar giving a rapid introduction to algebraic number theory and elliptic curves. Topics: Dedekind domains, rings of integers, scheme-theoretic curves, finite morphisms thereof, splitting and ramification, the Tchebotarov density theorem ...more>> Algebraic Number Theory Archives - Boston, Grayson Preprints about algebraic number theory and arithmetic geometry are accepted in electronic form for storage until publication. There are instructions for authors who wish to submit preprints to the archives and for for joining the mailing list (members ...more>> Algorithms for Modular Elliptic Curves - John Cremona View parts of the second edition of the book, with introduction, tables and software. Download the errata list for the second edition, available as a dvi file or a postscript file. ...more>> Analytic and Descriptive Geometry - Dave Rusin; The Mathematical Atlas An introduction to ordinary analytic geometry as studied in secondary school. (Also included here is a rather lengthy analysis of a result known as Poncelet\\'s Porism.) History; applications and related fields and subfields; textbooks, reference works, ...more>> Arbeitsgruppe Bachem/Schrader - University of Köln A working group of the Center for Parallel Computing (ZPR) at Köln. Site includes a list of members and contact address; informative write-ups on its projects, some in English. Topics include: Basic Research (meaning \"pure\" as opposed to \"applied\"); ...more>> Arithmetic Algebraic Geometry (AAG) - Training and Mobility of Researchers; European Community A partnership of 12 working groups from 5 European countries (England, France, Germany, Italy, Spain) working to develop methods taken from geometry to study the arithmetical properties of algebraic equations. Main research themes: p-adic cohomology theory; ...more>> arXiv.org e-Print archive - Los Alamos National Laboratory (LANL) A major site for mathematics preprints that has incorporated many formerly independent specialist archives including alg-geom, funct-an, dg-ga, q-alg, auto-fms, cd-hg, MAGNUS, Several Complex Variables, Logic E-prints, Commutative Algebra, Dynamical Systems, ...more>> Automated Deduction - William McCune Research on applications of automated deduction to problems in abstract algebra and algebraic geometry, algorithms and strategies for searching for proofs and for counterexamples, high-performance implementation of automated deduction algorithms, and ...more>> Automorphic Forms and Representations Virtual Study Group - Michael Harrison A virtual study group formed to explore the Langland\\'s Program by reading Automorphic Form and Representations by Daniel Bump. ...more>> Basic Geometry of Voting - Donald G. Saari A book that offers a large number of new results about voting theory, its emphasis purposely placed on three candidate settings so that the book can be read by almost everyone. (With four or more candidates, the results need more advanced mathematical ...more>> Bibliography for Automorphic and Modular Forms, L-Functions, Representations, and Number Theory - Paul Garrett A 1996 bibliography compiled for the author\\'s Ph.D. students at the University of Minnesota. ...more>> The Birch and Swinnerton-Dyer Conjecture - Clay Mathematics Institute A Clay Mathematics Institute Prize problem, with a description in pdf format by Andrew Wiles. The conjecture addresses the problem of enumerating rational points, such as on elliptic curves. ...more>> Books: Professional & Technical: Professional Science: Mathematics - Amazon.com Browse bestselling math books from the Professional and Technical Bookstore at Amazon.com, in such categories as Applied; Chaos & Systems; Geometry & Topology; Mathematical Analysis; Mathematical Physics; Number Systems; Pure Mathematics; Transformations; ...more>> Calabi-Yau Homepage - Sheldon Katz A page of information about Calabi-Yau manifolds: threefolds, fourfolds, software. ...more>> Categorical Geometry - Zhaohua Luo See a brief tour of categorical geometry, which studies the geometric properties of unitary categories. The categorical approach to algebraic geometry was initiated by Yves Diers in his pioneer book Categories of Commutative Algebras (Oxford University ...more>> Counting Points on Elliptic Curves - Fouquet, Harley, Gaudry, Morain The authors, Ph.D. students at the Laboratoire d\\'informatique de l\\'École polytechnique, Paris, have established new records for point counting in characteristic 2, using a new algorithm due to Takakazu Satoh. ...more>> Course Notes - J. S. Milne Full course notes in dvi, pdf, and postscript formats for all the advanced courses J. S. Milne taught at the University of Michigan between 1986 and 1999: Group Theory; Fields and Galois Theory; Algebraic Number Theory; Class Field Theory; Modular Functions ...more>> David Pollack Download the author\\'s thesis, \"Explicit Hecke Actions on Modular Forms\" (Harvard, 1998) in DVI format. ...more>> Diophantine geometry in characteristic p: a survey - José Felipe Voloch An article that appeared in Arithmetic Geometry, F. Catanese, ed., Symposia Mathematica XXXVII, Cambridge Univ. Press, 1997, pp. 260-278. Introduction; Curves; Abelian varieties and their subvarieties; Diophantine Approximation in characteristic p; Omitted ...more>> Discrete and Computational Geometry - Springer-Verlag An international journal of mathematics and computer science that accepts research articles of high quality in discrete geometry and on the design and analysis of geometric algorithms; more specifically, DCG publishes papers on such topics as configurations ...more>> EAGER (European Algebraic Geometry Research Training Network) - Wolfram Decker The members of EAGER are algebraic geometers in mathematical centres spreading among most European countries. These centres are grouped into twelve geographical nodes which are responsible for the management of joint research projects and for the training ...more>> The ECMNET Project - Charron, Daminelli, Granlund, Leyland, Zimmermann The goal of the ECMNET Project is to find large factors using the elliptic curve method, mainly Cunningham numbers. ...more>> Elliptic Curve Discrete Logarithms (ECDL) Project - Robert Harley; 4K Associates The Project has solved ECC2K-108. ...more>> Elliptic Curves and Cryptography - Andreas Enge Includes the test of the author\\'s introductory book on elliptic curves and their applications in cryptography. Also links to further publications; the personal pages of number theorists and cryptographers; and companies, organisations and miscellaneous ...more>> Elliptic Curves and Cryptology - Marc Joye Links to elliptic curve resources: people (an extensive list of personal pages), bibliography, software, and links to relevant Web pages. Many preprints are available from the site. ...more>> Elliptic Curves and Elliptic Functions - Charles Daney From The Mathematics of Fermat\\'s Last Theorem. With a glossary; contents include: What is an elliptic curve?; The group structure of an elliptic curve; Arithmetic on elliptic curves; Further basic concepts and results. ...more>> Elliptic Curves and Formal Groups - Lubin, Serre, Tate Lecture notes from a seminar, part of the Lecture notes prepared in connection with the seminars held at the Summer Institute on Algebraic Geometry, Whitney Estate, Woods Hole, Massachusetts, July 6-July 31, 1964. ...more>> Elliptic Curves and Right Triangles - Karl Rubin; Stanford University Slides (GIF) of lectures. ...more>> Elliptic Curves - Dave Rusin; The Mathematical Atlas An area of algebraic geometry that deals with nonsingular curves of genus 1 - in English, solutions to equations y^2 = x^3 + A x + B. It has important connections to number theory and in particular to factorization of ordinary integers (and thus to cryptography). ...more>> Elliptic Curves II - Johan P. Hansen; Dept. of Mathematics, Ny Munkegade, Aarhus Lecture notes in English and German, a continuation of the course: \"Elliptic curves over Q and C,\" on modular forms and elliptic curves. ...more>> Elliptic Curves - Stéfane Fermigier A collection of links to research articles on elliptic curves and related topics (applications, modular forms, Fermat\\'s last theorem). ...more>> Euler Systems - Karl Rubin One of the most exciting new subjects in Algebraic Number Theory and Arithmetic Algebraic Geometry is the theory of Euler systems. Euler systems are special collections of cohomology classes attached to p-adic Galois representations. Here, in the first ...more>> Explicit Approaches to Modular Abelian Varieties - William Stein Stein\\'s Ph.D. thesis (Berkeley, 2000). An investigation of the Birch and Swinnerton-Dyer conjecture, which ties together the constellation of invariants attached to an abelian variety. The author attempts to verify this conjecture for certain specific ...more>> Front for the Mathematics ArXiv - Univ. of California, Davis U.C. Davis front end for the xxx.lanl.gov e-Print archive, a major site for mathematics preprints that has incorporated many formerly independent specialist archives including alg-geom, funct-an, dg-ga, q-alg, auto-fms, cd-hg, MAGNUS, Several Complex ...more>> Geometric Algorithms - Susan Landau; Univ. of Massachusetts at Amherst Landau\\'s interest in geometric algorithms has been in problems of an algebraic flavor: computer-aided design, robotics, and other geometric-related applications that have created a need for methods for embeddings, visualization, and algorithms for construction ...more>> Geometry and Topology: Recent Papers 1989-1993 - Univ. of Melbourne, Australia A bibliography of e-prints from the Dept. of Mathematics and Statistics, Univ. of Melbourne, Australia. ...more>> Helena A. Verrill - Dept. of Mathematics, University of Copenhagen, Denmark Links to Mathematics papers (Root lattices and pencils of varieties; The L-series of certain Calabi-Yau threefolds; Some congruences related to modular forms; Picard-Fuchs equations of some families of elliptic curves; On modular mod l Galois representations ...more>> The history of voting - MacTutor Math History Archives Linked essay on the history of electoral systems. ...more>> Homogeneous Transformation Matrices - Daniel W. VanArsdale Explicit n-dimensional homogeneous matrices for projection, dilation, reflection, shear, strain, rotation and other familiar transformations. ...more>> How to Fix an Election - Ivars Peterson (MathTrek) Voting sounds like a simple matter. Just pick a candidate, then count the ballots and announce the tally. When there are three or more candidates (or choices), however, the results may not actually reflect the true preferences of the voters. Suppose ...more>> Ideals, Varieties, and Algorithms - Cox, Little, O\\'Shea An introduction to computational algebraic geometry and commutative algebra at the undergraduate level, with discussions of systems of polynomial equations (\"ideals\"), their solutions (\"varieties\"), and how these objects can be manipulated (\"algorithms\"). ...more>> Innovative Teaching of Mathematics - ITM 2003 - Eckhard Hitzer September 20-21, 2003, Research Institute for Mathematical Sciences, Kyoto, Japan. An international symposium with an explicit focus on Clifford geometric algebra for teaching. A second major focus is to present new ways of innovative cooperation between ...more>> Page: 1 2 [ next> ] Search for these keywords: Click only once for faster results: all keywords, in any order at least one, that exact phrase parts of words whole words Choose a Math Topic all math topics algebra analysis arithmetic/early math calculus (single variable) calculus (multivariable) communicating math differential equations discrete math dynamical systems general geometry history and biography logic/foundations number theory numerical analysis operations research pre-calculus probability/statistics topology applications/connections algebraic geometry all math topics Choose a Math Education Topic all math education topics teaching issues/strategies assessment/testing general programs/approaches materials-reviews/recommendations activities psychological/affective issues special contexts specific math concepts/techniques teaching styles/practices technology in math ed writing/communication in math professional ed/career development continuing ed degrees/higher ed job placement/job market pre-service staff/prof development math ed research/reform curriculum/materials development pedagogical research psychological research reform social issues/public policy community outreach educational systems equity public understanding of math all math education topics Choose a Resource Type all resource types educational materials net-based resources organizations publications recreations reference sources software all resource types Choose a Level all levels elementary early elem. (prek-2) late elem. (3-5) middle school (6-8) high school (9-12) college early college late college research all levels Power Search [ Privacy Policy ] [ Terms of Use ] Suggestion Box || Home || The Math Library || Help Desk || Quick Reference || Search © 1994-2004 The Math Forum http://mathforum.org/ webmaster@mathforum.org \\n\\t\\n\\n-->\\n\\n\\n\\n\\n\\n\\nThe Math Forum - Math Library - Algebraic Geom.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBrowse and Search the Library\\n\\nHome\\n\\n   \\t\\t\\t: Math Topics\\n   \\t\\t\\t: Geometry\\n\\t\\t\\t: Algebraic Geom.\\n\\t\\t\\n\\n\\n\\nLibrary Home  ||  \\n\\nSearch ||\\n\\nFull Table of Contents ||\\n\\nSuggest a Link ||\\n\\nLibrary Help\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0 Selected Sites\\n\\xa0 (see also All Sites in this category)\\n\\n\\n\\n\\n\\n\\n\\nAlgebraic Geometry - \\nDave Rusin; The Mathematical Atlas\\n\\n\\t\\t\\tA short article designed to provide an introduction to algebraic geometry, which combines the algebraic with the geometric for the benefit of both. Thus the recent proof of \"Fermat\\'s Last Theorem\" - ostensibly a statement in number theory - was proved with geometric tools. Conversely, the geometry of sets defined by equations is studied using quite sophisticated algebraic machinery. This is an enticing area but the important topics are quite deep. This area includes elliptic curves. Applications and related fields and subfields; textbooks, reference works, and tutorials; software and tables; other web sites with this focus.\\n\\t\\t\\t more>>\\n\\nThe Algebraic Geometry Notebooks - Aksel Sogstad\\n\\n\\t\\t\\tFor non-experts. Contents: What is algebraic geometry?; What can algebraic geometry be used for?; Classification of curves in the Weierstrass familiy; Bezout\\'s Theorem; Varieties, orbits and orbit-spaces; Automatic theorem proving using Singular.\\n\\t\\t\\t more>>\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0 All Sites\\n\\n - 97 items\\n found, showing 1 to 50\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nA 27-Vertex Graph That Is Vertex-Transitive and Edge-Transitive But Not 1-Transitive - Peter Doyle\\n\\n\\t\\t\\tHypertext and Postscript versions of a paper describing a 27-vertex graph that is vertex-transitive and edge-transitive but not 1-transitive. While all vertices and edges of the graph are similar, there are no edge-reversing automorphisms.\\n\\n\\t\\t\\t...more>>\\n\\nAdvanced Geometry - Math Forum\\n\\n\\t\\t\\tLinks to some of the best Internet resources for advanced geometry: Web sites, software, Internet projects, publications, and public forums for discussion.\\n\\n\\t\\t\\t...more>>\\n\\nAG Algebraic Geometry (Front for the Mathematics ArXiv) - Univ. of California, Davis\\n\\n\\t\\t\\tAlgebraic Geometry preprints, from the U.C. Davis front end for the xxx.lanl.gov e-Print archive, a major site for mathematics preprints that has incorporated many formerly independent specialist archives. Search by keyword or browse by topic. \\n\\n\\t\\t\\t...more>>\\n\\nAGATHOS - Algebraic Geometry: A Total Hypertext Online System - Kevin R. Coombes; Department of Biostatistics, University of Texas M.D.Anderson Cancer Center\\n\\n\\t\\t\\tThis course assumes that the reader has completed a year-long graduate course in abstract algebra. It can be approached as a linear text, a list of interesting examples, a list of important theorems in algebraic geometry, key concepts, or exercises that\\n\\n\\t\\t\\t...more>>\\n\\nAlgebraic Curves - Silvio Levy; The Geometry Center\\n\\n\\t\\t\\tCurves that can be given in implicit form as f(x,y)=0, where f is a polynomial, are called algebraic. This site provides an overview, with illustrations.\\n\\n\\t\\t\\t...more>>\\n\\nAlgebraic Geometry - Peter Stiller; Dept. of Mathematics & Computer Science, Texas A&M Univ.\\n\\n\\t\\t\\tThis page answers the question, \"What is Algebraic Geometry?\" - a subject with historical roots in analytic geometry, concerned with the geometry of the solutions of a system of polynomial equations. With a link to a bibliography of related publications.\\n\\n\\t\\t\\t...more>>\\n\\nAlgebraic Geometry Seminar - Michael Harrison; Univ. of Washington\\n\\n\\t\\t\\tOnline notes for a seminar, including handouts in dvi and postscript formats, solutions to the problems in Chapter 1 of Hartshorne\\'s Algebraic Geometry, and notes on algebraic geometry by Professor J. S. Milne, Univ. of Michigan.\\n\\n\\t\\t\\t...more>>\\n\\nAlgebraic Geometry - Tayler Jarvis\\n\\n\\t\\t\\tA page of links to general algebraic geometry sites, sites on algebraic curves, algebraic surfaces, algebraic geometers and the like, and related sites.\\n\\n\\t\\t\\t...more>>\\n\\nAlgebraic Number Theory and Elliptic Curves - Ghitza, Osserman; Massachusetts Institute of Technology\\n\\n\\t\\t\\tA semester-long seminar giving a rapid introduction to algebraic number theory and elliptic curves. Topics: Dedekind domains, rings of integers, scheme-theoretic curves, finite morphisms thereof, splitting and ramification, the Tchebotarov density theorem\\n\\n\\t\\t\\t...more>>\\n\\nAlgebraic Number Theory Archives - Boston, Grayson\\n\\n\\t\\t\\tPreprints about algebraic number theory and arithmetic geometry are accepted in electronic form for storage until publication. There are instructions for authors who wish to submit preprints to the archives and for for joining the mailing list (members\\n\\n\\t\\t\\t...more>>\\n\\nAlgorithms for Modular Elliptic Curves - John Cremona\\n\\n\\t\\t\\tView parts of the second edition of the book, with introduction, tables and software. Download the errata list for the second edition, available as a dvi file or a postscript file.\\n\\n\\t\\t\\t...more>>\\n\\nAnalytic and Descriptive Geometry - Dave Rusin; The Mathematical Atlas\\n\\n\\t\\t\\tAn introduction to ordinary analytic geometry as studied in secondary school. (Also included here is a rather lengthy analysis of a result known as Poncelet\\'s Porism.) History; applications and related fields and subfields; textbooks, reference works,\\n\\n\\t\\t\\t...more>>\\n\\nArbeitsgruppe Bachem/Schrader - University of Köln\\n\\n\\t\\t\\tA working group of the Center for Parallel Computing (ZPR) at Köln. Site includes a list of members and contact address; informative write-ups on its projects, some in English. Topics include: Basic Research (meaning \"pure\" as opposed to \"applied\");\\n\\n\\t\\t\\t...more>>\\n\\nArithmetic Algebraic Geometry (AAG) - Training and Mobility of Researchers; European Community\\n\\n\\t\\t\\tA partnership of 12 working groups from 5 European countries (England, France, Germany, Italy, Spain) working to develop methods taken from geometry to study the arithmetical properties of algebraic equations. Main research themes: p-adic cohomology theory;\\n\\n\\t\\t\\t...more>>\\n\\narXiv.org e-Print archive - Los Alamos National Laboratory (LANL)\\n\\n\\t\\t\\tA major site for mathematics preprints that has incorporated many formerly independent specialist archives including alg-geom, funct-an, dg-ga, q-alg, auto-fms, cd-hg, MAGNUS, Several Complex Variables, Logic E-prints, Commutative Algebra, Dynamical Systems,\\n\\n\\t\\t\\t...more>>\\n\\nAutomated Deduction - William McCune\\n\\n\\t\\t\\tResearch on applications of automated deduction to problems in abstract algebra and algebraic geometry, algorithms and strategies for searching for proofs and for counterexamples, high-performance implementation of automated deduction algorithms, and\\n\\n\\t\\t\\t...more>>\\n\\nAutomorphic Forms and Representations Virtual Study Group - Michael Harrison\\n\\n\\t\\t\\tA virtual study group formed to explore the Langland\\'s Program by reading Automorphic Form and Representations by Daniel Bump.\\n\\n\\t\\t\\t...more>>\\n\\nBasic Geometry of Voting - Donald G. Saari\\n\\n\\t\\t\\tA book that offers a large number of new results about voting theory, its emphasis purposely placed on three candidate settings so that the book can be read by almost everyone. (With four or more candidates, the results need more advanced mathematical\\n\\n\\t\\t\\t...more>>\\n\\nBibliography for Automorphic and Modular Forms, L-Functions, Representations, and Number Theory - Paul Garrett\\n\\n\\t\\t\\tA 1996 bibliography compiled for the author\\'s Ph.D. students at the University of Minnesota.\\n\\n\\t\\t\\t...more>>\\n\\nThe Birch and Swinnerton-Dyer Conjecture - Clay Mathematics Institute\\n\\n\\t\\t\\tA Clay Mathematics Institute Prize problem, with a description in pdf format by Andrew Wiles. The conjecture addresses the problem of enumerating rational points, such as on elliptic curves.\\n\\n\\t\\t\\t...more>>\\n\\nBooks: Professional & Technical: Professional Science: Mathematics - Amazon.com\\n\\n\\t\\t\\tBrowse bestselling math books from the Professional and Technical Bookstore at Amazon.com, in such categories as Applied; Chaos & Systems; Geometry & Topology; Mathematical Analysis; Mathematical Physics; Number Systems; Pure Mathematics; Transformations;\\n\\n\\t\\t\\t...more>>\\n\\nCalabi-Yau Homepage - Sheldon Katz\\n\\n\\t\\t\\tA page of information about Calabi-Yau manifolds: threefolds, fourfolds, software.\\n\\n\\t\\t\\t...more>>\\n\\nCategorical Geometry - Zhaohua Luo\\n\\n\\t\\t\\tSee a brief tour of categorical geometry, which studies the geometric properties of unitary categories. The categorical approach to algebraic geometry was initiated by Yves Diers in his pioneer book Categories of Commutative Algebras (Oxford University\\n\\n\\t\\t\\t...more>>\\n\\nCounting Points on Elliptic Curves - Fouquet, Harley, Gaudry, Morain\\n\\n\\t\\t\\tThe authors, Ph.D. students at the Laboratoire d\\'informatique de l\\'École polytechnique, Paris, have established new records for point counting in characteristic 2, using a new algorithm due to Takakazu Satoh.\\n\\n\\t\\t\\t...more>>\\n\\nCourse Notes - J. S. Milne\\n\\n\\t\\t\\tFull course notes in dvi, pdf, and postscript formats for all the advanced courses J. S. Milne taught at the University of Michigan between 1986 and 1999: Group Theory; Fields and Galois Theory; Algebraic Number Theory; Class Field Theory; Modular Functions\\n\\n\\t\\t\\t...more>>\\n\\nDavid Pollack\\n\\n\\t\\t\\tDownload the author\\'s thesis, \"Explicit Hecke Actions on Modular Forms\" (Harvard, 1998) in DVI format.\\n\\n\\t\\t\\t...more>>\\n\\nDiophantine geometry in characteristic p: a survey - José Felipe Voloch\\n\\n\\t\\t\\tAn article that appeared in Arithmetic Geometry, F. Catanese, ed., Symposia Mathematica XXXVII, Cambridge Univ. Press, 1997, pp. 260-278. Introduction; Curves; Abelian varieties and their subvarieties; Diophantine Approximation in characteristic p; Omitted\\n\\n\\t\\t\\t...more>>\\n\\nDiscrete and Computational Geometry - Springer-Verlag\\n\\n\\t\\t\\tAn international journal of mathematics and computer science that accepts research articles of high quality in discrete geometry and on the design and analysis of geometric algorithms; more specifically, DCG publishes papers on such topics as configurations\\n\\n\\t\\t\\t...more>>\\n\\nEAGER (European Algebraic Geometry Research Training Network) - Wolfram Decker\\n\\n\\t\\t\\tThe members of EAGER are algebraic geometers in mathematical centres spreading among most European countries. These centres are grouped into twelve geographical nodes which are responsible for the management of joint research projects and for the training\\n\\n\\t\\t\\t...more>>\\n\\nThe ECMNET Project - Charron, Daminelli, Granlund, Leyland, Zimmermann\\n\\n\\t\\t\\tThe goal of the ECMNET Project is to find large factors using the elliptic curve method, mainly Cunningham numbers.\\n\\n\\t\\t\\t...more>>\\n\\nElliptic Curve Discrete Logarithms (ECDL) Project - Robert Harley; 4K Associates\\n\\n\\t\\t\\tThe Project has solved ECC2K-108.\\n\\n\\t\\t\\t...more>>\\n\\nElliptic Curves and Cryptography - Andreas Enge\\n\\n\\t\\t\\tIncludes the test of the author\\'s introductory book on elliptic curves and their applications in cryptography. Also links to further publications; the personal pages of number theorists and cryptographers; and companies, organisations and miscellaneous\\n\\n\\t\\t\\t...more>>\\n\\nElliptic Curves and Cryptology - Marc Joye\\n\\n\\t\\t\\tLinks to elliptic curve resources: people (an extensive list of personal pages), bibliography, software, and links to relevant Web pages. Many preprints are available from the site.\\n\\n\\t\\t\\t...more>>\\n\\nElliptic Curves and Elliptic Functions - Charles Daney\\n\\n\\t\\t\\tFrom The Mathematics of Fermat\\'s Last Theorem. With a glossary; contents include: What is an elliptic curve?; The group structure of an elliptic curve; Arithmetic on elliptic curves; Further basic concepts and results.\\n\\n\\t\\t\\t...more>>\\n\\nElliptic Curves and Formal Groups - Lubin, Serre, Tate\\n\\n\\t\\t\\tLecture notes from a seminar, part of the Lecture notes prepared in connection with the seminars held at the Summer Institute on Algebraic Geometry, Whitney Estate, Woods Hole, Massachusetts, July 6-July 31, 1964.\\n\\n\\t\\t\\t...more>>\\n\\nElliptic Curves and Right Triangles - Karl Rubin; Stanford University\\n\\n\\t\\t\\tSlides (GIF) of lectures.\\n\\n\\t\\t\\t...more>>\\n\\nElliptic Curves - Dave Rusin; The Mathematical Atlas\\n\\n\\t\\t\\tAn area of algebraic geometry that deals with nonsingular curves of genus 1 - in\\nEnglish, solutions to equations y^2 = x^3 + A x + B. It has important connections to number theory and in particular to factorization of ordinary integers (and thus to cryptography).\\n\\n\\t\\t\\t...more>>\\n\\nElliptic Curves II - Johan P. Hansen; Dept. of Mathematics, Ny Munkegade, Aarhus\\n\\n\\t\\t\\tLecture notes in English and German, a continuation of the course: \"Elliptic curves over Q and C,\" on modular forms and elliptic curves.\\n\\n\\t\\t\\t...more>>\\n\\nElliptic Curves - Stéfane Fermigier\\n\\n\\t\\t\\tA collection of links to research articles on elliptic curves and related topics (applications, modular forms, Fermat\\'s last theorem).\\n\\n\\t\\t\\t...more>>\\n\\nEuler Systems - Karl Rubin\\n\\n\\t\\t\\tOne of the most exciting new subjects in Algebraic Number Theory and Arithmetic Algebraic Geometry is the theory of Euler systems. Euler systems are special collections of cohomology classes attached to p-adic Galois representations. Here, in the first\\n\\n\\t\\t\\t...more>>\\n\\nExplicit Approaches to Modular Abelian Varieties - William Stein\\n\\n\\t\\t\\tStein\\'s Ph.D. thesis (Berkeley, 2000). An investigation of the Birch and Swinnerton-Dyer conjecture, which ties together the constellation of invariants attached to an abelian variety. The author attempts to verify this conjecture for certain specific\\n\\n\\t\\t\\t...more>>\\n\\nFront for the Mathematics ArXiv - Univ. of California, Davis\\n\\n\\t\\t\\tU.C. Davis front end for the xxx.lanl.gov e-Print archive, a major site for mathematics preprints that has incorporated many formerly independent specialist archives including alg-geom, funct-an, dg-ga, q-alg, auto-fms, cd-hg, MAGNUS, Several Complex\\n\\n\\t\\t\\t...more>>\\n\\nGeometric Algorithms - Susan Landau; Univ. of Massachusetts at Amherst\\n\\n\\t\\t\\tLandau\\'s interest in geometric algorithms has been in problems of an algebraic flavor: computer-aided design, robotics, and other geometric-related applications that have created a need for methods for embeddings, visualization, and algorithms for construction\\n\\n\\t\\t\\t...more>>\\n\\nGeometry and Topology: Recent Papers 1989-1993 - Univ. of Melbourne, Australia\\n\\n\\t\\t\\tA bibliography of e-prints from the Dept. of Mathematics and Statistics, Univ. of Melbourne, Australia.\\n\\n\\t\\t\\t...more>>\\n\\nHelena A. Verrill - Dept. of Mathematics, University of Copenhagen, Denmark\\n\\n\\t\\t\\tLinks to Mathematics papers (Root lattices and pencils of varieties; The L-series of certain Calabi-Yau threefolds; Some congruences related to modular forms; Picard-Fuchs equations of some families of elliptic curves; On modular mod l Galois representations\\n\\n\\t\\t\\t...more>>\\n\\nThe history of voting\\n - MacTutor Math History Archives\\n\\n\\t\\t\\tLinked essay on the history of electoral systems.\\n\\n\\t\\t\\t...more>>\\n\\nHomogeneous Transformation Matrices - Daniel W. VanArsdale\\n\\n\\t\\t\\tExplicit n-dimensional homogeneous matrices for projection, dilation, reflection, shear, strain, rotation and other familiar transformations.\\n\\n\\t\\t\\t...more>>\\n\\nHow to Fix an Election - Ivars Peterson (MathTrek)\\n\\n\\t\\t\\tVoting sounds like a simple matter. Just pick a candidate, then count the ballots and announce the tally. When there are three or more candidates (or choices), however, the results may not actually reflect the true preferences of the voters.\\nSuppose\\n\\n\\n\\t\\t\\t...more>>\\n\\nIdeals, Varieties, and Algorithms - Cox, Little, O\\'Shea\\n\\n\\t\\t\\tAn introduction to computational algebraic geometry and commutative algebra at the undergraduate level, with discussions of systems of polynomial equations (\"ideals\"), their solutions (\"varieties\"), and how these objects can be manipulated (\"algorithms\").\\n\\n\\t\\t\\t...more>>\\n\\nInnovative Teaching of Mathematics - ITM 2003 - Eckhard Hitzer\\n\\n\\t\\t\\tSeptember 20-21, 2003, Research Institute for Mathematical Sciences, Kyoto, Japan. An international symposium with an explicit focus on Clifford geometric algebra for teaching. A second major focus is to present new ways of innovative cooperation between\\n\\n\\n\\t\\t\\t...more>>\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPage: \\xa01\\n\\xa02\\n[next>]\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\tSearch for these keywords:\\n\\n\\n\\n                        \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\n\\t\\t\\t\\xa0 \\xa0\\n\\nClick only once for faster results:\\n\\n\\n\\n\\n\\n\\n all keywords, in any order\\n at least one,\\n that exact phrase\\n\\n\\t\\n parts of words \\n whole words \\n\\n\\t\\n\\n\\n\\n\\n\\n\\n\\nChoose a Math Topic\\n\\tall math topics\\n\\talgebra\\n\\tanalysis\\n\\tarithmetic/early math\\n\\tcalculus (single variable)\\n\\tcalculus (multivariable)\\n\\tcommunicating math\\n\\tdifferential equations\\n\\tdiscrete math\\n\\tdynamical systems\\n\\tgeneral\\n\\tgeometry\\n\\thistory and biography\\n\\tlogic/foundations\\n\\tnumber theory\\n\\tnumerical analysis\\n\\toperations research\\n\\tpre-calculus\\n\\tprobability/statistics\\n\\ttopology\\n\\tapplications/connections\\n\\talgebraic geometry\\n\\tall math topics\\n\\n\\nChoose a Math Education Topic\\n\\tall math education topics\\n\\tteaching issues/strategies\\n\\tassessment/testing\\n\\tgeneral programs/approaches\\n\\tmaterials-reviews/recommendations\\n\\tactivities\\n\\tpsychological/affective issues\\n\\tspecial contexts\\n\\tspecific math concepts/techniques\\n\\tteaching styles/practices\\n\\ttechnology in math ed\\n\\twriting/communication in math\\n\\tprofessional ed/career development\\n\\tcontinuing ed\\n\\tdegrees/higher ed\\n\\tjob placement/job market\\n\\tpre-service\\n\\tstaff/prof development\\n\\tmath ed research/reform\\n\\tcurriculum/materials development\\n\\tpedagogical research\\n\\tpsychological research\\n\\treform\\n\\tsocial issues/public policy\\n\\tcommunity outreach\\n\\teducational systems\\n\\tequity\\n\\tpublic understanding of math\\n\\tall math education topics\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nChoose a Resource Type\\n\\tall resource types\\n\\teducational materials\\n\\tnet-based resources\\n\\torganizations\\n\\tpublications\\n\\trecreations\\n\\treference sources\\n\\tsoftware\\n\\tall resource types\\n\\n\\nChoose a Level\\n\\tall levels\\n\\telementary\\n\\tearly elem. (prek-2)\\n\\tlate elem. (3-5)\\n\\tmiddle school (6-8)\\n\\thigh school (9-12)\\n\\tcollege\\n\\tearly college\\n\\tlate college\\n\\tresearch\\n\\tall levels\\n\\n\\n\\n\\n\\n\\nPower Search\\n\\n\\n\\n\\n[Privacy Policy]\\n[Terms of Use]\\n\\nSuggestion Box ||\\n\\nHome  ||\\n\\nThe Math Library ||\\n\\nHelp Desk  ||\\n\\nQuick Reference ||\\n\\nSearch\\n\\n\\n© 1994-2004 The Math Forum\\nhttp://mathforum.org/\\nwebmaster@mathforum.org\\n\\n\\n\\n\\n\\n\\n',\n",
       " ' \\xa0 \\xa0 Search advanced search Table of contents Past issues Links to advertisers Products advertised Place an ad Buyers\\' guide About us Contact us Submit press release American Institute of Physics The Industrial Physicist Computing in Science & Engineering Journals Virtual Journals Article The Discovery of Rapid Climate Change Only within the past decade have researchers warmed to the possibility of abrupt shifts in Earth\\'s climate. Sometimes, it takes a while to see what one is not prepared to look for. Spencer Weart How fast can our planet\\'s climate change? Too slowly for humans to notice, according to the firm belief of most scientists through much of the 20th century. Any shift of weather patterns, even the Dust Bowl droughts that devastated the Great Plains in the 1930s, was seen as a temporary local excursion. To be sure, the entire world climate could change radically: The ice ages proved that. But common sense held that such transformations could only creep in over tens of thousands of years. In the 1950s, a few scientists found evidence that some of the great climate shifts in the past had taken only a few thousand years. During the 1960s and 1970s, other lines of research made it plausible that the global climate could shift radically within a few hundred years. In the 1980s and 1990s, further studies reduced the scale to the span of a single century. Today, there is evidence that severe change can take less than a decade. A committee of the National Academy of Sciences (NAS) has called this reorientation in the thinking of scientists a veritable \"paradigm shift.\" The new paradigm of abrupt global climate change, the committee reported in 2002, \"has been well established by research over the last decade, but this new thinking is little known and scarcely appreciated in the wider community of natural and social scientists and policymakers.\" 1 Much earlier in the 20th century, some specialists had evidence of abrupt climate change in front of their eyes. The evidence was meaningless to them. To appreciate change occurring within 10 years as significant, scientists first had to accept the possibility of change within 100 years. That, in turn, had to wait until they accepted the 1000-year time scale. The history of this evolution gives a good example of the stepwise fashion in which science commonly proceeds, contrary to the familiar heroic myths of discoveries springing forth in an instant. The history also suggests why, as the NAS committee worried, most people still fail to realize just how badly the world\\'s climate might misbehave. Was a 1000-year climate change possible? During the early decades of the 20th century, a very few meteorologists did speculate about possibilities for rapid change. The most striking scenario was offered in 1925 by the respected climate expert C. E. P. Brooks, who suggested that a slight change of conditions might set off a self-sustaining shift between climate states. Suppose, he said, some random decrease of snow cover in northern latitudes exposed dark ground. Then the ground would absorb more sunlight, which would warm the air, which would melt still more snow--a vicious feedback cycle. An abrupt and catastrophic rise of tens of degrees was conceivable, Brooks wrote, \"perhaps in the course of a single season.\" 2 Run the cycle backward, and an ice age might suddenly descend. Most other professional climatologists dismissed the idea as preposterous. The continental glaciers of an ice age, a kilometer thick, would surely require vast lengths of time to build up or melt away. Beyond that elementary reasoning lay a deeper rejection of all such speculations. It was the climatologists\\' trade to compile statistics on past weather in order to advise a farmer what crops to grow or tell an engineer what sort of floods were likely over the lifetime of a bridge. The climatologist\\'s career thus rested on a conviction that the experience of the recent past reliably described future conditions. That belief was supported by a paucity of data, for hardly any accurate records of daily temperatures and the like went back more than half a century or so. The limitation scarcely worried climatologists, who assumed that significant changes took place only over thousands of years. In their textbooks, climate was introduced as the long-term average of weather over time, by definition, static over centuries. The experts held a traditional belief that the natural world is self-regulating: If anything started to perturb a grand planetary system like the atmosphere, natural forces would automatically compensate. Scientists came up with various plausible self-regulating mechanisms. For example, if temperatures rose, then more water would evaporate from the seas; in response, clouds would thicken and reflect more sunlight, which would restore normal temperatures. The perception of self-regulation reflected a view of the world held deeply in almost every human culture: Stability was guaranteed, if not by Divine Providence, then by the suprahuman power of a benevolent \"balance of nature.\" Those beliefs were not disturbed by the few long-term climate records available at the time. The best of those data were compiled in the 1920s by an Arizona astronomer, Andrew Ellicott Douglass, who noted that the rings in trees were thinner in dry years. Analyzing old logs, Douglass reported a major century-long climate perturbation around the 17th century. But most other scientists doubted that tree rings (if they reflected climate at all) gave information about anything beyond random regional variations. Signs of climate shifts were also visible in varves, a Swedish word for the layers laid down each year in the mud on the bottom of northern lakes. From bogs and outcrops where the beds of fossil lakes were exposed, or from cores of slick clay drilled out of living lakes, the layers were painstakingly counted and measured. Ancient pollen told what plants had lived in the region when the layers were laid down. Major changes in vegetation suggested that the last ice age had not ended with a uniformly steady warming, but with peculiar oscillations of temperature. Scandinavian data revealed a particularly striking shift around 12 000 years ago, when a warm period gave way to a spell of bitterly cold weather, dubbed the Younger Dryas, after Dryas octopetala , a hardy Arctic flower whose pollen signals frigid tundra. In 1955, the timing was pinned down by a radiocarbon-dating study, which revealed that the temperature change had been rapid; for climate scientists at midcentury, \"rapid\" meant a change that took place over as little as 1000 years. 3 Ice-age changes over a thousand years or so in a restricted region, although surprising, seemed acceptable. The rate of advance and retreat of the great glaciers would be no faster than present-day mountain glaciers were seen to move. That perception was compatible with the so-called uniformitarian principle, a geological tenet that the forces that molded ice, rock, sea, and air did not vary over time. Through most of the 20th century, the uniformitarian principle was cherished by geologists as the very foundation of their science: How could one study anything scientifically unless the rules stayed the same? The idea had become central to their training and theories during a century of disputes, when scientists painfully gave up traditions that explained certain geological features by invoking Noah\\'s Flood or other supernatural interventions. In human experience, temperatures apparently did not rise or fall radically in less than millennia, so the uniformitarian principle declared that such changes had never happened in the past. Scientists found themselves insisting on this principle as they were confronted, time and again, by cranks and religious fundamentalists who publicly proclaimed ideas about apocalyptic global cataclysms. Something resembling catastrophic climate jumps could in fact show up in varves. But the silt layers could have been distorted in countless ways that had nothing to do with climate: a forest fire perhaps, or a shift of stream drainage. Scientists saw the jumps not as climate data to be analyzed, but as mere local noise. They did not worry about the fact that old radiocarbon dates were accurate only within a thousand years or so, so that the chronologies of different sites could not be matched well enough to point to any rapid and widespread change. Figure 1 In 1956, studying variations in the shells of plankton that were embedded in cores of clay pulled from the deep seabed (see figure 1 ), radiocarbon expert Hans Suess discovered what was at the time the fastest change that anyone expected. Suess reported that the last glacial period had ended with a relatively rapid rise of temperature, about 1°C per thousand years. 4 It scarcely bothered him and his colleagues that no faster change could have been seen in most cores. In many places, the mud was constantly stirred by burrowing worms or by seafloor currents and slumping, which blurred any differences between layers. Yet the data curves did sharpen as cores were pulled from regions of rapid deposition and as radiocarbon dating improved. By 1960, a trio of scientists at what is now the Lamont-Doherty Observatory--Wallace Broecker, Maurice Ewing, and Bruce Heezen--were reporting a variety of evidence, from deep-sea and lake deposits, that a global climate shift of as much as 5-10°C had taken place in less than a thousand years. 5 Most of their colleagues found such a rise barely plausible. Making sense of rapid change Evidence of a climate shift could only be accepted if it made sense--that is, if there existed some plausible theory of the climate system that could explain the shift. Broecker suspected that the cause might be a rapid turnover of North Atlantic ocean waters, but that was just hand-waving speculation. More influential was a 1956 paper by Ewing and William Donn, who built an elaborate model for the coming and going of ice ages. 6 Like Brooks and others before them, Ewing and Donn began with the notion that a retreat of reflective snow and ice would bring more warming by sunlight. Their new idea was that the feedback mechanism had a hair trigger set off by ocean currents. As ice sheets melted and the sea level rose, warm water would spill into the Arctic Ocean and melt its ice cover, thus speeding up the warming. But once the Arctic Ocean was free of ice, they argued, so much moisture would evaporate that snow would fall heavily all around the Arctic, switching the feedback to cooling. Ewing and Donn thought it conceivable that the polar ocean might become ice-free and launch us into a new ice age within the next few hundred years. Journalists alerted the public to the risk of a glacial advance within the foreseeable future. People were prepared to believe it, for they were already abandoning their old ideas about an imperturbable balance of nature. The headlong advances of population and industry were making themselves felt in ever more widespread pollution. More ominous still was the global radioactive fallout from nuclear weapons tests, alongside scientists\\' warnings that a nuclear war could wreck the entire planet. It was no longer inconceivable that some perturbation--even one produced from human industry--might alter the entire planet. In fact, Ewing and Donn\\'s theory was erroneous, as other scientists quickly pointed out. Nevertheless, it had served a useful function. For the first time, there was respectable scientific backing for a picture of rapid, even disastrous, climate change. Other scientists, even as they rejected the theory, were stimulated to broaden their thinking and to inspect data for new kinds of information. Further stimulation came from entirely different studies. In the late 1950s, a group led by Dave Fultz at the University of Chicago carried out tabletop \"dishpan\" experiments in which they used a rotating fluid to simulate the circulation of the atmosphere. They created a simulacrum complete with a miniature jet stream and cyclonic storms. But when they perturbed the rotating liquid with a pencil, they found that the circulation pattern could flip between distinct modes. If the actual atmospheric circulation did that, weather patterns in many regions would shift almost instantly. In the early 1960s, climatologist Mikhail Budyko in Leningrad got disturbing results on a still larger scale from some simple equations for Earth\\'s energy budget. His calculations indicated that feedbacks involving snow cover could indeed bring extraordinary climate changes within a short time. Other geophysical models turned up more possibilities for rapid change. Figure 2 The most influential idea for what might bring rapid change was developed from old speculations about the circulation of the North Atlantic Ocean. In 1966, Broecker (pictured in figure 2 ), taking a close look at deep-sea cores, reported evidence for an \"abrupt transition between two stable modes of operation of the ocean-atmosphere system.\" 7 Nowadays, warm tropical water flows northward near the surface of the Atlantic; a large quantity, heavy with cold and salt, sinks near Iceland and returns southward in the deep. A change of temperature or salinity might shut down the circulation, cut off the northward transport of a huge amount of heat, and bring severe climate change. Simple numerical models involving the transport of fresh water by a changed pattern of winds showed that such a change could be self-sustaining. At the University of Wisconsin-Madison, Reid Bryson scrutinized entirely different types of data. In the late 1950s, he had been struck by the wide variability of climates as recorded in the varying width of tree rings. He was also familiar with the dishpan experiments that showed how a circulation pattern might change almost instantaneously. To take a new, interdisciplinary look at climate, Bryson brought together a group that even included an anthropologist who studied the ancient Native American cultures of the Midwest. From radiocarbon-dated bones and pollen, they deduced that a prodigious drought had struck the region in the 1200s--the very period when flourishing towns of the Mound Builders had gone into decline. Compared to that drought, the Dust Bowl of the 1930s had been mild and temporary. By the mid-1960s, Bryson was announcing that \"climatic changes do not come about by slow, gradual change, but rather by apparently discrete \\'jumps\\' from one atmospheric circulation regime to another.\" 8 His group further reported pollen studies showing a rapid shift around 10 500 years ago; by \"rapid\" they meant a change in the mix of tree species within less than a century. Perhaps the Younger Dryas was not just a local Scandinavian anomaly. Still, no major climate change was required to transform any particular forest. Many experts continued to believe it was sheer speculation to imagine that the climate of a region, let alone of the entire world, could change in less than a thousand years or so. But confirmation of changes at that rate, at least, was coming from a variety of studies. As the respected climatologist J. Murray Mitchell Jr explained in 1972, in place of the old view of \"a grand, rhythmic cycle,\" the new evidence showed a \"much more rapid and irregular succession\" in which Earth \"can swing between glacial and interglacial conditions in a surprisingly short span of millennia (some would say centuries).\" 9 Figure 3 The most convincing evidence came from a long core of ice drilled at Camp Century, Greenland, by Willi Dansgaard\\'s Danish group, in cooperation with Americans led by Chester Langway Jr. The proportions of different oxygen isotopes in the layers of ice gave a fairly straightforward record of temperature. Mixed in with the expected gradual cycles were what the group called \"spectacular\" shorter-term shifts, including the Younger Dryas oscillation. Some of the shifts seemed to have taken as little as a century or two (see figure 3 ). During the early 1970s, most climate experts came to agree that interglacial periods tended to end more abruptly than had been supposed. Many concluded that the current warm period could end in a rapid cooling, possibly even within the next few hundred years. Bryson (pictured in figure 4 ), Stephen Schneider, and a few others took this new concern to the public. They insisted that the climate we had experienced in the past century or so, mild and equable, was not the only sort of climate the planet knew. For all anyone could say, the next decade might start a plunge into a cataclysmic freeze, drought, or other change unprecedented in recent memory, although not without precedent in the archaeological and geological record. Figure 4 Cooling was not the only change that experts were starting to worry about. Since the late 1950s, attentive scientists had acknowledged the potential value of the old idea that human emissions of carbon dioxide gas (CO 2 ) might lead to global warming. (See Physics Today, January 1997, page 34 .) Most experts assumed that if such a greenhouse-effect warming did occur, it would come as they expected for any climate change--gradually over the course of a few centuries. But some suggested swifter possibilities. In 1972, pursuing his calculations of ice-cover feedbacks, Budyko declared that, at the rate we were pumping CO 2 into the atmosphere, the ice covering the Arctic Ocean might melt entirely by 2050. And glacier experts were developing models that suggested how warming might cause the ice sheets of Antarctica to break up swiftly and shock the climate system. Bryson and others worked harder than ever to bring their concerns to the attention of the broader scientific community and the public. Most scientists spoke more cautiously. When leading experts had to state a consensus opinion, as in a 1975 NAS report on climate research, 10 they reported that they saw nothing that would bring anything beyond relatively small changes that would take centuries or longer to develop. They did warn that there could be significant noise, the usual irregularities of weather patterns. And they admitted that they might have failed to recognize some mechanisms of change. If there was a threat, experts in the 1970s could not agree whether it was from global warming or cooling. The one thing that all scientists agreed on was that they were seriously ignorant about how the climate system worked. So the only step they recommended to policymakers was to pursue research more aggressively. Jumps within centuries--or less In the late 1970s and early 1980s, a variety of new data revealed surprising climate shifts. To take one example, a study of beetles that had been preserved in peat bogs since the end of the last glacial epoch turned up changes in the mix of species; those changes represented climate shifts of 3°C in well under 1000 years. Meanwhile, computer modelers produced plausible calculations for rapid climate shifts involving snow-cover feedbacks, a shutdown of North Atlantic circulation, or ice-sheet collapse. 11 During the 1980s, the list of plausible mechanisms grew. Perhaps a rise in global temperature would cause methane to bubble out of the vast expanses of boggy tundra. Because methane is a greenhouse gas that blocks heat radiation even more effectively than CO 2 , such a release would cause even more warming in a vicious feedback cycle. Or what about the clathrates--peculiar ices that lock up huge volumes of methane in the muck of seabeds? Perhaps those would disintegrate and release greenhouse gases. Many scientists continued to look on such speculations as little more than science fiction. The evidence for rapid shifts, as it sometimes turned up in odd data sources like bog beetles, was never entirely convincing. Any single record could be subject to all kinds of accidental errors. The best example of a problem was in the best data on climate shifts, the odd wiggles in measurements from the Camp Century core. Those data came from near the bottom of the hole. Skeptics argued that the ice layers there, squeezed tissue-thin, were folded and distorted as they flowed over the bedrock. To get more reliable data, the ice drillers went to a second location, some 1400 kilometers distant from Camp Century. By 1981, after a decade of tenacious labor, they hit bedrock and extracted gleaming cylinders of ice 10 cm in diameter and more than two km deep; the deepest ice came from the last ice age, 14 000 years ago. The ratios of oxygen isotopes within the ice layers gave a temperature record showing what the researchers called \"violent\" changes. The most prominent of those, corresponding to the Younger Dryas oscillation, showed \"a dramatic cooling of rather short duration, perhaps only a few hundred years.\" 12 Since the 1950s, jumps had persistently turned up in weather and climate models, whether built from rotating dishpans or from sets of equations run through computers. Scientists could have dismissed those models as too crude to say anything reliable--but the historical data showed that the notion of radical climate instability was not absurd after all. And scientists could have dismissed the jumps in the scattered data as artifacts, due to merely regional changes or simple errors--but the models showed that global jumps were physically plausible. Figure 5 Nevertheless, experts were scarcely prepared for the shock that came from the Greenland ice plateau in 1993. Plans had been laid to drill at the summit of the ice cap, where irregularities due to the deep flow of ice would have been minimal. Early hopes for a new cooperative program joining Americans and Europeans broke down and each team drilled its own hole, some 3 km deep (see figure 5 ). Competition was transmuted into cooperation by a decision to put the two boreholes just far enough apart (30 km) so that anything that showed up in both cores must represent a real climate effect, not an artifact due to bedrock conditions. The match turned out to be remarkably exact for most of the way down. The comparison between cores showed convincingly that climate could change more rapidly than almost any scientist had imagined. Swings of temperature that were believed in the 1950s to take tens of thousands of years, in the 1970s to take thousands of years, and in the 1980s to take hundreds of years, were now found to take only decades. Greenland had sometimes warmed a shocking 7°C within a span of less than 50 years. More recent studies have reported that, during the Younger Dryas transition, drastic shifts in the entire North Atlantic climate could be seen within five snow layers, that is, as little as five years! 13 Studies of pollen and other indicators--at locations ranging from Ohio to Japan to Tierra del Fuego, and dated with greatly improved radiocarbon techniques--suggested that the Younger Dryas event affected climates around the world. The extent of the climate variations was controversial (and to some extent remains so). Likewise uncertain was whether such variations could occur not only in glacial times, but also in warm periods like the present. Computer modelers, now fully alerted to the delicate balance of salinity and temperature that drove the North Atlantic circulation, found that global warming might bring future changes in precipitation that could shut down the current heat transport. The 2001 report of the Intergovernmental Panel on Climate Change, pronouncing the official consensus of the world\\'s governments and their climate experts, reported that a shutdown in the coming century was \"unlikely\" but \"cannot be ruled out.\" If such a shutdown did occur, it would change climates all around the North Atlantic--a dangerous cooling brought on by global warming. 14 Now that the ice had been broken, so to speak, most experts were prepared to consider that rapid climate change--huge and global change--could come at any time. \"The abrupt changes of the past are not fully explained yet,\" wrote the NAS committee in its 2002 report, \"and climate models typically underestimate the size, speed, and extent of those changes. Hence, . . . climate surprises are to be expected.\" 1 Despite the profound implications of this new viewpoint, hardly anyone rose to dispute it. 15 Although people did not deny the facts head-on, many denied them more subtly by failing to revise their accustomed ways of thinking. \"Geoscientists are just beginning to accept and adapt to the new paradigm of highly variable climate systems,\" wrote the NAS committee. And beyond geoscientists, \"this new paradigm has not yet penetrated the impacts community\"--the economists and other specialists who try to calculate the consequences of climate change. 16 Policymakers and the public lagged even farther behind in grasping what the new scientific view could mean. As a geologist once remarked, \"To imagine that turmoil is in the past and somehow we are now in a more stable time seems to be a psychological need.\" 17 A gradual discovery process How abrupt was the discovery of abrupt climate change? Many climate experts would put their finger on one moment: the day they read the 1993 report of the analysis of Greenland ice cores. Before that, almost nobody confidently believed that the climate could change massively within a decade or two; after the report, almost nobody felt sure that it could not. So wasn\\'t the preceding half-century of research a waste of effort? If only scientists had enough foresight, couldn\\'t they have waited until they were able to get good ice cores and settle the matter once and for all with a single unimpeachable study? The actual history shows that even the best scientific data are never that definitive. People can see only what they find believable. Over the decades, many scientists who looked at tree rings, varves, ice layers, and such had held evidence of decade-scale climate shifts before their eyes. They easily dismissed it. There were plausible reasons to dismiss global calamity as nothing but a crackpot fantasy. Sometimes the scientists\\' assumptions were actually built into their procedures: When pollen specialists routinely analyzed their clay cores in 10-cm slices, they could not possibly see changes that took place within a centimeter\\'s worth of layers. If the conventional beliefs had been the same in 1993 as in 1953--that significant climate change always takes many thousands of years--the short-term fluctuations in ice cores would have been passed over as meaningless noise. First, scientists had to convince themselves, by shuttling back and forth between historical data and studies of possible mechanisms, that rapid shifts made sense, with the meaning of \"rapid\" gradually changing from millennia to centuries to decades. Without that gradual shift of understanding, the Greenland cores would never have been drilled. The funds required for those heroic projects became available only after scientists reported that climate could change in damaging ways on a time scale meaningful to governments. In an area as difficult as climate science, in which all is complex and befogged, it takes a while to see what one is not prepared to look for. This article is based on The Discovery of Global Warming by Spencer Weart (Harvard U. Press, 2003) and was supported in part by the NSF program in history and philosophy of science. A more complete account, with full bibliographic references to the scientific work, may be found in hypertext online at http://www.aip.org/history/climate/rapid.htm . Spencer Weart ( sweart@aip.org ) directs the Center for History of Physics at the American Institute of Physics. References 1. National Academy of Sciences, Committee on Abrupt Climate Change, Abrupt Climate Change: Inevitable Surprises , National Academy Press, Washington, DC (2002), pp. 1, 16. Available online at http://books.nap.edu/books/0309074347/html . 2. C. E. P. Brooks, Q. J. R. Meteorol. Soc. 51 , 83 (1925). 3. See, for example, R. F. Flint, Am. J. Sci. 253 , 249 (1955). 4. H. E. Suess, Science 123 , 355 (1956). 5. W. S. Broecker, M. Ewing, B. C. Heezen, Am. J. Sci. 258 , 429 (1960). 6. M. Ewing, W. L. Donn, Science 123 , 1061 (1956). 7. W. S. Broecker, Science 151 , 299 (1966). 8. D. A. Barreis, R. A. Bryson, Wis. Archeologist 46 , 204 (Dec. 1965). 9. J. M. Mitchell Jr, Quaternary Res. 2 , 436 (1972). 10. National Research Council, US Committee for the Global Atmospheric Research Program, Understanding Climatic Change: A Program for Action , National Academy of Sciences, Washington, DC, (1975), appendix A. 11. For beetle data, see G. R. Coope, Philos. Trans. R. Soc. London, Ser. B 280 , 313 (1977); for albedo feedback theory, see M. I. Budyko, EOS Trans. Am. Geophys. Union 53 , 868 (1972); for a discussion on ocean circulation, see K. Bryan, M. J. Spelman, J. Geophys. Res. 90 , 11679 (1985) [INSPEC] ; for ice-sheet theory, see H. Flohn, Quaternary Res. 4 , 385 (1974). 12. W. Dansgaard et al., Science 218 , 1273 (1982) [INSPEC] . 13. For a summary and popular account, see P. A. Mayewski, F. White, The Ice Chronicles: The Quest to Understand Global Climate Change , University Press of New England, Hanover, N.H. (2002). 14. Intergovernmental Panel on Climate Change, Climate Change 2001--The Scientific Basis: Contribution of Working Group I to the Third Assessment Report of the Intergovernmental Panel on Climate Change , J. T. Houghton et al., eds., Cambridge U. Press, New York (2001), p. 420. Available online at http://www.ipcc.ch/pub/reports.htm . 15. Current thinking is reviewed in R. B. Alley et al., Science 299 , 2005 (2003) [INSPEC] . 16. See ref. 1, p. 121. 17. E. Moores, quoted in J. McPhee, Annals of the Former World , Farrar, Straus and Giroux, New York (1998), p. 605. 18. W. Dansgaard et al., in The Late Cenozoic Glacial Ages , K. K. Turekian, ed., Yale U. Press, New Haven, Conn. (1971), chap. 3. © 2003 American Institute of Physics Free this month The Discovery of Rapid Climate Change South Dakota Governor Pushes for Underground Lab as Homestake Water Rises Science Fashions and Scientific Fact Obituary: Robert Lull Forward Letters Also this month The Discovery of Rapid Climate Change A Topological Look at the Quantum Hall Effect Quantum Physics Under Control Sponsored links For your conference travel needs, try: Hotels London Hotels Miami Hotels Reno Hotels Boston Hotels Orlando Hotels Vancouver Hotels Chicago Hotels Las Vegas Hotels Toronto Hotels About Physics Today Contact Us FAQ Disclaimer Terms and Conditions Privacy Policy Jump to .. Home Find a Job Post a Job Place An Ad Subscribe Past Contents Event Calendar ___________________ Feature Articles Physics Update Letters Reference Frame Search & Discovery Issues & Events Opinion Books New Products We Hear That Obituaries \\n\\t\\n\\n-->\\n\\n\\n\\nThe Discovery of Rapid Climate Change - Physics Today\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\');\\xa0\\ndocument.write(\\'\\');\\ndocument.close();\\n//-->\\n\\n\\xa0\\n  \\xa0\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nadvanced search\\n\\n\\n\\n\\xa0 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTable \\n            of contents\\n\\n\\nPast \\n            issues\\n\\n\\n\\n\\n\\nLinks \\n            to advertisers\\n\\n\\nProducts \\n            advertised\\n\\n\\nPlace \\n            an ad\\n\\n\\nBuyers\\' \\n      guide\\n\\n\\n\\n\\n\\nAbout \\n            us\\n\\n\\nContact \\n            us\\n\\n\\nSubmit press release\\n\\n\\n\\n\\n\\nAmerican Institute \\n            of Physics\\n\\n\\nThe Industrial \\n            Physicist\\n\\n\\nComputing in \\n            Science & Engineering\\n\\n\\nJournals\\n\\n\\nVirtual \\n            Journals\\n\\n\\n\\n\\nArticle\\n\\n\\n\\n\\nThe Discovery of Rapid Climate Change\\nOnly within the past decade have researchers\\n              warmed to the possibility of abrupt shifts in Earth\\'s climate.\\n              Sometimes, it takes a while to see what one is not prepared to\\n              look for.\\nSpencer Weart\\n\\n            How fast can our planet\\'s climate change? Too slowly for humans to\\n            notice, according to the firm belief of most scientists through much\\n            of the 20th century. Any shift of weather patterns, even the Dust\\n            Bowl droughts that devastated the Great Plains in the 1930s, was\\n            seen as a temporary local excursion. To be sure, the entire world\\n            climate could change radically: The ice ages proved that. But common\\n            sense held that such transformations could only creep in over tens\\n            of thousands of years.\\n             In the 1950s, a few scientists found evidence that some of the\\n              great climate shifts in the past had taken only a few thousand\\n              years. During the 1960s and 1970s, other lines of research made\\n              it plausible that the global climate could shift radically within\\n              a few hundred years. In the 1980s and 1990s, further studies reduced\\n              the scale to the span of a single century. Today, there is evidence\\n              that severe change can take less than a decade. A committee of\\n              the National Academy of Sciences (NAS) has called this reorientation\\n              in the thinking of scientists a veritable \"paradigm shift.\" The\\n              new paradigm of abrupt global climate change, the committee reported\\n              in 2002, \"has been well established by research over the last decade,\\n              but this new thinking is little known and scarcely appreciated\\n              in the wider community of natural and social scientists and policymakers.\"1\\n Much earlier in the 20th century, some specialists had evidence\\n              of abrupt climate change in front of their eyes. The evidence was\\n              meaningless to them. To appreciate change occurring within 10 years\\n              as significant, scientists first had to accept the possibility\\n              of change within 100 years. That, in turn, had to wait until they\\n              accepted the 1000-year time scale. The history of this evolution\\n              gives a good example of the stepwise fashion in which science commonly\\n              proceeds, contrary to the familiar heroic myths of discoveries\\n              springing forth in an instant. The history also suggests why, as\\n              the NAS committee worried, most people still fail to realize just\\n              how badly the world\\'s climate might misbehave.\\n            \\nWas a 1000-year climate change possible?\\n During the early decades of the 20th century, a very few meteorologists\\n              did speculate about possibilities for rapid change. The most striking\\n              scenario was offered in 1925 by the respected climate expert C.\\n              E. P. Brooks, who suggested that a slight change of conditions\\n              might set off a self-sustaining shift between climate states. Suppose,\\n              he said, some random decrease of snow cover in northern latitudes\\n              exposed dark ground. Then the ground would absorb more sunlight,\\n              which would warm the air, which would melt still more snow--a vicious\\n              feedback cycle. An abrupt and catastrophic rise of tens of degrees\\n              was conceivable, Brooks wrote, \"perhaps in the course of a single\\n              season.\"2 Run the cycle backward,\\n              and an ice age might suddenly descend.\\n             Most other professional climatologists dismissed the idea as\\n              preposterous. The continental glaciers of an ice age, a kilometer\\n              thick, would surely require vast lengths of time to build up or\\n              melt away. Beyond that elementary reasoning lay a deeper rejection\\n              of all such speculations. It was the climatologists\\' trade to compile\\n              statistics on past weather in order to advise a farmer what crops\\n              to grow or tell an engineer what sort of floods were likely over\\n              the lifetime of a bridge. The climatologist\\'s career thus rested\\n              on a conviction that the experience of the recent past reliably\\n              described future conditions. That belief was supported by a paucity\\n              of data, for hardly any accurate records of daily temperatures\\n              and the like went back more than half a century or so. The limitation\\n              scarcely worried climatologists, who assumed that significant changes\\n              took place only over thousands of years. In their textbooks, climate\\n              was introduced as the long-term average of weather over time, by\\n              definition, static over centuries.\\n             The experts held a traditional belief that the natural world\\n              is self-regulating: If anything started to perturb a grand planetary\\n              system like the atmosphere, natural forces would automatically\\n              compensate. Scientists came up with various plausible self-regulating\\n              mechanisms. For example, if temperatures rose, then more water\\n              would evaporate from the seas; in response, clouds would thicken\\n              and reflect more sunlight, which would restore normal temperatures.\\n              The perception of self-regulation reflected a view of the world\\n              held deeply in almost every human culture: Stability was guaranteed,\\n              if not by Divine Providence, then by the suprahuman power of a\\n              benevolent \"balance of nature.\"\\n             Those beliefs were not disturbed by the few long-term climate\\n              records available at the time. The best of those data were compiled\\n              in the 1920s by an Arizona astronomer, Andrew Ellicott Douglass,\\n              who noted that the rings in trees were thinner in dry years. Analyzing\\n              old logs, Douglass reported a major century-long climate perturbation\\n              around the 17th century. But most other scientists doubted that\\n              tree rings (if they reflected climate at all) gave information\\n              about anything beyond random regional variations.\\n             Signs of climate shifts were also visible in varves, a Swedish\\n              word for the layers laid down each year in the mud on the bottom\\n              of northern lakes. From bogs and outcrops where the beds of fossil\\n              lakes were exposed, or from cores of slick clay drilled out of\\n              living lakes, the layers were painstakingly counted and measured.\\n              Ancient pollen told what plants had lived in the region when the\\n              layers were laid down. Major changes in vegetation suggested that\\n              the last ice age had not ended with a uniformly steady warming,\\n              but with peculiar oscillations of temperature. Scandinavian data\\n              revealed a particularly striking shift around 12 000 years ago,\\n              when a warm period gave way to a spell of bitterly cold weather,\\n              dubbed the Younger Dryas, after Dryas octopetala, a hardy\\n              Arctic flower whose pollen signals frigid tundra. In 1955, the\\n              timing was pinned down by a radiocarbon-dating study, which revealed\\n              that the temperature change had been rapid; for climate scientists\\n              at midcentury, \"rapid\" meant a change that took place over as little\\n              as 1000 years.3\\n Ice-age changes over a thousand years or so in a restricted region,\\n              although surprising, seemed acceptable. The rate of advance and\\n              retreat of the great glaciers would be no faster than present-day\\n              mountain glaciers were seen to move. That perception was compatible\\n              with the so-called uniformitarian principle, a geological tenet\\n              that the forces that molded ice, rock, sea, and air did not vary\\n              over time. Through most of the 20th century, the uniformitarian\\n              principle was cherished by geologists as the very foundation of\\n              their science: How could one study anything scientifically unless\\n              the rules stayed the same? The idea had become central to their\\n              training and theories during a century of disputes, when scientists\\n              painfully gave up traditions that explained certain geological\\n              features by invoking Noah\\'s Flood or other supernatural interventions.\\n              In human experience, temperatures apparently did not rise or fall\\n              radically in less than millennia, so the uniformitarian principle\\n              declared that such changes had never happened in the past. Scientists\\n              found themselves insisting on this principle as they were confronted,\\n              time and again, by cranks and religious fundamentalists who publicly\\n              proclaimed ideas about apocalyptic global cataclysms.\\n             Something resembling catastrophic climate jumps could in fact\\n              show up in varves. But the silt layers could have been distorted\\n              in countless ways that had nothing to do with climate: a forest\\n              fire perhaps, or a shift of stream drainage. Scientists saw the\\n              jumps not as climate data to be analyzed, but as mere local noise.\\n              They did not worry about the fact that old radiocarbon dates were\\n              accurate only within a thousand years or so, so that the chronologies\\n              of different sites could not be matched well enough to point to\\n              any rapid and widespread change.\\n             \\n\\n\\n\\n\\n\\nFigure\\n                    1\\n\\n\\n            In 1956, studying variations in the shells of plankton that were\\n            embedded in cores of clay pulled from the deep seabed (see figure\\n            1), radiocarbon expert Hans Suess discovered what was at the\\n            time the fastest change that anyone expected. Suess reported that\\n            the last glacial period had ended with a relatively rapid rise of\\n            temperature, about 1°C per thousand years.4 It\\n            scarcely bothered him and his colleagues that no faster change could\\n            have been seen in most cores. In many places, the mud was constantly\\n            stirred by burrowing worms or by seafloor currents and slumping,\\n            which blurred any differences between layers. Yet the data curves\\n            did sharpen as cores were pulled from regions of rapid deposition\\n            and as radiocarbon dating improved. By 1960, a trio of scientists\\n            at what is now the Lamont-Doherty Observatory--Wallace Broecker,\\n            Maurice Ewing, and Bruce Heezen--were reporting a variety of evidence,\\n            from deep-sea and lake deposits, that a global climate shift of as\\n            much as 5-10°C had taken place in less than a thousand years.5 Most\\n            of their colleagues found such a rise barely plausible.\\n            \\nMaking sense of rapid change\\n Evidence of a climate shift could only be accepted if it made\\n              sense--that is, if there existed some plausible theory of the climate\\n              system that could explain the shift. Broecker suspected that the\\n              cause might be a rapid turnover of North Atlantic ocean waters,\\n              but that was just hand-waving speculation. More influential was\\n              a 1956 paper by Ewing and William Donn, who built an elaborate\\n              model for the coming and going of ice ages.6 Like\\n              Brooks and others before them, Ewing and Donn began with the notion\\n              that a retreat of reflective snow and ice would bring more warming\\n              by sunlight. Their new idea was that the feedback mechanism had\\n              a hair trigger set off by ocean currents. As ice sheets melted\\n              and the sea level rose, warm water would spill into the Arctic\\n              Ocean and melt its ice cover, thus speeding up the warming. But\\n              once the Arctic Ocean was free of ice, they argued, so much moisture\\n              would evaporate that snow would fall heavily all around the Arctic,\\n              switching the feedback to cooling. Ewing and Donn thought it conceivable\\n              that the polar ocean might become ice-free and launch us into a\\n              new ice age within the next few hundred years.\\n             Journalists alerted the public to the risk of a glacial advance\\n              within the foreseeable future. People were prepared to believe\\n              it, for they were already abandoning their old ideas about an imperturbable\\n              balance of nature. The headlong advances of population and industry\\n              were making themselves felt in ever more widespread pollution.\\n              More ominous still was the global radioactive fallout from nuclear\\n              weapons tests, alongside scientists\\' warnings that a nuclear war\\n              could wreck the entire planet. It was no longer inconceivable that\\n              some perturbation--even one produced from human industry--might\\n              alter the entire planet.\\n             In fact, Ewing and Donn\\'s theory was erroneous, as other scientists\\n              quickly pointed out. Nevertheless, it had served a useful function.\\n              For the first time, there was respectable scientific backing for\\n              a picture of rapid, even disastrous, climate change. Other scientists,\\n              even as they rejected the theory, were stimulated to broaden their\\n              thinking and to inspect data for new kinds of information.\\n             Further stimulation came from entirely different studies. In\\n              the late 1950s, a group led by Dave Fultz at the University of\\n              Chicago carried out tabletop \"dishpan\" experiments in which they\\n              used a rotating fluid to simulate the circulation of the atmosphere.\\n              They created a simulacrum complete with a miniature jet stream\\n              and cyclonic storms. But when they perturbed the rotating liquid\\n              with a pencil, they found that the circulation pattern could flip\\n              between distinct modes. If the actual atmospheric circulation did\\n              that, weather patterns in many regions would shift almost instantly.\\n              In the early 1960s, climatologist Mikhail Budyko in Leningrad got\\n              disturbing results on a still larger scale from some simple equations\\n              for Earth\\'s energy budget. His calculations indicated that feedbacks\\n              involving snow cover could indeed bring extraordinary climate changes\\n              within a short time. Other geophysical models turned up more possibilities\\n              for rapid change.\\n            \\n \\n\\n\\n\\n\\n\\nFigure\\n                    2\\n\\n\\n            The most influential idea for what might bring rapid change was developed\\n            from old speculations about the circulation of the North Atlantic\\n            Ocean. In 1966, Broecker (pictured in figure\\n            2), taking a close look at deep-sea cores, reported evidence\\n            for an \"abrupt transition between two stable modes of operation of\\n            the ocean-atmosphere system.\"7 Nowadays,\\n            warm tropical water flows northward near the surface of the Atlantic;\\n            a large quantity, heavy with cold and salt, sinks near Iceland and\\n            returns southward in the deep. A change of temperature or salinity\\n            might shut down the circulation, cut off the northward transport\\n            of a huge amount of heat, and bring severe climate change. Simple\\n            numerical models involving the transport of fresh water by a changed\\n            pattern of winds showed that such a change could be self-sustaining.\\n             At the University of Wisconsin-Madison, Reid Bryson scrutinized\\n              entirely different types of data. In the late 1950s, he had been\\n              struck by the wide variability of climates as recorded in the varying\\n              width of tree rings. He was also familiar with the dishpan experiments\\n              that showed how a circulation pattern might change almost instantaneously.\\n              To take a new, interdisciplinary look at climate, Bryson brought\\n              together a group that even included an anthropologist who studied\\n              the ancient Native American cultures of the Midwest. From radiocarbon-dated\\n              bones and pollen, they deduced that a prodigious drought had struck\\n              the region in the 1200s--the very period when flourishing towns\\n              of the Mound Builders had gone into decline. Compared to that drought,\\n              the Dust Bowl of the 1930s had been mild and temporary. By the\\n              mid-1960s, Bryson was announcing that \"climatic changes do not\\n              come about by slow, gradual change, but rather by apparently discrete\\n              \\'jumps\\' from one atmospheric circulation regime to another.\"8 His\\n              group further reported pollen studies showing a rapid shift around\\n              10 500 years ago; by \"rapid\" they meant a change in the mix of\\n              tree species within less than a century. Perhaps the Younger Dryas\\n              was not just a local Scandinavian anomaly.\\n             Still, no major climate change was required to transform any\\n              particular forest. Many experts continued to believe it was sheer\\n              speculation to imagine that the climate of a region, let alone\\n              of the entire world, could change in less than a thousand years\\n              or so. But confirmation of changes at that rate, at least, was\\n              coming from a variety of studies. As the respected climatologist\\n              J. Murray Mitchell Jr explained in 1972, in place of the old view\\n              of \"a grand, rhythmic cycle,\" the new evidence showed a \"much more\\n              rapid and irregular succession\" in which Earth \"can swing between\\n              glacial and interglacial conditions in a surprisingly short span\\n              of millennia (some would say centuries).\"9\\n\\n \\n\\n\\n\\n\\n\\nFigure\\n                    3\\n\\n\\n            The most convincing evidence came from a long core of ice drilled\\n            at Camp Century, Greenland, by Willi Dansgaard\\'s Danish group, in\\n            cooperation with Americans led by Chester Langway Jr. The proportions\\n            of different oxygen isotopes in the layers of ice gave a fairly straightforward\\n            record of temperature. Mixed in with the expected gradual cycles\\n            were what the group called \"spectacular\" shorter-term shifts, including\\n            the Younger Dryas oscillation. Some of the shifts seemed to have\\n            taken as little as a century or two (see figure\\n            3).\\n             During the early 1970s, most climate experts came to agree that\\n              interglacial periods tended to end more abruptly than had been\\n              supposed. Many concluded that the current warm period could end\\n              in a rapid cooling, possibly even within the next few hundred years.\\n              Bryson (pictured in figure 4),\\n              Stephen Schneider, and a few others took this new concern to the\\n              public. They insisted that the climate we had experienced in the\\n              past century or so, mild and equable, was not the only sort of\\n              climate the planet knew. For all anyone could say, the next decade\\n              might start a plunge into a cataclysmic freeze, drought, or other\\n              change unprecedented in recent memory, although not without precedent\\n              in the archaeological and geological record.\\n            \\n \\n\\n\\n\\n\\n\\nFigure\\n                    4\\n\\n\\n            Cooling was not the only change that experts were starting to worry\\n            about. Since the late 1950s, attentive scientists had acknowledged\\n            the potential value of the old idea that human emissions of carbon\\n            dioxide gas (CO2) might lead to global warming. (See Physics\\n            Today, January 1997, page 34.) Most experts assumed that if such\\n            a greenhouse-effect warming did occur, it would come as they expected\\n            for any climate change--gradually over the course of a few centuries.\\n            But some suggested swifter possibilities. In 1972, pursuing his calculations\\n            of ice-cover feedbacks, Budyko declared that, at the rate we were\\n            pumping CO2 into the atmosphere, the ice covering the\\n            Arctic Ocean might melt entirely by 2050. And glacier experts were\\n            developing models that suggested how warming might cause the ice\\n            sheets of Antarctica to break up swiftly and shock the climate system.\\n            Bryson and others worked harder than ever to bring their concerns\\n            to the attention of the broader scientific community and the public.\\n             Most scientists spoke more cautiously. When leading experts had\\n              to state a consensus opinion, as in a 1975 NAS report on climate\\n              research,10 they reported that they\\n              saw nothing that would bring anything beyond relatively small changes\\n              that would take centuries or longer to develop. They did warn that\\n              there could be significant noise, the usual irregularities of weather\\n              patterns. And they admitted that they might have failed to recognize\\n              some mechanisms of change. If there was a threat, experts in the\\n              1970s could not agree whether it was from global warming or cooling.\\n              The one thing that all scientists agreed on was that they were\\n              seriously ignorant about how the climate system worked. So the\\n              only step they recommended to policymakers was to pursue research\\n              more aggressively.\\n            \\nJumps within centuries--or less\\n In the late 1970s and early 1980s, a variety of new data revealed\\n              surprising climate shifts. To take one example, a study of beetles\\n              that had been preserved in peat bogs since the end of the last\\n              glacial epoch turned up changes in the mix of species; those changes\\n              represented climate shifts of 3°C in well under 1000 years.\\n              Meanwhile, computer modelers produced plausible calculations for\\n              rapid climate shifts involving snow-cover feedbacks, a shutdown\\n              of North Atlantic circulation, or ice-sheet collapse.11 During\\n              the 1980s, the list of plausible mechanisms grew. Perhaps a rise\\n              in global temperature would cause methane to bubble out of the\\n              vast expanses of boggy tundra. Because methane is a greenhouse\\n              gas that blocks heat radiation even more effectively than CO2,\\n              such a release would cause even more warming in a vicious feedback\\n              cycle. Or what about the clathrates--peculiar ices that lock up\\n              huge volumes of methane in the muck of seabeds? Perhaps those would\\n              disintegrate and release greenhouse gases.\\n             Many scientists continued to look on such speculations as little\\n              more than science fiction. The evidence for rapid shifts, as it\\n              sometimes turned up in odd data sources like bog beetles, was never\\n              entirely convincing. Any single record could be subject to all\\n              kinds of accidental errors. The best example of a problem was in\\n              the best data on climate shifts, the odd wiggles in measurements\\n              from the Camp Century core. Those data came from near the bottom\\n              of the hole. Skeptics argued that the ice layers there, squeezed\\n              tissue-thin, were folded and distorted as they flowed over the\\n              bedrock.\\n             To get more reliable data, the ice drillers went to a second\\n              location, some 1400 kilometers distant from Camp Century. By 1981,\\n              after a decade of tenacious labor, they hit bedrock and extracted\\n              gleaming cylinders of ice 10 cm in diameter and more than two km\\n              deep; the deepest ice came from the last ice age, 14 000 years\\n              ago. The ratios of oxygen isotopes within the ice layers gave a\\n              temperature record showing what the researchers called \"violent\" changes.\\n              The most prominent of those, corresponding to the Younger Dryas\\n              oscillation, showed \"a dramatic cooling of rather short duration,\\n              perhaps only a few hundred years.\"12\\n Since the 1950s, jumps had persistently turned up in weather\\n              and climate models, whether built from rotating dishpans or from\\n              sets of equations run through computers. Scientists could have\\n              dismissed those models as too crude to say anything reliable--but\\n              the historical data showed that the notion of radical climate instability\\n              was not absurd after all. And scientists could have dismissed the\\n              jumps in the scattered data as artifacts, due to merely regional\\n              changes or simple errors--but the models showed that global jumps\\n              were physically plausible.\\n            \\n \\n\\n\\n\\n\\n\\nFigure\\n                    5\\n\\n\\n            Nevertheless, experts were scarcely prepared for the shock that came\\n            from the Greenland ice plateau in 1993. Plans had been laid to drill\\n            at the summit of the ice cap, where irregularities due to the deep\\n            flow of ice would have been minimal. Early hopes for a new cooperative\\n            program joining Americans and Europeans broke down and each team\\n            drilled its own hole, some 3 km deep (see figure\\n            5). Competition was transmuted into cooperation by a decision\\n            to put the two boreholes just far enough apart (30 km) so that anything\\n            that showed up in both cores must represent a real climate effect,\\n            not an artifact due to bedrock conditions. The match turned out to\\n            be remarkably exact for most of the way down. The comparison between\\n            cores showed convincingly that climate could change more rapidly\\n            than almost any scientist had imagined. Swings of temperature that\\n            were believed in the 1950s to take tens of thousands of years, in\\n            the 1970s to take thousands of years, and in the 1980s to take hundreds\\n            of years, were now found to take only decades. Greenland had sometimes\\n            warmed a shocking 7°C within a span of less than 50 years. More\\n            recent studies have reported that, during the Younger Dryas transition,\\n            drastic shifts in the entire North Atlantic climate could be seen\\n            within five snow layers, that is, as little as five years!13\\n Studies of pollen and other indicators--at locations ranging\\n              from Ohio to Japan to Tierra del Fuego, and dated with greatly\\n              improved radiocarbon techniques--suggested that the Younger Dryas\\n              event affected climates around the world. The extent of the climate\\n              variations was controversial (and to some extent remains so). Likewise\\n              uncertain was whether such variations could occur not only in glacial\\n              times, but also in warm periods like the present. Computer modelers,\\n              now fully alerted to the delicate balance of salinity and temperature\\n              that drove the North Atlantic circulation, found that global warming\\n              might bring future changes in precipitation that could shut down\\n              the current heat transport. The 2001 report of the Intergovernmental\\n              Panel on Climate Change, pronouncing the official consensus of\\n              the world\\'s governments and their climate experts, reported that\\n              a shutdown in the coming century was \"unlikely\" but \"cannot be\\n              ruled out.\" If such a shutdown did occur, it would change climates\\n              all around the North Atlantic--a dangerous cooling brought on by\\n              global warming.14\\n Now that the ice had been broken, so to speak, most experts were\\n              prepared to consider that rapid climate change--huge and global\\n              change--could come at any time. \"The abrupt changes of the past\\n              are not fully explained yet,\" wrote the NAS committee in its 2002\\n              report, \"and climate models typically underestimate the size, speed,\\n              and extent of those changes. Hence, . . . climate surprises are\\n              to be expected.\"1 Despite the profound\\n              implications of this new viewpoint, hardly anyone rose to dispute\\n              it.15\\n Although people did not deny the facts head-on, many denied them\\n              more subtly by failing to revise their accustomed ways of thinking. \"Geoscientists\\n              are just beginning to accept and adapt to the new paradigm of highly\\n              variable climate systems,\" wrote the NAS committee. And beyond\\n              geoscientists, \"this new paradigm has not yet penetrated the impacts\\n              community\"--the economists and other specialists who try to calculate\\n              the consequences of climate change.16 Policymakers\\n              and the public lagged even farther behind in grasping what the\\n              new scientific view could mean. As a geologist once remarked, \"To\\n              imagine that turmoil is in the past and somehow we are now in a\\n              more stable time seems to be a psychological need.\"17\\n\\nA gradual discovery process\\n How abrupt was the discovery of abrupt climate change? Many climate\\n              experts would put their finger on one moment: the day they read\\n              the 1993 report of the analysis of Greenland ice cores. Before\\n              that, almost nobody confidently believed that the climate could\\n              change massively within a decade or two; after the report, almost\\n              nobody felt sure that it could not. So wasn\\'t the preceding half-century\\n              of research a waste of effort? If only scientists had enough foresight,\\n              couldn\\'t they have waited until they were able to get good ice\\n              cores and settle the matter once and for all with a single unimpeachable\\n              study?\\n             The actual history shows that even the best scientific data are\\n              never that definitive. People can see only what they find believable.\\n              Over the decades, many scientists who looked at tree rings, varves,\\n              ice layers, and such had held evidence of decade-scale climate\\n              shifts before their eyes. They easily dismissed it. There were\\n              plausible reasons to dismiss global calamity as nothing but a crackpot\\n              fantasy. Sometimes the scientists\\' assumptions were actually built\\n              into their procedures: When pollen specialists routinely analyzed\\n              their clay cores in 10-cm slices, they could not possibly see changes\\n              that took place within a centimeter\\'s worth of layers. If the conventional\\n              beliefs had been the same in 1993 as in 1953--that significant\\n              climate change always takes many thousands of years--the short-term\\n              fluctuations in ice cores would have been passed over as meaningless\\n              noise.\\n             First, scientists had to convince themselves, by shuttling back\\n              and forth between historical data and studies of possible mechanisms,\\n              that rapid shifts made sense, with the meaning of \"rapid\" gradually\\n              changing from millennia to centuries to decades. Without that gradual\\n              shift of understanding, the Greenland cores would never have been\\n              drilled. The funds required for those heroic projects became available\\n              only after scientists reported that climate could change in damaging\\n              ways on a time scale meaningful to governments. In an area as difficult\\n              as climate science, in which all is complex and befogged, it takes\\n              a while to see what one is not prepared to look for.\\n             This article is based on The Discovery of Global Warming by\\n                Spencer Weart (Harvard U. Press, 2003) and was supported in part\\n                by the NSF program in history and philosophy of science. A more\\n                complete account, with full bibliographic references to the scientific\\n                work, may be found in hypertext online at http://www.aip.org/history/climate/rapid.htm. \\n\\n\\n\\n Spencer Weart (sweart@aip.org)\\n                directs the Center for History of Physics at the American Institute\\n                of Physics.\\n\\n\\n\\nReferences\\n 1. National Academy of Sciences, Committee on Abrupt\\n              Climate Change, Abrupt Climate Change: Inevitable Surprises,\\n              National Academy Press, Washington, DC (2002), pp. 1, 16. Available\\n              online at http://books.nap.edu/books/0309074347/html.\\n 2. C. E. P. Brooks, Q. J. R. Meteorol. Soc. 51,\\n              83 (1925).\\n 3. See, for example, R. F. Flint, Am. J. Sci. 253,\\n              249 (1955).\\n 4. H. E. Suess, Science 123, 355 (1956).\\n 5. W. S. Broecker, M. Ewing, B. C. Heezen, Am.\\n                J. Sci. 258, 429 (1960).\\n 6. M. Ewing, W. L. Donn, Science 123,\\n              1061 (1956).\\n 7. W. S. Broecker, Science 151, 299\\n              (1966).\\n 8. D. A. Barreis, R. A. Bryson, Wis. Archeologist 46,\\n              204 (Dec. 1965).\\n 9. J. M. Mitchell Jr, Quaternary Res. 2,\\n              436 (1972).\\n10. National Research Council, US Committee for the\\n              Global Atmospheric Research Program, Understanding Climatic\\n              Change: A Program for Action, National Academy of Sciences,\\n              Washington, DC, (1975), appendix A.\\n11. For beetle data, see G. R. Coope, Philos. Trans.\\n                R. Soc. London, Ser. B 280, 313 (1977); for albedo\\n                feedback theory, see M. I. Budyko, EOS Trans. Am. Geophys.\\n                Union 53, 868 (1972); for a discussion on ocean circulation,\\n                see K. Bryan, M. J. Spelman, J. Geophys. Res. 90,\\n                11679 (1985)\\xa0[INSPEC];\\n                for ice-sheet theory, see H. Flohn, Quaternary Res. 4,\\n                385 (1974).\\n12. W. Dansgaard et al., Science 218,\\n              1273 (1982)\\xa0[INSPEC].\\n13. For a summary and popular account, see P. A. Mayewski,\\n              F. White, The Ice Chronicles: The Quest to Understand Global\\n              Climate Change, University Press of New England, Hanover, N.H.\\n              (2002).\\n14. Intergovernmental Panel on Climate Change, Climate\\n                Change 2001--The Scientific Basis: Contribution of Working Group\\n                I to the Third Assessment Report of the Intergovernmental Panel\\n                on Climate Change, J. T. Houghton et al., eds., Cambridge\\n                U. Press, New York (2001), p. 420. Available online at http://www.ipcc.ch/pub/reports.htm.\\n15. Current thinking is reviewed in R. B. Alley et\\n              al., Science 299,\\n              2005 (2003)\\xa0[INSPEC].\\n16. See ref. 1, p. 121.\\n17. E. Moores, quoted in J. McPhee, Annals of the\\n                Former World, Farrar, Straus and Giroux, New York (1998),\\n                p. 605.\\n18. W. Dansgaard et al., in The Late Cenozoic Glacial\\n                Ages, K. K. Turekian, ed., Yale U. Press, New Haven, Conn.\\n                (1971), chap. 3.\\n\\xa0 \\n            \\n© 2003 American Institute of Physics\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\nFree this month\\n\\n\\nThe\\n                          Discovery of Rapid Climate Change\\n\\n\\nSouth\\n                          Dakota Governor Pushes for Underground Lab as Homestake\\n                          Water Rises\\n\\n\\nScience\\n                          Fashions and Scientific Fact\\n\\n\\nObituary:\\n                          Robert Lull Forward\\n\\n\\nLetters\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAlso this month\\n\\n\\nThe\\n                          Discovery of Rapid Climate Change\\n\\n\\nA Topological Look\\n                          at the Quantum Hall Effect\\n\\n\\nQuantum\\n                          Physics Under Control\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSponsored links\\n\\n\\n\\n\\nFor your conference\\n              travel needs, try:\\n\\n\\nHotels\\nLondon Hotels\\nMiami Hotels\\nReno Hotels\\nBoston\\n                          Hotels\\nOrlando\\n                          Hotels\\nVancouver\\n                          Hotels\\nChicago\\n                          Hotels\\nLas Vegas\\n                          Hotels\\nToronto\\n                        Hotels\\n\\n\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\n\\n\\nAbout Physics Today\\xa0\\xa0 Contact Us\\xa0\\xa0 \\n                        FAQ\\n\\n\\nDisclaimer\\xa0\\xa0 \\n                        Terms and \\n                        Conditions\\xa0\\xa0\\xa0Privacy \\n                        Policy\\xa0 \\n\\n\\n\\n\\n\\n\\n\\nJump to .. Home Find a Job Post a Job Place An Ad Subscribe\\nPast \\n\\n        Contents Event Calendar ___________________ Feature Articles\\nPhysics \\n\\n        Update Letters Reference Frame\\nSearch & \\n\\n        Discovery Issues & Events\\nOpinion\\nBooks New Products\\nWe Hear That\\nObituaries\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " \"\\n\\n\\n\\n\\nA First Artificial Network\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Next: The Perceptron Paradigm\\n Up: 27-642 Artificial Neural Networks \\n Previous: 27-642 Artificial Neural Networks \\n\\n\\n\\n\\nSubsections\\n\\nMcCulloch and Pitts, 1943\\n\\nThe Setting\\nThe Premises\\nTranslation\\nSome Example Neurons\\nDegrees of Inhibition\\nRepercussions\\n\\n\\n\\n\\nA First Artificial Network\\n\\n\\nIn previous lectures we have seen a general overview of Artificial Neural\\nNetworks (ANNs) and the relation of the discipline to Artificial Intelligence \\n(AI).  This lectures explores the development of one particular network design\\nfrom its origins to its current incarnation.\\n\\n\\n\\nMcCulloch and Pitts, 1943\\n\\n\\n\\nThe Setting\\n\\n\\nbeginnings of single cell recordings\\nno intracellular recordings\\nionic and electrical basis of neural activity unclear\\ndominant observation:  ``all or none action potential''\\nno understanding of how thought, intelligence, reasoning, etc., \\n\\toccurred in the neural substrate\\nmodels of intelligence largely based in logic\\n\\n\\nWarren S. McCulloch and Walter Pitts, ``A logical calculus of the \\nideas immanent in nervous activity'', Bulletin of Mathematical Biophysics,\\n5: 115-133.\\n\\n\\n\\nThe Premises\\n\\nAll based on 1943 understandings of neuro-biology.\\n\\n\\n\\n1.\\nThe activity of the neuron is an ``all-or-none'' process.\\n2.\\nA certain fixed number of synapses must be excited within the period\\nof latent addition in order to excite a neuron at any time, and this \\n\\tnumber is independent of previous activity and position on the\\n\\tneuron.\\n3.\\nThe only significant delay within the nervous system is synaptic delay.\\n4.\\nThe activity of any inhibitory synapse absolutely prevents excitation\\n\\tof the neuron at that time.\\n5.\\nThe structure of the net does not change with time.\\n\\n\\nGoal:  to postulate a mathematical model of networks of neurons that was \\nbiologically accurate and could account for high level cognitive tasks\\n(logic).\\n\\n\\n\\nTranslation\\n\\nThe mathematical formalism used is very ornate.  Minsky (1967), used a\\nmore straightforward formulation to develop an entire theory of computation.\\n\\n\\n\\n1.\\nThe activity is -1 or +1.\\n2.\\nExcitatory connections:\\n\\n\\n\\n\\n\\n\\n(1)\\n\\n\\n3.\\nConnections are associated with a delay.\\n4.\\nIf an neuron with an inhibitory connection to this neuron is active (+1),\\n\\tthen this neuron does not fire (-1).\\n5.\\nSystem is static.\\n\\n\\n\\nSome Example Neurons\\n\\n\\nNo excitatory connections, bi = -1, 1 inhibitory connection.\\n\\n\\nNOT.\\n\\n\\nTwo excitatory connections, \\n\\nwi,1 = 1, wi,2 = 1, \\n\\nbi = -1/2,\\n\\tno inhibitory connections.\\n\\n\\nOR.\\n\\n\\nTwo excitatory connections, \\n\\nwi,1 = 1, wi,2 = 1, \\n\\nbi = +1/2,\\n\\tno inhibitory connections.\\n\\n\\nAND.\\n\\n\\nWith NOT, OR, and AND gates, any logic circuit can be created.\\n\\n\\n\\nDegrees of Inhibition\\n\\nMcCulloch and Pitts also showed that networks with degrees of inhibition\\nhave equivalent computational power.  This means for every net with \\nall-or-nothing inhibition, there exists a different net with degrees of \\ninhibition that does exactly the same thing, and vice versa.  How would\\nwe do this construction?\\n\\n\\n\\nRepercussions\\n\\n\\ninfluenced von Neumann (1945)\\nMinsky (1967) theory of computation\\nbasis for biological models of neurons\\nled to generalized artificial neuron models used today (first \\n\\t\\tperceptron)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Next: The Perceptron Paradigm\\n Up: 27-642 Artificial Neural Networks \\n Previous: 27-642 Artificial Neural Networks \\n\\n\\n\\n1999-01-26\\n\\n\\n\\n\",\n",
       " '\\n\\nSlashdot | Open Source Computer Algebra Systems\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0OSDN\\xa0:\\xa0Browse SourceForge\\xa0-\\xa0Shop ThinkGeek\\xa0-\\xa0freshmeat Downloads\\xa0-\\xa0Newsletters\\xa0-\\xa0Personals\\nSearch\\xa0»\\xa0\\n\\xa0X\\xa0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0Login\\n\\n\\n\\n\\n         \\xa0Why Login?\\n         \\xa0Why Subscribe?\\n\\n\\n\\n\\n\\xa0Sections\\n\\n\\xa0Main\\n \\n\\xa0Apache\\n \\n\\xa0Apple\\xa0\\xa02 more\\n \\n\\xa0Askslashdot\\xa0\\xa06 more\\n \\n\\xa0Books\\n \\n\\xa0BSD\\xa0\\xa01 more\\n\\n\\n\\xa0Developers\\n \\n\\xa0Games\\xa0\\xa08 more\\n \\n\\xa0Interviews\\n \\n\\xa0Science\\xa0\\xa01 more\\n \\n\\xa0YRO\\xa0\\xa02 more\\n\\n\\xa0\\n\\n\\xa0Help\\n\\n\\n\\n      \\xa0FAQ\\n      \\xa0Bugs\\n\\n\\n\\n\\xa0Stories\\n\\n\\n\\n      \\xa0Old Stories\\n      \\xa0Old Polls \\n      \\xa0Topics \\n      \\xa0Hall of Fame \\n      \\xa0Submit Story\\n\\n\\n\\n\\xa0About\\n\\n\\n\\n       \\xa0Supporters\\n       \\xa0Code\\n       \\xa0Awards\\n\\n\\n\\n\\xa0Services\\n\\n\\n\\n       \\xa0Tech Jobs\\n       \\xa0Advertising\\n       \\xa0PriceGrabber\\n       \\xa0Personals\\n\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\nOpen Source Computer Algebra Systems\\n\\n\\n\\n\\n\\nPosted by\\ntimothy\\non Sat Mar 30, \\'02 06:19 AM\\nfrom the add-divide-multiply-sum dept.\\ntimdaly writes \"A while back Slashdot had an article decrying\\nthe lack of a good open source computer algebra\\nsystem. That is changing. There is a \\nconference scheduled for the end of May to define the development model and strategy for future work. Students of math, science and engineering will find this valuable. If you\\'re tired of hacking open source editors and want something with a real technical challenge this is the area for you.\"\\n\\n\\n\\n\\n\\n\\xa0\\n\\xa0\\n\\n\\n\\n\\nSlashdot Login\\n\\n\\n\\n\\nNickname:\\n\\nPassword:\\n\\n\\n\\n\\n\\n[ Create a new account ]\\n\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\nRelated Links\\n\\n\\n\\ntimdaly\\nlack of a good open source computer algebra\\nsystem\\n\\nconference\\nMore on Programming\\nAlso by timothy\\n\\n\\n\\n\\n\\n\\nDevelopers\\n\\n\\n\\n\\n· Announcing Cooperative Linux· freedesktop.org xlibs 1.0 Released· Open Source Operating System For Smart Cards· Man Page Project Can Now Use Official POSIX Docs· Are 64-bit Binaries Slower than 32-bit Binaries?· Perens on Patents· Effect of Using 64-bit Pointers?· MySQL Official GUI Interface· Creating A Mobile News Portal· Crack the Code and Win a Million Bucks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\n\\n\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\\tThis discussion has been archived.\\n\\t\\t\\t\\tNo new comments can be posted.\\n\\t\\t\\t\\n\\t\\t\\n\\t\\n\\n\\n\\nOpen Source Computer Algebra Systems\\n\\n\\n\\t|\\n\\n\\n\\t\\t\\nLog in/Create an Account\\n\\n\\n\\n\\t\\t| Top\\n\\n\\n\\n\\n\\n\\t\\t| 36 comments\\n\\n\\n\\n\\n\\n\\n\\t\\t| \\nSearch Discussion\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\tThreshold: \\n-1: 36 comments\\n0: 36 comments\\n1: 28 comments\\n2: 11 comments\\n3: 3 comments\\n4: 1 comments\\n5: 0 comments\\n\\nFlat\\nNested\\nNo Comments\\nThreaded\\n\\nOldest First\\nNewest First\\nHighest Scores First\\nOldest First (Ignore Threads)\\nNewest First (Ignore Threads)\\n\\n\\n\\n\\n\\n\\n\\nThe Fine Print:\\nThe following comments are owned by whoever posted them.\\nWe are not responsible for them in any way.\\n\\n\\nwhat will it include? (Score:1)\\n\\t\\t\\t\\t\\nby abdulla (523920) \\n\\t\\t\\t\\ton Saturday March 30, @06:32AM (#3253025)\\n\\t\\t\\t\\t(Last Journal: Wednesday April 23, @06:01AM)\\n\\n\\n\\t\\t\\t\\tmaybe i\\'m not understanding it properly, but will this be a large maths library? if so what will it include exactly?\"\\n\\t\\t\\t\\nRe:what will it include? by timdaly (Score:1) Saturday March 30, @12:02PM \\nRe:what will it include? by njdj (Score:2) Saturday March 30, @04:47PM \\nRe:Maxima by timdaly (Score:1) Saturday March 30, @05:07PM \\n \\n\\nGiNaC (Score:1)\\n\\t\\t\\t\\t\\nby sultanoslack (320583) \\n\\t\\t\\t\\ton Saturday March 30, @07:41AM (#3253120)\\n\\t\\t\\t\\t(http://www.slackorama.net)\\n\\n\\n\\t\\t\\t\\tWell, though it stands for GiNaC is Not a CAS (Computer Algebre System).  GiNaC [ginac.de], which is a library for advanced symbolic calculations certainly would make a fine backend for a CAS.  It\\'s a pretty powerful tool wainging for an interface.\\n\\t\\t\\t\\nRe:GiNaC, C++ and Aldor by timdaly (Score:2) Saturday March 30, @01:15PM \\n \\n\\nBad Idea (Score:2, Insightful)\\n\\t\\t\\t\\t\\nby cperciva (102828) \\n\\t\\t\\t\\ton Saturday March 30, @10:34AM (#3253698)\\n\\t\\t\\t\\t(http://www.daemonology.net/)\\n\\n\\n\\t\\t\\t\\tA computer algebra system which is built in a bazaar is a Bad Idea.With an operating system, it isn\\'t all that critical if it crashes occasionally.  Ok, it\\'s a nuisance... but it won\\'t go unnoticed, and someone will track down why it\\'s crashing, and it will get fixed.Computer algebra systems are rather more prone to undiscovered errors.  It\\'s very easy, for example, to write a long integer multiplication routine which works perfectly for integers less than 2^20 digits long, but starts to fail (deterministically, but without obvious pattern) for very rare inputs above that size.  In a bazaar, where code is accepted from anyone, you\\'re very likely to see this sort of buggy code get introduced.With a closed (commercial) or pseudo-closed (not necessarily commercial, but within a university where everyone has scrutinized each other\\'s credentials) environment, such errors are far less likely to exist.  Computational mathematicians are paranoid about such errors; computational mathematicians will not introduce a piece of code unless they can *prove* that it will work.Given enough eyes, all detected bugs will be fixed; but actually detecting those bugs in the first place is far from certain.\\n\\t\\t\\t\\n \\n\\nRe:Bad Idea (Score:4, Insightful)\\n\\t\\t\\t\\t\\nby swillden (191260) \\n\\t\\t\\t\\ton Saturday March 30, @11:25AM (#3253912)\\n\\t\\t\\t\\t \\n\\t\\t\\t\\n\\n Computer algebra systems are rather more prone to undiscovered errors. [...] In a bazaar, where code is accepted from anyone, you\\'re very likely to see this sort of buggy code get introduced.\\n With a closed (commercial) or pseudo-closed (not necessarily commercial, but within a university where everyone has scrutinized each other\\'s credentials) environment, such errors are far less likely to exist. [...] computational mathematicians will not introduce a piece of code unless they can *prove* that it will work.\\n Given enough eyes, all detected bugs will be fixed; but actually detecting those bugs in the first place is far from certain.\\nNo offense, but you don\\'t understand the open source process.  I know of no open source project anywhere in which \"code is accepted from anyone\".  Patches can be offered by anyone, but there are gatekeepers that decide what is and is not allowed into the source tree.  For a project like this, I would expect the gatekeepers to be computational mathematicians and I would expect that they would reject out of hand any non-trivial submission that isn\\'t accompanied by a formal proof of correctness.\\n\\nFurther, it wouldn\\'t surprise me if *this* project used some extensive and formal peer review process, so that before (or maybe after) the gatekeeper accepts a submission there\\'s oversight from a group of other mathematicians.\\n\\nI think open source is *perfect* for this sort of application.  It\\'s certainly much more likely to produce correct code than a closed source shop where only a small number of people can review the code and there are marketing people trying to push the product out the door.\\n\\t\\t\\t\\n\\n\\n\\t\\t\\t\\t\\t[ Parent\\n ]\\n\\t\\t\\t\\t\\t\\n\\n\\nRe:Bad Idea by cperciva (Score:2) Saturday March 30, @12:36PM \\nRe:Bad Idea by swillden (Score:2) Saturday March 30, @05:51PM \\nRe:Bad Idea by timdaly (Score:1) Saturday March 30, @11:41AM \\nRe:Bad Idea by xerofud (Score:1) Sunday April 07, @01:35AM \\nRe:Bad Idea by mmusn (Score:2) Monday April 01, @01:03AM \\n2 replies\\n beneath your current threshold. \\n\\n MAXIMA (Score:1)\\n\\t\\t\\t\\t\\nby Jess (11386)  <gehinjcNO@SPAMalum.mit.edu>\\n\\t\\t\\t\\ton Saturday March 30, @11:28AM (#3253933)\\n\\t\\t\\t\\t \\n\\t\\t\\t\\n\\n\\t\\t\\t\\tThere is alread an open source (GPL) computer algebra and calculus system available, called MAXIMA.  It\\'s based on MACSYMA.\\n\\nFor more info check out:\\n\\nhttp://maxima.sourceforge.net [sourceforge.net]\\n\\nI\\'ve used it for some basic things, and it seems to be quite powerful.  Not as full-featured as MAPLE or Mathematica, but covers most of the common needs for a CAS.\\n\\t\\t\\t\\nRe: MAXIMA by DGolden (Score:1) Sunday March 31, @02:37PM \\nRe: MAXIMA by sir99 (Score:1) Tuesday April 02, @11:13AM \\n \\n\\nRPMS (redhat 7.2) of mathematics packages (Score:3, Informative)\\n\\t\\t\\t\\t\\nby rdieter (112462)  <`rdieter\\' `at\\' `math.unl.edu\\'>\\n\\t\\t\\t\\ton Saturday March 30, @12:51PM (#3254365)\\n\\t\\t\\t\\t(http://www.math.unl.edu/~rdieter/ | Last Journal: Friday June 21, @02:57PM)\\n\\n\\n\\t\\t\\t\\tI have a collection of self-built RPMS for mathematics packages to try out if you like.  Included are: drgenius, gap, geomview, gtkmathview, Macaulay2, maxima (a minor pain \\'cause it depends on lisp), and others available at  \\n\\nhttp://www.math.unl.edu/~rdieter [unl.edu].  Enjoy.\\n\\t\\t\\t\\n \\n\\nHmmm.... (Score:1)\\n\\t\\t\\t\\t\\nby redhatbox (569534)  <redhatbox AT myrealbox DOT com>\\n\\t\\t\\t\\ton Saturday March 30, @07:46PM (#3257212)\\n\\t\\t\\t\\t \\n\\t\\t\\t\\n\\nI think they call it \"FORTRAN\", or something like that... ;)(yes, it\\'s a joke) \\n\\nRe:Hmmm.... by fparnold (Score:1) Thursday April 04, @12:03PM \\n \\n\\nLisp && Prolog (Score:1)\\n\\t\\t\\t\\t\\nby NeuroMorphus (463324)  <.neuromorphus. .at. .yahoo.com.>\\n\\t\\t\\t\\ton Sunday March 31, @02:12AM (#3258971)\\n\\t\\t\\t\\t(http://slashdot.org/)\\n\\n\\n\\t\\t\\t\\tLISP and Prolog anyone?\\n\\n1 reply\\n beneath your current threshold. \\n\\nFrontends to computer algebra systems (Score:1)\\n\\t\\t\\t\\t\\nby Joris van der Hoeven (566677) \\n\\t\\t\\t\\ton Sunday March 31, @05:48PM (#3262111)\\n\\t\\t\\t\\t(http://www.math.u-psud.fr/~vdhoeven)\\n\\n\\n\\t\\t\\t\\tFor anyone interested in free computer algebra systems, I would like to point to GNU TeXmacs [texmacs.org], whose version 1.0 has just been released. This program can be used as a frontend for most free computer algebra systems. Please contact us if you want to help us supporting other systems or improving the current interfaces (or adding interfaces to other scientific computation systems).\\n\\t\\t\\t\\n \\n\\nMathemagix (Score:2, Interesting)\\n\\t\\t\\t\\t\\nby Joris van der Hoeven (566677) \\n\\t\\t\\t\\ton Sunday March 31, @05:53PM (#3262141)\\n\\t\\t\\t\\t(http://www.math.u-psud.fr/~vdhoeven)\\n\\n\\n\\t\\t\\t\\tWe plan to write a compiler for a new computer algebra system called mathemagix [www.mathemagix].\\nWe are searching potential contributors or\\npeople who would just like to give us some\\nuseful suggestions; the development is still\\nin a very early stage due to my work on TeXmacs.\\n\\t\\t\\t\\n2 replies\\n beneath your current threshold. \\n\\nMaxima (Score:2)\\n\\t\\t\\t\\t\\nby mmusn (567069) \\n\\t\\t\\t\\ton Monday April 01, @01:10AM (#3264343)\\n\\t\\t\\t\\t \\n\\t\\t\\t\\n\\n\\t\\t\\t\\tLet me put in another word for Maxima.  It is based on Macsyma, and it is very powerful.  \\n\\nStill, people have learned quite a bit about algorithms, software engineering, and software reuse since Macsyma, Maple, and Mathematica were originally implemented, and it might well be worth thinking about having another go at a modern computer algebra system.  I suspect that implementing it in something like ML or Haskell might help a lot with correctness and extensibility.\\n\\t\\t\\t\\n \\n\\nRe:Development model (Score:1)\\n\\t\\t\\t\\t\\nby timdaly (539918) \\n\\t\\t\\t\\ton Saturday March 30, @11:54AM (#3254063)\\n\\t\\t\\t\\t \\n\\t\\t\\t\\n\\n\\t\\t\\t\\tThere is a subtle point here. There will be manydifferent computer algebra systems presented andthey each have a community.I don\\'t expect therewill be an uber-system that converges them. Butit is well worth discussing the future directionsand shared concerns (like a common front-end (e.g.TeXmacs)), common test suites, common graphicspackages. I also expect the license issues tobe wonderfully warmth-generating :-)\\n\\t\\t\\t\\n\\n\\n\\t\\t\\t\\t\\t[ Parent\\n ]\\n\\t\\t\\t\\t\\t\\n\\n\\nRe:Development model by Anonymous Coward (Score:1) Saturday March 30, @04:26PM \\nRe: GAP by timdaly (Score:1) Saturday March 30, @04:52PM \\n1 reply\\n beneath your current threshold.Re:Development model by joto (Score:3) Saturday March 30, @05:24PM \\nRe:Development model by Anonymous Coward (Score:1) Sunday March 31, @12:26AM \\n2 replies\\n beneath your current threshold.\\n\\n\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\xa0 \\n\\nAd astra per aspera.\\n\\t[To the stars by aspiration.]\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n All trademarks and copyrights on this\\n  page are owned by their respective owners.  Comments\\n  are owned by the Poster.\\n  The Rest © 1997-2003 OSDN.\\n\\n\\n\\n\\n [\\n\\t\\thome |\\n\\t\\tawards |\\n\\t\\tcontribute story |\\n\\t\\tolder articles |\\n\\t\\tOSDN |\\n\\t\\tadvertise |\\n\\t\\tself serve ad system |\\n\\t\\tabout |\\n\\t\\tterms of service |\\n\\t\\tprivacy |\\n\\t\\tfaq |\\n\\t\\trss ]\\n\\t\\n\\n\\n\\n\\n',\n",
       " '\\n\\n\\n\\n\\t\\t\\tBayesian Networks Pragmatics\\n\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\tI.D.I.S.\\n\\t\\t\\t\\t\\t\\n\\n\\n\\t\\t\\t\\t\\t\\tArtificial Intelligence Resources\\n\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\tBayesian Networks\\n\\t\\t\\t\\t\\t\\tPragmatics\\n\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\t\\tThis page provides discussions on the pragmatics of building and\\n\\t\\tusing Bayesian Networks. Special thanks for Russ Greiner\\n\\t\\t\\t(greiner@scr.siemens.com)\\n\\t\\tfor gathering much of this information.\\n\\n\\n\\n\\n\\n\\n Table of Contents \\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\tWhy Bayesian Networks?\\n\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\tKnowledge Acquisition/Engineering\\n\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\tGeneral Discussion\\n\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\tWhy Bayesian Networks?\\n\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\nFrom Knowledge Industries \\nhome page -\\n\\n\\n\\nComparison with Rule-Based Expert Systems:\\n\\n\\t\\n\\n   [...], it is worth pointing out some differences between traditional\\n   expert systems and probability-based diagnosis systems.  In an expert\\n   system, the knowledge engineer attempts to capture the reasoning process of\\n   that an expert uses during diagnosis.  Probability-based systems, on the\\n   other hand, capture the expert\\'s qualitative physical understanding of the\\n   system under diagnosis and use this knowledge to construct diagnostic\\n   models.  While it is possible to capture diagnostic information directly in\\n   a probabilistic system, our experience shows that it is much faster and\\n   easier to assess causal models.\\n\\t\\n\\n\\n\\n\\n\\nEric Horvitz (horvitz@microsoft.com) writes:\\n\\n\\n\\n\\n  One of the strongest reasons is\\n  that uncertainty is indeed prevalent in the real world and it is\\n  critical to represent that uncertainty in at least a semi-coherent\\n  manner.   Another reason is that Bayesian networks are a fundamentally\\n  more modular representation of uncertain knowledge than rule-based\\n  systems.  This makes them easier to maintain, and to adapt them to\\n  different contexts, than there rule-based relatives.\\n\\t\\n  [...]\\n\\t\\n  The uncertain decision making context, and the modularity and ease of\\n  maintenance, make the Bayesian networks quite attractive.\\n\\t\\n\\n  [..]\\n\\n\\t\\n  Bayesian networks are very intuitive for nonexperts.   We had a United\\n  Airlines engine person drawing and debugging these models within a few\\n  hours of exposure to them.\\n\\n\\t\\n  The NASA Vista people needed to have a way to fuse together the\\n  implications of multiple sensors, some noisy, when we began the Vista\\n  project.  There was no good way to take into consideration all of the\\n  sensors and their failure modes with a fault tree.\\n\\n\\n\\n\\n\\nBruce D\\'Ambrosio (dambrosi@chert.cs.orst.edu) writes:\\n\\n\\n\\n\\n  I am currently doing some work with Litton Data Systems.  The strong\\n  arguments there are primarily technological: improved performance,\\n  better communication with experts (model-based reps are modular and\\n  easy to work with).\\n\\n\\n\\n\\n\\nStuart Russell (russell@cs.berkeley.edu) writes:\\n\\n\\t\\n\\n  The commonsense case for BNs over eg NNs is\\n\\t\\n the expert can provide knowledge in the form of causal structure\\n\\t\\t\\tquite easily\\n  \\t\\t the resulting network after training is understandable and\\n\\t\\t\\textensible and provides probabilities on variables of interest, and\\n\\t\\t\\tcan also be used easily with arbitrary missing data (in both\\n\\t\\t\\ttraining and use).\\n\\t\\n\\n\\n\\n\\n\\nRuss Greiner (greiner@scr.siemens.com) writes:\\n\\n\\t\\n I was told that many of the papers at the recent\\n\\n\\t\\t\\n NIPS\\'95 Workshop on Learning in Bayesian Networks and\\n\\t\\t\\t\\tOther Graphical Models\\n\\ncontained (often implicit) comparisions between Bayesian nets and\\nneural nets, especially in terms of learning.\\n\\n\\t\\t\\t\\t\\t\\n\\n\\n\\nIn response to \\n\\t\\n I am looking for a concrete BUSINESS arguments for using Bayesian\\n\\t\\t\\tnets. In particular, it would be useful to have a host of\\n\\t\\t\\ttestimonials, of the form [...]\\n\\t\\n\\nWray Buntine (wray@ptolemy-ethernet.arc.nasa.gov) writes:\\n\\n\\t\\n Note on what follows: I\\'m all in the question and the positive\\n\\t\\t\\tresponses, I\\'m just being flippant and trying to inject\\n\\t\\t\\ta different perspective.\\n\\n\\t\\t I always thought that concrete BUSINESS arguments for using\\nBayesian nets are rather like concrete BUSINESS arguments for using\\nthe product rule of calculus. Bayesian nets in the narrow sense are the kind of\\ngraph plus probability tables that have dominated UAI conferences\\nin previous years.  I\\'d have a hard time putting a case for these\\non anything but very specific narrow problems.   Bayesian networks in the\\nbroader sense of \"probabilistic networks\" are a set of\\nmethods and representation for probabilistic calculation that\\napplies to most problems with probabilities.  On any serious\\nsized problem, people will grind through similar operations anyway.\\nWhether you call this Bayesian nets or not is a matter of which \\nscientific tribe you belong to.   I would never recommend a business\\nuse probabilistic networks.   But I would recommend a business employ\\nR&D people with some good graduate training in probabilistic networks.\\n\\nSo instead, here are some more apt questions and answers I believe.\\nI\\'m going to replace the phrase \"Bayesian nets\", by the phrase\\n\"The Magic Button\" in what follows.   If you object to this\\nphrase, then use \"Genetic Algorithms\" or \"Neural Networks\" instead\\nor any of the other magic buttons that the research community is\\nbusily constructing for industry to push.\\n\\t\\n Q1. I am looking for a concrete BUSINESS arguments for \\n\\t  using The Magic Button.\\n\\t\\t A1. Put smart people on the job and have them use the right\\n\\t  tools.\\tDon\\'t necessarily use The Magic Button, but\\n\\t  if your specific problem has a need for the kind of \\n\\t  analysis that The Magic Button can help with, then\\n\\t  by all means push the The Magic Button.\\n\\ni.e.,   I believe there are no concrete BUSINESS arguments for\\n\\t\\tusing probabilistic networks in general.\\n\\t\\t Q2.   Should I employ an expert in The Magic Button to solve\\n\\t\\t\\tmy problem.\\n\\t\\t A2. Most likely \"no\". Most problems require a combination of\\n\\t  different skills, database work, systems programming,\\n\\t  Windows-UNIX interfaces, visualization, probabilistic reasoning.\\n\\t  Have the expert act as a consultant, maybe.\\n\\t\\n\\nNow this is rather quickly starting to sound like I believe\\nno-one should be using probabilistic networks.  Course I don\\'t\\nbelieve this.   For the following reasons.\\n\\nProbabilistic networks in the broader sense are the way to address issues of\\nprobabilistic calculation.   They are unavoidable.   If you are\\naddressing any problem with uncertainty (vision, image processing,\\ndatabase mining, diagnosis, etc.) then you are invariably\\ndoing something that is at least a rough approximation to probabilistic\\ncomputation.   So if your smart people are using the right tools,\\nthen they will be using variations of probabilistic networks anyway.\\nThe question is, whether they have been properly trained in the\\nmore elegant framework of probabilistic networks, or whether they grind\\nthrough the same kinds of calculations on their own.   My experience\\nis that applications out in industry aren\\'t solved properly because\\nFEW people are willing to go out and acquire the right tools\\nfor the particular problem they are\\nlooking at, and are rarely trained in the different neighboring fields\\nwhere they could get useful ideas.   Furthermore, experts in the\\nThe Magic Button run around looking for applications suited for their\\nMagic Button, and a large pool of applications exist that require\\na 5% input from The Magic Button A, another 5% from The Magic Button B,\\nanother 5% from The Magic Button C, and then 85% of good old sweat and toil.\\n\\nSo instead the relevant questions are:\\n\\t\\n Q3.  Are probabilistic networks the right tools for\\n\\t\\t\\taddressing uncertainty (in its many forms) ?\\n\\n\\t (Rhetorical question only, don\\'t want to start THIS discussion)\\n\\t\\t Q4.  If the answer to Q3 is yes, then why aren\\'t\\n\\t\\tprobabilistic networks \\n\\t a basic option for all computer scientists, statisticians, etc., in\\n\\t graduate school ?\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\tKnowledge Acquisition\\n\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\nEdward J. Wright (EJWright@smtpgate.read.tasc.com) writes:\\n\\n\\t\\n\\n  [We tried] to encode the \"rules\" for some simple photo-geology\\n  [within a public domain expert system shell].  But I could never get it to\\n  work.  Later I found another expert system shell that included certainty\\n  factors.  I could not get that to work either.\\n\\n\\n\\n  [I recently] used the same problem as a class project for a Bayesian network\\n  class.  I will emphasize that the system I built is a very simple subset of\\n  the photo-geology problem, but it does provide answers that are always\\n  reasonable and usually correct.  And I spent much less time building the\\n  Bayes net model then I did playing with the rule based systems.\\n\\n\\n\\n  I think the problem was too many interrelated variables and no\\n  way to deal with uncertain relationships, but this was no challenge\\n  for the Bayesian net.\\n\\n\\t\\t\\t\\t\\t\\n\\n\\n\\n\\nEric Horvitz (horvitz@microsoft.com) writes:\\n\\n\\t\\n [...] thought you might find reading\\n\\t\\t\\t[Henrion 87]\\n\\t\\t\\tinteresting, given your recent question on rationale for using\\n\\t\\t\\tinfluence diagrams and belief networks. [Also] a discussion of\\n\\t\\t\\tefficiency of knowledge acquisition and modularity and its\\n\\t\\t\\timplications for maintaining knowledge bases [can be found] in\\n\\t\\t\\t[Horvitz 88].\\n\\t\\n\\nRuss Greiner (greiner@scr.siemens.com) writes:\\n\\n\\t\\n\\n   As the title suggests, the first article compares the challenge of building\\n   an effective diagnostic system (for the diagnosis and treatment of root\\n   disorders of apple trees) within first a standard rule-based expert system\\n   framework (ES), and then an influence diagram (ID).\\n\\n\\t\\t\\n\\n   There were several relevant findings:\\n\\n\\t\\n The two systems produced had similar graphical structures, modulo\\n\\t\\t\\tthe directions of the links.\\n\\t\\t While an ID can, in principle, require a great many numbers, here\\n\\t\\t\\tthe expert had to specify only a few.  (Most of the values\\n\\t\\t\\tdegenerated to either 0 or 1, or were determined by other values.)\\n\\t\\t\\tMoreover, a sensitivity analysis suggests that even rough\\n\\t\\t\\tassessments were usually sufficient.\\n\\t\\t While it was easier to produce an *initial* ES than an ID (as the\\n\\t   rule-based ES requires fewer numbers, etc), the ES requires more\\n\\t   extensive testing, debugging and tuning, as the initial ES will\\n\\t   cover only the combinations that the expert explicitly anticipated.\\n\\t   This also means an ID can handle situations that the expert did\\n\\t   expect, perhaps better than the initial expert could have.\\n\\t\\n\\n  The second article is also superb.  Among other things, it discusses the\\n  need for handling uncertainties, motivates the use of probabilities and\\n  decision theory, and reviews some early attempts.  It also states that\\n\\t\\n \"although recent research on the application of decision-science\\n\\t\\t\\tseems promising, for the most part only prototype systems have been\\n\\t\\t\\tdemonstrated\"\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\t\\tGeneral Discussion\\n\\t\\n\\nExpert Systems and Probabilistic Network Models (1997)\\n          E. Castillo, J. M. Gutirrez, and A. S. Hadi.\\n          Springer-Verlag, New York.\\nSpanish version:\\nSistemas Expertos y Modelos de Redes Probabilsticas (1997)\\n          E. Castillo, J. M. Gutirrez, and A. S. Hadi.\\n          Monografas de la Academia Espa1ola de Ingeniera, Madrid.\\n\\t\\n\\n\\tSymbolic Propagation Algorithms:\\n\"Sensitivity Analysis in Discrete Bayesian Networks\" (1997)\\n          E. Castillo,J.M. Gutirrez, and A.S. Hadi\\n          IEEE Transactions on Man, Cybernetics and Systems 27(4), 403-412.\\n\"A new Method for Symbolic Propagation in Bayesian Networks\" (1996)\\n          E. Castillo,J.M. Gutirrez, and A.S. Hadi\\n          Networks 28, 31-43.\\n\\t\\n\\n\\tLearning:\\n\"Learning and Updating of Uncertainty in Dirichlet Models\" (1997)\\n          Castillo, E., Hadi, A. S., and Solares, C.\\n          Machine Learning, in press.\\n\\n\\n\\tModeling Continuous and Discrete-Continuous Models:\\n\"Modeling Networks of Discrete and Continuous Variables with an\\nApplication to\\n     Damage Assessment\" (1997)\\n          E. Castillo,J.M. Gutirrez, and A.S. Hadi\\n          Multivariate Analysis, in press.\\n\"Symbolic Propagation and Sensitivity Analysis in Gaussian Bayesian\\nNetworks\\n     with Application to Damage Assessment\" (1997)\\n          E. Castillo,J.M. Gutirrez, and A.S. Hadi\\n          Artificial Intelligence in Engineering 11, 173-181.\\n\\t\\n\\n\\n\\n\\n\\n\\tAdministrator:  Dr. Eugene Santos ---\\n\\teugene@cse.uconn.edu\\n \\n\\tWebmaster:  Mitchell Saba ---\\n\\t\\tsaba@cse.uconn.edu\\n\\n\\nLast Update: \\n\\n\\n\\n\\n',\n",
       " '\\n\\n\\nWhat is Artificial Intelligence\\n\\n\\n\\n\\nWhat is Artificial Intelligence?\\n\\n\\nBy\\n\\n\\nIstván S. N. Berkeley Ph.D.\\n\\n(istvan@USL.edu)\\n\\n\\n\\n\\nPhilosophy,\\n\\nThe University of Southwestern Louisiana\\n\\n\\n\\n\\nHistorical Perspective: It All Sounds So Good….\\n\\nProbably everybody has heard of Artificial Intelligence (AI for\\nshort), but relatively few people have a really good idea of what\\nthe term really means. For most people, AI is associated with\\nartifacts like the Hal 9000 Computer in the movie 2001: A Space\\nodyssey. Such images are the product of Hollywood, rather\\nthan the kind of thing which actually happens in the research\\nlabs of the world today. My purpose here is to introduce a few\\nof the basic ideas behind AI, and to try and offer a means by\\nwhich people can come to grips with the current state of the art\\nin the field.\\n\\nRoughly speaking, Artificial Intelligence is the study of man-made\\ncomputational devices and systems which can be made to act in\\na manner which we would be inclined to call intelligent. The birth\\nof the field can be traced back to the early 1950s. Arguably,\\nthe first significant event in the history of AI was the publication\\nof a paper entitled \"Computing Machinery and Intelligence\"\\nby the British Mathematician Alan Turing.\\nIn this paper, Turing argued that if a machine could past a certain\\ntest (which has become known as the \\'Turing test\\') then we would\\nhave grounds to say that the computer was intelligent. The Turing test\\ninvolves a human being (known as the \\'judge\\') asking questions\\nvia a computer terminal to two other entities, one of which is\\na human being and the other of which is a computer. If the judge\\nregularly failed to correctly distinguish the computer from the\\nhuman, then the computer was said to have passed the test. In\\nthis paper Turing also considered a number of arguments for, and\\nobjections to, the idea that computers could exhibit intelligence.\\n\\n\\nIt is commonly believed that AI was born as a discipline at a\\nconference called \"The Dartmouth Summer research Project on Artificial Intelligence\",\\norganized by amongst others, John McCarthy\\nand Marvin Minsky.\\nAt this conference a system known as LOGIC THEORIST was demonstrated\\nby Alan Newell and Herb Simon.\\nLOGIC THEORIST was a system which discovered proofs to theorems\\nin symbolic logic. The significance of this system was that, in\\nthe words of Feigenbaum and Feldman (1963: p. 108) LOGIC THEORIST\\nwas \"…the first foray by artificial intelligence into\\nhigh-order intellectual processes.\" This initial success\\nwas rapidly followed by a number of other systems which could\\nperform apparently intelligent tasks. For example, a system known\\nas \"DENDRAL\"\\nwas able to mechanize aspects of the scientific reasoning found\\nin organic chemistry. Another program, known as \"MYCIN\",\\nwas able to interactively diagnose infectious diseases.\\n\\nThe fundamental strategy which lay behind all these successes\\nled to the proposal of what is known as the Physical Symbol Systems\\nHypothesis, by Newell and Simon in 1976. The Physical Symbol System\\nHypothesis amounts to a distillation of the theory which lay behind\\nmuch of the work which had gone on up until that date and was\\nproposed as a general scientific hypothesis. Newell and Simon\\n(1976: p. 41) wrote;\\n\\n\"A physical symbol system has the necessary and sufficient\\nmeans for general intelligent action.\"\\n\\nAlthough there has been a great deal of controversy about exactly\\nhow this hypothesis should be interpreted, there are two important\\nconclusions which have been drawn from it. The first conclusion\\nis that computers are physical symbol systems, in the relevant\\nsense, and thus there are grounds (should the hypothesis be correct)\\nto believe that they should be able to exhibit intelligence. The\\nsecond conclusion is that, as we humans also are intelligent,\\nwe too must be physical symbol systems and thus are in a significant\\nsense, similar to computers.\\n\\nCurrent Perspective: The Problems and the Successes\\n\\nWith all these apparently positive results and interesting theoretical\\nwork, a fairly obvious question seems to be \\'Where are the intelligent\\nmachines, like the HAL 9000\\'? Although there have been many impressive\\nsuccesses in the field, there have also been a number of significant\\nproblems which AI research has run into. As yet, there is no HAL\\n9000 and realistically, it will be a good while before such systems\\nbecome available, if indeed they ever prove to be possible at\\nall. \\n\\nThe early successes in AI led researchers in the field to be wildly\\noptimistic. Unfortunately, the optimism was somewhat misplaced.\\nFor example, in 1957 Simon predicted that it would take only ten\\nyears for a computer to be the world\\'s chess champion. Of course,\\nthis particular feat was not accomplished until this year, by\\nthe Deep Blue system.\\nThere are deeper problems which AI has run into however.\\n\\nFor most people, if they know that President Clinton is in Washington,\\nthen they also know that President Clinton\\'s right knee is also\\nin Washington. This may seem like a trivial fact, and indeed it\\nis for humans, but it is not trivial when it comes to AI systems.\\nIn fact, this is an instance of what has come to be known as \\'The\\nCommon Sense Knowledge Problem\\'. A computational system only knows\\nwhat it has been explicity told. No matter what the capacities\\nof a computational system, if that system knows that President\\nClinton was in Washington, but doesn\\'t know that his left knee\\nis there too, then the system will not appear to be too clever.\\nOf course, it is perfectly possible to tell a computer that if\\na person is in one place, then their left knee is in the same\\nplace, but this is only the beginning of the problem. There are\\na huge number of similar facts which would also need to be programmed\\nin. For example, we also know that if President Clinton is in\\nWashington, then his hair is also in Washington, his lips are\\nin Washington and so on. The difficulty, from the perspective\\nof AI, is to find a way to capture all these facts. The Problem\\nof Common Sense Knowledge is one of the main reasons why we do\\nnot have as yet the intelligent computers predicted by science\\nfiction, like the HAL 9000.\\n\\nThe Problem of Common Sense Knowledge runs very deep in AI. For\\nexample, it would be very difficult for a computer to pass the\\nTuring test, if it lacked the kind of knowledge described above.\\nThe point can be illustrated by considering the case of ELIZA.\\nELIZA\\nis an AI system designed by Weizenbaum in 1966 which was supposed\\nto emulate a psychotherapist. There are many variants of this\\nsoftware these days, quite a few of which can be downloaded.\\nAlthough in some senses ELIZA can be quite impressive, it doesn\\'t\\ntake much to get the system confused, or off track. It becomes\\nclear very quickly that the system is far from intelligent.\\n\\nThere have been a number of responses to The Problem of Common\\nSense Knowledge within the AI research community. One strategy\\nis to attempt to build systems which are only designed to operate\\nin limited domains. This is the strategy which lies behind the\\nLoebner Prize,\\na modern day competition based upon a limited version of the Turing\\ntest. Some recent entries to this contest, such as the TIPS\\nsystem are really quite impressive, when compared to ELIZA. \\n\\nAnother more ambitious strategy has been adopted by AI researcher\\nDoug Lenat.\\nLenat and his colleagues have been working for a number of years\\non a system which is known as CYC.\\nThe goal of the CYC project is to develop a large computational\\ndatabase and search tools which enables AI systems to access all\\nthe knowledge which makes up common sense. The CYC project tries\\nto meet the Problem of Common Sense Knowledge head on. At the\\ncurrent time, the results of the project are just beginning to\\nemerge. It is not yet clear whether the massive effort has been\\na success.\\n\\nOther researchers have adopted a different tack to try and deal\\nwith the problem. They reason that human being have common sense,\\nbecause of the vast wealth of experiences which we have as we\\ngrow up and learn. They prefer to attempt to deal with the Problem\\nof Common Sense by adopting a machine learning\\nstrategy. Perhaps, if a computer could learn, in a manner similar\\nto a human being, the it too would develop common sense. This\\nstrategy is still being pursued and it is too early to tell if\\nit will be successful.\\n\\nAnother problem which AI research has run into is that tasks which\\nare hard for human beings, like mathematics, or playing chess,\\nturn out to be quite easy for computers. On the other hand, tasks\\nwhich human beings find easy, like learning to navigate through\\na room full of furniture, or recognizing faces, computers find\\ncomparatively hard to do. This has inspired some researchers to\\ntry and develop systems which have (at least superficially) brain-like\\nproperties. The research based upon this strategy has come to\\nbe known as the field of Artificial Neural Networks\\n(also called Connectionism),\\nand is currently one of the major specialist sub-areas within\\nAI. On interesting aspect of Artificial Neural Networks is that\\nmany of these systems also learn, thereby incorporating some of\\nthe advantages of the machine learning strategy to solving the\\nCommon Sense Knowledge Problem. Artificial Neural Network systems\\nhave been successful at solving many problems, such as those involving\\npattern recognition, which have proved hard for other approaches.\\n\\n\\nIt is important to realize though that not everybody accepts the\\npremises which AI research operates under. The whole project of\\nAI has come under sharp criticisms from time to time. One well-known\\ncritic is Herbert Dreyfus.\\nHe has argued on a variety of grounds that the whole enterprise\\nof AI is doomed to failure, as it makes assumptions about the\\nworld and minds which are not tenable, when critically assessed.\\nAnother well-known critic of AI is John Searle.\\nSearle has proposed an argument based on a thought experiment,\\nknown as the Chinese Room argument.\\nThis argument purports to show that the goal of building intelligent\\nmachines is not possible. Even though this argument was originally\\npublished in the 1980s, it is still a hot topic of discussion\\non internet newsgroups such as comp.ai.philosophy. \\n\\nWhether the critics of AI are correct or not, only time will tell.\\nHowever, there have been two important sets of consequences which\\nhave arisen since the initial inception of the field. The first\\nof these has been the birth of a new and exciting academic discipline\\nwhich  has come to be known as \\'Cognitive Science\\'.\\nCognitive Science shares with AI the fundamental premise that,\\nin some sense, mental activity is computational in nature. The\\ngoal of Cognitive Science though is different from that of AI.\\nCognitive scientists set themselves the goal of unraveling the\\nmysteries of the human mind. This is no small task, given that\\nthe human brain is the most complicated device known to mankind.\\nFor example, even when various simplifying assumptions are made,\\nit seem highly likely that the number of distinct possible states\\nof a single human brain is actually greater than the number of\\natoms of the Universe! Nonetheless, the lessons learned and progress\\nmade in the pursuit of the goal of AI, along with progress in\\nother disciplines, seem to show that project of Cognitive Science\\nis viable, though hard to attain. \\n\\nThe second set of consequences which have arisen from the study\\nof AI are perhaps a little less obvious. There are many programs\\nand systems around today which make use of the fruits of AI research.\\nAlthough we do not have a HAL 9000 as yet, many of the early goals\\nof AI have been achieved, albeit not in a single grand system.\\nPerhaps the saddest thing though is that AI seldom gets credit\\nfor its contribution to other areas. There is a saying in academic\\ncircles that \"The best fruits of AI, become plain old computer\\nscience\". As we learn to do more and more, what was once\\nalmost miraculous, becomes mundane. Now that the goal of a really\\nfine chess playing computer has been realized, it is likely that\\nthis too will no longer thrill or surprise us. However, there\\nare still many challenging and exciting frontiers to be conquered\\nwithin AI. There are also numerous thorny questions which need\\nto be thought through. In the articles which follow this one,\\nI will try and introduce some of the fascinating  work which is\\nbeing done in AI, so that the contribution of this research program\\nto the world as we know it will be better known and understood.\\n\\n\\n© István S. N. Berkeley Ph.D. 1997. All\\nrights reserved.\\n\\n\\nSuggested Further Reading\\n\\n\\nCampbell, J., (1989), The Improbable Machine, Simon &\\nSchuster (New York).\\n\\nCopeland, J. (1993), Artificial Intelligence, Blackwells\\n(Oxford).\\n\\nChurchland, P. (1988), Matter and Consciousness, MIT Press\\n(Cambridge, MA).\\n\\nHaugeland, J. (1985), Artificial Intelligence: The Very Idea,\\nMIT Press (Cambridge MA).\\n\\n\\nBibliography\\n\\nFeigenbaum, E. and Feldman, J. (1963), Computers and Thought,\\nMcGraw-Hill (New York).\\n\\nHaugeland, J. (1981) Mind Design, MIT Press (Cambridge,\\nMA).\\n\\nNewell, A. and Simon, H., (1976), \"Computer Science as Empirical\\nInquiry: Symbols and Search\" reprinted in Haugeland (1981:\\npp. 35-66).\\n\\n\\n\\n',\n",
       " '\\n\\n\\n\\nDaylight Clustering Manual\\n\\n\\nDaylight Clustering Manual\\n\\nDavid Weininger, Jack Delany\\nDaylight Version 4.82\\nRelease Date 06/16/03\\n\\nDaylight Chemical Information Systems, Inc.\\n\\nCopyright Notice\\n\\nThis document and the programs described herein are\\nCopyright © 1992-2003, Daylight Chemical Information\\nSystems, Inc. of Mission Viejo, CA .  Daylight explicitly\\ngrants permission to reproduce this document under the\\ncondition that it is reproduced in its entirety, including\\nthis notice.  All other rights are reserved.\\n\\n\\n\\n\\n\\nTable of Contents\\n\\n\\n\\n1.  Intended audience\\n\\n2.  Introduction to the Daylight Clustering \\nPackage\\n2.1.  Clustering Package contents\\n2.2.  Applicability\\n2.3.  Uses of structural clustering\\n3. Methodology\\n3.1.  Strucutural characterization\\n3.2.  Structural similarity\\n3.3.  Jarvis-Patrick clustering\\n3.4.  Cluster statistics\\n3.5.  Singletons\\n4.  Cluster package usage\\n4.1.  Relationships of Cluster package \\nprograms\\n4.2.  Preparing data for clustering\\n4.3.  Structural characterization\\n4.4.  Clustering\\n4.5.  Postprocessing and analysis\\n5.  Using clustering data with other Daylight\\nsoftware\\n6.  Maintaining clustered databases and \\nother advanced topics\\nA1.  Appendix: Examples\\nA2.  Appendix: References\\nA3.  Appendix: Release notes\\n\\n\\n\\n\\nDaylight Chemical Information Systems, Inc.\\ninfo@daylight.com\\n\\n',\n",
       " '\\n\\n\\n\\nFraunhofer IESE - Core Competencies: Experimentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWelcome\\n\\nto Fraunhofer IESE in Kaiserslautern, Germany - the leading competence center for applied research and technology transfer in experimental software engineering.\\nKeywords: Offerings, offer, institut, institute, kaiserslautern, kl, fraunhofer, iese, experimental software engineering, services, customers, develop, software, guidance, evaluation, quality improvement, engineering-based, state-of-the-art, development, processes, techniques, training, education, monitoring, help, individual assistance, products \\n\\n\\n\\n\\n\\n',\n",
       " '\\n\\n\\n\\ncomputer-magazin Nr. 10\\n\\n\\n\\n\\n\\n\\nTips & News \\n\\nPraxishandbuch abH \\n\\nVorankündigung: Die Bundesanstalt für\\nArbeit wird Ende 1996/Anfang 1997 ein Praxishandbuch \"Ausbildungsbegleitende\\nHilfen\" veröffentlichen. Autor: Christoph Eckardt.\\n\\n Dieses Handbuch entstand im Rahmen des vom Bundesministerium\\nfür Bildung, Wissenschaft, Forschung und Technologie finanzierten\\nProjektes \"Maßnahmen zur inhaltlichen Gestaltung der\\nBerufsausbildung benachteiligter Jugendlicher (Prozeßbegleitung)\".\\nDieses Projekt wurde durchgeführt vom Institut für berufliche\\nBildung, Arbeitsmarkt und Sozialpolitik (INBAS GmbH, Frankfurt/Main)\\n\\nEin Kapitel des Handbuches widmet sich dem \"Lernen\\nmit Computern\". Darin werden eine Reihe von Ansatzpunkten\\nfür die Nutzung von Personalcomputern in abH, Grundsätze\\nzum Einsatz von Lernprogrammen sowie Kategorien von Lernsoftware\\nvorgestellt. \\n\\n\\n\\nNeue Version des Computer-Programmes\\n\\n\"X - Auf der Suche nach der Zukunft\\n\\nAls CD-ROM ist kürzlich die Version 2 des oben\\ngenannten Computer-Programmes erschienen. Es wendet sich an die\\nZielgruppe der lernbehinderten Schüler/innen und dient der\\nBerufsorientierung. Es stellt  u.a. die Informationsmöglichkeiten\\nder Berufsinformationszentren der Berufsberatung vor. Das Programm,\\ndas in der Form eines Adventure-Spieles aufgebaut ist (Version\\n1 haben wir im Computer-Magazin Nr. 9 vorgestellt), ist jetzt\\nmit Soundressourcen ausgestattet. Das Programm ist bei der Berufsberatung\\nder Arbeitsämter oder beim Verlag erhältlich:\\n\\nDKF Multimedia - Münch & Partner GmbH,\\nMainzerstr. 35, 65239 Hochheim/Main, Telefon 06146-8347-0 Telefax\\n06146-8347-14 eMail: dkfmm@t-online.de  Internet: http://www.dkf.de/\\n(Wir danken Rudolf Weidmann von DKF Multimedia recht herzlich\\nfür die Bereitstellung der CD-ROM)\\n\\n\\ndidacta mit elektronischen Informationen.\\n\\nIm Vorfeld der Bildungsmesse didacta, die vom 17.\\nbis 21. Februar 1997 in Düsseldorf stattfindet, können\\nsich Interessierte vom dortigen Pressereferat mit speziellen Artikeln\\nversorgen lassen. Zum erstenmal werden diese Artikel als Manuskript\\noder alternativ auf Diskette (MS-DOS, WinWord 6.0) angeboten.\\nHier einige Textbeispiele: \\n\\n\"Schulen ans Netz\"; \\n\\n\"Lernen am Computer - hat das Schulbuch ausgedient?\";\\n\\n\\n\"Das duale Ausbildungssystem in wirtschaftlichen\\nKrisenzeiten\"; \\n\\n\"Weiterbildungsmaßnahmenwerden noch zu\\nselten auf ihren Nutzen hin überprüft. Betriebliche\\nWeiterbildung auf der didacta 97\".\\n\\n(Quelle: Pressereferat didacta 97, Bernd Kunzelmann)\\n\\n\\n\\n\\nErstes Europäisches  Stahlbaulehrprogramm\\n\\nWas ändert  sich durch die neuen Eurocodes im\\nStahl- und Verbundbau? Und was muß sich deshalb in der universitären\\nund betrieblichen Bildung verändern? Das erste  Stahlbau-Lehrprogramm\\nsoll  Hochschulen, Unternehmen und Weiterbildungseinrichtungen\\nbei  der Umstellung  helfen. Rund  220  Wissenschaftler und  Ingenieure\\naus  18 europäischen Staaten haben an dem Programm mitgearbeitet.\\nAuf einer  CD-ROM, die einem Textumfang von rund 6000 Seiten entspricht,\\nwerden alle  wichtigen Bereiche  des Stahlbaus  detailliert dargestellt.\\n\\nDarüber hinaus  bietet das  Programm mit 1000\\nDias und 21 Videokassetten anschauliches Unterrichtsmaterial.\\nWeitere Informationen zu dem Lernprogramm sind erhältlich\\nbeim Stahl-Informationszentrum, Breite  Straße 69,  40213\\nDüsseldorf, Tel.: 02 11 / 829-3 78, Fax: 02 11 /8 29-3 44.\\n\\n(Quelle: EU-AKTUELL, Februar 1996, eine Information\\ndes EURO INFO CENTER der HLT Hessen (c) HLT Wirtschaftsförderung\\nHessen, Investitionsbank AG, Abraham-Lincoln-Str. 38-42, 65189\\nWiesbaden)\\n\\n\\n\\n\"Die Bundesregierung informiert\" / \"Informationen\\nfür junge Leute\"\\n\\nDas Presse- und Informationsamt der Bundesregierung\\ninformiert nun auch umfassend in multimedialer Form auf CD-ROM.\\nDie CD-ROM \"Die Bundesregierung informiert\" bietet mit\\nText, Animationen, Ton und Videos aufbereitete Informationen zu\\nThemen aus der Politik. ... Hinzu kommt eine Reihe von elektronischen\\nInformationsmaterialien des Bundespresseamtes. ... Die CD-ROM\\n\"Informationen\" für junge Leute\" bietet fast\\nalles, was junge Leute von Ausbildung bis Zivildienst interessiert,\\nFacts und Unterhaltung. Bestellt werden können die CD-ROM\\'s\\n(kostenlos) beim: \\n\\nPresse- und Informationsamt der Bundesregierung,\\n53105 Bonn, Telefon 01805-22-1996 (rund um die Uhr), Telefax 01805-22-1997.\\nHinzu kommt die Bestellmöglichkeit in T-Online (*Bundesregierung#)\\nund über das Internet-Informationsangebot (http://www.bundesregierung.de).\\n\\n(Quelle: Sozialpolitische Umschau, hrsg. vom Presse-\\nund Informationsamt der Bundesregierung, Welckerstr. 11, 53113\\nBonn, Nr. 350/1996 vom 26. August 1996).\\n\\n\\nDr. Fluency\\n\\n - Computerprogramm soll Stotterern die Zunge lösen\\n\\n\"Alexander Wolff von Gudenberg sitzt vor dem\\ntragbaren PC und spricht ganz langsam in ein Mikrofon. Jede einzelne\\nSilbe dehnt er auf zwei Sekunden, der kleine Computer zeichnet\\nden merkwürdig anmutenden Singsang auf. Gudenberg erprobt\\neine neuartige Therapie, die mit Hilfe des Programmes \\'Dr. Fluency\\'\\nStotterern die Zunge lösen soll. Die Gesamthochschule Kassel\\ntestet das Verfahren in einem Forschungsprojekt für den deutschen\\nSprachraum.\\n\\nErfolgreiche Technik - die Technik wird seit über\\n20 Jahren erfolgreich in den USA angewandt, sagt  der 39jährige\\nGudenberg, der sie selbst in Virginia ausprobiert hat. Zunächst\\nlernen die Stotterer, einzelne Silben ganz langsam und bewußt\\nzu sprechen,  später wird das Sprechtempo erhöht. \\'Ich\\nkonnte das erste Mal in meinem Leben in der Kneipe eine Cola bestellen\\',\\nberichtet der promovierte Mediziner von seinen persönlichen\\nErfolgen. Vorher war er immer am \\'C\\' hängengeblieben, hatte\\neine unüberwindliche Blockade. Das eigentlich Neue an \\'Dr.\\nFluency\\' ist der Computereinsatz, der zumindest in Teilbereichen\\nden Sprecherzieher ersetzen kann. \\n\\nDie Ursache der Sprechstörung, an der in Deutschland\\nschätzungsweise 800.000 Menschen leiden, ist weitgehend unbekannt.\\nDie meisten deutschen Therapeuten versuchen, den Stotterer mit\\nseinem Manko zu versöhnen nach dem Motto: \\'Lerne stottern\\nohne zu leiden\\'. In den USA dagegen wird das Stottern eher als\\nTeilleistungsstörung wie etwa die Lese- und Rechtschreibschwäche\\nLegasthenie gesehen, die man zumindest zum Teil wegtrainieren\\nkann. Das von dem Israeli Arve Friedmann auf dieser Grundlage\\nentwickelte Programm bietet nun die Möglichkeit, auch außerhalb\\nder Therapie unter Anleitung gezielt zu Üben, in dem es je\\nnach Können Aufgaben vorgibt, die Ergebnisse festhält\\nund bewertet. Kollege Computer übernimmt noch eine weitere,\\nnicht zu unterschätzende Funktion: die Kontrolle. \\'Es ist\\nkein Problem, einen Stotterer dazu zu bringen, für eine Zeitlang\\nflüssig zu sprechen\\', meint der Kasseler Logopäde und\\nProjektleiter Rüdiger Rother. Schwierig sei vielmehr, den\\neinmal erzielten Erfolg festzuhalten. Das Programm speichert daher\\nbei jeder Übungseinheit die genaue Dauer sowie Erfolge und\\nMißerfolge der Benutzer. Hoffen auf Besserung: einige der\\nzunächst fünf Probanden des Projektes haben bereits\\nmehrere Therapieversuche hinter sich und erzielten dabei durchaus\\nErfolge. \\'Früher bin ich Gesprächen ausgewichen, heute\\nstürze ich mich darauf\\', sagt Barbara Stahl. Da sie aber\\nnach wie vor stottert, hofft sie nun auf  \\'Dr. Fluency\\'.\"\\n\\n(Quelle: dpa, Christian Ebner in einem Artikel\\nfür den Trierischen Volksfreund, 09. Mai 1996)\\n\\n\\n\\nDeutscher Bildungssoftware-Preis auf der 48. Frankfurter Buchmesse\\n1996\\n\\nDie Gemeinschaftsinitiative von \"bild der wissenschaft\",\\n\"Institut für Bildung in der Informationsgesellschaft\"\\n(IBI) und \"Stiftung Lesen\" hat ihre Preise vergeben.\\n\\nIn der Sparte \"Berufliche Aus- und Fortbildung\"\\nhat für den Bereich Wirtschaft das Programm \"cabs. 3.0\"\\nden \"digita 96\" gewonnen. Version \"professional\",\\nVersion \"advanced\". Entwicklung der Technologie unter\\nVerantwortung von Thomas Lehnert, Jörg Neubauer - Virtual\\nManagement Simulation Software GmbH, Klever Straße 84, 40477\\nDüsseldorf; Preise (Direktvertrieb): professional DM 2499,-\\nzzgl. MwSt.; advanced: DM 599,- Schüler, Auszubildende, Studenten:\\n349,- DM\\n\\ncabs ist ein innovatives multimediales Lern- und\\nSimulationsprogramm für die Anwendung und Vertiefung betriebswirtschaftlicher\\n(Grund)-Kenntnisse in der beruflichen Aus- und Weiterbildung an\\nSchulen, Hochschulen und in Unternehmen. Es ermöglicht handlungsorientiertes\\nLernen auf hohem Niveau: Planspiele aus wesentlichen Managementbereichen\\nvermitteln die gesamte Komplexität unternehmerischen Handelns.\\nDie jeweiligen Kenntnisse des Lernenden werden hier in übergreifende\\nZusammenhänge gestellt. Der Anwender kann durch selbst erstellte\\nFallstudien eigene Schwerpunkte setzen. cabs eignet sich für\\n\"Selbstlerner\" ebenso wie für Lerngruppen, wobei\\nhier der Anreiz hinzukommt, in Konkurrenz zu anderen Nutzern agieren\\nzu können.\\n\\nIn der Sparte \"Berufliche Aus- und Fortbildung\"\\ngab es im Bereich \"Technik\" den folgenden Gewinner:\\n\"FluidSIM Pneumatik\", Entwicklung: Universität\\nGesamthochschule Paderborn, Art Systems Software GmbH Paderborn,\\nFesto Didactic Esslingen. Herausgeber: Festo Didactic KG, PF 624,\\n73734 Esslingen. Preise zzgl. MwSt.: Schülerversion: 99 DM,\\nVollversion: 1200 DM, Klassenraumlizenz: 5040 DM.\\n\\n\\nDas Programm ist Abbild eines umfassend bestückten (Elektro)Pneumatiklabors.\\nEs führt in die Schaltbilder, Funktionsweisen und Abläufe\\nin Schaltungen ein und erklärt Elemente, Komponenten und\\nBauteile der (Elektro)Pneumatik. Mit FluidSIM-P läßt\\nsich eine beliebige Anzahl von Schaltungen am Bildschirm aufbauen\\nund über Simulation eine Funktionsanalyse vornehmen. Durch\\ndie gelungene Verbindung von Animationen, elektronischen Folien,\\nSimulationen, Videos und Arbeitsumgebungen zum Aufbau eigener\\nSchaltungen vermittelt dieses Multimedia-Programm fachliche und\\ndidaktische Kompetenz. Es genügt damit zugleich den Ansprüchen\\nder technischen Realität als auch der Gestaltung von Lernprozessen.\\n\\n(Quelle: Faltblatt der Geschäftsstelle der\\ndigita 96, Institut für Bildung in der Informationsgesellschaft,\\nc/o TU Berlin, Franklinstr. 28/29, 10587 Berlin)  \\n\\n\\nEin Angebot der Computer Werkstatt München\\n\\nDie \"Computer Werkstatt\" der INBUS GmbH\\n- Innovation in Bildung und Sozialarbeit bietet preisgünstige\\nLernsoftware für benachteiligte Jugendliche in der beruflichen\\nAusbildung.. Die Programme wurden im Rahmen eines von der EU mitfinanzierten\\nModellprojektes entwickelt.  Die Programme können gegen eine\\nKostenentschädigung von 10 DM je Programm zzgl. 7 % MwSt\\nbestellt werden:\\n\\nÜbungsprogramme\\n\\n- Flächenberechnen\\n\\n- Formelumstellen (mit Maus)\\n\\n- Formelumstellen\\n\\n- Einheiten umrechnen\\n\\n- Dreisatz\\n\\n- Mischungsrechnen (Aufgaben)\\n\\n- Maschinenberechnen für Schreiner\\n\\n- Kopfrechnen (Windows-Programm)\\n\\nTutorielle Programme\\n\\n- Prozent - Was ist das eigentlich?\\n\\n- Mischungsrechnen - was ist das eigentlich?\\n\\n- Das deutsche Sozialversicherungssystem\\n\\n- Holz arbeitet\\n\\n- Rechtsfähigkeit (Windows-Programm)\\n\\n- Geschäftsfähigkeit (Windows-Programm)\\n- ab Dezember 1996\\n\\n (Bestelladresse: INBUS GmbH - Computer &\\nAusbildung, Müllerstr. 43, 80469 München, verantwortlich:\\nSusanne Prell.)\\n\\n\\n\\nMultimediales Sprachlernprogramm für das Metallhandwerk\\n\\nMetallhandwerker in Deutschland, England und Dänemark\\nsollen in spätestens 3 Jahren den Fachwortschatz ihrer Branche\\nmit der Hilfe  einer Computer-CD erlernen können. Diese soll\\nin einem dreijährigen Modellprojekt realisiert werden, an\\ndem die Technologie-Transferstelle der HWK der Pfalz, das Zentrum\\nfür Fernstudien und Universitäre Weiterbildung der Uni\\nKaiserslautern, das Odense Technical College in Dänemark\\nund die Euro-Media-Call aus Bristol in England - ein Multimediaunternehmen,\\ndas sich auf die Konzeption von multimedialen Lernprogrammen spezialisiert\\nhat - teilnehmen. Gefördert wird das Projekt mit rund 360.000\\nDM aus dem Leonardo-Programm. Im Rahmen des Projektes soll ein\\nmultimedialer Selbstlernkurs entwickelt werden, der es Metallhandwerkern\\naus den beteiligten Ländern ermöglicht, einen berufsspezifischen\\nfremdsprachlichen Fachwortschatz zu erwerben.\\n\\n(Quelle: HZ-Deutsches Wirtschaftsblatt, Ausgabe\\nRheinland-Pfalz, Zeitung der HWK Trier, Ausgabe 9 vom 09. Mai\\n1996)\\n\\n \\n\\n\\nComputergestütztes Planspiel \"Europäische Integration\\nim Spiel (EIS)\"\\n\\nDas Ruhrforschungszentrum Düsseldorf hat in\\nKooperation mit dem Studienkreis Schule/Wirtschaft Nordrhein-Westfalen\\nund mit Förderung der europäischen Kommission ein computergestütztes\\nPlanspiel entwickelt, das Schüler der Sekundarstufe II (16\\nJahre und älter) auf anschauliche und aktive Art und Weise\\nüber die Europäische Union, ihre Institutionen und Politiken\\ninformiert. Das Planspiel wird Ende 1996/Anfang 1997 in einem\\nnamhaften Schulbuchverlag erscheinen.\\n\\n(Für weitere Informationen: Trägerverein\\ndes Ruhrforschungszentrums e.V., z.Hd. Frau Heinrich, Uerdinger\\nStraße 58-62, 40474 Düsseldorf)\\n\\n\\n\\nSchulfernsehen CD-ROM\\n\\nDie CD-ROM, für die ein Multimedia-Computer\\nmit Soundkarte erforderlich ist, bietet:\\n\\n * neue Möglichkeiten für die Unterrichtsvorbereitung:\\nVideos, Fotos, Quellenmaterial, Statistiken, Grafiken und Tondokumente\\n\\n * ausführliches Material zu einer Vielzahl\\nvon Schulfernseh-Sendungen\\n\\n * Vorschläge für Arbeitsmaterialien\\n\\n * Grafiken, Fotos etc. zum Erstellen und Ergänzen\\nvon Arbeitsblättern\\n\\n * Druckfunktion für Texte, Bilder, Arbeitsblätter\\n\\n(Informationen: SÜDWESTFUNK, Geschäftsstelle\\nSchulfernsehen SÜDWEST 3, Hans-Bredow-Straße, 76530\\nBaden-Baden, T 07221-923486, F 07221-922027)\\n\\n\\nCD-ROMs des Modellversuches \"BENNO\"\\n\\nDer Bremer Modellversuch \"BENNO - Ausbildung\\nbenachteiligter Jugendlicher in neugeordneten Metallberufen und\\nWeiterbildung  für Ausbilder\"  läuft seit 01.10.1990\\n und endet am. 31.12.1996. Die Ziele, Projekte und Resultate des\\nModellversuchs liegen kompakt und anschaulich aufbereitet als\\nmultimediale Präsentation vor.\\n\\n- Diskettenversion DM 10,00\\n\\n- CD-ROM Version (mit Leittextbeispielen zur Fehlersuche\\nund Pneumatik)  DM 20,00\\n\\nIm Verlauf des Modellversuches wurden Materialien\\nfür die Bereiche Berufsvorbereitung, Metallgrundbildung,\\nFachbildung, Pneumatikgrundlehrgang, Prüfungsvorbereitung\\nentwickelt (die Autoren Norbert Kampe und Gerd Cordes veröffentlichten\\ndie Materialien als Modellversuchsreihe Band I-V (ISBN 3-930284-00-6\\n... 3-930284-04-9) Drei weitere Veröffentlichungen stellen\\nSeminarkonzepte zur Ausbilderförderung, die Qualifikationsdebatte\\nsowie Forschungsergebnisse zur Dyskalkulie/Rechenschwäche\\nvor. (Autorinnen/Autoren: Christiane Koch, Kathrin Hensge, Rolf\\nRöhrig, Norbert Kampe).\\n\\nDie Materialien können in der betrieblichen\\nund überbetrieblichen Ausbildung von Konstruktions- und Industriemechanikern\\neingesetzt werden. Von besonderem Interesse sind die während\\ndes Modellprojektes durchgeführten Projekte: \"Power\\nJeep/automatische Tür\" (Berufsvorbereitung), \"Drehmobil\"\\n(Metallgrundbildung), \"Systematische Fehlersuche - Reparatur\\neiner Maschinensäge\" (Metallfachbildung), \"Pneumatisierung\\neiner Bohrmaschine\", \"Simulation berufstypischer Planungsaufgaben\"\\n(Prüfungsvorbereitung). \\n\\nDer Projektträger, das \"Arbeiter-Bildungs-Centrum\\n(ABC) der Arbeiterkammer Bremen GmbH\" hat eine weitere CD-ROM\\nveröffentlicht, auf der sich die gesamten Leittextmaterialien\\nder Ausbildungsprojekte zur systematischen Fehlersuche und  Pneumatikgrundbildung\\nbefinden.\\n\\nDie Veröffentlichung der Leittexte auf CD-ROM\\nverfolgt zwei Zielsetzungen:\\n\\n1. Die entwickelten Materialien vorzustellen\\n\\n2. Eine Möglichkeit anzubieten, mit Hilfe gängiger\\nSoftware die Leittextmaterialien für die eigenen Ausbildungs-\\noder Förderbedürfnisse zu modifizieren und situativ\\numzubauen.  \\n\\nHardwarevoraussetzungen: CD-ROM Laufwerk, PC (ab\\n386), Softwarevoraussetzungen: Windows 3ff.; Page-Maker oder beliebige\\nTextverarbeitungssoftware, Preis: DM 84 (plus Versandkosten)\\n\\nWir hatten kurz vor Redaktionsschluß Gelegenheit,\\nuns die erste CD-ROM mit der Modellversuchspräsentation und\\nden Leittext-Beispielen anzusehen. Die entwickelten Materialien\\nsind von hoher Qualität und zeugen von einer außergewöhnlichen\\nProduktivität der Mitarbeiter/innen des Modellprojektes und\\nder wissenschaftlichen Begleitung. Die CD-ROM ist informativ und\\ngut gemacht, die Bedienungsroutinen könnten allerdings etwas\\nverbessert werden. Etwas mehr Hilfestellung für den Umgang\\nmit dem Programm wäre empfehlenswert.\\n\\nBezugsquelle für die beiden CD-ROMs: Arbeiter-Bildungs-Centrum,\\nSchiffbauerweg 4, 28237 Bremen, Tel.: 04 21 / 6 18 05 19, Herr\\nNorbert Kampe.\\n\\n(Quelle: Unterlagen vom ABC, BENNO-News 2/96 und\\nbibbmail, Verzeichnis LERNSOFT, Meldungen Nr. 490 und 492 vom\\n14.10. und 18.10.1996)\\n\\n\\nKatalog des Deutschen Buchhandels: \"CDs und Disketten\\n\\n - Software\\n\\n- (ohne Audio-CDs) 5300 Titel\\n\\nDer Deutsche Buchhandel hat diesen fast 600 Seiten\\numfassenden Katalog vorgelegt. Er  enthält die Computer-Programme,\\ndie über den Buchhandel vertrieben werden. Ein Beispiel aus\\ndem Bereich Elektrotechnik, Elektronik, Maschinenbau: Franzis\\nSystembibliothek für den Maschinenbau, 1 CD-ROM mit Begleitbuch.\\nMit über 500 maßstabsgetreuen Symbolen in DXF-Format.\\n\\n\\nFür Windows 3.1. Bookware. Von Wolfgang Andratschke\\nu. Norbert Grossek. 1996. ISBN 3-7723-8052-2 (Franzis, F.) 658,-\\nöS  98,- DM.  \\n(Der Katalog ist beim Buchhandel erhältlich. Hrsg.: K.F.\\nKoehler Verlag GmbH, Stuttgart, Werbeabteilung).\\n\\n\\nCD-ROM des Bundesinstitutes für Berufsbildung\\n\\nÜber 600 Seiten Folien, Info-Materialien, Arbeitsblätter\\nfür den PC und Mac. Diese CD-ROM wurde als Arbeitshilfe für\\ndie Planung und Durchführung von Seminaren entwickelt. Sie\\nenthält Auszüge aller Seminarpakete zur Ausbilderförderung,\\nsoweit diese zur Vorbereitung und Durchführung von Seminaren\\nbenötigt werden, also Arbeitstransparente, Arbeitsblätter\\nmit Begleitmaterialien, Seminarankündigungen und dazu gehörige\\nKurzinformationen.\\n\\nSeminarthemen: Ausbilden im Verbund, Heterogene Gruppen,\\nLernbeeinträchtigte, Aus der Situation lernen, Industrielle\\nMetallberufe, Lehrlinge lernen planen, Motivation zum Lernen,\\nCNC-Technik, Kreative Aufgaben, Leittexte, Türkische Jugendliche.\\nDie CD-ROM hat die Bestellnummer 112.618 und kann für 69,50\\nDM beim W. Bertelsmann Verlag, Postfach 100633, 33506 Bielefeld,\\nbestellt werden.\\n\\n(Quelle: bibbmail, die Mailbox des Bundesinstitutes\\nfür Berufsbildung, Verzeichnis AF -Ausbilderförderung,\\nDatei CD-ROM.SEM vom 27.11.96)\\n\\n(wsk)\\n\\nzurück zur Homepage\\n\\n\\nKritik, Hinweise, Kommentare bitte an das Computer-Magazin\\n\\n www.weiterbildung.com\\nCopyright © 1996 Internet Publishing GbR \\nJan Allers & Peter Esser-Krapp, Hamburg, Germany \\n\\n\\n',\n",
       " ' \\xa0 \\xa0 Search advanced search Table of contents Past issues Links to advertisers Products advertised Place an ad Buyers\\' guide About us Contact us Submit press release American Institute of Physics The Industrial Physicist Computing in Science & Engineering Journals Virtual Journals Letters Science for All Americans: Seeking a Common Knowledge Core Across Disciplines It is not difficult to see why K-12 science education is in trouble. In his letter, Art Hobson ( Physics Today, December 2002, page 12 ) approvingly quotes the authors of Science for All Americans for their view that, \"without a scientifically literate population, the outlook for a better world is not promising.\" 1 However the authors of that document promote obsolete and incorrect information about the foundations of physics. On page 47, for example, they write, \"Scientists continue to investigate atoms and have discovered even smaller constituents of which electrons . . . are made.\" That same sentence is repeated in Benchmarks for Science Literacy , 2 which is currently used as the foundation for curriculum reform across the US. Much more serious than any specific erroneous statement is the document\\'s almost grotesque failure at the foundation of physics. You probably thought that the basic premise of the modern theory of matter was quantum mechanics, or at least that it was the standard model, right? Well, Science for All Americans says, \"The basic premise of the modern theory of matter is that the elements consist of a few different kinds of atoms--particles far too tiny to see in a microscope--that join together in different configurations to form substances. There are one or more--but never many--kinds of these atoms for each of the approximately 100 elements.\" The first sentence indicates that the way you form \"substances\" is by combining different isotopes of a single element! Is it any wonder that our K-12 education system is in such bad shape when such an illiterate, antiquated report has been circulated and used for more than a decade? NASA\\'s Maryland Space Grant Consortium has been trying for the past decade to bring university-level science professionalism into Maryland\\'s K-12 school system. It is an uphill task. References 1. F. J. Rutherford, A. Ahlgren, Science for All Americans , Oxford U. Press, New York (1990). Available online at http://www.project2061.org/tools/sfaaol/sfaatoc.htm 2. American Association for the Advancement of Science, Benchmarks for Science Literacy , Oxford U. Press, New York (1993), p. 80. Richard Conn Henry ( henry@jhu.edu ) Maryland Space Grant Consortium Baltimore The director of Project 2061 replies: Richard Conn Henry makes two points in his letter, one identifying awkward language and one criticizing the exclusion of advanced physics topics from Science for All Americans . We will certainly fix the awkward language, as we have done in the past, and thank him for pointing it out. However, we stand by our choice of learning goals. Our authors attempted to define a common core of knowledge, across science, mathematics, and technology, that would be optimally useful and enlightening (given constraints of students\\' time, interests, and abilities) for every high-school graduate, not just for those who aspire to scientific or technical careers. Difficult decisions about what was both important and possible included input from numerous chemistry and physics faculty, who have found that few college students understand the particulate nature of matter and the significance of the periodic table, let alone the significance of the standard model. Science for All Americans does not claim to be a ceiling, but a floor; high-school and college faculty consider it quite ambitious. Perhaps with better materials and teaching we can achieve universal science literacy on which to build understanding of more sophisticated ideas such as quantum mechanics and biological signal transduction. Project 2061 views Science for All Americans and Benchmarks for Science Literacy as living documents, subject to ongoing review and suggested revisions by scientists and educators. We welcome criticism and take it seriously. Interested readers can compare sections on cells, structure of matter, and energy transformations in their original 1989 print version with those in the current online version at our Web site, http://www.project2061.org . Jo Ellen Roseman ( jroseman@aaas.org ) Project 2061 American Association for the Advancement of Science Washington, DC In the continuing discussion about K-12 physics education, one or two sentences in Leon Lederman\\'s response ( Physics Today, August 2002, page 74 ) caught my eye. Lederman says, \"Here I only insist that the design be for all children.\" That got my attention, because I have been working for 25 years at that goal of science for all Americans. It was also the goal of F. James Rutherford, former assistant director of NSF and former assistant secretary of the US Department of Education, perhaps the most knowledgeable physicist in science education that the country has ever produced. From plenary lectures on science education that I have presented at major scientific and teachers\\' society meetings, I summarize some questions that I do not see answered in the writers\\' comments in the August 2002 Letters. For scientists interested in having all Americans learn science, I pose the following: For whom are you designing your reform? All students? Including the urban poor? The science able? The nonscientist/engineer (NSE) population? Future physicists? If you are concerned about all students, can you demonstrate to yourself even some knowledge of their typical educational background, lives, and challenges? What should students know more or better? Is physics more important for all than English, geography, or music? What is the evidence? CEOs, Cabinet officials, and media stars seem to do well with essentially no clue about physics. In what way would they improve by learning physics? As a scientist, have you reviewed the enormous amount of available literature on these topics from our university colleagues in education? Let me provide a reference to some baffling and scary data relevant to our hopes and ambitions. Rent or purchase copies of the videos A Private Universe and Minds of Their Own from the Annenberg Project series. Watch them and answer this question: Will your reform improve the performance of these students? If not, what value do you perceive in putting physics in ninth grade for all students? The students in those videos are graduating seniors of Harvard University and MIT, most with superb high-school experience. Yet more than 90% thought that the reason Earth had summer and winter was that it came closer to (or farther from) from the Sun. Even more could not identify photosynthesis as the mechanism by which a tree accumulates mass. The same percentage of MIT engineering graduates in their caps and gowns could not light a bulb with a battery and one wire. I\\'ve concluded that less than 10% of the American populace can handle any kind of abstraction. Fortunately, almost 100% can learn by using other senses and right-brain pathways. Hence my recent focus, and my recommendation to those who want to get more students into physics, is to start with reality and touch: touch-science. This recommendation builds on the unchangeable reality of the sequence of human sensory development, which starts with touch in the womb. The hands-on approach puts students in touch with such real sciences as agriculture, Earth, health, and materials. And as data from the University of Washington show, 1 using materials as the gateway to abstract science is valuable to both citizens and protophysicists. Out of the many who become interested spring more, and possibly better, physicist-citizens. © 2003 American Institute of Physics Free this month The Discovery of Rapid Climate Change South Dakota Governor Pushes for Underground Lab as Homestake Water Rises Science Fashions and Scientific Fact Obituary: Robert Lull Forward Letters Also this month Science for All Americans: Seeking a Common Knowledge Core Across Disciplines Ronald Richter, Genius or Nut? Baade, Shapley, and the Doubling of the Universe Karl Brown\\'s Role in TRANSPORT Group Velocity Is Not Signal Velocity Correction Sponsored links For your conference travel needs, try: Hotels London Hotels Miami Hotels Reno Hotels Boston Hotels Orlando Hotels Vancouver Hotels Chicago Hotels Las Vegas Hotels Toronto Hotels About Physics Today Contact Us FAQ Disclaimer Terms and Conditions Privacy Policy Jump to .. Home Find a Job Post a Job Place An Ad Subscribe Past Contents Event Calendar ___________________ Feature Articles Physics Update Letters Reference Frame Search & Discovery Issues & Events Opinion Books New Products We Hear That Obituaries \\n\\t\\n\\n-->\\n\\n\\n\\nPhysics Today - Letters\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\');\\xa0\\ndocument.write(\\'\\');\\ndocument.close();\\n//-->\\n\\n\\xa0\\n  \\xa0\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nadvanced search\\n\\n\\n\\n\\xa0 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTable \\n            of contents\\n\\n\\nPast \\n            issues\\n\\n\\n\\n\\n\\nLinks \\n            to advertisers\\n\\n\\nProducts \\n            advertised\\n\\n\\nPlace \\n            an ad\\n\\n\\nBuyers\\' \\n      guide\\n\\n\\n\\n\\n\\nAbout \\n            us\\n\\n\\nContact \\n            us\\n\\n\\nSubmit press release\\n\\n\\n\\n\\n\\nAmerican Institute \\n            of Physics\\n\\n\\nThe Industrial \\n            Physicist\\n\\n\\nComputing in \\n            Science & Engineering\\n\\n\\nJournals\\n\\n\\nVirtual \\n            Journals\\n\\n\\n\\n\\nLetters\\n\\n\\n\\n\\nScience for All Americans: Seeking a Common Knowledge\\n              Core Across Disciplines\\n It is not difficult to see why K-12 science education is in trouble.\\n              In his letter, Art Hobson (Physics\\n              Today, December 2002, page 12) approvingly quotes the authors\\n              of Science for All Americans for their view that, \"without\\n              a scientifically literate population, the outlook for a better\\n              world is not promising.\"1 However\\n              the authors of that document promote obsolete and incorrect information\\n              about the foundations of physics. On page 47, for example, they\\n              write, \"Scientists continue to investigate atoms and have discovered\\n              even smaller constituents of which electrons . . . are made.\" That\\n              same sentence is repeated in Benchmarks for Science Literacy,2 which\\n              is currently used as the foundation for curriculum reform across\\n              the US.\\n             Much more serious than any specific erroneous statement is the\\n              document\\'s almost grotesque failure at the foundation of physics.\\n              You probably thought that the basic premise of the modern theory\\n              of matter was quantum mechanics, or at least that it was the standard\\n              model, right? Well, Science for All Americans says, \"The\\n              basic premise of the modern theory of matter is that the elements\\n              consist of a few different kinds of atoms--particles far too tiny\\n              to see in a microscope--that join together in different configurations\\n              to form substances. There are one or more--but never many--kinds\\n              of these atoms for each of the approximately 100 elements.\" The\\n              first sentence indicates that the way you form \"substances\" is\\n              by combining different isotopes of a single element!\\n             Is it any wonder that our K-12 education system is in such bad\\n              shape when such an illiterate, antiquated report has been circulated\\n              and used for more than a decade?\\n             NASA\\'s Maryland Space Grant Consortium has been trying for the\\n              past decade to bring university-level science professionalism into\\n              Maryland\\'s K-12 school system. It is an uphill task.\\n            \\nReferences\\n1. F. J. Rutherford, A. Ahlgren, Science for All\\n                Americans, Oxford U. Press, New York (1990). Available online\\n                at http://www.project2061.org/tools/sfaaol/sfaatoc.htm\\n2. American Association for the Advancement of Science, Benchmarks\\n                for Science Literacy, Oxford U. Press, New York (1993), p.\\n                80.\\n\\nRichard Conn Henry\\n(henry@jhu.edu)\\nMaryland Space Grant Consortium\\nBaltimore\\n\\n\\n The director of Project 2061 replies: Richard Conn Henry\\n              makes two points in his letter, one identifying awkward language\\n              and one criticizing the exclusion of advanced physics topics from Science\\n              for All Americans. We will certainly fix the awkward language,\\n              as we have done in the past, and thank him for pointing it out.\\n              However, we stand by our choice of learning goals.\\n             Our authors attempted to define a common core of knowledge, across\\n              science, mathematics, and technology, that would be optimally useful\\n              and enlightening (given constraints of students\\' time, interests,\\n              and abilities) for every high-school graduate, not just for those\\n              who aspire to scientific or technical careers. Difficult decisions\\n              about what was both important and possible included input from\\n              numerous chemistry and physics faculty, who have found that few\\n              college students understand the particulate nature of matter and\\n              the significance of the periodic table, let alone the significance\\n              of the standard model.\\n             Science for All Americans does not claim to be a ceiling,\\n              but a floor; high-school and college faculty consider it quite\\n              ambitious. Perhaps with better materials and teaching we can achieve\\n              universal science literacy on which to build understanding of more\\n              sophisticated ideas such as quantum mechanics and biological signal\\n              transduction.\\n             Project 2061 views Science for All Americans and Benchmarks\\n                for Science Literacy as living documents, subject to ongoing\\n                review and suggested revisions by scientists and educators. We\\n                welcome criticism and take it seriously. Interested readers can\\n                compare sections on cells, structure of matter, and energy transformations\\n                in their original 1989 print version with those in the current\\n                online version at our Web site, http://www.project2061.org.\\n            \\nJo Ellen Roseman\\n(jroseman@aaas.org)\\nProject 2061\\nAmerican Association for the\\nAdvancement of Science\\nWashington, DC\\n\\n\\n\\n In the continuing discussion about K-12 physics education, one\\n              or two sentences in Leon Lederman\\'s response (Physics\\n              Today, August 2002, page 74) caught my eye. Lederman says, \"Here\\n              I only insist that the design be for all children.\" That\\n              got my attention, because I have been working for 25 years at that\\n              goal of science for all Americans. It was also the goal of F. James\\n              Rutherford, former assistant director of NSF and former assistant\\n              secretary of the US Department of Education, perhaps the most knowledgeable\\n              physicist in science education that the country has ever produced.\\n             From plenary lectures on science education that I have presented\\n              at major scientific and teachers\\' society meetings, I summarize\\n              some questions that I do not see answered in the writers\\' comments\\n              in the August 2002 Letters. For scientists interested in having\\n              all Americans learn science, I pose the following:\\n            \\n\\nFor whom are you designing your reform? All students? Including\\n                the urban poor? The science able? The nonscientist/engineer (NSE)\\n                population? Future physicists?\\n\\n\\n\\nIf you are concerned about all students, can you demonstrate\\n                to yourself even some knowledge of their typical educational\\n                background, lives, and challenges?\\n\\n\\n\\nWhat should students know more or better? Is physics more important\\n                for all than English, geography, or music? What is the evidence?\\n                CEOs, Cabinet officials, and media stars seem to do well with\\n                essentially no clue about physics. In what way would they improve\\n                by learning physics?\\n\\n\\n\\nAs a scientist, have you reviewed the enormous amount of available\\n                literature on these topics from our university colleagues in\\n                education?\\n\\n Let me provide a reference to some baffling and scary data relevant\\n              to our hopes and ambitions. Rent or purchase copies of the videos A\\n              Private Universe and Minds of Their Own from the Annenberg\\n              Project series. Watch them and answer this question: Will your\\n              reform improve the performance of these students? If not, what\\n              value do you perceive in putting physics in ninth grade for all\\n              students?\\n             The students in those videos are graduating seniors of Harvard\\n              University and MIT, most with superb high-school experience. Yet\\n              more than 90% thought that the reason Earth had summer and winter\\n              was that it came closer to (or farther from) from the Sun. Even\\n              more could not identify photosynthesis as the mechanism by which\\n              a tree accumulates mass. The same percentage of MIT engineering\\n              graduates in their caps and gowns could not light a bulb with a\\n              battery and one wire.\\n             I\\'ve concluded that less than 10% of the American populace can\\n              handle any kind of abstraction. Fortunately, almost 100% can learn\\n              by using other senses and right-brain pathways. Hence my recent\\n              focus, and my recommendation to those who want to get more students\\n              into physics, is to start with reality and touch: touch-science.\\n              This recommendation builds on the unchangeable reality of the sequence\\n              of human sensory development, which starts with touch in the womb.\\n              The hands-on approach puts students in touch with such real sciences\\n              as agriculture, Earth, health, and materials. And as data from\\n              the University of Washington show,1 using\\n              materials as the gateway to abstract science is valuable to both\\n              citizens and protophysicists. Out of the many who become interested\\n              spring more, and possibly better, physicist-citizens.\\n                       \\n            \\xa0            \\n            \\xa0 \\n\\xa0 \\n\\n\\n© 2003 American Institute of Physics\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\nFree this month\\n\\n\\nThe\\n                        Discovery of Rapid Climate Change\\n\\n\\nSouth\\n                        Dakota Governor Pushes for Underground Lab as Homestake\\n                        Water Rises\\n\\n\\nScience\\n                        Fashions and Scientific Fact\\n\\n\\nObituary:\\n                        Robert Lull Forward\\n\\n\\nLetters\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAlso this month\\n\\n\\nScience\\n                          for All Americans: Seeking a Common Knowledge Core\\n                          Across Disciplines\\n\\n\\nRonald\\n                          Richter, Genius or Nut?\\n\\n\\nBaade,\\n                          Shapley, and the Doubling of the Universe\\n\\n\\nKarl\\n                          Brown\\'s Role in TRANSPORT\\n\\n\\nGroup\\n                          Velocity Is Not Signal Velocity\\n\\n\\nCorrection\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSponsored links\\n\\n\\n\\n\\nFor your conference\\n              travel needs, try:\\n\\n\\nHotels\\nLondon Hotels\\nMiami Hotels\\nReno Hotels\\nBoston\\n                          Hotels\\nOrlando\\n                          Hotels\\nVancouver\\n                          Hotels\\nChicago\\n                          Hotels\\nLas Vegas\\n                          Hotels\\nToronto\\n                        Hotels\\n\\n\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\n\\n\\nAbout Physics Today\\xa0\\xa0 Contact Us\\xa0\\xa0 \\n                        FAQ\\n\\n\\nDisclaimer\\xa0\\xa0 \\n                        Terms and \\n                        Conditions\\xa0\\xa0\\xa0Privacy \\n                        Policy\\xa0 \\n\\n\\n\\n\\n\\n\\n\\nJump to .. Home Find a Job Post a Job Place An Ad Subscribe\\nPast \\n\\n        Contents Event Calendar ___________________ Feature Articles\\nPhysics \\n\\n        Update Letters Reference Frame\\nSearch & \\n\\n        Discovery Issues & Events\\nOpinion\\nBooks New Products\\nWe Hear That\\nObituaries\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " ' Site Awards About this Site Google Field Search Problems July 10, 2003: For more than a month now, the intitle: and inurl: field searches have been broken. I first heard of this on May 27, 2003. The advantage of intitle: and inurl: over the advanced search page Occurrences section or the allintitle: and allinurl: field searches was that they applied to only a single term and could be combined with other search terms that would look through the record. So now, searchers can not do a search that looks for one word in the title and another in the body. A search that tries like \"market research\" intitle:tourism retrieves many results that do not include \\'tourism\\' in the title. At first I thought this was a temporary glitch from the strange May update, but it has persisted through the June update and has continued for some time. Hopefully it will be correct sometime soon. I\\'ve updated the Google Inconsistencies page with this problem and several others long term problems. In addition, I updated several parts of the Google Review , including the addition of several language limits added in early 2002 that I had missed: Croatian, Indonesian, Serbian, Slovak, and Slovenian. Subject(s): Google ; Site Updates by Greg R. Archives October 2003 September 2003 August 2003 July 2003 June 2003 May 2003 April 2003 March 2003 February 2003 January 2003 December 2002 November 2002 October 2002 September 2002 August 2002 July 2002 June 2002 May 2002 April 2002 March 2002 February 2002 January 2002 December 2001 November 2001 October 2001 September 2001 August 2001 July 2001 June 2001 May 2001 April 2001 March 2001 February 2001 January 2001 December 2000 November 2000 October 2000 August 2000 July 2000 Older News: Jan-June 2000 Sept-Dec. 1999 Jan-Aug 1999 By Subject Alerts AlltheWeb AltaVista Ask Jeeves Excite FAST Gigablast Google HotBot Infoseek/Go Inktomi LookSmart Lycos MSN Search Meta Search Engines New Search Engines News Search Engines Openfind Opinion Searching Other News Overture Search Features Site Updates Teoma WiseNut Yahoo! Syndicate [XML] A Notess.com Web Site ©1999-2003 by Greg R. , all rights reserved Search Engine Showdown Greg\\'s Writings Greg\\'s Presentations \\n\\t\\n\\n-->\\n\\n\\n\\n\\n\\nSearch Engine Showdown News: Google Field Search Problems\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFeature Chart\\nSEs by Feature\\nNews Searches\\nMulti-Search\\nDirectories\\nOpinions/Usenet\\nPhone Numbers\\nOthers\\n\\n\\n\\n\\n\\nSize\\nFreshness\\nInconsistencies\\nAnd more\\n\\n\\n\\n\\nGoogle\\nAlltheWeb\\nAltaVista\\nGigablast\\n\\n\\n\\n\\nNews Archive\\nEmail Lists\\nAlerts \\n\\n\\n\\n\\n\\nOn the Net columns\\nSearch Strategies\\nBooks on Searching\\n\\n\\n\\nSite Awards \\nAbout this Site \\n\\n\\n\\n\\n\\n\\n\\n\\nGoogle Field Search Problems\\n\\n\\nJuly 10, 2003: \\nFor more than a month now, the intitle: and inurl: field searches have been broken. I first heard of this on May 27, 2003. The advantage of intitle: and inurl: over the advanced search page Occurrences section or the allintitle: and allinurl: field searches was that they applied to only a single term and could be combined with other search terms that would look through the record. So now, searchers can not do a search that looks for one word in the title and another in the body. A search that tries like \"market research\" intitle:tourism retrieves many results that do not include \\'tourism\\' in the title. \\nAt first I thought this was a temporary glitch from the strange May update, but it has persisted through the June update and has continued for some time. Hopefully it will be correct sometime soon. I\\'ve updated the Google Inconsistencies page with this problem and several others long term problems.\\nIn addition, I updated several parts of the Google Review, including the addition of several language limits added in early 2002 that I had missed: Croatian, Indonesian, Serbian, Slovak, and Slovenian.\\n\\n\\nSubject(s): \\n\\nGoogle\\n ; \\nSite Updates\\n\\n\\n\\n\\n\\n\\n\\n\\n  by Greg R. \\n\\nArchives\\n\\n\\nOctober 2003\\nSeptember 2003\\nAugust 2003\\nJuly 2003\\nJune 2003\\nMay 2003\\nApril 2003\\nMarch 2003\\nFebruary 2003\\nJanuary 2003\\nDecember 2002\\nNovember 2002\\nOctober 2002\\nSeptember 2002\\nAugust 2002\\nJuly 2002\\nJune 2002\\nMay 2002\\nApril 2002\\nMarch 2002\\nFebruary 2002\\nJanuary 2002\\nDecember 2001\\nNovember 2001\\nOctober 2001\\nSeptember 2001\\nAugust 2001\\nJuly 2001\\nJune 2001\\nMay 2001\\nApril 2001\\nMarch 2001\\nFebruary 2001\\nJanuary 2001\\nDecember 2000\\nNovember 2000\\nOctober 2000\\nAugust 2000\\nJuly 2000\\n\\n\\nOlder News:\\nJan-June 2000 \\nSept-Dec. 1999 \\nJan-Aug 1999\\n\\n\\n\\nBy Subject\\n\\n\\nAlerts\\nAlltheWeb\\nAltaVista\\nAsk Jeeves\\nExcite\\nFAST\\nGigablast\\nGoogle\\nHotBot\\nInfoseek/Go\\nInktomi\\nLookSmart\\nLycos\\nMSN Search\\nMeta Search Engines\\nNew Search Engines\\nNews Search Engines\\nOpenfind\\nOpinion Searching\\nOther News\\nOverture\\nSearch Features\\nSite Updates\\nTeoma\\nWiseNut\\nYahoo!\\n\\n\\n\\nSyndicate [XML]\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    A Notess.com Web Site\\n    ©1999-2003 by Greg R. , all \\n    rights reserved\\n\\n\\nSearch Engine Showdown\\n    Greg\\'s Writings\\n    Greg\\'s Presentations\\n\\n\\n\\n\\n',\n",
       " ' Introduction An HTML element may include a name, some attributes and some text or hypertext, and will appear in an HTML document as <tag_name> text </tag_name> <tag_name attribute_name=argument> text </tag_name> , or just <tag_name> For example: <title> My Useful Document </title> and <a href=\"argument\"> text </a> An HTML document is composed of a single element: <html> . . . </html> that is, in turn, composed of head and body elements: <head> . . . </head> and <body> . . . </body> To allow older HTML documents to remain readable, <html> , <head> , and <body> are actually optional within HTML documents. Elements usually placed in the head element <isindex> Specifies that the current document describes a database that can be searched using the index search method appropriate for whatever client is being used to read the document. For example, a Lynx user will use the \"s\" keyboard command. <title> . . . </title> Specify a document title. Note that the title will not appear on the document as is customary on printed documents. It will usually appear in a window bar identifying the contents of the window. HTML header tags perform the functions usually reserved for titles. <base href=\"URL\"> Specify the name of the file relative to which partially qualified pathnames in URLs should be interpreted. If not otherwise specified the URL containing the document being displayed is used as the base. <link rev=\"RELATIONSHIP\" rel=\"RELATIONSHIP\" href=\"URL\"> The link tag allows you to define relationships between the document containing the link tag and the document specified in the \"URL\". The rel attribute specifies the relationship between the HTML file and the Uniform Resource Locator (URL). The rev attribute (for \"reverse\") specifies the relationship between the URL and the HTML file. For example, <link rev=\"made\" href=\"URL\"> indicates that the file maker or owner is described in the document identified by the URL. (Note that link tags are not displayed on the screen as part of the document. They define static relationships, not hypertext links.) Elements usually placed in the body element The following sections describe elements that can be used in the body of the document. Text Elements <p> The end of a paragraph that will be formatted before it is displayed on the screen. <pre> . . . </pre> Identifies text that has already been formatted (preformatted) by some other system and must be displayed as is. Preformatted text may include embedded tags, but not all tag types are permitted. The <pre> tag can be used to include tables in documents. <listing> . . . </listing> Example computer listing; embedded tags will be ignored, but embedded tabs will work. This is an archaic tag. <xmp> . . . </xmp> Similar to <pre> except no embedded tags will be recognized. <plaintext> Similar to <pre> except no embedded tags will be recognized, and since there is no end tag, the remainder of the document will be rendered as plain text. This is an archaic tag. Note that some browsers actually recognize a </plaintext> tag, even though it is not defined by the standard. <blockquote> . . . </blockquote> Include a section of text quoted from some other source. Hyperlinks or Anchors <a name=\"anchor_name\"> . . . </a> Define a target location in a document <a href=\"#anchor_name\"> . . . </a> Link to a location in the base document, which is the document containing the anchor tag itself, unless a base tag has been specified. <a href=\"URL\"> . . . </a> Link to another file or resource <a href=\"URL#anchor_name\"> . . . </a> Link to a target location in another document <a href=\"URL?search_word+search_word\"> . . . </a> Send a search string to a server. Different servers may interpret the search string differently. In the case of word-oriented search engines, multiple search words might be specified by separating individual words with a plus sign (+). An anchor must include a name or href attribute, and may include both. There are several optional attributes, but they are rarely encountered. The structure of a Uniform Resource Locator (URL) may be expressed as: resource_type:additional_information where the possible resource types include: file , http , news , gopher , telnet , ftp , and wais , among others, and each resource type relates to a specific server type. Since each server performs a unique function, each resource type requires different additional_information . For example http and gopher URLs will have a structure like: resource_type://host.domain:port/pathname The colon followed by an integer TCP port number is optional, and is used when a server is listening on a non-standard port. Strictly speaking, the anchor_name and search_word information included in the name and href attributes in the examples above are part of the URL. They are presented as separate entities for simplicity. A more complete description of URLs is presented in http://www.w3.org/hypertext/WWW/Addressing/Addressing.html Headers <h1> . . . </h1> Most prominent header <h2> . . . </h2> <h3> . . . </h3> <h4> . . . </h4> <h5> . . . </h5> <h6> . . . </h6> Least prominent header Logical Styles <em> . . . </em> Emphasis <strong> . . . </strong> Stronger emphasis <code> . . . </code> Display an HTML directive <samp> . . . </samp> Include sample output <kbd> . . . </kbd> Display a keyboard key <var> . . . </var> Define a variable <dfn> . . . </dfn> Display a definition (not widely supported) <cite> . . . </cite> Display a citation Physical Styles <b> . . . </b> Boldface <i> . . . </i> Italics <u> . . . </u> Underline <tt> . . . </tt> Typewriter font Definition list/glossary: <dl> <dl> <dt> First term to be defined <dd> Definition of first term <dt> Next term to be defined <dd> Next definition </dl> The <dl> attribute compact can be used to generate a definition list requiring less space. Present an unordered list: <ul> <ul> <li> First item in the list <li> Next item in the list </ul> Present an ordered list: <ol> <ol> <li> First item in the list <li> Next item in the list </ol> Present an interactive menu: <menu> <menu> <li> First item in the menu <li> Next item </menu> Present a directory list of items: <dir> <dir> <li> First item in the list <li> Second item in the list <li> Next item in the list </dir> Items should be less than 20 characters long. Entities & keyword ; Display a particular character identified by a special keyword. For example the entity &amp; specifies the ampersand ( & ), and the entity &lt; specifies the less than ( < ) character. Note that the semicolon following the keyword is required, and the keyword must be one from the lists presented in: http://www.w3.org/pub/WWW/MarkUp/html-spec/html-spec_9.html#SEC9.7 &# ascii_equivalent ; Use a character literally. Again note that the semicolon following the ASCII numeric value is required. HTML Forms Interface The HTML forms interface allows document creators to define HTML documents containing forms to be filled out by users. When a user fills out the form and presses a button indicating the form should be \"submitted,\" the information on the form is sent to a server for processing. The server will usually prepare an HTML document using the information supplied by the user and return it to the client for display. The following tags implement the forms interface: <form> . . . </form> <input> <select> . . . </select> <option> <textarea> . . . </textarea> The last four tags can only be used within a <form> . . . </form> element. Define a form <form> . . . </form> Defines a form within an HTML document. A document may contain multiple <form> elements, but <form> elements may not be nested. Note that non-form tags can be used within a <form> element. Attributes and their arguments: action=\"URL\" : The location of the program that will process the form. method=data_exchange method The method chosen to exchange data between the client and the program started to process the form: One of get or post . post is preferred for most applications. Example: <form action=\"http://www.ku.edu/cgi-bin/register\" method=post> . . . </form> Define an input field <input> (there is no ending tag) Defines an input field where the user may enter information on the form. Each input field assigns a value to a variable which has a specified name and a specified data type . Attributes and their arguments: type=\"variable_type\" Specifies the data type for the variable, where: type=\"text\" and type=\"password\" fields accept character data type=\"checkbox\" fields are either selected or not type=\"radio\" fields of the same name allow selection of only one of the associated values type=\"submit\" defines an action button that sends the completed form to the query server type=\"reset\" defines a button that resets the form variables to their default values type=\"hidden\" defines an invisible input field whose value will be sent along with the other form values when the form is submitted. This is used to pass state information from one script or form to another. type=\"image\" defines an image map within a form and returns the coordinates of a mouse click within the image. name=\"textstring\" where textstring is a symbolic name (not displayed) identifying the input variable as in: <input type=\"checkbox\" name=\"box1\"> value=\"textstring\" where the meaning of textstring depends on the argument for type . For type=\"text\" or type=\"password\" , textstring is the default value for the input variable. Password values will not be shown on the user\\'s form. Anything entered by the user will replace any default value defined with this attribute. If type=\"checkbox\" or type=\"radio\" , textstring is the value that will be sent to the server if the checkbox is \"checked\". For type=\"reset\" or type=\"submit\" , textstring is a label that will appear on the submit or reset button in place of the words \" submit \" and \" reset \". checked No arguments. For type=\"checkbox\" or type=\"radio\" , if checked is present the input field is \"checked\" by default. size=\"display_width\" where display_width is an integer value representing the number of characters displayed for the type=\"text\" or type=\"password\" input field. maxlength=\"string_length\" where string_length is the maximum number of characters allowed within type=\"text\" or type=\"password\" variable values. This attribute is only valid for single line \"text\" or \"password\" fields. Define a select field <select> . . . </select> Defines and displays a set of optional list items from which the user can select one or more items. This element requires an <option> element for each item in the list. Attributes and their arguments: name=\"textstring\" where textstring is the symbolic identifier for the select field variable. size=\"list_length\" where list_length is an integer value representing the number of <option> items that will be displayed at one time. multiple No arguments. If present, the multiple attribute allows selection of more than one <option> value. Define a select field option <option> Within the <select> element the <option> tags are used to define the possible values for the select field. If the attribute selected is present then the option value is selected by default. In the following example all three options may be chosen but bananas are selected by default. <select multiple> <option>Apples <option selected>Bananas <option>Cherries </select> Define a text area <textarea> . . . default text . . . </textarea> Defines a rectangular field where the user may enter text data. If \"default text\" is present it will be displayed when the field appears. Otherwise the field will be blank. Attributes and their values: name=\"textstring\" textstring is a symbolic name that identifies the <textarea> variable. rows=\"num_rows\" and cols=\"numcols\" Both attributes take an integer value which represents the lines and number of characters per line in the <textarea> to be displayed. Miscellaneous <!-- text --> Place a comment in the HTML source <address> . . . </address> Present address information <img src=\"URL\" alt=\"Alternate Text\"> Embed a graphic image in the document. Attributes: src Specifies the location of the image. alt Allows a text string to be put in place of the image in clients that cannot display images. align Specify a relationship to surrounding text. The argument for align can be one of top , middle , or bottom . ismap If ismap is present and the image tag is within an anchor, the image will become a \"clickable image\". The pixel coordinates of the cursor will be appended to the URL specified in the anchor if the user clicks within the ismap image. The resulting URL will take the form \"URL?m,n\" where m and n are integer coordinates, and the URL will specify the location of a program that will examine the pixel coordinates, and return an appropriate document. <br> Forces a line break immediately and retains the same style. <hr> Places a horizontal rule or separator between sections of text. Additional Information For a tutorial introduction to HTML see: http://www.ncsa.uiuc.edu/General/Internet/WWW/HTMLPrimer.html. For an introduction to forms within HTML see: An Instantaneous Introduction to CGI Scripts and HTML Forms . For general information about HTML, see http://www.w3.org/hypertext/WWW/MarkUp/MarkUp.html The current URL is /~acs/docs/other/HTML_quick.shtml This file was last modified Tuesday, 03-Jul-2001 09:55:56 CDT. Questions about computing to question@ku.edu. Problems, comments about this Web site to acsweb@ku.edu . HTML> \\n\\t\\n\\n-->\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHTML Quick Reference\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEmail\\nInternet\\nWeb \\n      Authoring\\n Computer \\n      Labs\\nComputing \\n      Classes\\nHelp\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\nHTML Quick Reference\\n\\n\\n\\n\\n\\n\\nThe HyperText Markup Language (HTML) is composed of a set of elements \\n        that define a document and guide its display. This document presents a \\n        concise reference guide to HTML, listing the most commonly used elements \\n        from Versions 1 and 2 of HTML, and giving a brief description of those \\n        elements. \\n Users should be aware that HTML is an evolving language, and different \\n        World-Wide Web browsers may recognize slightly different sets of HTML \\n        elements. For general information about HTML including plans for new versions, \\n        see  http://www.w3.org/hypertext/WWW/MarkUp/MarkUp.html.\\n\\n\\xa0 \\n      \\n\\nIntroduction\\n An HTML element may include a name, some attributes and some text or \\n        hypertext, and will appear in an HTML document as \\n      \\n<tag_name> text </tag_name>\\n<tag_name attribute_name=argument> text </tag_name>, \\n          or just \\n        <tag_name>\\n\\n      For example: \\n      \\n<title> My Useful Document </title>\\n\\n      and \\n      \\n<a href=\"argument\"> text </a>\\n\\n      An HTML document is composed of a single element: \\n      \\n<html> . . . </html>\\n\\n      that is, in turn, composed of head and body elements: \\n      \\n<head> . . . </head>\\n\\n      and \\n      \\n<body> . . . </body>\\n\\n      To allow older HTML documents to remain readable, <html>, \\n      <head>, and <body> are actually optional \\n      within HTML documents. \\n      Elements usually placed in the head element\\n\\n<isindex>\\nSpecifies that the current document describes a database that can \\n          be searched using the index search method appropriate for whatever client \\n          is being used to read the document. For example, a Lynx user will use \\n          the \"s\" keyboard command. \\n        <title> . . . </title>\\nSpecify a document title. Note that the title will not appear on the \\n          document as is customary on printed documents. It will usually appear \\n          in a window bar identifying the contents of the window. HTML header \\n          tags perform the functions usually reserved for titles. \\n        <base href=\"URL\">\\nSpecify the name of the file relative to which partially qualified \\n          pathnames in URLs should be interpreted. If not otherwise specified \\n          the URL containing the document being displayed is used as the base. \\n        <link rev=\"RELATIONSHIP\" rel=\"RELATIONSHIP\" href=\"URL\">\\nThe link tag allows you to define relationships between the document \\n          containing the link tag and the document specified in the \"URL\". The \\n          rel attribute specifies the relationship between the HTML \\n          file and the Uniform Resource Locator (URL). The rev attribute \\n          (for \"reverse\") specifies the relationship between the URL and the HTML \\n          file. For example, <link rev=\"made\" href=\"URL\"> indicates \\n          that the file maker or owner is described in the document identified \\n          by the URL. (Note that link tags are not displayed on the screen as \\n          part of the document. They define static relationships, not hypertext \\n          links.) \\n      \\nElements usually placed in the body element\\n      The following sections describe elements that can be used in the body of \\n      the document.\\n      \\nText Elements\\n\\n<p>\\nThe end of a paragraph that will be formatted before it is displayed \\n          on the screen. \\n        <pre> . . . </pre>\\nIdentifies text that has already been formatted (preformatted) by \\n          some other system and must be displayed as is. Preformatted text may \\n          include embedded tags, but not all tag types are permitted. The <pre> \\n          tag can be used to include tables in documents. \\n        <listing> . . . </listing>\\nExample computer listing; embedded tags will be ignored, but embedded \\n          tabs will work. This is an archaic tag. \\n        <xmp> . . . </xmp>\\nSimilar to <pre> except no embedded tags will be \\n          recognized. \\n        <plaintext>\\nSimilar to <pre> except no embedded tags will be \\n          recognized, and since there is no end tag, the remainder of the document \\n          will be rendered as plain text. This is an archaic tag. Note that some \\n          browsers actually recognize a </plaintext> tag, even \\n          though it is not defined by the standard. \\n        <blockquote> . . . </blockquote>\\nInclude a section of text quoted from some other source. \\n      \\nHyperlinks or Anchors\\n\\n<a name=\"anchor_name\"> . . . </a>\\nDefine a target location in a document \\n        <a href=\"#anchor_name\"> . . . </a>\\nLink to a location in the base document, which is the document containing \\n          the anchor tag itself, unless a base tag has been specified. \\n        <a href=\"URL\"> . . . </a>\\nLink to another file or resource \\n        <a href=\"URL#anchor_name\"> . . . </a>\\nLink to a target location in another document \\n        <a href=\"URL?search_word+search_word\"> . . . </a>\\nSend a search string to a server. Different servers may interpret \\n          the search string differently. In the case of word-oriented search engines, \\n          multiple search words might be specified by separating individual words \\n          with a plus sign (+). \\n      \\n      An anchor must include a name or href attribute, \\n      and may include both. There are several optional attributes, but they are \\n      rarely encountered. \\n       The structure of a Uniform Resource Locator (URL) may be expressed as: \\n      \\n\\nresource_type:additional_information \\n      \\n      where the possible resource types include: file, http, \\n      news, gopher, telnet, ftp, \\n      and wais, among others, and each resource type relates to a \\n      specific server type. Since each server performs a unique function, each \\n      resource type requires different additional_information. For \\n      example http and gopher URLs will have a structure \\n      like: \\n      \\n\\nresource_type://host.domain:port/pathname \\n      \\n      The colon followed by an integer TCP port number is optional, and is used \\n      when a server is listening on a non-standard port. \\n       Strictly speaking, the anchor_name  and search_word \\n        information included in the name and href attributes \\n        in the examples above are part of the URL. They are presented as separate \\n        entities for simplicity. A more complete description of URLs is presented \\n        in  \\n        http://www.w3.org/hypertext/WWW/Addressing/Addressing.html \\nHeaders\\n\\n<h1> . . . </h1> Most prominent \\n          header \\n        <h2> . . . </h2>\\n<h3> . . . </h3>\\n<h4> . . . </h4>\\n<h5> . . . </h5>\\n<h6> . . . </h6> Least prominent \\n          header \\n      \\nLogical Styles\\n\\n<em> . . . </em>\\nEmphasis \\n        <strong> . . . </strong>\\nStronger emphasis \\n        <code> . . . </code>\\nDisplay an HTML directive \\n        <samp> . . . </samp>\\nInclude sample output \\n        <kbd> . . . </kbd>\\nDisplay a keyboard key \\n        <var> . . . </var>\\nDefine a variable \\n        <dfn> . . . </dfn>\\nDisplay a definition (not widely supported) \\n        <cite> . . . </cite>\\nDisplay a citation \\n      \\nPhysical Styles\\n\\n<b> . . . </b>\\n Boldface\\n<i> . . . </i>\\n Italics\\n<u> . . . </u>\\n Underline\\n<tt> . . . </tt>\\n Typewriter font\\n\\nDefinition list/glossary: <dl>\\n\\n<dl>\\n<dt> First term to be defined \\n        <dd> Definition of first term \\n        <dt> Next term to be defined \\n        <dd> Next definition \\n        </dl>\\n\\n      The <dl> attribute compact can be used to \\n      generate a definition list requiring less space.\\n      \\nPresent an unordered list: <ul>\\n\\n<ul>\\n<li> First item in the list \\n        <li> Next item in the list \\n        </ul>\\n\\nPresent an ordered list: <ol>\\n\\n<ol>\\n<li> First item in the list \\n        <li> Next item in the list \\n        </ol>\\n\\nPresent an interactive menu: <menu>\\n\\n<menu>\\n<li> First item in the menu \\n        <li> Next item \\n        </menu>\\n\\nPresent a directory list of items: <dir>\\n\\n<dir>\\n<li> First item in the list \\n        <li> Second item in the list \\n        <li> Next item in the list \\n        </dir>\\n\\n      Items should be less than 20 characters long.\\n      \\nEntities\\n\\n&keyword;\\nDisplay a particular character identified by a special keyword. For \\n          example the entity &amp; specifies the ampersand ( \\n          & ), and the entity &lt; specifies the less than \\n          ( < ) character. Note that the semicolon following the keyword is \\n          required, and the keyword must be one from the lists presented in: \\n         http://www.w3.org/pub/WWW/MarkUp/html-spec/html-spec_9.html#SEC9.7 \\n          \\n&#ascii_equivalent;\\nUse a character literally. Again note that the semicolon following \\n          the ASCII numeric value is required. \\n      \\nHTML Forms Interface\\n      The HTML forms interface allows document creators to define HTML documents \\n      containing forms to be filled out by users. When a user fills out the form \\n      and presses a button indicating the form should be \"submitted,\" the information \\n      on the form is sent to a server for processing. The server will usually \\n      prepare an HTML document using the information supplied by the user and \\n      return it to the client for display. \\n       The following tags implement the forms interface: \\n      \\n<form> . . . </form>\\n<input>\\n<select> . . . </select>\\n<option>\\n<textarea> . . . </textarea>\\n\\n      The last four tags can only be used within a <form> . \\n      . . </form> element. \\n      Define a form\\n<form> . . . </form>\\n Defines a form within an HTML document. A document may contain multiple \\n        <form> elements, but <form> elements \\n        may not be nested. Note that non-form tags can be used within a <form> \\n        element. Attributes and their arguments: \\n      \\naction=\"URL\": \\n        The location of the program that will process the form. \\n        method=data_exchange method\\nThe method chosen to exchange data between the client and the program \\n          started to process the form: One of get or post. \\n          post is preferred for most applications. \\n      \\n\\nExample: \\n        \\n<form action=\"http://www.ku.edu/cgi-bin/register\" method=post> \\n          . . . </form>\\n\\n\\nDefine an input field\\n <input> (there is no ending tag) \\n       Defines an input field where the user may enter information on the form. \\n        Each input field assigns a value to a variable which has a specified name \\n        and a specified data type. Attributes and their arguments: \\n      \\ntype=\"variable_type\"\\nSpecifies the data type for the variable, where: \\n          \\n type=\"text\" and type=\"password\" fields \\n              accept character data \\n             type=\"checkbox\" fields are either selected or not \\n             type=\"radio\" fields of the same name allow selection \\n              of only one of the associated values \\n             type=\"submit\" defines an action button that sends \\n              the completed form to the query server \\n            type=\"reset\" defines a button that resets the form \\n              variables to their default values \\n            type=\"hidden\" defines an invisible input field whose \\n              value will be sent along with the other form values when the form \\n              is submitted. This is used to pass state information from one script \\n              or form to another. \\n            type=\"image\"defines an image map within a form and \\n              returns the coordinates of a mouse click within the image. \\n          \\nname=\"textstring\"\\nwhere textstring is a symbolic name (not displayed) identifying \\n          the input variable as in: \\n<input type=\"checkbox\" name=\"box1\">\\nvalue=\"textstring\"\\nwhere the meaning of textstring depends on the argument \\n          for type. \\n          \\n\\nFor type=\"text\" or type=\"password\", \\n              textstring is the default value for the input \\n              variable. Password values will not be shown on the user\\'s form. \\n              Anything entered by the user will replace any default value defined \\n              with this attribute. \\n            If type=\"checkbox\" or type=\"radio\", \\n              textstring is the value that will be sent to the server \\n              if the checkbox is \"checked\". \\n            For type=\"reset\" or type=\"submit\", textstring \\n              is a label that will appear on the submit or reset button in place \\n              of the words \"submit\" and \"reset\". \\n          \\nchecked\\nNo arguments. For type=\"checkbox\" or type=\"radio\", \\n          if checked is present the input field is \"checked\" \\n          by default. \\n        size=\"display_width\"\\nwhere display_width is an integer value representing \\n          the number of characters displayed for the type=\"text\" \\n          or type=\"password\" input field. \\n        maxlength=\"string_length\"\\nwhere string_length is the maximum number of characters \\n          allowed within type=\"text\" or type=\"password\" \\n          variable values. This attribute is only valid for single line \"text\" \\n          or \"password\" fields. \\n      \\nDefine a select field\\n<select> . . . </select>\\n Defines and displays a set of optional list items from which the user \\n        can select one or more items. This element requires an <option> \\n        element for each item in the list. Attributes and their arguments: \\n      \\nname=\"textstring\"\\nwhere textstring is the symbolic identifier for the select \\n          field variable. \\n        size=\"list_length\"\\nwhere list_length is an integer value representing the \\n          number of <option> items that will be displayed at \\n          one time. \\n        multiple\\nNo arguments. If present, the multiple attribute allows \\n          selection of more than one <option> value. \\n      \\nDefine a select field option\\n<option>\\n Within the <select> element the <option> \\n        tags are used to define the possible values for the select \\n        field. If the attribute selected is present then the option \\n        value is selected by default. In the following example all three options \\n        may be chosen but bananas are selected by default. \\n       <select multiple>\\n<option>Apples\\n<option selected>Bananas\\n<option>Cherries\\n</select>\\n\\nDefine a text area\\n<textarea> . . . default text . . . </textarea>\\n Defines a rectangular field where the user may enter text data. If \"default \\n        text\" is present it will be displayed when the field appears. Otherwise \\n        the field will be blank. Attributes and their values: \\n      \\nname=\"textstring\"\\ntextstring is a symbolic name that identifies the <textarea> \\n          variable. \\n        rows=\"num_rows\" and cols=\"numcols\"\\nBoth attributes take an integer value which represents the lines and \\n          number of characters per line in the <textarea> to \\n          be displayed. \\n      \\nMiscellaneous\\n\\n<!-- text -->\\nPlace a comment in the HTML source \\n        <address> . . . </address>\\nPresent address information \\n        <img src=\"URL\" alt=\"Alternate Text\">\\nEmbed a graphic image in the document. Attributes: \\n          \\nsrc\\nSpecifies the location of the image. \\n            alt\\nAllows a text string to be put in place of the image in clients \\n              that cannot display images. \\n            align\\nSpecify a relationship to surrounding text. The argument for align \\n              can be one of top, middle, or bottom. \\n            ismap\\nIf ismap is present and the image tag is within an \\n              anchor, the image will become a \"clickable image\". The pixel coordinates \\n              of the cursor will be appended to the URL specified in the anchor \\n              if the user clicks within the ismap image. The resulting URL will \\n              take the form \"URL?m,n\" where m and n are integer coordinates, and \\n              the URL will specify the location of a program that will examine \\n              the pixel coordinates, and return an appropriate document. \\n          \\n<br>\\nForces a line break immediately and retains the same style. \\n        <hr>\\nPlaces a horizontal rule or separator between sections of text. \\n      \\nAdditional Information\\n      For a tutorial introduction to HTML see:  \\n      http://www.ncsa.uiuc.edu/General/Internet/WWW/HTMLPrimer.html.\\n For an introduction to forms within HTML see:  \\n        An Instantaneous Introduction to CGI Scripts and HTML Forms. \\n       For general information about HTML, see  \\n        http://www.w3.org/hypertext/WWW/MarkUp/MarkUp.html\\n\\n\\n\\n The current URL is /~acs/docs/other/HTML_quick.shtml \\n  This file was last modified Tuesday, 03-Jul-2001 09:55:56 CDT.\\nQuestions about computing to   \\nquestion@ku.edu.\\n Problems, comments about this Web site to  \\nacsweb@ku.edu.\\n\\n\\n\\n\\nHTML>\\n\\n',\n",
       " \"\\n\\n\\n\\n\\n\\n\\n\\n Lower Bounds for Searching in Streets and Generalized Streets \\n\\n\\n\\n\\n\\nA\\nLinear Lower Bound on Index Size for Text Retrieval. E. D. Demaine and A.\\nLópez-Ortiz. Proceedings of 12th Symposium on Discrete Algorithms (SODA\\n2001), Washington, DC, 2001. Invited to special issue of Journal of Algorithms\\non selected papers from SODA. PostScript\\nfile. \\n\\xa0\\nAbstract. Most\\ninformation-retrieval systems preprocess the data to produce an auxiliary index\\nstructure. Empirically, it has been observed that there is a tradeoff between\\nquery response time and the size of the index. When indexing a large corpus,\\nsuch as the web, the size of the index is an important consideration. In this\\ncase it would be ideal to produce an index that is substantially smaller than\\nthe text. \\nIn this work we prove a linear lower bound on the\\nsize of any index that reports the location (if any) of a substring in the text\\nin time proportional to the length of the pattern. In other words, an index\\nsupporting linear-time substring searches requires about as much space as the\\noriginal text. Here ``time'' is measured in the number of bit probes to the\\ntext; an arbitrary amount of computation may be done on an arbitrary amount of\\nthe index. Our lower bound applies to inverted word indices as well. \\n\\xa0\\n\\n\\n\\n\",\n",
       " ' 96.018 2 s 2 2 p 5 2 s 2 2 p 4 ( 1 D)3 s 2 P°- 2 D 3/2 3/2 98% 4.19E-03 0.97 -3 77.412 2 s 2 2 p 5 2 s 2 2 p 4 ( 1 S)3 d 2 P° - 2 D 3/2 3/2 98% 6.95E-03 1.05 5 91.370 2 s 2 2 p 5 2 s 2 2 p 4 ( 1 S)3 s 2 P° - 2 S 3/2 1/2 97% 1.23E-02 0.89 -11 83.284 2 s 2 2 p 5 2 s 2 2 p 4 ( 3 P)3 d 2 P° - 2 P 3/2 1/2 87% 1.55E-02 1.01 1 91.797 2 s 2 2 p 5 2 s 2 2 p 4 ( 1 S)3 s 2 P° - 2 S 1/2 1/2 97% 1.55E-02 0.90 -10 83.005 2 s 2 2 p 5 2 s 2 2 p 4 ( 3 P)3 d 2 P° - 2 P 3/2 3/2 68% 1.82E-02 0.48 -52 99.096 2 s 2 2 p 5 2 s 2 2 p 4 ( 3 P)3 s 2 P° - 2 P 3/2 1/2 98% 2.08E-02 0.99 -1 99.966 2 s 2 2 p 5 2 s 2 2 p 4 ( 3 P)3 s 2 P° - 2 P 1/2 3/2 96% 3.28E-02 1.00 -1 80.490 2 s 2 2 p 5 2 s 2 2 p 4 ( 1 D)3 d 2 P° - 2 P 3/2 1/2 85% 3.33E-02 1.58 58 80.395 2 s 2 2 p 5 2 s 2 2 p 4 ( 1 D)3 d 2 P° - 2 D 3/2 3/2 91% 3.39E-02 1.18 18 83.639 2 s 2 2 p 5 2 s 2 2 p 4 ( 3 P)3 d 2 P° - 2 P 1/2 1/2 87% 4.57E-02 0.93 -7 96.023 2 s 2 2 p 5 2 s 2 2 p 4 ( 1 D)3 s 2 P° - 2 D 3/2 5/2 98% 6.39E-02 0.97 -3 77.429 2 s 2 2 p 5 2 s 2 2 p 4 ( 3 P)3 d 2 P° - 2 D 3/2 5/2 98% 6.81E-02 1.06 6 99.599 2 s 2 2 p 5 2 s 2 2 p 4 ( 3 P)3 s 2 P° - 2 P 1/2 1/2 98% 7.66E-02 0.99 -1 96.490 2 s 2 2 p 5 2 s 2 2 p 4 ( 1 D)3 s 2 P° - 2 D 1/2 3/2 98% 7.80E-02 0.97 -3 81.031 2 s 2 2 p 5 2 s 2 2 p 4 ( 1 D)3 d 2 P° - 2 S 1/2 1/2 95% 8.32E-02 1.25 25 83.257 2 s 2 2 p 5 2 s 2 2 p 4 ( 3 P)3 d 2 P° - 2 D 3/2 3/2 68% 9.98E-02 1.10 9 99.460 2 s 2 2 p 5 2 s 2 2 p 4 ( 3 P)3 s 2 P° - 2 P 3/2 3/2 96% 1.00E-01 1.00 -1 77.718 2 s 2 2 p 5 2 s 2 2 p 4 ( 1 S)3 d 2 P° - 2 D 1/2 3/2 97% 1.01E-01 1.10 10 80.909 2 s 2 2 p 5 2 s 2 2 p 4 ( 1 D)3 d 2 P° - 2 P 1/2 3/2 90% 1.11E-01 1.11 11 80.698 2 s 2 2 p 5 2 s 2 2 p 4 ( 1 D)3 d 2 P° - 2 S 3/2 1/2 95% 1.35E-01 0.89 -11 83.128 2 s 2 2 p 5 2 s 2 2 p 4 ( 3 P)3 d 2 P° - 2 D 3/2 5/2 86% 1.80E-01 2.16 116 80.449 2 s 2 2 p 5 2 s 2 2 p 4 ( 1 D)3 d 2 P° - 2 D 3/2 5/2 77% 1.90E-01 1.76 76 83.611 2 s 2 2 p 5 2 s 2 2 p 4 ( 3 P)3 d 2 P° - 2 D 1/2 3/2 68% 1.99E-01 0.81 -19 83.358 2 s 2 2 p 5 2 s 2 2 p 4 ( 3 P)3 d 2 P° - 2 P 1/2 3/2 68% 2.03E-01 1.20 20 80.577 2 s 2 2 p 5 2 s 2 2 p 4 ( 1 D)3 d 2 P° - 2 P 3/2 3/2 90% 2.67E-01 1.09 9 80.821 2 s 2 2 p 5 2 s 2 2 p 4 ( 1 D)3 d 2 P° - 2 P 1/2 1/2 85% 2.73E-01 0.96 -4 80.725 2 s 2 2 p 5 2 s 2 2 p 4 ( 1 D)3 d 2 P° - 2 D 1/2 3/2 91% 3.68E-01 1.14 14 Problems concerning large discrepancies in transition probabilities for fluorine-like spectra between the OP [ 8 ] and CIV3 [ 11 ] results were discussed earlier in [ 21 ]. At that time extended relativistic calculations for individual lines were available only from CIV3 [ 11 ] calculations. Last year new MCHF [ 10 ] data became available. The dependence of accuracy on the purity of LS coupling is illustrated by an example for the fluorine-like ion S VIII . The following three plots show detailed comparisons of oscillator strengths for allowed transitions of S VIII . In Fig. 4 the ratios of OP [ 8 ] and CIV3 [ 11 ] oscillator strengths to the MCHF [ 10 ] values are plotted on a logarithmic scale against the logarithm of the MCHF oscillator strength. The dashed lines indicate a band of 50% around a perfect ratio of 1.00. Some large disagreements are observed with the OP data, even for the stronger lines. The agreement between MCHF and CIV3 is clearly better, but for many transitions the agreement is still not good. Figure 4: Comparison of oscillator strengths for the F-like ion S VIII . In studying these transitions for which the agreement is not good, we found that for almost all of them, one or both of the levels involved in the transition could be considered as mixed. By mixed we mean that the main contribution to the wave function of the level is less than 80%. Correspondingly, a pure level here means that that the main contribution to the wave function composition of this level is more than 80%. Figure 5: Comparison of oscillator strengths of allowed transitions between mixed levels for the F-like ion S VIII . Figure 5 shows a comparison of oscillator strengths of allowed transitions between mixed levels for the F-like ion S VIII . The ratios of CIV3 [ 11 ] oscillator strengths to the corresponding MCHF [ 10 ] values are plotted on a logarithmic scale versus the logarithm of the MCHF oscillator strength. The dashed lines indicate a band of 50% around a perfect ratio of 1.00. It is seen that for most transitions the agreement is better than 50%, which is within the range of data listed in the NIST reference tables. Figure 6 shows the comparison of oscillator strengths of allowed transitions between pure levels for the F-like ion S VIII . The ratios of CIV3 [ 11 ] oscillator strengths to the corresponding MCHF [ 10 ] values are plotted on a logarithmic scale against the logarithm of the MCHF oscillator strength. Out of 33 transitions, 31 have agreements between the CIV3 and MCHF calculations of better than 10%. The other two agree within 20%. Figure 6: Comparison of oscillator strengths of allowed transitions between pure levels for the F-like ion S VIII . The study of such dependence on LS coupling is important when we have transition probabilities available only from one source and need to estimate their accuracy on the basis of extrapolation from comparisons with other sources in overlapping areas. Introduction | Comparison | Table Arrangement | References \\n\\t\\n\\n-->\\n\\nNIST: Spect. Data for Chandra X-Ray Observ. - Comparisons\\n\\n\\n\\n\\nGraphical and Numerical Comparisons in Support of Assessment \\nProcedure\\n\\nIn order to reaffirm the uncertainty estimates of transition probabilities for \\nthe present compilation we made graphical and numerical comparisons of the \\nresults of different advanced calculations for as many transitions as possible, \\nregardless of wavelength. Later we selected data for the Chandra spectral range \\n20\\xa0Å to 170\\xa0Å. To fit the data into systematic trends, or deviations \\nfrom them, we found useful the theoretically predicted scaling of data along \\nisoelectronic sequences. If available we always selected data from detailed \\nconfiguration-interaction calculations with intermediate coupling. As usual \\nthese calculations are performed for transitions to the ground state or between \\nlow excited configurations. For the transitions involving high-lying \\nconfigurations, only Opacity Project (OP) data are available. For the stronger \\ntransitions of many spectra, good agreement exists between the OP data and data \\nfrom more detailed calculations that consider spin-orbit interactions. \\n\\n\\n\\n\\n\\nFigure 1: Comparison of oscillator strengths of the individual lines for \\nthe B-like ion Mg\\xa0VIII.\\n \\xa0 \\nFigure 1 shows a comparison of oscillator strengths of the individual lines \\nfor the B-like ion Mg\\xa0VIII. The ratios of the OP\\n[8] oscillator strengths in combination with \\nLS coupling and selected calculations for individual lines \\nto the [16,19,20] \\ncorresponding MCHF\\xa0[10] values are \\nplotted on a logarithmic scale versus the logarithm of the MCHF oscillator \\nstrengths. \\nThe dashed and dotted lines indicate bands of 10% and 25% around a perfect \\nratio of 1.00. For most transitions the agreement is better than 10% with \\ncalculations for the individual lines and better than 25% with the OP data.\\n\\n\\n\\n\\nThe agreement among the OP calculations and different relativistic calculations \\ngets worse for the weaker transitions and for transitions between those levels \\nwhere one or both are appreciably mixed due to breakdown of LS coupling. \\nLarge disagreements are often observed for weaker transitions when calculations \\nfor the transition probabilities of weak transitions have encountered \\nconsiderable problems due to appreciable cancellations of positive and negative \\ncomponents of the transition integral. A comparison of oscillator strengths for \\nthe C-like spectrum of Si\\xa0IX is given in Fig. 2. \\nThe ratios of the CIV3\\xa0[13] oscillator \\nstrengths to the corresponding MCHF\\xa0[10] \\nvalues are plotted on a logarithmic scale versus the logarithm of the MCHF \\noscillator strengths. The dashed-dotted lines indicate a band of 50% around a \\nperfect ratio of 1.00. Transitions \\n2s22p2-2s22p3s, \\n2s22p2-2s22p3d, \\n2p4-2s22p3s, and\\n2p4-2s22p3d in the \\nChandra range 54\\xa0Å to 125\\xa0Å are presented. This \\ncomparison shows good agreement for the transitions with large oscillator \\nstrengths but the scatter increases sharply for transitions with lower \\noscillator strengths.\\n \\xa0 \\n\\n\\n\\nFigure 2: Comparison of oscillator strengths for the C-like ion \\nSi\\xa0IX.\\n\\n\\n\\n\\n\\nFigure 3: Comparison of oscillator strengths for the F-like ion \\nSi\\xa0VI.\\n\\nFigure 3 shows a comparison of oscillator strengths for the F-like ion \\nSi\\xa0VI in the range of 77\\xa0Å to \\n146\\xa0Å. The ratios of CIV3\\xa0[11] \\noscillator strengths to the corresponding MCHF\\xa0[10] values are plotted on a logarithmic scale \\nversus the logarithm of the MCHF oscillator strengths. The dotted and \\ndashed-dotted lines indicate bands of 25% and 50% around a perfect ratio of \\n1.00. It is seen that agreements are better than 25% for most transitions. \\nHowever for four transitions (spectroscopic designations for them are given on \\nthe plot) there is disagreement of more than 50%.  \\n\\n\\n\\nThe numerical values associated with Fig.\\xa03 along with wavelengths and \\ntransition classifications are given in Table\\xa01. (i indicates the \\nlower level and k the upper level.) The largest contribution to the \\neigenvector percentage composition of each upper level is also given. We do not \\nprovide these values for the lower levels, which are well described by \\nLS coupling. From the table it is seen that the percentage contributions \\nof the main terms to the upper levels belonging to transitions with large \\ndeviations are \\n2s22p4(3P)3d\\n2P3/2 68%; \\n2s22p4(1D)3d\\n2P1/2 85%; \\n2s22p4(1D)3d\\n2D5/2 77%; \\n2s22p4(3P)3d\\n2D5/2 86%. Thus, we may conclude for this case that when \\nboth levels have a main contribution of more than 85%, the agreement is better \\nthan 10%. If at least one level has a main contribution of less than 85%, the \\noscillator strengths may disagree up to 50%. \\n\\n\\n\\n\\nTable 1. Oscillator Strengths of Si VI\\n\\n\\n\\n (Å)\\n \\xa0\\nConfig.\\n    (i)\\n \\xa0\\nConfig.\\n    (k)\\n \\xa0\\nTerms\\ni-k\\n \\xa0\\nJi\\n \\xa0\\nJk\\n \\xa0\\n\\nLS principal\\n    contribution\\n    to\\xa0upper\\n    level\\xa0k\\n \\xa0\\nf (MCHF)\\n \\xa0\\nf (CIV3)/\\nf (MCHF)\\n \\xa0\\nAgreement\\xa0%\\n\\n\\n\\n\\n96.018\\n\\n2s22p5\\n\\n2s22p4(1D)3s\\n\\n2P°-2D\\n3/2\\n3/2\\n98%\\n4.19E-03\\n0.97\\n-3\\n\\n\\n77.412\\n\\n2s22p5\\n\\n2s22p4(1S)3d\\n\\n2P°\\n-2D\\n3/2\\n3/2\\n98%\\n6.95E-03\\n1.05\\n5\\n\\n\\n91.370\\n\\n2s22p5\\n\\n2s22p4(1S)3s\\n\\n2P°\\n-2S\\n3/2\\n1/2\\n97%\\n1.23E-02\\n0.89\\n-11\\n\\n\\n83.284\\n\\n2s22p5\\n\\n2s22p4(3P)3d\\n\\n2P°\\n-2P\\n3/2\\n1/2\\n87%\\n1.55E-02\\n1.01\\n1\\n\\n\\n91.797\\n\\n2s22p5\\n\\n2s22p4(1S)3s\\n\\n2P°\\n-2S\\n1/2\\n1/2\\n97%\\n1.55E-02\\n0.90\\n-10\\n\\n\\n83.005\\n\\n2s22p5\\n\\n2s22p4(3P)3d\\n\\n2P°\\n-2P\\n3/2\\n3/2\\n68%\\n1.82E-02\\n0.48\\n-52\\n\\n\\n99.096\\n\\n2s22p5\\n\\n2s22p4(3P)3s\\n\\n2P°\\n-2P\\n3/2\\n1/2\\n98%\\n2.08E-02\\n0.99\\n-1\\n\\n\\n99.966\\n\\n2s22p5\\n\\n2s22p4(3P)3s\\n\\n2P°\\n-2P\\n1/2\\n3/2\\n96%\\n3.28E-02\\n1.00\\n-1\\n\\n\\n80.490\\n\\n2s22p5\\n\\n2s22p4(1D)3d\\n\\n2P°\\n-2P\\n3/2\\n1/2\\n85%\\n3.33E-02\\n1.58\\n58\\n\\n\\n80.395\\n\\n2s22p5\\n\\n2s22p4(1D)3d\\n\\n2P°\\n-2D\\n3/2\\n3/2\\n91%\\n3.39E-02\\n1.18\\n18\\n\\n\\n83.639\\n\\n2s22p5\\n\\n2s22p4(3P)3d\\n\\n2P°\\n-2P\\n1/2\\n1/2\\n87%\\n4.57E-02\\n0.93\\n-7\\n\\n\\n96.023\\n\\n2s22p5\\n\\n2s22p4(1D)3s\\n\\n2P°\\n-2D\\n3/2\\n5/2\\n98%\\n6.39E-02\\n0.97\\n-3\\n\\n\\n77.429\\n\\n2s22p5\\n\\n2s22p4(3P)3d\\n\\n2P°\\n-2D\\n3/2\\n5/2\\n98%\\n6.81E-02\\n1.06\\n6\\n\\n\\n99.599\\n\\n2s22p5\\n\\n2s22p4(3P)3s\\n\\n2P°\\n-2P\\n1/2\\n1/2\\n98%\\n7.66E-02\\n0.99\\n-1\\n\\n\\n96.490\\n\\n2s22p5\\n\\n2s22p4(1D)3s\\n\\n2P°\\n-2D\\n1/2\\n3/2\\n98%\\n7.80E-02\\n0.97\\n-3\\n\\n\\n81.031\\n\\n2s22p5\\n\\n2s22p4(1D)3d\\n\\n2P°\\n-2S\\n1/2\\n1/2\\n95%\\n8.32E-02\\n1.25\\n25\\n\\n\\n83.257\\n\\n2s22p5\\n\\n2s22p4(3P)3d\\n\\n2P°\\n-2D\\n3/2\\n3/2\\n68%\\n9.98E-02\\n1.10\\n9\\n\\n\\n99.460\\n\\n2s22p5\\n\\n2s22p4(3P)3s\\n\\n2P°\\n-2P\\n3/2\\n3/2\\n96%\\n1.00E-01\\n1.00\\n-1\\n\\n\\n77.718\\n\\n2s22p5\\n\\n2s22p4(1S)3d\\n\\n2P°\\n-2D\\n1/2\\n3/2\\n97%\\n1.01E-01\\n1.10\\n10\\n\\n\\n80.909\\n\\n2s22p5\\n\\n2s22p4(1D)3d\\n\\n2P°\\n-2P\\n1/2\\n3/2\\n90%\\n1.11E-01\\n1.11\\n11\\n\\n\\n80.698\\n\\n2s22p5\\n\\n2s22p4(1D)3d\\n\\n2P°\\n-2S\\n3/2\\n1/2\\n95%\\n1.35E-01\\n0.89\\n-11\\n\\n\\n83.128\\n\\n2s22p5\\n\\n2s22p4(3P)3d\\n\\n2P°\\n-2D\\n3/2\\n5/2\\n86%\\n1.80E-01\\n2.16\\n116\\n\\n\\n80.449\\n\\n2s22p5\\n\\n2s22p4(1D)3d\\n\\n2P°\\n-2D\\n3/2\\n5/2\\n77%\\n1.90E-01\\n1.76\\n76\\n\\n\\n83.611\\n\\n2s22p5\\n\\n2s22p4(3P)3d\\n\\n2P°\\n-2D\\n1/2\\n3/2\\n68%\\n1.99E-01\\n0.81\\n-19\\n\\n\\n83.358\\n\\n2s22p5\\n\\n2s22p4(3P)3d\\n\\n2P°\\n-2P\\n1/2\\n3/2\\n68%\\n2.03E-01\\n1.20\\n20\\n\\n\\n80.577\\n\\n2s22p5\\n\\n2s22p4(1D)3d\\n\\n2P°\\n-2P\\n3/2\\n3/2\\n90%\\n2.67E-01\\n1.09\\n9\\n\\n\\n80.821\\n\\n2s22p5\\n\\n2s22p4(1D)3d\\n\\n2P°\\n-2P\\n1/2\\n1/2\\n85%\\n2.73E-01\\n0.96\\n-4\\n\\n\\n80.725\\n\\n2s22p5\\n\\n2s22p4(1D)3d\\n\\n2P°\\n-2D\\n1/2\\n3/2\\n91%\\n3.68E-01\\n1.14\\n14\\n\\n\\n\\n\\n\\nProblems concerning large discrepancies in transition probabilities for \\nfluorine-like spectra between the OP\\xa0[8] \\nand CIV3\\xa0[11] results were discussed \\nearlier in [21]. At that time extended \\nrelativistic calculations for individual lines were available only from \\nCIV3\\xa0[11] calculations. Last year new \\nMCHF\\xa0[10] data became available. The \\ndependence of accuracy on the purity of LS coupling is illustrated by an \\nexample for the fluorine-like ion S\\xa0VIII. The \\nfollowing three plots show detailed comparisons of oscillator strengths for \\nallowed transitions of S\\xa0VIII.\\n\\n\\n\\nIn Fig. 4 the ratios of OP\\xa0[8] and \\nCIV3\\xa0[11] oscillator strengths to the \\nMCHF\\xa0[10] values are plotted on a \\nlogarithmic scale against the logarithm of the MCHF oscillator strength. The \\ndashed lines indicate a band of 50% around a perfect ratio of 1.00. Some large \\ndisagreements are observed with the OP data, even for the stronger lines. The \\nagreement between MCHF and CIV3 is clearly better, but for many transitions the \\nagreement is still not good.\\n \\xa0 \\n\\n\\n\\nFigure 4: Comparison of oscillator strengths for the F-like ion \\nS\\xa0VIII. \\n\\n\\nIn studying these transitions for which the agreement is not good, we found \\nthat for almost all of them, one or both of the levels involved in the \\ntransition could be considered as mixed. By mixed we mean that the main \\ncontribution to the wave function of the level is less than 80%. \\nCorrespondingly, a pure level here means that that the main contribution to the \\nwave function composition of this level is more than 80%.\\n\\n\\n\\n\\n\\n\\n\\nFigure 5: Comparison of oscillator strengths of allowed transitions \\nbetween mixed levels for the F-like ion S\\xa0VIII.\\n \\xa0 \\nFigure 5 shows a comparison of oscillator strengths of allowed transitions \\nbetween mixed levels for the F-like ion S\\xa0VIII. The \\nratios of CIV3\\xa0[11] oscillator strengths \\nto the corresponding MCHF\\xa0[10] values are \\nplotted on a logarithmic scale versus the logarithm of the MCHF oscillator \\nstrength. The dashed lines indicate a band of 50% around a perfect ratio of \\n1.00. It is seen that for most transitions the agreement is better than 50%, \\nwhich is within the range of data listed in the NIST reference tables.\\n\\n\\n\\nFigure 6 shows the comparison of oscillator strengths of allowed transitions\\nbetween pure levels for the F-like ion S\\xa0VIII. The \\nratios of CIV3\\xa0[11] oscillator strengths \\nto the corresponding MCHF [10] values are \\nplotted on a logarithmic scale against the logarithm of the MCHF oscillator \\nstrength. Out of 33 transitions, 31 have agreements between the CIV3 and \\nMCHF calculations of better than 10%. The other two agree within 20%.\\n\\n \\xa0 \\n\\n\\n\\nFigure 6: Comparison of oscillator strengths of allowed transitions \\nbetween pure levels for the F-like ion S\\xa0VIII.\\n\\n\\nThe study of such dependence on LS coupling is important when we have \\ntransition probabilities available only from one source and need to estimate \\ntheir accuracy on the basis of extrapolation from comparisons with other \\nsources in overlapping areas. \\n\\n\\n\\nIntroduction \\xa0 | \\xa0\\nComparison \\xa0 | \\xa0\\nTable Arrangement \\xa0 | \\xa0\\nReferences\\n\\n\\n\\n\\n\\n',\n",
       " '\\n\\n\\n\\nPercent and Probability\\n\\n\\n\\n\\nPercent and Probability\\n Percent \\nWhat is a percent? \\nPercent as a fraction \\nPercent as a decimal \\nEstimating percents \\nInterest \\nSimple interest \\nCompound interest \\nPercent increase and decrease\\n\\nPercent discount\\n Chances and probability \\nWhat is an event? \\nPossible outcomes of an event\\n\\nProbability\\n\\xa0 \\n\\n\\n\\n\\n\\nMath Contests\\xa0\\n\\n\\nSchool League Competitions\\n\\n\\n\\n\\nContest Problem \\n\\t\\t  Books\\xa0\\n\\n\\nChallenging, fun math practice\\n\\n\\n\\n\\nEducational \\n\\t\\t  Software\\xa0\\n\\n\\nComprehensive Learning Tools\\n\\n\\n\\n\\n\\n\\n\\n \\n\\tVisit the Math League \\n  \\n\\n\\nWhat is a Percent?\\n A percent is a ratio of a number to 100. A percent can be expressed using the \\n  percent symbol %. \\n Example: 10 percent or 10% are both the same, and stand \\n  for the ratio 10:100. \\n \\n\\nPercent as a fraction\\n A percent is equivalent to a fraction with denominator 100. \\n Example: 5% of something = 5/100 of that thing. \\n Example: 2 1/2% is equal to what fraction? \\n  Answer:\\n  2 1/2%\\xa0=\\xa0(2 1/2)/100\\xa0=\\xa05/200\\xa0=\\xa01/40 \\n Example: 52% most nearly equals which one of 1/2, 1/4, \\n  2, 8, or 1/5?\\n  Answer: 52%\\xa0=\\xa052/100. This is very close to 50/100, or 1/2. \\n Example: 13/25 is what %?\\n  We want to convert 13/25 to a fraction with 100 in the denominator: 13/25\\xa0=\\xa0(13\\xa0×\\xa04)/(25\\xa0×\\xa04)\\xa0=\\xa052/100, \\n  so 13/25\\xa0=\\xa052%. \\n  Alternatively, we could say: Let 13/25 be n%, and let us find n. Then \\n  13/25\\xa0=\\xa0n/100, so cross multiplying, 13\\xa0×\\xa0100\\xa0=\\xa025\\xa0×\\xa0n, \\n  so 25n\\xa0=\\xa013\\xa0×\\xa0100\\xa0=\\xa01300. Then 25n\\xa0÷\\xa025\\xa0=\\xa01300\\xa0÷\\xa025, \\n  so n\\xa0=\\xa01300\\xa0÷\\xa025\\xa0=\\xa052. So 13/25\\xa0=\\xa0n%\\xa0=\\xa052%. \\n Example: 8/200 is what %?\\n  Method 1: 8/200\\xa0=\\xa0(4\\xa0×\\xa02)/(100\\xa0×\\xa02), so \\n  8/200\\xa0=\\xa04/100\\xa0=\\xa04%.\\n  Method 2: Let 8/200 be n%. Then 8/200\\xa0=\\xa0n/100, so 200\\xa0×\\xa0n\\xa0=\\xa0800, \\n  and 200n\\xa0÷\\xa0200\\xa0=\\xa0800\\xa0÷\\xa0200\\xa0=\\xa04, \\n  so n%\\xa0=\\xa04%. \\n Example: Write 80% as a fraction in lowest terms.\\n  80%\\xa0=\\xa080/100, which is equal to 4/5 in lowest terms. \\n\\nPercent as a decimal\\n Percent and hundredths are basically equivalent. This makes conversion between \\n  percent and decimals very easy. \\n To convert from a decimal to a percent, just move the decimal 2 places to the \\n  right. For example, 0.15 = 15 hundredths = 15%. \\n Example:\\n 0.0006 = 0.06% \\n Converting from percent to decimal form is similar, only you move the decimal \\n  point 2 places to the left. You must also be sure, before doing this, that the percentage \\n  itself is expressed in decimal form, without fractions.\\n Example:\\n Express 3% in decimal form. Moving the decimal 2 to the left (and adding in 0\\'s \\n  to the left of the 3 as place holders,) we get 0.03. \\n Example:\\n Express 97 1/4% in decimal form. First we write 97 1/4 in decimal form: 97.25. \\n  Then we move the decimal 2 places to the left to get 0.9725, so 97 1/4%\\xa0=\\xa00.9725. \\n  This makes sense, since 97 1/4% is nearly 100%, and 0.9725 is nearly 1. \\n\\nEstimating percents\\n When estimating percents, it is helpful to remember the fractional equivalent \\n  of some simple percents. \\n 100%\\xa0=\\xa01\\n  (100% of any number equals that number.) \\n 50%\\xa0=\\xa01/2\\xa0=\\xa00.5\\n  (50% of any number equals half of that number.) \\n 25%\\xa0=\\xa01/4\\xa0=\\xa00.25\\n  (25% of any number equals one-fourth of that number.) \\n 10%\\xa0=\\xa01/10\\xa0=\\xa00.1\\n  (10% of any number equals one-tenth of that number.) \\n 1%\\xa0=\\xa01/100\\xa0=\\xa00.01\\n  (1% of any number equals one-hundredth of that number.) \\n Because it is very easy to switch between a decimal and a percent, estimating \\n  a percent is as easy as estimating a fraction as a decimal, and converting to a \\n  percent by multiplying by 100. \\n Example:\\n Estimate 19 as a percent of 80.\\n  As a fraction, 19/80\\xa0\\xa020/80\\xa0=\\xa01/4\\xa0=0.25\\xa0=\\xa025%. \\n  The step used to estimate the percent occurred when we estimated 19/80 as 20/80. \\n  \\n  The exact percent is actually 23.75%, so the estimate of 25% is only 1.25% off. \\n  (About 1 part in 100.) \\n Example:\\n Estimate 7 as a percent of 960.\\n  As a fraction, 7/960\\xa0\\xa07/100\\xa0=\\xa00.007\\xa0=\\xa00.7%. \\n  The step used to estimate the percent occurred when we estimated 7/960 as 7/1000. \\n  \\n  The exact percent, to the nearest thousandth of a percent, is actually 0.729%.\\n  To estimate the percent of a number, we may convert the percent to a fraction, if \\n  useful, to estimate the percent. \\n Example:\\n Estimate 13% of 72.\\n  Twice 13% is 26%, which is very close to 25%, and 25%=1/4. We may multiply both \\n  sides by 1/2 to get an estimate for 13%: 13%\\xa0\\xa012.5%\\xa0=\\xa01/2\\xa0×\\xa025%\\xa0=\\xa01/2\\xa0×\\xa01/4\\xa0=\\xa01/8. \\n  Using our estimate of 1/8 for 13%, 1/8\\xa0×\\xa072\\xa0=\\xa09, so we \\n  get an estimate of 9 for 13% of 72. \\n  If we had calculated this exactly, 13% of 72 equals 9.36. It may look like we did \\n  a lot more work to get the estimate of 9 that just multiplying 72 by 0.13, but with \\n  practice, keeping in mind some simple percents and the fractions they are equal \\n  to will enable you to estimate some number combinations very quickly. \\n Example:\\n Estimate 9.6% of 51.\\n  Method 1: We could estimate 9.6% of 50. It would be easy to estimate 9.6% of 100, \\n  which is just 9.6. Since 50 is half of 100, we can just take half of 9.6, which \\n  is 4.8. The actual value of 9.6% of 51 is 4.896, so an estimate of 4.8 is pretty \\n  good. \\n Method 2: We could estimate 10% of 51, which is just 5.1. This is not as close \\n  an estimate as method 1, but is still a good estimate of the actual answer of 4.896. \\n\\nInterest\\n Interest is a fee paid to borrow money. It is usually charged as a percent of \\n  the total amount borrowed. The percent charged is called the interest rate. The \\n  amount of money borrowed is called the principal. There are two types of interest, \\n  simple interest and compound \\n  interest. \\n Example: A bank charges 7% interest on a $1000 loan. \\n  It will cost the borrower 7% of $1000, which is $70, for each year the money is \\n  borrowed. Note that when the loan is up, the borrower must pay back the original \\n  $1000. \\n\\nSimple Interest\\n Simple interest is interest figured on the principal only, for the duration of \\n  the loan. Figure the interest on the loan for one year, and multiply this amount \\n  by the number of years the money is borrowed for.\\n Example: A bank charges 8% simple interest on a $600 \\n  loan, which is to be paid back in two years. It will cost the borrower 8% of $600, \\n  which is $48, for each year the money is borrowed. Since it is borrowed for two \\n  years, the total charge for borrowing the money will be $96. After the two years \\n  the borrower will still have to pay back the original $600. \\n\\nCompound Interest\\n Compound interest is interest figured on the principal and any interest owed from \\n  previous years. The interest charged the first year is just the interest rate times \\n  the amount of the loan. The interest charged the second year is the interest rate, \\n  times the sum of the loan and the interest from the first year. The interest charged \\n  the third year is the interest rate, times the sum of the loan and the first two \\n  years\\' interest amounts. Continue figuring the interest in this way for any additional \\n  years of the loan. \\n Example: A bank charges 8% compound interest on a $600 \\n  loan, which is to be paid back in two years. It will cost the borrower 8% of $600 \\n  the first year, which is $48. The second year, it will cost 8% of $600\\xa0+\\xa0$48\\xa0=\\xa0$648, \\n  which is $51.84. The total amount of interest owed after the two years is $48\\xa0+\\xa0$51.84\\xa0=\\xa0$99.84. \\n  Note that this is more than the $96 that would be owed if the bank was charging \\n  simple interest. \\n Example: A bank charges 4% compound interest on a $1000 \\n  loan, which is to be paid back in three years. It will cost the borrower 4% of $1000 \\n  the first year, which is $40. The second year, it will cost 4% of $1000\\xa0+\\xa0$40\\xa0=\\xa0$1040, \\n  which is $41.60. The third year, it will cost 4% of $1040\\xa0+\\xa0$41.60\\xa0=\\xa0$1081.60, \\n  which is $43.26 (with rounding). The total amount of interest owed after the three \\n  years is $40\\xa0+\\xa0$41.60\\xa0+\\xa043.26\\xa0=\\xa0$124.86. \\n\\nPercent increase and decrease\\n Percent increase and decrease of a value measure how that value changes, as a \\n  percentage of its original value. \\n Example: A collectors\\' comic book is worth $120 in \\n  1994, and in 1995 its value is $132. The change is $132\\xa0-\\xa0$120\\xa0=\\xa0$12, \\n  an increase in price of $12; since $12 is 10% of $120, we say its value increased \\n  by 10% from 1994 to 1995. \\n Example: A bakery makes a chocolate cake that has 8 \\n  grams of fat per slice. A new change in the recipe lowers the fat to 6 grams of \\n  fat per slice. The change is 8g\\xa0-\\xa06g\\xa0=\\xa02g, a decrease of 2 grams; \\n  since 2 grams is 25% of 8, we say that the new cake recipe has 25% less fat, or \\n  a 25% decrease in fat. \\n Example: Amy is training for the 1500 meter run. When \\n  she started training she could run 1500 meters in 5 minutes and 50 seconds. After \\n  a year of practice her time decreased by 8%. How fast can she run the race now? \\n  Her old time was 5\\xa0×\\xa060\\xa0+ 50\\xa0=\\xa0350 seconds, and 8% \\n  of 350 is 28, so she can run the race in 350\\xa0-\\xa028\\xa0=\\xa0322 seconds \\n  (5 minutes and 22 seconds). \\n Example: A fishing magazine sells 110000 copies each \\n  month. The company\\'s president wants to increase the sales by 6%. How many extra \\n  magazines would they have to sell to reach this goal? This problem is easy, since \\n  it only asks for the change in sales: 6% of 110000 equals 6600 more magazines. \\n\\nPercent Discount\\n A discount is a decrease in price, so percent discount is the percent decrease \\n  in price.\\n Example: Chocolate bars normally cost 80 cents each, \\n  but are on sale for 40 cents each, which is 50% of 80, so the chocolate is on sale \\n  at a 50% discount. \\n Example: A compact disc that sells for $12 is on sale \\n  at a 20% discount. How much does the disc cost on sale? The amount of the discount \\n  is 20% of $12, which is $2.40, so the sale price is $12.00\\xa0-\\xa0$2.40\\xa0=\\xa0$9.60. \\n Example: Movie tickets sell for $8.00 each, but if \\n  you buy 4 or more you get $1.00 off each ticket. What percent discount is this? \\n  We figure $1 as a percentage of $8: $1.00/$8.00\\xa0×\\xa0100%\\xa0=\\xa012.5%, \\n  so this is a 12.5% discount. \\n\\nWhat is an event?\\n An event is an experiment or collection of experiments. \\n Examples:\\n The following are examples of events. \\n 1) A coin toss.\\n  2) Rolling a die.\\n  3) Rolling 5 dice.\\n  4) Drawing a card from a deck of cards.\\n  5) Drawing 3 cards from a deck.\\n  6) Drawing a marble from a bag of different colored marbles.\\n  7) Spinning a spinner in a board game.\\n  8) Tossing a coin and rolling a die. \\n\\nPossible Outcomes of an Event\\n Possible outcomes of an event are the results which may occur from any event. \\n  (Remember, they may not occur.) \\n Examples:\\n The following are possible outcomes of events. \\n 1) A coin toss has two possible outcomes. The outcomes are \"heads\" and \\n  \"tails\".\\n  2) Rolling a regular six-sided die has six possible outcomes. You may get a side \\n  with 1, 2, 3, 4, 5, or 6 dots.\\n  3) Drawing a card from a regular deck of 52 playing cards has 52 possible outcomes. \\n  Each of the 52 playing cards is different, so there are 52 possible outcomes for \\n  drawing a card.\\n  4) How many different outcomes are there for the color of marble that may be drawn \\n  from a bag containing 3 red, 4 green, and 5 blue marbles? This event has 3 possible \\n  outcomes. You may get a red marble, a green marble, or a blue marble. Even if the \\n  marbles are different sizes, the outcome we are considering is the color \\n  of the marble that is drawn. \\n  5) How many different outcomes are there for the colors of two marbles that may \\n  be drawn from a bag containing 3 red, 4 green, and 5 blue marble? This event has \\n  6 possible outcomes: you may get two reds, two greens, two blues, a red and blue, \\n  a red and green, or a blue and green.\\n  6) Rolling two regular dice, one of them red and one of them blue, has 36 possible \\n  outcomes. The outcomes are listed in the table below. \\n\\n\\n\\n\\n\\n\\nRed Die\\n\\n\\n\\n\\n\\nResult:\\nRed1 \\nRed2\\nRed3\\nRed4\\nRed5\\nRed6 \\n\\n\\n\\nBlue1\\nBlue1, Red1 \\nBlue1, Red2\\nBlue1, Red3\\nBlue1, Red4\\nBlue1, Red5\\nBlue1, Red6 \\n\\n\\n\\nBlue2\\nBlue2, Red1 \\nBlue2, Red2\\nBlue2, Red3\\nBlue2, Red4\\nBlue2, Red5\\nBlue2, Red6 \\n\\n\\n\\n\\nBlue \\n\\t\\tDie\\n\\n\\nBlue3\\nBlue3, Red1\\nBlue3, Red2\\nBlue3, Red3\\nBlue3, Red4 \\nBlue3, Red5\\nBlue3, Red6\\n\\n\\n\\nBlue4\\nBlue4, Red1 \\nBlue4, Red2\\nBlue4, Red3\\nBlue4, Red4\\nBlue4, Red5\\nBlue4, Red6 \\n\\n\\n\\nBlue5\\nBlue5, Red1 \\nBlue5, Red2\\nBlue5, Red3\\nBlue5, Red4\\nBlue5, Red5\\nBlue5, Red6 \\n\\n\\n\\nBlue6\\nBlue6, Red1 \\nBlue6, Red2\\nBlue6, Red3\\nBlue6, Red4\\nBlue6, Red5\\nBlue6, Red6 \\n\\n\\n Note that the event tells us how to think of the outcomes. Even though there are \\n  12 different marbles in example 4, the event tells us to count only the color of \\n  the die, so there are three outcomes. In example 6, the two dice are different, \\n  and there are 36 possible outcomes. Suppose we don\\'t care about the color of the \\n  dice in example 6. Then we would only see 21 different outcomes: 1-1, 1-2, 1-3, \\n  1-4, 1-5, 1-6, 2-2, 2-3, 2-4, 2-5, 2-6, 3-3, 3-4, 3-5, 3-6, 4-4, 4-5, 4-6, 5-5, \\n  5-6, and 6-6. (We think of a 1 and a 2, a 1-2, as being the same as a 2 and a 1.) \\n\\nProbability of an Outcome\\n The probability of an outcome for a particular event is a number telling us how \\n  likely a particular outcome is to occur. This number is the ratio of the number \\n  of ways the outcome may occur to the number of total possible outcomes for the event. \\n  Probability is usually expressed as a fraction or decimal. Since the number of ways \\n  a certain outcome may occur is always smaller or equal to the total number of outcomes, \\n  the probability of an event is some number from 0 through 1. \\n Example:\\n Suppose there are 10 balls in a bucket numbered as follows: 1, 1, 2, 3, 4, 4, \\n  4, 5, 6, and 6. A single ball is randomly chosen from the bucket. What is the probability \\n  of drawing a ball numbered 1? There are 2 ways to draw a 1, since there are two \\n  balls numbered 1. The total possible number of outcomes is 10, since there are 10 \\n  balls. \\n The probability of drawing a 1 is the ratio 2/10\\xa0=\\xa01/5. \\n Example:\\n Suppose there are 10 balls in a bucket numbered as follows: 1, 1, 2, 3, 4, 4, \\n  4, 5, 6, and 6. A single ball is randomly chosen from the bucket. What is the probability \\n  of drawing a ball with a number greater than 4? There are 3 ways this may happen, \\n  since 3 of the balls are numbered greater than 4. The total possible number of outcomes \\n  is 10, since there are 10 balls. The probability of drawing a number greater than \\n  4 is the ratio 3/10. Since this ratio is larger than the one in the previous example, \\n  we say that this event has a greater chance of occurring than drawing a 1. \\n Example:\\n Suppose there are 10 balls in a bucket numbered as follows: 1, 1, 2, 3, 4, 4, \\n  4, 5, 6, and 6. A single ball is randomly chosen from the bucket. What is the probability \\n  of drawing a ball with a number greater than 6? Since none of the balls are numbered \\n  greater than 6, this can occur in 0 ways. The total possible number of outcomes \\n  is 10, since there are 10 balls. The probability of drawing a number greater than \\n  6 is the ratio 0/10\\xa0=\\xa00. \\n Example:\\n Suppose there are 10 balls in a bucket numbered as follows: 1, 1, 2, 3, 4, 4, \\n  4, 5, 6, and 6. A single ball is randomly chosen from the bucket. What is the probability \\n  of drawing a ball with a number less than 7? Since all of the balls are numbered \\n  greater than 7, this can occur in 10 ways. The total possible number of outcomes \\n  is 10, since there are 10 balls. The probability of drawing a number less than 7 \\n  is the ratio 10/10\\xa0=\\xa01.\\n  Note in the last two examples that a probability of 0 meant that the event would \\n  not occur, and a probability of 1 meant the event definitely would occur. \\n Example:\\n Suppose a card is drawn at random from a regular deck of 52 cards. What is the \\n  probability that the card is an ace? There are 4 different ways that the card can \\n  be an ace, since 4 of the 52 cards are aces. There are 52 different total outcomes, \\n  one for each card in the deck. The probability of drawing an ace is the ratio 4/52\\xa0=\\xa01/13. \\n Example:\\n Suppose a regular die is rolled. What is the probability of getting a 3 or a 6? \\n  There are a total of 6 possible outcomes. Rolling a 3 or a 6 are two of them, so \\n  the probability is the ratio of 2/6\\xa0=\\xa01/3. \\n\\n\\n\\n\\n \\n\\tVisit the Math League \\n\\n \\n\\n\\n© 1997-2001 by Math League Multimedia \\n\\tThis page may not be mirrored or reproduced on any other internet site.\\n\\tLast updated March 2001 by Mark Motyka.\\n\\n\\n\\n',\n",
       " \"\\n  \\nAssumptions in Model-based Diagnosis \\n  Assumptions in Model-based\\nDiagnosis  Dieter Fensel and Richard Benjamins  University of Amsterdam, Department SWIRoetersstraat 15, 1018\\nWB Amsterdam, the Netherlands, E-mail: {fensel | richard}@swi.psy.uva.nl\\n\\n   Abstract. Mostly, papers on\\nproblem-solving methods focus on the description of reasoning strategies and\\ndiscuss their underlying assumptions as a side aspect. We take a complementary\\npoint of view and focus on these underlying assumptions as they play three\\nimportant roles: first, assumptions are necessary to characterise the precise\\ncompetence of a problem-solving method in terms of the tasks that can be solved\\nby it, and in terms of the domain knowledge that is required by it. Second,\\nassumptions are necessary to enable tractable problem solving for complex\\nproblems. Third, assumptions are necessary for appropriate interaction of the\\nproblem solver with its environment. Their introduction and refinement can be\\nused to develop new problem-solving methods, or to adapt existing ones according\\nto task and domain-specific circumstances of a given application. For this\\npurpose, one requires a framework for dealing with these assumptions. This paper\\nmakes a step in this direction by summarising the assumptions that can be found\\nin the literature on diagnosis with component-oriented device models. The main\\ncontribution of the paper is to collect these assumptions, to make their role in\\nthe reasoning process explicit, and to classify them. \\n  1\\t There Is No Such Thing As An Assumption-Free\\nReasoning Strategy   Reasoning about real-world problems\\nis only possible by introducing assumptions about these problems. Such\\nassumptions are necessary to relate the input and the output of the reasoning\\nprocess with the actual problem. Assumptions refer to the provided case data and\\nthe precise task that should be executed by the reasoner. In the case of\\nknowledge-based reasoners, additional assumptions appear that are formulated as\\nrequirements on knowledge as it is necessary for the reasoning process. Another\\ntype of assumptions deals with the complexity of the reasoning task. In general,\\nmost problems tackled with knowledge-based systems are inherently complex and\\nintractable ([Bylander, 1991], [Bylander et\\nal., 1991], [Nebel, 1996]). A knowledge-based reasoner\\ncan only solve such tasks with reasonable computational effort by introducing\\nassumptions that restrict the complexity of the problem, or strengthen the\\nrequirements on domain knowledge (cf. [Benjamins et al.,\\n1996]). Therefore, this second type of assumptions either weakens the task\\ndefinition or introduces additional requirements on the domain knowledge that is\\nexpected by the problem solver. In the first case, the task is weakened (i.e.,\\nthe application scope is reduced) to meet the competence of the reasoning system\\nand in the second case, the competence of the reasoner is strengthened by the\\nassumed domain knowledge.\\n\\n   In [Fensel\\n& Straatman, 1996] we discussed the idea of viewing the development\\nprocess of an efficient knowledge-based reasoner as a process of introducing and\\nrefining assumptions. A simple generate & test problem solver is modified to\\na more efficient problem-solving method (a variant of propose & revise for\\nparametric design by introducing assumptions that weaken the task and that\\nstrengthen the requirements on domain knowledge. In this paper, we review the\\nwork on diagnostic reasoning systems. We focus on assumptions that are introduced\\nto define the effect of diagnostic reasoners and to improve their efficiency. The\\nwork on diagnostic reasoning provides an interesting empirical basis for our\\napproach as it provides a more than ten years long history on developing\\nknowledge-based reasoners for a complex task. During this process, several\\nassumptions have been identified and refined in the literature on model-based\\ndiagnostic systems.\\n\\n   The first diagnostic systems built\\nwere heuristic systems, in the sense that they contained compiled knowledge which\\nlinked symptoms directly to hypotheses (usually in rules). In these systems, only\\nforeseen symptoms can be diagnosed and heuristic knowledge that links symptoms\\nwith possible faults needs to be available. One of the main principles underlying\\nmodel-based diagnosis is the use of a domain model (called SBF models in [Chandrasekaran, 1991]). Heuristic knowledge that links symptoms\\nwith causes is no longer necessarily. The domain model is used for predicting the\\ndesired device behaviour, which is then compared to the observed device\\nbehaviour. A discrepancy indicates a symptom. General reasoning techniques as\\nconstraint satisfaction or truth maintenance can be used to derive diagnoses that\\nexplain the actual behaviour of the device using its model. Because the reasoning\\npart is represented separately from domain knowledge, it can be reused for\\ndifferent domains. This paradigm of model-based diagnosis gave rise to the\\ndevelopment of general approaches to diagnosis, such as ``constraint suspension``\\n[Davis, 1984], DART [Genesereth, 1984],\\nGDE [de Kleer & Williams, 1987], and several extensions\\nto GDE (GDE+ [Struss & Dressler, 1989], Sherlock [de Kleer & Williams, 1989]).\\n\\n   In\\nthis paper, we will focus on assumptions underlying approaches to diagnostic\\nproblem solving. In Section 2, we discuss assumptions that are necessary to\\nrelate a diagnostic system with its environment. That is, assumptions on the\\navailable case data, the required domain knowledge and the problem type. In\\nSection 3, we discuss assumptions introduced to reduce the complexity of the\\nreasoning process necessary to execute the diagnostic task. Such assumptions are\\nintroduced to either change the worst-case complexity or the average-case\\nbehaviour of problem solving. In Section 4, we sketch further assumptions that\\nare related to the efficiency of the interaction of the problem solver with its\\nenvironment. Section 5 sketches a general framework for specifying reasoning\\nsystems and their underlying assumptions. It discusses how assumptions can be\\nused to weaken or strengthen the problem-solving method competence. In section 6,\\nwe present conclusions.\\n\\n   2\\t The Task: Assumptions\\nfor Effect   In model-based diagnosis (cf. [de Kleer et al., 1992]), the definition of the task of the KBS\\nrequires a system description of the device under consideration and a\\nset of observations, where some indicate normal and other\\nabnormal behaviour. The goal of the task is to find a diagnosis\\nthat, together with the system description, explains the observations.\\nIn the following, we discuss four different aspects of such a task definition and\\nshow the assumptions related to each of them. The four aspects are: identifying\\nabnormalities, identifying causes of these abnormalities, defining hypotheses,\\nand defining diagnoses.\\n\\n   2.1\\t Identifying\\nAbnormalities   Identification of abnormal behaviour is\\nnecessary before a diagnostic process can be started to find explanations for the\\nabnormalities. This identification task requires three kinds of knowledge, of\\nwhich two are related to the type of input, and one to the interpretation of\\npossible discrepancies (see [Benjamins, 1993]):\\n\\n  \\n observations of the behaviour of the device must be\\nprovided to the diagnostic reasoner;   a\\nbehavioural description of the device must be provided to the\\ndiagnostic reasoner;   knowledge concerning the\\n(im)preciseness of the observations and the behavioural\\ndescription as well as comparison knowledge (thresholds etc.) are\\nnecessary (for comparison) to decide whether a discrepancy is significant. Other\\nrequired knowledge concerns the interpretation of missing values, and\\nwhether an observation can have several values (i.e., its value type).  \\n Relevant assumptions state that the two types of inputs need to\\nbe reliable. Otherwise, the discrepancy could be explained by a\\nmeasuring fault or a modelling fault. In other words, these assumptions guarantee\\nthat if a prediction yields a different behaviour than the observed behaviour of\\nthe artefact, then the artefact has a defect [Davis &\\nHamscher, 1988]).\\n\\n   These assumptions are also\\nnecessary for the meta-level decision whether a diagnosis problem is given at all\\n(i.e., whether there is an abnormality in system behaviour). This decision relies\\non a further assumption: the no-design-error assumption [Davis, 1984] which says that if no fault occurs, then the\\ndevice must be able to achieve the desired behaviour. In other words, the\\ndiscrepancy must be the result of a fault situation where some parts of the\\nsystem are defect. It cannot be the result of a situation where the system works\\ncorrectly, but cannot achieve the desired functionality because it is not\\ndesigned for this. If this assumption does not hold, one has a design problem and\\nnot a diagnostic problem.\\n\\n   2.2\\t Identifying\\nCauses   Another purpose of the system description is the\\nidentification of causes of faulty behaviour. This cause-identification knowledge\\nmust be reliable [Davis & Hamscher, 1988], or,\\nin other words, the knowledge used in model-based diagnosis is assumed to be a\\ncorrect and complete description of the artefact. Correct and complete in the\\nsense, that it enables the derivation of correct and complete diagnoses if\\ndiscrepancies appear. In accordance with different types of device models and\\ndiagnostic methods, these assumptions wear different clothes. During the\\nfollowing we restrict our attention to component-oriented device models that\\ndescribe a device in terms of components, their behaviours (a functional\\ndescription), and their connections. The set of all possible hypotheses is the\\npower-set of the set of annotated components\\n\\n  \\n{(c1), (c1), ..., (cn)},   where modeji(cj) describes that the j-th\\ncomponent is in mode i. [Davis, 1984] has pointed\\nout that one should be aware of the underlying assumptions for such a diagnostic\\napproach and listed a number of them.\\n\\n   First, the\\nlocalised-failure-of-function assumption: the device must be\\ndecomposable in well-defined and localised entities (i.e., a component) that can\\nbe treated as causes of faulty behaviour. Second, these components have a\\nfunctional description that provides the (correct) output for their possible\\ninputs. If this functional description is local, that is, it does not refer to\\nthe functioning of the whole device, the no-function-in-structure\\nassumption [de Kleer & Brown, 1984] is satisfied. Several\\ndiagnostic methods can also use the reverse of the functional descriptions, thus,\\nrules that derive the expected input from the provided output. If only\\ncorrect functional descriptions are available, fault behaviour is defined as any\\nother behaviour than the correct one. Fault behaviour of components can be\\nconstrained by including fault models, that is, functional descriptions of\\nthe components in case they are broken (cf. [de Kleer &\\nWilliams, 1989], [Struss & Dressler, 1989]). If one\\nassumes that these functional descriptions are complete (the complete\\nfault-knowledge assumption), then components can be considered innocent if\\nnone of their fault descriptions is consistent with the observed fault behaviour.\\nA result of using fault models is that all kinds of non-specified behaviours of a\\ncomponent are excluded as diagnosis. For example, using fault models, it becomes\\nimpossible to conclude that a fault (one of two light bulbs is not working) is\\nexplained by a defect battery that does not provide power and a defect lamp that\\nlights without electricity (cf. [Struss & Dressler,\\n1989]).\\n\\n   Further assumptions that are related to the\\nfunctional descriptions of components are the no-fault-masking and the\\nnon-intermittency assumption. The former states that the defect of an\\nindividual or composite component, or of the entire device must be visible by\\nchanged outputs (cf. [Davis & Hamscher, 1988], [Raiman, 1992]). According to the latter, a component that gets\\nidentical inputs at different points of time, must produce identical outputs. In\\nother words, the output is a function of the input (cf. [Raiman\\net al., 1991]). They argue that intermittency results from incomplete input\\nspecifications of components, but that it is impossible to get rid of it (it is\\nimpossible to represent the required additional inputs).\\n\\n  \\nA third assumption underlying many diagnostic approaches is the\\nno-faults-in-structure assumption (cf. [Davis &\\nHamscher, 1988]) that manifests itself in different variants according to the\\nparticular domain. The assumption states that the interactions of the components\\nare correctly modelled and that they are complete. This assumption gives rise to\\nthree different classes of more specific assumptions. First, the\\nno-broken-interaction assumption states that connections between the\\ncomponents work correctly (e.g. no wires between components are broken). If this\\nis not the case, the assumption can be weakened by representing the connections\\nthemselves as components too. Second, the no-unexpected-directions\\nassumption (or existence of a causal-pathway assumption) states that the\\ndirections of the interactions are correctly modelled and are complete. For\\nexample, a light-bulb gets power from a battery and there is no interaction in\\nthe opposite direction. The no-hidden-interactions assumption (cf. [Böttcher, 1996]) assumes that there are no non-represented\\ninteractions (i.e., closed-world assumptions on connections). A bridge fault [Davis, 1984] is an example of a violation of this assumption in\\nthe electronic domain. Electronic devices whose components unintendedly interact\\nthrough heat exchange, is another example [Böttcher,\\n1996]. In the worst case, all potential unintended interaction paths between\\ncomponents are represented [Preist & Welhalm, 1990]. The\\nno-hidden-interactions assumption is critical since most models (like design\\nmodels of the device) describe correctly working devices and unexpected\\ninteractions are therefore precisely not mentioned. A refinement of this\\nassumptions is that there are no assembly errors (i.e., every individual\\ncomponent works but they have been wired up incorrectly).\\n\\n  \\n2.3\\t Defining Hypotheses   In addition to\\nknowledge that is required to identify a discrepancy and knowledge that provides\\nhypotheses used to explain these discrepancies, one requires further knowledge to\\ndecide which type of explanation is required. [Console &\\nTorasso, 1992] distinguish two types of explanations: weak explanations, that\\nare consistent with the observations (no contradiction can be derived\\nfrom the union of the device model, the observations, and the hypothesis), and\\nstrong explanations, that imply the observations (the observations can\\nbe derived from the device model and the hypothesis). Both types of explanation\\ncan be combined by dividing observations in two classes: observations that need\\nto be explained by deriving them from a hypothesis, and observations that need\\nonly be consistent with the hypothesis. In this case one requires knowledge\\nthat allows to divide the set of observations. The decision which type of\\nexplanation to use, can only be made based on assumptions about the environment\\nin which the KBS is used.\\n\\n   2.4\\t Defining\\nDiagnoses   Having established observations, hypotheses\\nand an explanatory relation that relates hypotheses with observations, one must\\nestablish the notion of diagnosis. Not each hypothesis that correctly\\nexplains all observations needs to be a desired diagnosis. One could accept only\\nparsimonious hypotheses as diagnoses (cf. [Bylander et al., 1991]). An hypothesis or explanation H is\\nparsimonious if H is an explanation and there exists no other hypothesis\\nH' that also is an explanation and H' < H.\\nH is a diagnosis if H is a parsimonious explanation. One has to\\nmake assumptions about the desired diagnosis (cf. [McIlraith,\\n1994]) in order to define the partial ordering (<) on hypotheses. For\\nexample, whether the diagnostic task is concerned with finding all components\\nthat are necessarily fault to explain the system behaviour, or whether it is\\nconcerned with finding all components that are necessarily correct to explain the\\nsystem behaviour. In the first case, we aim at economy in repair, whereas in\\nsafety critical applications (e.g., monitoring of nuclear power plants) one\\nshould obviously choose for the second case. \\n\\n   As shown by\\n[McIlraith, 1994], the assumptions about the type of\\nexplanation relation (i.e., consistency versus derivability) and about the\\nexplanations (i.e., definition of parsimony) make also strong commitments on the\\ndomain knowledge (the device model) that is used to describe the system. If we\\nask for a consistent explanation with minimal sets of fault components (i.e.,\\nH1 < H2 if\\nH1 assumes less components as being fault than\\nH2), we need knowledge that constrains the normal behaviour\\nof components. If we ask for a consistent explanation with minimal sets of\\ncorrect components (i.e., H1 < H2\\nif H1 assumes less components as being correct than\\nH2), we need knowledge that constrains the abnormal\\nbehaviour of components.\\n\\n   The definition of parsimonious\\nhypotheses introduces a preference on hypotheses. This could be extended\\nby defining further preferences on diagnoses to select one optimal one (e.g., by\\nintroducing assumptions related to the probability of faults). Again, knowledge\\nabout preferences must be available to define a preference function and a\\ncorresponding ordering.\\n\\n   2.5\\t Summary   Figure 1 summarises the assumptions that are\\ndiscussed above and groups them according to their purpose. All these assumptions\\nare necessary to relate the definition of the functionality of the diagnostic\\nsystem with the diagnostic problem (i.e., the task) to be solved and with the\\ndomain knowledge that is required to define the task. Table\\n1 provides an explanation of the assumptions along with the role they play\\n(function), the domain they are about (case data, domain knowledge or task), and\\nsome references where they are discussed in more detail.  \\n\\n     Figure 1 Assumptions for Effect\\n \\n  \\n All these assumptions are necessary to relate a model of the\\ndevice with the actual device under concern.  ``There is no such thing as an\\nassumption-free representation. Every model, every representation contains\\nsimplifying assumptions`` [Davis & Hamscher, 1988]. If\\nthe assumptions are too strong, one could consider weakening them. However, this\\nraises another problem in model-based diagnosis, namely its high complexity\\nor intractability. This will be discussed in the following section.\\n\\n \\n 3\\t The Problem Solver: Assumptions for Efficiency\\n  Besides the assumptions that are necessary to define the\\ntask, further assumptions are necessary because of the complexity of model-based\\ndiagnosis. Component-based diagnosis is in the worst case exponential in the\\nnumber of annotated components ([Bylander et al., 1991]).\\nEvery element of the power-set of the set of annotated components is a possible\\nhypothesis. As we are not interested in problem-solving in principle but in\\npractice, further assumptions have to be introduced that either decrease the\\nworst-case, or at least the average-case behaviour.\\n\\n  \\n3.1\\t Reducing the Worst-Case Complexity: The Single-Fault Assumption\\n  A drastic way to reduce the complexity of the diagnostic\\ntask is achieved by the single-fault or N-fault assumption\\n(SFA) [Davis, 1984], which reduces the complexity to\\npolynomial in the number of components. If the single-fault assumption holds, the\\nincorrect behaviour of the device is completely explainable by one failing\\ncomponent. This assumption defines either strong requirements on the provided\\ndomain knowledge, or significantly restricts the diagnostic problems that can\\ncorrectly be handled by the diagnostic system (cf. [Benjamins et\\nal., 1996]).\\n\\n   If the SFA has to be satisfied by the\\ndomain knowledge, then each possible fault has to be represented as a\\nsingle entity. In principle this causes complexity problems for the domain\\nknowledge as each fault combination (combination of fault components) has to be\\nrepresented. However, additional domain knowledge can be used to restrict the\\nexponential increase. [Davis, 1984] discusses an example of a\\nrepresentation change where a 4-fault case (i.e., 15 different combinations of\\nfaults) is transformed into a single fault. A chip with four ports can cause\\nfaults on each port. When we know that the individual ports never fail, but only\\nthe chip as a whole, a fault on four ports can be represented as one fault of the\\nchip. Even without such a representation change, we do not necessarily have to\\nrepresent all possible fault combinations. We can, for example, exclude all\\ncombinations that are not possible or likely in the specific domain (expert\\nknowledge).\\n\\n   Instead of formulating the requirement above\\non the domain knowledge, one can also weaken the task definition by this\\nassumption. This means that the competence of the PSM meets the task definition\\nunder the assumption that only single fault occurs. That is, only in cases where\\na single fault occurs, the method works correctly and complete. This would imply\\nthat the diagnostic system is designed for simple routine diagnoses.\\n\\n   3.2\\t Reducing the Average-Case Behaviour: The Minimality\\nAssumption of GDE   As the SFA might be too strong an\\nassumption for several applications, either as a requirement on the domain\\nknowledge or as a restriction on the task, [Reiter, 1987] and\\n[de Kleer & Williams, 1987] provide approaches able to\\ndeal with multiple faults. However, this re-introduces the complexity problems of\\nMBD. To deal with this problem, GDE [de Kleer & Williams,\\n1987] exploits the minimality assumption, which reduces, in\\npractical cases, the exponential worst case behaviour to a complexity that grows\\nwith the square of the number of components. In GDE, this assumptions helps\\nreducing the complexity in two ways. First, a conflict is a set of components\\nwhere it cannot be the case that all components work correctly given the provided\\ndomain knowledge and the observed behaviour. Under the minimality assumption,\\neach super-set of a conflict is also a conflict and all conflicts can be\\nrepresented by minimal conflicts. Second, a hypothesis contains at least one\\ncomponent of each conflict. Every super-set of such a hypothesis is again a\\nhypothesis. Therefore, diagnoses can be represented by minimal diagnoses. The\\nminimality assumption requires that diagnoses are independent or monotonic (see\\n[Bylander et al., 1991]): a diagnosis that assumes more\\ncomponents as being faulty, explains more observations.\\n\\n   A\\ndrastic way to ensure that the minimality assumption holds, is to neglect any\\nknowledge about the behaviour of faulty components. Thus, any behaviour that is\\nnot correct is considered as fault. A disadvantage of this is that physical rules\\nmay be violated (i.e., existing knowledge about faulty behaviour). We already\\nmentioned the example provided in [Struss & Dressler,\\n1989], where a fault (one of two bulbs does not light) is explained by a\\nbroken battery that does not provide power and a broken bulb that lights without\\npower. Knowledge about how components behave when they are faulty (called fault\\nmodels) could be used to constrain the set of diagnoses derived by the system. On\\nthe other hand, it increases the complexity of the task. If for one component\\nm possible fault behaviours are provided, this leads to m+1\\npossible states instead of two (correct and fault). The maximum number of\\ncandidates increases from 2n to (m+1)n.\\n\\n\\n  A similar extension of GDE that includes fault models, is\\nthe Sherlock system (cf. [de Kleer & Williams, 1989]).\\nWith fault models, it is no longer guaranteed that every super-set of the faulty\\ncomponents that constitute the diagnosis, is also a diagnosis, and therefore the\\nminimality assumption as such cannot be exploited. In Sherlock, a diagnosis does\\nnot only contain fault components (and implicitly assumes that all other, not\\nmentioned, components are correct), but it contains a set of components assumed\\nto work correctly and a set of components assumed to be fault. A conflict is now\\na set of some correct and fault components that is inconsistent with the provided\\ndomain knowledge and the observations. In order to accommodate to this situation,\\n[de Kleer et al., 1992] extend the concept of minimal\\ndiagnoses to kernel diagnoses and characterise the conditions under which the\\nminimality assumption still holds. The kernel diagnoses are given by the prime\\nimplicants of the minimal conflicts. Moreover, the minimal sets of kernel\\ndiagnoses sufficient to cover every diagnosis correspond to the irredundant sets\\nof prime implicants of all minimal conflicts. These extensions cause drastic\\nadditional effort, because there can be exponentially more kernel diagnoses than\\nminimal diagnoses, and finding irredundant sets of prime implicants is NP-hard.\\nTherefore, [de Kleer et al., 1992] characterise two\\nassumptions under which the kernel diagnoses are identical to the minimal\\ndiagnoses. The kernel diagnoses are identical to the minimal diagnoses if all\\nconflicts contain only fault components. In this case, there is again only one\\nirredundant set of minimal diagnoses (the set containing all minimal diagnoses).\\nThe two assumptions that can ensure these properties are the\\nignorance-of-abnormal-behaviour assumption and the\\nlimited-knowledge-of-abnormal-behaviour assumption.\\n\\n   The ignorance-of-abnormal-behaviour assumption excludes knowledge\\nabout faulty behaviour and thus characterises the original situation of GDE. The\\nlimited-knowledge-of-abnormal-behaviour assumption states that the knowledge of\\nabnormal behaviour does not rule out any diagnosis indicating a set of fault\\ncomponents, if there exist a valid diagnosis indicating a subset of them as\\nfaulty components, and if the additional assumed fault components are not\\ninconsistent with the observations and the system description. The latter\\nassumption is a refinement of the former, that is, the truth of the\\nignorance-of-abnormal-behaviour assumption implies the truth of the\\nlimited-knowledge-of-abnormal-behaviour assumption.\\n\\n   A\\nsimilar type of assumption is used by [Bylander et al., 1991]\\nto characterise different complexity classes of component-based diagnosis. In\\ngeneral, finding one or all diagnoses is intractable. The independent\\nand monotonic assumption, which have the same effect as the\\nlimited-knowledge-of-abnormal-behaviour assumption, require that each super-set\\nof a diagnosis indicating a set of faulty components is also a diagnosis. In this\\ncase, the worst-case complexity of finding one minimal diagnosis grows\\npolynomially with the square of the number of components. However, the task of\\nfinding all minimal diagnoses is still NP-hard in the number of components. This\\ncorresponds to the fact that the minimality assumption of GDE (i.e., the\\nignorance-of-abnormal-behaviour and limited-knowledge-of-abnormal-behaviour\\nassumptions), that searches for all diagnoses, does not change the worst-case but\\nonly the average-case behaviour of the diagnostic reasoner.\\n\\n   3.3\\t Search Guidance   The complexity\\nof component-based diagnosis (especially when working with fault models) requires\\nfurther assumptions that enable efficient reasoning for practical cases (cf. [Struss, 1992], [Böttcher & Dressler,\\n1994]). Again, these assumptions do not change the worst case complexity but\\nshould reduce the necessary effort in practical cases. A well-known notion to\\nincrease efficiency is a reasoning focus. Defining a focus for the reasoning\\nprocess can be achieved by exploiting hierarchies or probability information. The\\nhierarchically-layered device-model assumption assumes the existence of\\nhierarchically layered models that allow step wise refinement of diagnosis to\\nreduce the complexity of the diagnostic process (cf. the complexity analysis of\\nhierarchical structures of [Goel et al., 1987]). The large\\nnumber of components at the lowest level of refinement is replaced by a small\\nnumber of components at a higher level. Only the relevant parts of the model are\\nrefined during the problem-solving process. The hierarchically-layered\\nbehavioural-model assumption assumes the existence of more abstract\\ndescriptions of the behaviour that can improve the efficiency because reasoning\\ncan be performed at a more coarse grained, and thus simpler, level (cf. [Abu-Hanna, 1994]). The existence-of-probabilities\\nassumption assumes knowledge about the probability of faults that can be used to\\nguide the search process for diagnoses by focusing on faults with high\\nprobabilities. Usually, these probabilities introduce new assumptions (e.g., the\\ncomponents-fail-independently assumption [de Kleer &\\nWilliams, 1989]).\\n\\n   All these knowledge types and their\\nrelated assumptions rely on further assumptions concerning the utility of this\\nsearch control knowledge. For example, the hierarchically-layered device model\\nimproves only the search process when the faults are not distributed in a way\\nthat enforces the problem solver to expand each abstract component descriptions\\nto their lowest levels. It significantly improves the search process if the\\nproblem solver need to refine only one abstract component description at each\\nlevel.\\n\\n   3.4\\t Summary   Figure 2 summarises the assumptions and groups them\\naccording to their purpose. All these assumptions are introduced to reduce the\\ncomputational effort required to solve the problem. Table\\n2 provides an explanation of the assumptions along with the role they play\\n(function), the domain they are about (case data, domain knowledge or task), and\\nsome references where they are discussed in more detail.\\n\\n \\n Figure 2 Assumptions for Efficiency\\n  \\n 4\\t Efficient Interaction with the Environment: Hypothesis\\nDiscrimination   Hypothesis discrimination\\nbecomes necessary if the number of hypotheses found, exceeds the desired number\\n(cf. Table [Davis & Hamscher, 1988]). Additional\\nobservations must be provided as the initial observations were not strong enough\\nto discriminate between existing hypotheses. Assumptions related to this activity\\ninclude the following (see Table [Benjamins, 1993]). First,\\nit must be possible to obtain additional observations. Examples of more\\nspecific versions of this assumption are: can the device be unfastened, are\\nmeasuring points reachable, can components be replaced easily to test behaviour,\\ncan new input be provided to the device, etc. Second, assumptions can be made\\nabout the utility of additional observations. One can assume cost\\ninformation of additional measurements and knowledge about their discriminatory\\npower (i.e., knowledge about dependencies between hypotheses) to optimise their\\nselection. Again, NP-hard problems arise if one tries to optimise these\\ndecisions. Therefore, assumptions concerning heuristic knowledge that\\nguide this process are necessary.\\n\\n \\n  \\n  All these assumptions are necessary to optimise the\\ncooperation of the diagnostic system with its environment. In principle, one\\ncould assume that all observations that are possible are provided to the system\\nbefore it starts its diagnostic reasoning. However, collecting observations is\\noften a major cost-determining factor. Therefore, assumptions are introduced\\nconcerning the efficiency of gathering information with minimal costs.\\n\\n   5\\t The Role of Assumptions in Specifying Knowledge-Based\\nReasoning Systems   In [Fensel et al.,\\n1996], we provided different aspects of a specification of knowledge-based\\nsystem which are related by assumptions (see Figure 3): a\\ntask definition defines the problem to be solved by the knowledge-based\\nsystem; a problem-solving method defines the reasoning process of the\\nknowledge-based system; and a domain model describes the domain\\nknowledge of the knowledge-based system.\\n\\n\\n \\n Figure 3 The different elements of a specification of a\\nknowledge-based system\\n\\n  The task definition specifies the goals\\nthat should be achieved in order to solve a given problem, which are functionally\\nspecified as an input-output relation. A task definition also defines assumptions\\nabout the domain knowledge. For example, a task that concerns the selection of\\nthe maximal element of a set of elements, requires a preference relation as\\ndomain knowledge. Assumptions are also used to define the requirements on such a\\nrelation (e.g. transitivity, symmetry, etc.).\\n\\n   The\\nreasoning of a knowledge-based system can be described by a problem-solving\\nmethod (PSM). A PSM consists of three parts. First, a definition of\\nthe functionality defines the competence of the PSM independent of its\\nrealisation. Second, an operational description defines the dynamic\\nreasoning process. Such an operational description describes how the competence\\ncan be achieved in terms of the reasoning steps and their dynamic interaction\\n(i.e., the knowledge and control flow). The third part of a PSM concerns\\nassumptions about the domain knowledge. Each inference step requires a\\nspecific type of domain knowledge with specific characteristics. These complex\\nrequirements on the input of a problem-solving method distinguish it from usual\\nsoftware products. Preconditions on inputs (used in normal software to guarantee\\nvalid inputs) are in PSMs extended to complex requirements on available domain\\nknowledge. A problem-solving method can only solve a complex task with reasonable\\ncomputational effort by introducing assumptions. Such assumptions can work in two\\ndirections to achieve this result. First, they can restrict the complexity of the\\nproblem, that is, weaken the task definition in such a way that the PSM\\ncompetence is sufficient to realise the task. Second, they can strengthen the\\ncompetence of the PSM by the assumed (extra) domain knowledge. This is called the\\nlaw of conservation of assumptions in [Benjamins\\net al., 1996]. In terms of dynamic logic [Harel,\\n1984], this law can be formulated as follows:\\n\\n   assumptionsdomain-knowledge \\n([problem-solving method] assumptionstask  goal)\\n  The formula states that, if the\\nassumptions on domain knowledge hold in the initial state, then the\\nproblem-solving method must terminate in a state that fulfils the goals of the\\ntask, if the assumptions on the task are fulfilled. This formula could be\\nweakened by either strengthening the assumptions on domain knowledge (i.e., the\\nproblem-solving method must behave well for less initial states) or strengthening\\nthe assumptions on the task (i.e., the problem-solving method must achieve the\\ngoal in less terminal states).\\n\\n   The description of the\\ndomain model introduces the domain knowledge as it is required by the\\nproblem-solving method and the task definition. Three elements are needed to\\ndefine a domain model. First, a description of properties of the domain knowledge\\nat a meta-level. The meta-knowledge characterises properties of the\\ndomain knowledge. It is the counterpart of the assumptions on domain knowledge\\nmade by the other parts of a KBS specification: assumptions made about domain\\nknowledge by these parts, must be stated as properties of the domain knowledge.\\nThese properties define goals for the modelling process of domain knowledge in\\nthe case of knowledge acquisition. The second element of a domain model concerns\\nthe domain knowledge and case data necessary to define the task\\nin the given application domain, and necessary to carry out the inference steps\\nof the chosen problem-solving method. The third element is formed by external\\nassumptions that link the domain knowledge with the actual domain. These\\nexternal assumptions can be viewed as the missing pieces in the proof that the\\ndomain knowledge fulfils its meta-level characterisations.\\n\\n   The assumptions of the different parts of a KBS specification define\\ntheir proper relationships and the adequate relationship of the overall\\nspecification with its environment. Their detection, verification, and validation\\nis an important part for developing correct reasoning systems. In [Fensel et al., 1996], we used the Karlsruhe Interactive\\nVerifier (KIV) [Reif, 1995], developed in the specifications.\\nBesides verification, KIV was used to detect hidden assumptions which were\\nnecessary to relate the competence of a problem-solving method to the task\\ndefinition. Hidden assumptions could be found by analysing failed proof attempts.\\nThe analysis of partial proofs gives some hints for the construction of possible\\ncounter examples, but also for repairing the proof by introducing further\\nassumptions. These assumptions are the missing pieces in proofing the\\ncorrectness of the specification. Verifying these specification is therefore a\\nway to detect underlying hidden assumptions.\\n\\n   6\\nConclusion   It is essential to know the underlying\\nassumptions of a reasoning system in order to know when it is applicable.\\nMoreover, assumptions are good ways to characterise them and they can be used to\\nguide the acquisition process of domain knowledge. They define the type of\\nknowledge and its properties as it is are required by the reasoner.\\n\\n   In [Fensel, 1995] we provided the analysis of\\nthe assumptions of one problem-solving method. In this paper we have provided a\\nstudy of the role of assumptions used by a the group of problem-solving methods\\nfor model-based diagnosis. Model-based diagnostic systems were first introduced\\nto overcome the limitations of heuristic systems. However, research on\\nmodel-based systems revealed immediately that ``pure`` model-based diagnostic\\nsystems were too inefficient to be useful. Model-based diagnosis is in principle\\nintractable, and in the last ten years, substantial effort has been devoted to\\noptimise diagnostic algorithms in order to make them tractable. Assumptions can\\nbe viewed as the re-introduction, however controlled, of heuristics for reasons\\nof efficiency. Looking at the history of model-based diagnostic systems in\\nretrospect, we could describe it as follows: researchers have started with a\\ngeneral, but inefficient, model-based diagnostic reasoner, and then incrementally\\nintroduced and modified assumptions, until an efficient reasoner was achieved.\\nThis approach is strikingly similar to the assumption-driven development process\\nof problem solvers recently put forward in the knowledge engineering community [Fensel & Straatman, 1996], where one starts with a general,\\nbut inefficient, specification of the reasoning process and gradually introduces\\nand modifies assumptions until a efficient reasoner is achieved. In [Fensel et al., 1996] is shown how verification tools can be\\nused to support the process of detecting and introducing assumptions of reasoning\\nsystems.\\n\\n   In future work we will use the assumptions\\nidentified in this paper to construct, in an assumption-driven manner, efficient\\nknowledge-based reasoners. We will also investigate whether the identified\\nassumptions can be structured in a such a way that they support this process. In\\nother words, we are looking for libraries of reusable assumptions. Still, our\\ncurrent work suffers from serious shortcomings. We have not provided a proper\\ndefinition of the term assumption nor a real justification for the\\nclassification we applied in the paper. The assumptions where collected as they\\ncould be found in papers on model-based diagnosis but it lacks a\\ntheoretical-based framework that would allow to detect gaps in this list and to\\ndecide when a level of (relative) completeness is achieved.\\n\\n \\nAcknowledgement. We would like to thank Annette ten Teije for\\nguiding us through the literature on model-based diagnosis and for helpful\\ncomments on early drafts of the paper. Helpful comments were also provided by\\nClaudia Böttcher, Bert Bredeweg, Joost Breuker, Kees de Koning, Remco\\nStraatman, and anonymous reviewers.\\n\\n  References   A. Abu-Hanna: Multiple\\nDomain Models in Diagnostic Reasoning, PhD thesis, University of Amsterdam,\\n1994.\\n\\n   R. Benjamins: Problem-Solving Methods for\\nDiagnosis, PhD Thesis, University of Amsterdam, Amsterdam, the Netherlands,\\n1993.\\n\\n   R. Benjamins, D. Fensel, and R. Straatman:\\nAssumptions of Problem-Solving Methods and Their Role in Knowledge Engineering.\\nIn Proceedings of the 12. European Conference on Artificial Intelligence\\n(ECAI-96), Budapest, August 12-16, 1996.\\n\\n   R.\\nBenjamins and C. Pierret-Golbreich: Assumptions of Problem-Solving Method. In N.\\nShadbolt et al. (eds.), Advances in Knowledge Acquisition, Lecture Notes\\nin Artificial Intelligence (LNAI), no 1076, Springer-Verlag, Berlin, 1996.\\n\\n\\n  C. Böttcher: No Faults in Structure? - How to Diagnose\\nHidden Interactions, 1996.\\n\\n   C. Böttcher and O.\\nDressler: A Framework For Controlling Model-Based Diagnosis Systems with Multiple\\nActions, Annals of Mathematics and Artificial Intelligence, 11:241-261,\\n1994.\\n\\n   T. Bylander: Complexity Results for Planning. In\\nProceedings of the 12th International Joint Conference on Artificial\\nIntelligence (IJCAI-91), Sydney, Australia, August 1991.\\n\\n   T. Bylander, D. Allemang, M. C. Tanner, and J. R. Josephson: The\\nComputational Complexity of Abduction, Artificial Intelligence, 49:\\n25-60, 1991.\\n\\n   B. Chandrasekaran: Models versus rules, deep\\nversus compiled, content versus form, IEEE-Expert, 6(2): 75--79.\\n\\n \\n L. Console and P. Torasso: A Spectrum of Logical Definitions of\\nModel-Based Diagnosis. In W. Hamscher et al. (eds.), Readings in Model-based\\nDiagnosis, Morgan Kaufman Publ., San Mateo, CA, 1992.\\n\\n   R. Davis: Diagnostic Reasoning Based on Structure and Behavior,\\nArtificial Intelligence, 24: 347-410, 1984.\\n\\n   R.\\nDavis and W. Hamscher: Model-based Reasoning: Troubleshooting. In H. E. Shrobe\\n(ed.), Exploring AI: Survey Talks from the National Conference on AI,\\nMorgen Kaufman, San Mateo, CA, 1988.\\n\\n   D. Fensel:\\nAssumptions and Limitations of a Problem-Solving Method: A Case Study.  In\\nProceedings of the 9th Banff Knowledge Acquisition for Knowledge-Based System\\nWorkshop (KAW-95), Banff, Canada, February 26th - February 3th, 1995.\\n\\n\\n  D. Fensel, A. Schönegge, R. Groenboom and B. Wielinga:\\nSpecification and Verification of Knowledge-Based Systems. In Proceedings of\\nthe ECAI-96 Workshop Validation, Verification and Refinement of Knowledge-Based\\nSystems, at the 12th European Conference on Artificial Intelligence\\n(ECAI-96), Budapest, 12.-16. August 1996. See also: Proceedings of the\\n10th Banff Knowledge Acquisition for Knowledge-Based System Workshop\\n(KAW-96), Banff, Canada, November 9-14, 1996\\n\\n   D.\\nFensel und R. Straatman: The Essence of Problem-Solving Methods: Making\\nAssumptions for Efficiency Reasons. In N. Shadbolt et al. (eds.), Advances in\\nKnowledge Acquisition, Lecture Notes in Artificial Intelligence (LNAI), no\\n1076, Springer-Verlag, Berlin, 1996.\\n\\n   M. R. Genesereth:\\nThe Use of Design Descriptions in Automated Diagnosis, Artificial\\nIntelligence (AI), 24:411-436, 1984.\\n\\n   A. Goel, N.\\nSoundararajan, and B. Chandrasekaran: Complexity in Classificatory Reasoning. In\\nProceedings of the 6th National Conference on Artificial Intelligence\\n(AAAI-87), Seattle, Washington, July 13-17, 1987.\\n\\n   D.\\nHarel: Dynamic Logic. In D. Gabby et al. (eds.), Handbook of Philosophical\\nLogic, vol. II, Extensions of Classical Logic, Publishing Company, Dordrecht\\n(NL), 1984.\\n\\n   J. de Kleer and J. S. Brown: A Qualitative\\nPhysics Based on Confluences, Artificial Intelligence, 24:7-83, 1984.\\n\\n\\n  J. de Kleer, A. K. Mackworth, and R. Reiter: Characterizing\\nDiagnoses and Systems, Artificial Intelligence, 56, 1992.\\n\\n   J. de Kleer and B. C. Williams: Diagnosing Multiple Faults,\\nArtificial Intelligence, 32:97-130, 1987.\\n\\n   J. de\\nKleer and B. C. Williams: Diagnosis with Behavioral Modes. In Proceedings of\\nthe 11th International Joint Conference on AI (IJCAI-89), Detroit, MI,\\n1989.\\n\\n   E. J. McCluskey: Minimizing of Boolean Functions,\\nBell Systems Technology Journal, 35(5):1417-1444, 1956.\\n\\n   S. McIlraith: Further Contribution to Characterizing Diagnosis,\\nAnnals of Mathematics and AI, special issues on model-based diagnosis,\\n11(1-4), 1994.\\n\\n   B. Nebel: Artificial intelligence: A\\nComputational Perspective. To appear in G. Brewka (ed.), Essentials in\\nKnowledge Representation.\\n\\n   W. Nejdl, P. Froehlich and\\nM. Schroeder: A Formal Framework For Representing Diagnosis Strategies in\\nModel-Based Diagnosis Systems. In Proceedings of the 14th International Joint\\nConference on AI (IJCAI-95), Montreal, Canada, August 20-25, 1995.\\n\\n \\n C. Preist and B. Welham: Modelling Bridge Faults fo Diagnosis in\\nElectronic Circuits. In Proceedings of the 1st International Workshop on\\nPrinciples of Diagnosis, Stanford, 1990.\\n\\n   O. Raiman:\\nDiagnosis as a Trial. In Proceedings of the Model Based Diagnosis\\nInternational Workshop, Paris, 1989.\\n\\n   O. Raiman: The\\nAlibi Principle. In W. Hamscher et al. (eds.), Readings in model-based diagnosis,\\nMorgan Kaufmann Publ., San Mateo, CA, 1992.\\n\\n   O. Raiman, J.\\nde Kleer, V. Saraswat, M. Shirley: Characterizing Non-Intermittent Faults. In\\nProceedings of the 9th National Conference on AI (AAAI-91), Anaheim, CA,\\nJuly 14-19, 1991.\\n\\n   W. Reif: The KIV Approach to Software\\nEngineering. In M. Broy and S. Jähnichen (eds.): Methods, Languages, and\\nTools for the Construction of Correct Software, Lecture Notes in Computer\\nScience (LNCS), no 1009, Springer-Verlag, Berlin, 1995.\\n\\n  \\nR. Reiter: A Theory of Diagnosis from First Principles, Artificial\\nIntelligence, 32:57-95,1987.\\n\\n   J. Sticklen, B.\\nChandrasekaran, and W.E. Bond: Applying a Functional Approach for Model Based\\nReasoning, Proc. of (IJCAI) Workshop on Model Based Reasoning, Detroit, 1989,\\n\\n\\n  P. Struss: Diagnosis as a Process. In W. Hamscher etal.\\n(eds.), Readings in Model-based Diagnosis, Morgan Kaufman Publ., San\\nMateo, CA, 1992.\\n\\n   P. Struss and O. Dressler: ``Physical\\nNegation`` - Integrating Fault Models into the General Diagnostic Engine. In\\nProceedings of the 11th International Joint Conference on AI (IJCAI-89),\\nDetroit, MI, 1989.\\n\\n   A. ten Teije and F. Van Harmelen:\\nUsing reflection techniques for flexible problem solving (with examples from\\ndiagnosis), Future Generation Computer Systems, to appear 1996.\\n\\n \\n\\nLast Modified: 02:22pm MET DST, October 05, 1996  \\n\",\n",
       " '\\n\\n\\n\\n\\nFrontiers in Conceptual Navigation 2\\n\\n\\n\\n\\n \\nDr. Kim H. Veltman\\nFrontiers in Conceptual Navigation 2: Interfaces for Cultural\\nHeritage\\n\\n\\n1. Introduction\\n2. Emerging Interface Technologies\\n3. Virtual Guides and Physical Museums\\n4. Virtual Museums, Libraries and Spatial Navigation\\n5. Historical Virtual Museums\\n6. Imaginary Museums\\n7. Metadata\\n8. Research and Knowledge\\n9. Challenges\\n10. Two, Three and Multiple Dimensions\\n11. Abstracts\\n12. Conclusions\\n\\n\\xa0\\n1. Introduction\\n\\n\\xa0\\xa0\\xa0 The enormous rise in new information has been paralleled by an\\nequally extraordinary rise in new methods for understanding that information, new ways of\\ntranslating data into information, and information into knowledge. New fields are\\nemerging. For instance, at the frontiers of science and in the military, scientific\\nvisualization is a thriving discipline with close connections to virtual reality,\\naugmented, enhanced and mixed reality. In business, database materials are being linked\\nwith spreadsheets to produce new three-dimensional visualisations of business statistics\\n(e.g. companies such as Visible Decisions). In industry, data mining is emerging as an\\nimportant new field. In the field of culture, where immediate profit is less obvious,\\nthese techniques remain largely unknown. \\n\\xa0\\xa0\\xa0 Interestingly enough, standard books on human computer interface by\\nShneiderman do not give a complete picture of techniques now available or in development,\\nnor even recent books with promising titles. There are a few journals, organizations and\\nsome conferences devoted to the subject of which the present is the most prestigious.\\nMeanwhile, on the world-wide-web itself, there are a series of useful sites, which offer\\nthe beginnings of a serious overview into these developments. For instance, Martin Dodge\\n(Centre for Advanced Spatial Analysis, University College, London), has produced a useful Atlas\\nof Cyberspace, with examples of at least four basic map(ping) techniques, namely,\\nconceptual, geographic, information (landscapes and spaces), and topology (including ISP\\nand web site). A more thorough survey is provided in an excellent study by Peter Young,\\n(Computer Science, Durham University), on Three Dimensional Information Visualisation.\\nHere he lists twelve basic techniques: surface plots, cityscapes, fish-eye views,\\nBenediktine space, perspective walls, cone trees and cam trees, sphere-visualisation,\\nrooms, emotional icons, self-organising graphs, spatial arrangement of data and\\ninformation cube. He also has a very useful list of research visualization systems. Chris\\nNorth (University of Maryland at College Park), has also produced a useful and important Taxonomy\\nof Information Visualization User Interfaces (see Appendix 1. Cf. the list of\\nindividuals in Appendix 2). Pat Hanrahan (Stanford) has made a taxonomy of information\\nvisualization, while Mark Levoy (Stanford) also has a taxonomy of scientific visualization\\ntechniques. \\n\\xa0\\xa0\\xa0 The significance of emerging interface technologies will be\\nconsidered, namely, voice activation, haptic force, mobile and nomadic, video activation,\\ndirect brain control, brain implants, and alternative methods. A problem with such\\ntaxonomies and the technologies which they class, is that they are mainly from the point\\nof view of the technology’s capabilities, as if we were dealing with solutions\\nlooking for a purpose.\\n\\xa0\\xa0\\xa0 In order to arrive at a taxonomy of users’ needs, a deeper\\nunderstanding of their potential purposes is required, the whys? This paper offers\\npreliminary thoughts in that direction. It begins with an outline of five basic functions\\nrelating to cultural interfaces, namely, virtual guides, virtual museums, libraries and\\nspatial navigation, historical virtual museums, imaginary museums and various kinds of\\ncultural research. The role of metadata is considered briefly. Particular attention is\\ngiven to the realms of research, since it is felt that the new technologies will transform\\nour definitions of knowledge. The conclusion raises some further questions and challenges.\\n\\xa0\\xa0\\xa0 Bibliographical references to Human Computer\\nInteraction specifically with respect to Graphic User Interfaces (GUI) and Network Centred\\nUser Interfaces (NUI) are provided in the notes. The appendices provide a taxonomy of\\ninformation visualization user interfaces by data type (Appendix 1), a list of individuals\\nand their contributions (Appendix 2) and a survey of other developments mainly in Canada,\\nGermany, Japan, the United Kingdom and the United States (Appendix 3).\\nTable of Contents\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n\\n2. Emerging Interface Technologies\\n\\xa0\\xa0\\xa0 It is generally assumed that the two-dimensional spaces of current\\ncomputers are largely a reflection of hardware limitations, which will soon be overcome.\\nHence there are numerous initiatives to create three-dimensional spaces. Alternative\\ninterfaces and input devices are also being developed. \\nThree Dimensional Spaces\\n\\xa0\\xa0\\xa0 Dr. Henry Lieberman (MIT) is exploring the use of very large\\nthree-dimensional navigation spaces, with new techniques which allow \"zooming and\\npanning in multiple translucent layers.\" Silicon Graphics Inc. (SGI) foresees the use\\nof landscapes. Dr. Stuart Card (Xerox PARC) and his team have been working on a series of\\ntools for visualizing retrieved information using techniques such as a galaxy\\nrepresentation, spiral calendar, perspective wall, document lens and cone tree .\\nThere is an analogous project at the Gesellschaft für Mathematik und Datenverarbeitung\\n(GMD) in Darmstadt called Lyberworld. This takes a concept, searches for related terms,\\nlinks these with the concept in question and presents them spatially in a cone.\\nAlternatively the concept in question is positioned in the centre while various related\\nterms are placed along the circumference of a circle where they exercise the equivalent of\\na centrifugal gravitational force. If all these surrounding terms are equal in strength\\nthey exercise an equal force on the central concept. As one of the terms becomes more\\nsignificant it exercises a greater force on the central concept. Another GMD project,\\nSEPIA, foresees a hypermedia authoring environment with four concurrent spaces: a content\\nspace, planning space, argumentation space and rhetoric space. \\n\\xa0\\xa0\\xa0 At the level of abstract ideas a series of new products are being\\ndeveloped. For instance, a group at Rensselaer Polytechnic is developing an Information\\nBase Modelling System (IBMS) which allows one to visualize relationships in parallel\\nperspective. At the Pacific Northwest National Laboratory (Richland, Washington) a team\\nled by Renie McVeety is developing a Spatial Paradigm for Information Retrieval and\\nExplanation (SPIRE, cf. Themescape), while John Risch is developing Text Data\\nVisualisation Techniques as part of the Starlight project. At Carnegie Mellon University\\nthe Visualization and Intelligent Interfaces Group is creating a System for Automated\\nGraphics and Explanation (SAGE) and related methods (Sagebrush, Visage) for Selective\\nDynamic Manipulation (SDM). At the Sandia National Laboratory, Chuck Myers, Brian Wylie\\nand a team at the Computational Sciences, Computer Sciences and Mathematical Center are\\nworking on three dimensional Data Analysis Data Fusion and Navigating Science, whereby\\nfrequency of articles can be visualized as hills in an information landscape. This is part\\nof their Advanced Data Visualization and Exploration initiative called EIGEN-VR. Another\\nproject at Sandia is an Enterprise Engineering Viewing Environment (EVE). This: \\n\\n\\n\\nmulti-dimensional user-oriented synthetic environment permits components of the model\\n    to be examined, manipulated and assembled into sub-systems and/or the final structure. A\\n    movable clipping plane allows internal structure examination. Craft wall displays provide\\n    schematic or cut-away views of an assembled model.\\n\\n\\n\\xa0\\xa0\\xa0 The Sandia team is also working on Laser Engineered Net Shaping\\n(LENS) and has been exploring the implications of these techniques for modelling and\\nsimulation in manufacturing and medicine. The implications thereof for culture are no less\\nimpressive as will be suggested in the section on research and knowledge (see below\\nsection 7).\\nAlternative Interfaces and Input Devices\\n\\xa0\\xa0\\xa0 While monitors controlled by a mouse remain the most popular form of\\nnavigation at the moment, a number of other alternatives are being developed. Bill Buxton\\nhas, for instance, produced what appears to be the most thorough list of existing input\\ndevices. This includes: aids for the disabled, armatures, bar code readers, boards, desks\\nand pads, character recognition, chord keyboards, digitizing tablets, eye and head\\nmovement trackers, foot controllers, force feedback (\"haptic\") devices, game\\ncontrollers, gloves, joysticks, keyboards and keypads, lightpens, mice, MIDI controllers\\nand accessories, miscellaneous, motion capture, speech recognition, touch screens, touch\\ntablets and trackballs. A full assessment of the pros and cons and\\nphilosophical implications of all these devices would be a book in itself. For our\\npurposes, it will suffice to refer to some of the main alternative human web interaction\\nsystems. \\nVideo Interaction\\n\\xa0\\xa0\\xa0 One very innovative technique entails using video cameras to capture\\nhuman movements and use these as cues for manipulating virtual environments. For instance,\\nDavid Rokeby, in the Very Nervous System, links human movements such as dance to\\nacoustic environments. As one moves more slowly or quickly, a different range of sounds is\\nproduced. \\n\\xa0\\xa0\\xa0 Vincent John Vincent and the Vivid Group have developed other\\naspects of this approach in their Mandela software, such that the video camera and\\na blue screen essentially allow the user’s movements in the real world to be\\ntransposed to the virtual space within the screen. This permits a person to interact as a\\nplayer in a virtual space on screen. For example, at the Hockey Hall of Fame in Toronto\\none can stand in a real goal, see oneself standing in a virtual goal on screen and\\ninteract with other virtual players there. This complex software requires customized\\nprogramming for each site or special event. By contrast, the Free Action and Human\\nObject Reactor software of a new company called Reality Fusion, offers more simplified\\nversions of this approach allowing persons \"to interact on screen with the body using\\nvideo cameras\". \\n\\xa0\\xa0\\xa0 Such techniques are potentially of great interest not just for\\nphysically challenged persons. One could imagine a museum or gallery carefully equipped\\nwith video cameras such that one needed only to point to an object, or part of a painting\\nand one’s notebook computer would give one an explanation at the level desired.\\nHence, if one had identified oneself as a grade school child at the outset there would be\\nan elementary explanation, whereas a research student would be given a much more thorough\\ndescription. \\nVoice Activated Interfaces and Visualization Space\\n\\xa0\\xa0\\xa0 In the 1960’s there was considerable fanfare about dictation\\nmachines which, it was claimed, would replace the need for secretaries. After more than\\nthirty years of hype, the first reliable products for the general public have been made\\navailable in the past year through companies such as Dragon Systems and IBM. Such systems\\npresently entail vocabularies of 10-20,000 words, but will soon expand to vocabularies of\\n100,000 words and more. At the same time, researchers such as Mark Lucente (IBM Watson),\\nworking in conjunction with MIT have been developing futuristic scenarios whereby a person\\ncan control a wall-sized computer screen using voice commands. \\n\\xa0\\xa0\\xa0 There are related projects elsewhere. The Gesellschaft für\\nMathematik und Datenverarbeitung (GMD) has an Institut für Integrierte Publikations und\\nInformationssysteme (IPSI), which is working on a Co-operative Retrieval Interface\\nbased on Natural Language Acts (CORINNA). Such methods are attracting attention\\nwithin the cultural community. In the United States, the Information Infrastructure Task\\nForce (IITF) has created a Linguistic Data Consortium to develop a Spoken Natural\\nLanguage Interface to Libraries.\\n\\xa0\\xa0\\xa0 Voice activation clearly opens many new possibilities. For instance,\\nmany lists are tree-like hierarchies, which means that choices inevitably require\\nburrowing down many levels until one has the set of choices one seeks. If these choices\\nare voice activated then one can go directly to the appropriate branch of a decision tree\\nand skip the levels in between. The effectiveness of the technique will, however, depend,\\nvery much on the situation. In the case of public lectures voice commands can help\\ndramatic effect. In a classroom, if everyone were talking to their computers the results\\nmight border on chaos. \\n\\xa0\\xa0\\xa0 Meanwhile, there is increasing study of the ways in which visual and\\nauditory cues can be combined. For instance, a team at the Pacific Northwest National\\nLaboratory (Richland, Washington) is working on the Auditory Display (AD) of\\nInformation \"to take advantage of known strengths of both visual and auditory\\nperceptual systems, increasing the user\\'s ability to glean meaning from large amounts of\\ndisplayed information\": \\n\\n\\n\\nAn Auditory Display Prototype adding non-speech sound to the human-computer\\n    interface opens a new set of challenges in the system\\'s visual design; however, there are\\n    many reasons why one would want to use auditory display. The human auditory system has\\n    acute temporal resolution, a three-dimensional eyes-free `orienting\\' capacity, and greater\\n    affective response than the visual system. Especially promising for analysis applications\\n    is the natural ability to listen to many audio streams simultaneously (parallel listening)\\n    and the rich quantity of auditory parameters (pitch, volume, timbre, etc.) that are\\n    intuitively apparent to musicians and non-musicians alike. Current software leaves the\\n    potential of audio at the interface almost completely unused, even while visual displays\\n    (subject to well-understood limitations) are increasingly cramped. Auditory display poses\\n    a way to expand the human-computer interface by taking advantage of innate properties of\\n    the human perceptual system.\\n\\n\\n\\xa0\\xa0\\xa0 Such combinations of visual and auditory cues, are also being\\nstudied by Richard L. McKinley (Wright Patterson Airforce Base) in the context of a new\\nfield of bio-communications. If we truly learn so much better when we see and hear things\\nin combination or at least in certain combinations then we clearly need to find ways of\\nincorporating such experiences within the learning framework.\\nHaptic Force and Tactile Feedback\\n\\xa0\\xa0\\xa0 Research into artificial arms and limbs, by pioneers such as\\nProfessor Steven J. Jacobsen (University of Utah, Salt Lake City) has led to new awareness\\nof haptic force and tactile feedback as potential aspects of input systems. Corde Lane and\\nJerry Smith have made a useful list of a number of these new devices. Grigore Burdea, in a\\nrecent book, offers a very useful survey of this emerging field, showing that present\\napplications are limited mainly to the military (combat simulation, flight simulator),\\nmedicine (eye surgery and arthroscopy training simulator) and entertainment (virtual\\nmotion three-dimensional platform). \\n\\xa0\\xa0\\xa0 In the military, these principles are leading to tele-presence in\\nthe sense of tele-manipulation or tele-operation, whereby one can carry out actions at a\\ndistance. In the case of a damaged nuclear reactor, for instance, from a distance a person\\ncould safely control a robot, which would enter a space lethal for humans and do a\\ncritical repair. In medicine, these same principles are leading to tele-surgery. \\n\\xa0\\xa0\\xa0 In the field of culture such haptic force and tactile feedback\\nmechanisms could well lead one day to new types of simulated conservation experiments.\\nBefore trying to restore the only extant example of a vase or painting, one creates a\\nmodel and has various simulations before attempting to do so with the actual object. Not\\ninfrequently, there will only be one or two experts in the world familiar with the\\ntechniques. These could give tele-demonstrations, which advanced students could then\\nimitate. \\n\\xa0\\xa0\\xa0 In the eighteenth century, the Encyclopédie of Diderot and\\nD’Alembert attempted to catalogue all the known trades and crafts. Within the next\\ngenerations it is likely that these will be recorded in virtual reality complete with\\nhaptic simulations. These techniques will continue to change with time, such that in\\nfuture one could, for instance, refer back to how things were being done at the turn of\\nthe twentieth century. \\nMobile and Nomadic Interfaces \\n\\xa0\\xa0\\xa0 The advent of cellular telephones and Personal Digital Assistants\\n(PDA’s) such as the Apple Newton or Texas Instruments’ Palm Pilot has\\nintroduced the public to the general idea of mobile communications, an emerging field,\\nwhich involves most of the major industry players. At the research level the Fraunhofer\\nGesellschaft (Darmstadt) is working on Mobile Information Visualization, which\\nincludes Active Multimedia Mail (Active M3) and Location Information Services\\n(LOCI)\\n\\xa0\\xa0\\xa0 To understand more fully the larger visions underlying mobile\\ncommunications it is useful to examine Mark Weiser’s (Xerox PARC) vision of\\nubiquitous computing. This goes far beyond the idea of simply having a portable phone or\\ncomputer. Instead of thinking of the computer as an isolated machine, he sees all the\\ntechnological functions of a room integrated by a whole series of co-ordinated gadgets,\\nwhich are effectively miniature computers. Employee A, for instance, might always like a\\nbig lamp shining at their desk, have their coffee promptly at 10:30 a.m. each morning and\\nnot take calls from 2-3 p.m. because that is a time when the person writes letters.\\nAssuming that the room could \"recognize\" the person, say through their badge,\\nall of these technology \"decisions\" could be activated automatically, without\\nemployee A needing to turn on the big lamp at 8:30, the coffee machine just before 10:30\\nand turn on the answering machine from 2-3 p.m. In Weiser’s vision this recognition\\nprocess would continue outside one’s own office. Hence, if employee A had walked down\\nthe hall and was visiting the office of employee C, the telephone would \"know\"\\nthat it should not ring in their now empty office and ring instead in C’s office for\\nemployee A, using a special ring to link it with A. Such challenges are leading to an\\nemerging field of adaptive and user modelling.\\n\\xa0\\xa0\\xa0 In the military, where mobile computing is frequently called nomadic\\ncomputing, this vision is taken to greater extremes. Here one of the leading visionaries\\nis a former director of the Defence Advanced Projects Agency (DARPA), Professor Leonard\\nKleinrock (University of California at Berkeley). In his vision, a computer should simply\\nbe able to plug into a system without worrying about different voltage (110, 220, 240) or\\nneeding new configurations of IP addresses. A soldier on the ground with their view\\nobstructed by a hill, could communicate with an aircraft overhead, which would then relay\\nto the soldier a bird’s eye view of the situation. Companies such as Virtual Vision\\nare exploring some of the non-military implications of this approach.\\n\\xa0\\xa0\\xa0 While museums and galleries are far removed from the\\nlife-threatening aspects of the battlefield, one can readily see how the greatly increased\\ninteroperability of devices being developed in a military context, has enormous\\nimplications for museums and galleries. Imagine a notebook computer that \"knows\"\\nwhich painting is in front of one, and thus downloads the appropriate information without\\nneeding to be asked. Imagine a computer that immediately sought the information one might\\nneed for a city the moment one arrived in that city. Hence, on landing in Rome, it would\\ndownload an appropriate map of Rome, complete with information about the relevant museums\\nand their collections. \\nDirect Brain Control and Brain Implants \\n\\xa0\\xa0\\xa0 Those concerned with universal access for persons with various\\ndisabilities have developed various devices such that one can, for instance, control\\ncomputers simply by eye movements or other minimal motions. \\n\\xa0\\xa0\\xa0 A number of projects are moving towards direct brain control whereby\\nintermediary devices such as a mouse are no longer necessary. In Germany, the main work is\\noccuring at the International Foundation of Neurobionics in the Nordstadt Hospital\\n(Hanover), at the Institute for Biomedical Technique (St. Ingbert) and at the Scientific\\nMedical Institute of Tübingen University (Reutlingen). In Japan, Dr. \\n\\xa0\\xa0\\xa0 Hinori Onishi (Technos and Himeji Institute of Technology) has\\nproduced a Mind Control Tool Operating System (MCTOS). In the United States,\\nMasahiro Kahata (New York) has developed an Interactive Brainwave Visual Analyser\\n(IBVA). At the Loma Linda Medical Center work is being done on controlling computers with\\nneural signals.\\n\\xa0\\xa0\\xa0 Dr. Grant McMillan (Wright Patterson Airforce Base) has been\\nexploring the potentials of brain waves (namely, Alpha, Beta, Theta, Delta and Mu) on\\ncontrol mechanisms. For example, a pilot may be in a flight simulator and find themselves\\nflying upside down. Every time one thinks, the brain produces electric pulses. By\\nharnessing these waves a pilot has only to think and the resulting waves can act as a\\ncommand to return the simulator to an upright position.\\n\\xa0\\xa0\\xa0 A more futuristic and potentially disturbing trend entails direct\\nbrain implants in a manner foreseen in the film Strange Days. Part seven of a BBC\\ntelevision series Future Fantastic directed by Gillian Anderson, entitled Brainstorm,\\ndiscusses the work on brain implants by Dr. Dick Norman and Dr. Christopher Gallen. \\n\\xa0\\xa0\\xa0 Given such developments, phrases such as \"I see what you\\nmean\", \"sharing an idea\", \"look at it from my viewpoint\" or\\n\"giving someone a piece of one’s mind\" a may one day be more literal than\\nwe now imagine. As noted above, it is already possible to activate certain commands simply\\nby eye movement or through bands which measure one’s thought waves. In future,\\ninstead of voice activation, there might well be thought activation. Dictation would then\\nsimply require thinking the words which could conceivably lead some to forget how to speak\\nproperly. Will we be able to let others into our dreams and daydreams? Such questions lead\\nquickly beyond the scope of this essay and yet the problems they entail may well become\\ncentral to interface design sooner than we think. In order to assess more realistically\\nthe potentials of such applications it will be useful to step back and explore some basic\\nfunctions of cultural interfaces.\\nTable of Contents\\xa0\\xa0\\xa0\\n\\n\\n\\xa0\\n3. Virtual Guides and Physical Museums\\n\\n\\xa0\\xa0\\xa0 At the simplest level, one can imagine a physical museum endowed\\nwith different kinds of virtual guides. Instead of having a traditional tour guide, trying\\nto shepherd a group of twenty or thirty visitors through various rooms, standing around a\\npainting and having to shout to make themselves heard above the noise of the crowd, a\\nvisitor could simply rent a walkman-like device and listen to descriptions of paintings as\\nthey go. At the Museum in Singapore, for instance, such a device is already available.\\nCertain displays and paintings are specially marked and for these a virtual guided tour is\\navailable. In Italy, the National Research Council (CNR) is developing a similar device,\\nwhich will function much like a push-button dial on a telephone. However, instead of\\ndialing a telephone number, one will key in the painting or monument number to receive the\\ndesired description. \\n\\xa0\\xa0\\xa0 It is foreseen that these descriptions will be on-line. Hence, when\\na tourist arrives in a new city such as Rome for the first time, they will simply download\\nthe appropriate tours for that city, not unlike the way one now buys cultural videos of\\nthe city in question, except that all this will be on-line over the Internet. Given new\\nelectronic billing procedures, the \"rental\" of the tour can be arranged to allow\\nonly one hearing, or be limited to a series of hearings, or to tours within a set\\ntime-frame of a day, a week or a month.\\n\\xa0\\xa0\\xa0 The walkman-like guide is but one possibility. As notebook computers\\nmove increasingly towards electronic versions of notepads (cf. the Newton and Palmtop),\\nmuch more than a pleasant description of a painting or monument is possible. The notepad\\ncomputer can give a visitor images of related paintings. For instance, standing in front\\nof Uccello’s Battle of San Romano in the National Gallery of England (London),\\nthe viewer can be reminded exactly how it differs from the two other versions by Uccello\\nin the Louvre and the Uffizi respectively. More advanced viewers could use this technology\\nto compare minute differences between originals, versions by students of the painter,\\nmembers of their workshop, copies and so on. \\n\\xa0\\xa0\\xa0 Those not able to visit an actual painting would still be able to do\\nsuch comparative study from their desktops even if these were far from major centres of\\nculture. To be sure, seeing the original has and always shall be preferable to seeing\\nsurrogates. But in the past those in remote areas were typically doomed to seeing nothing\\nother than occasional –usually poorly reproduced images in books. Now at least they\\nwill potentially have access to an enormous array of heritage wherever they happen to be. \\n\\xa0\\xa0\\xa0 For those able to visit the famous museums there are still numerous\\nbarriers to seeing the painting as directly as one might wish. In extreme cases such as\\nthe Mona Lisa the work resides in a cage behind a solid sheet of glass which often\\nrefracts light in a way that hinders careful viewing. In most cases there are ropes or\\nother barriers to keep one from getting very close to a picture. Even if one could get as\\nclose as one would like, many of the most intriguing aspects of paintings are invisible to\\nthe naked eye. Often, for example, there are subtle variations beneath the surface (pentimenti)\\nas a result of a painter having changed their mind: changes in the position of a figure,\\nor sometimes its complete removal. In the past, the only way of studying such changes was\\nby means of x-ray photographs, which were only seldom available to a general viewer.\\nRecently (1997), a new method called infrared reflectography allows one actually to see\\nthe different layers of paint beneath the surface. For instance, in Leonardo da\\nVinci’s Adoration of the Magi (Florence, Uffizi) there are elephants, which he\\ndrew and were subsequently painted over. It is likely that future tourists will rent a\\nnotepad computer, which allows them to see all the layers beneath the surface, thus giving\\nnew meaning to the concept of looking closely at pictures. \\n\\xa0\\xa0\\xa0 The role of virtual guides is, of course, not necessarily limited to\\nthe interfaces of hand held devices as one goes around a real museum. They can be adapted\\nfor virtual and imaginary museums. IBM’s pioneering reconstruction of Cluny Abbey,\\nhad such a virtual guide or avatar, in the form of a mediaeval nun, who took one around\\nthe virtual reality model of the famous church. If Phippe Quéau’s visions of\\ntele-virtuality come about, then we shall, in the near future, be able to choose the kind\\nof avatars we wish and have them take us around whichever monuments may interest us. \\n\\xa0\\xa0\\xa0 In the past, a day at a museum often ended with a visit to the\\nmuseum shop, where one bought postcards or posters of the images which one particularly\\nliked. Those available were typically a small selection of the holdings of a museum, and\\noften it seemed that these invariably omitted the ones one wanted. In future all the\\nimages of a museum can be available on line and can be printed on demand. These images\\nwill include three-dimensional objects. At the National Research Council of Canada\\n(Ottawa), a laser camera has been developed which produces fully three-dimensional images,\\nwhich can be rotated on screen. Using stereo-lithography, three-dimensional copies of such\\nobjects can be \"printed\" on demand. \\n\\xa0\\xa0\\xa0 Virtual reality permits one to create full-scale three-dimensional\\nsimulations of the physical world. Augmented reality goes one step further, allowing one\\nto superimpose on that reconstruction additional information or layers of information.\\nThere are a number of such projects around the world. For instance, at Columbia\\nUniversity, Steve Feiner has been exploring the architectural implications of augmented\\nreality in the context of various projects. One is termed Architectural Anatomy.\\nThis allows one to view a virtual reality version of a room and then see the position of\\nall the wires, pipes and other things hidden behind the walls. \\n\\xa0\\xa0\\xa0 A second is called Urban Anatomy and entails a method aptly\\ntermed X-Ray Vision. Here one can look at a virtual reality view of a street or a\\nwhole neighbourhood, superimposed or more precisely underlying which one sees the various\\nlayers of plumbing, wires and tunnels that one would see in a Geographical Information\\nSystem (GIS). Except that, in this case, it is as if the earth were fully transparent and\\none can see precisely how they are collocated with the actual space. Similar techniques\\nare being developed by researchers such as Didier Stricker at the Institut für Graphische\\nDatenverarbeitung (IGD, Munich) which is linked with the Fraunhofer Gesellschaft’s\\nZentrum für Graphische Datenverarbeitung e.V. (ZGDV, Darmstadt). In this case augmented\\nreality is being used to superimpose on real landscapes, proposed designs of bridges and\\nother person-made constructions. Other projects at the same institute are working on Multimedia\\nElectronic Documents (MEDoc) and Intelligent Online Services to create Multimedia\\nExtension[s] (MME).\\n\\xa0\\xa0\\xa0 Of even greater direct interest for cultural applications are the\\nresearch experiments of Jun Rekimoto at Sony (Tokyo). Using what he terms augmented\\ninteraction, he has created a Computer Augmented Bookshelf, with the aid of \\n\\n\\xa0\\xa0\\xa0 Navicam. This \"is a kind of palmtop computer, which has a\\nsmall video camera to detect real-world environments. This system allows a user to look at\\nthe real world with context sensitive information generated by a computer.\" Hence,\\nlooking at a shelf of magazines, the system can point out which ones arrived today, in the\\nlast week and so on. A related invention of Dr. Rekimoto for use in theatres is called the\\nKabuki guidance system:\\n\\xa0\\xa0\\xa0 The system supplies the audience with a real time narrative that\\ndescribes the drama to allow a better understanding of the action without disturbing\\noverall appreciation of the drama. Synchronizing the narration with the action is very\\nimportant and also very difficult. Currently, narrations are controlled manually, but it\\nis possible for the system to be automated.\\n\\xa0\\xa0\\xa0 Applied to libraries, versions of such a system could essentially\\nlead a new user through the complexities of a major collection. In the case of a regular\\nreader, it could remind them of the location of books previously consulted. The reader\\nmight know they were there last year in June and that the book was somewhere in section C.\\nThe system could then identify the books in question.\\n\\xa0\\xa0\\xa0 This approach also introduces new possibilities in terms of\\nbrowsing. Instead of just perusing the titles on a shelf, a person could ask their notepad\\ncomputer for abstracts and reviews with respect to the book in question using the SUMS\\ninterface. Alternatively, if a person were tele-browsing from their home computer they\\ncould call up these features while sitting at their desk at home.\\nTable of Contents\\n\\n4. Virtual Museums and Libraries\\n\\n\\xa0\\xa0\\xa0 Complementary to the above scenarios, are cases where virtual\\nmuseums and libraries create digital versions of their physical spaces. Perhaps the\\nearliest example of such an experiment was the Micro-Gallery at the National Gallery of\\nEngland (London), a small room within the physical gallery with a handful of computers,\\nwhere one could view images of the paintings in the collection and plan a tour in keeping\\nwith one’s particular interests. This approach has since been copied at the National\\nGallery in Washington and is being adapted by the Rijksmuseum at Amsterdam.\\n\\xa0\\xa0\\xa0 Some of the early experiments in the field of cultural heritage\\npursued one metaphor to the exclusion of others. For instance, the Corbis CD-ROM of the\\nCodex Leicester fixed on the image of a virtual museum for both paintings and books,\\nsuch that the manuscripts appeared on the walls as if they were paintings. While optically\\nappealing, such attempts were unsatisfactory because they eliminated many of the essential\\ncharacteristics of books. Physical books give important clues as to thickness, size, age\\nand so on. Their surrogates in terms of virtual books also need to convey these\\ncharacteristics. \\n\\xa0\\xa0\\xa0 Present research is actively engaged in creating such surrogates.\\nFor instance, Professor Mühlhauser (Johanneum Research, Graz), is working on virtual\\nbooks, which will indicate their thickness. Dr. Stuart Card and colleagues (Xerox PARC),\\nare exploring the book metaphor in virtual space and developing ways of moving from\\nrepresentations of concrete books to visualisations of abstract concepts which they\\ncontain. Companies such as Dynamic Diagrams have created a simulation of file cards\\nin axonometric perspective for the Britannica Online site and for IBM’s web\\nsite. IBM (Almaden, Visualization Lab) has developed views of pages in parallel\\nperspective as part of their \\n\\nVisualization Data Explorer, such that one can trace the number of occurences of a\\ngiven term in the course of a text. \\n\\xa0\\xa0\\xa0 Such virtual museums and libraries can exist at various levels of\\ncomplexity and their viewing need not, of course, be limited to some ante-room of the\\nactual museum. As noted above, a number of museums include Quick-Time Virtual Reality (VR)\\ntours on CD-ROMS of their collections. Meanwhile, others such as the Uffizi, have\\nrecreated online a version of their entire museum complete with simple Quick Time VR\\nmodels of each room, such that one can look around to each of the walls as if one were\\nthere. These relatively simple images reflect the present day limitations of Internet\\nconnectivity, which will probably be overcome within the next decades.\\n\\xa0\\xa0\\xa0 At the frontiers, an Italian company, Infobyte, is developing\\nsoftware called Virtual Exhibitor, which will allow museums to create such virtual\\ngalleries with a minimum of effort. Although this presently requires a Silicon Graphics\\nmachine, within two years regular PCs will be powerful enough to perform the same tasks.\\nThis software, along with SUMS are part of the European Commission’s Museums over\\nStates in Virtual Culture (MOSAIC) project in the context of their Trans European Networks\\n(TEN) intitiative. \\n\\xa0\\xa0\\xa0 Such virtual visits can go much further than simply visiting the\\nrooms of museums ahead of time. In Tarkowsky’s famous film (1972) of Stanislaw\\nLem’s Solaris (1961), the viewers of Breughel’s Winter Landscape\\n(Vienna, Kunsthistorisches Museum) enter into the painting and walk around in the\\nlandscape. Professor Robert Stone (VR Solutions, Salford), in a project called Virtual\\nLowry, uses virtual reality to take viewers through the spaces of Lowry’s\\npainting. In Infobyte’s version of Raphael’s Stanze, viewers are able to\\nview the School of Athens and then enter into the space and listen to lectures by\\nfamous ancient philosophers and mathematicians. Museums and galleries typically have one\\nor more rooms where visitors can watch slide-shows, videos, or attend lectures pertaining\\nto some aspect of their collections. In future such virtual visits could reasonably occur\\nin such rooms or halls. \\n\\xa0\\xa0\\xa0 In the context of museums, a series of cultural interfaces thus\\npresent themselves. In the equivalent of an ante-room, viewers are able to prepare for\\ntours using monitors or more elaborate technology. For on-site tours there will be\\ncomputer notepads. Monitors linked to printers in the museum shop will allow one to print\\npostcards and full size posters on demand. For research purposes visits will occur\\nsometimes on a computer screen, a large display panel, an IMAX type screen (which will\\nprobably be available on-line in the next generation), on planetarium ceilings or in\\nentirely immersive CAVE environments (cf. below in section 7), within the museum or\\ngallery rooms. In future as bandwidth increases these materials will become available\\non-line such that visitors (children and adults alike), can prepare for visits to museums\\nand galleries by studying some their highlights or their detailed contents ahead of time,\\neither at school or in the comfort of their homes.\\n\\xa0\\xa0\\xa0 Museums and galleries have traditionally been famous for their\\n\"do not touch\" signs. Many visitors, especially children, want to know how\\nthings feel. This is an area where virtual reality reconstructions of objects, linked with\\nhaptic feedback, could be of great help, thus adding experiences to museum visits which\\nwould not be possible in the case of original objects. Prostheses of sculptures, statues,\\nvases and other objects can provide visitors with a sense of how they feel without\\nthreatening the original pieces.\\n\\xa0\\xa0\\xa0 In most cases, these museum interfaces increase interest in seeing\\nthe original. Their purpose is to prepare us to see the \"real\" artifacts. Only\\nin the case of special sites such as the caves at Lascaux or the Tomb of Nefertari, will\\nthe new technologies serve as a substitute for seeing the actual objects in order to\\nprotect the originals. By contrast, in the case of library interfaces, virtual libraries\\nwill very probably replace many functions of traditional libraries. Instead of using card\\ncatalogues to find a title and then searching the shelves for related books on a given\\ntopic, readers will use on-line catalogues and then do tele-browsing. Having found a book\\nof interest, they will print them on demand.\\n\\xa0\\xa0\\xa0 The continuing role of libraries will be defined in part by the kind\\nof information being sought. Much of the time readers are searching for a reference, fact,\\na quote or a passage. Such cases can readily be computerized and replaced by on-line\\nfacilities. On the other hand historians of palaeography and of the book are frequently\\nconcerned with the feel of the cover, details of the binding or subtle aspects of\\nhand-painted miniatures. In such cases, electronic facsimiles may help them answer\\npreliminary questions, but consultation of the actual manuscript or book will remain an\\nimportant part of their profession which only libraries can fulfill. \\n\\xa0\\xa0\\xa0 Why even print when one can read on screen? Physiological\\nexperiments have shown that one sees about a third less when light comes to the eye\\ndirectly from a monitor screen rather than being reflected from the surface of a page.\\nHence, while computer monitors are an excellent interim measure, they are not an optimal\\ninterface for detailed cultural research. A new kind of device, similar to a slide or film\\nprojector, is needed that projects images onto a solid surface. \\nSpatial Navigation\\n\\xa0\\xa0\\xa0 Knowing how to get there, spatial navigation, is one of the\\nfundamental concerns in the organization and retrieval of all knowledge including culture.\\nThe use of maps for this purpose is almost as old as civilization itself. Since the\\nRenaissance there have been enormous advances in relating different scales of maps. In the\\npast decades rapid developments in Geographical Information Systems (GIS) have begun\\nlinking these scales electronically (as vector images). Parallel with this has been a\\nlinking of scales of satellite and aerial photographs ( as raster images). The 1990’s\\nhave seen increasing translation between raster and vector images such that there is a\\npotential interoperability between maps and photographs (figure 1).\\n\\xa0\\xa0\\xa0 Projects such as Terravision in the United States and T-Vision\\nin Germany can be seen as first steps in this direction. This means a potentially\\nseamless integration of all spatial data such that we could move at will from views in\\nspace down to any painting on a wall or sculpture in a room. In Powers of Ten, a\\nfamous film by Charles and Ray Eames, a viewer was taken along one such visionary set of\\nconnections using photographs alone. Today it is technically feasible to do this\\ninteractively with any object in the world. \\nScales of Abstract\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nScales of Concrete\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nMap of-\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nSatellite Photos of -\\xa0\\xa0\\xa0 World\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n- \\xa0\\xa0\\xa0 Continent\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n- \\xa0\\xa0\\xa0 Country\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n- \\xa0\\xa0\\xa0 Province\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nAerial Photos of\\xa0\\xa0\\xa0 -\\xa0\\xa0\\xa0 City\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nPlan of -\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n- \\xa0\\xa0\\xa0 Building (GIS)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nGuicktime VR of\\xa0\\xa0\\xa0 -\\xa0\\xa0\\xa0 Room\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n- \\xa0\\xa0\\xa0 Objects in Room\\nFigure 1. Basic scheme of scales of abstract images (maps and plans) and concrete\\nimages (satellite photographs, aerial photos and Quick Time VR images).\\n\\xa0\\xa0\\xa0 Implicit in these breakthroughs is a reconciliation of methods which\\nearlier generations perceived as different and even potentially incompatible. For\\ninstance, Gombrich (1975), in his Royal Society lecture distinguished between\\nthe mirror (photographs) and the map. Dodge, in his Atlas of Cyberspace,\\ndistinguishes between topological maps and the photographic type maps of information\\nlandscapes. While such distinctions may continue, the breakthroughs mentioned above will\\nincreasingly permit us to move seamlessly between categories, such that we can switch from\\nviewing a topological map to a topographical map or an aerial photograph. By this same\\nprinciple it will be possible to move seamlessly between photographs of physical rooms and\\nComputer Aided Design (CAD) reconstructions of those same rooms used for Area\\nManagement/Facilities Management (AM/FM). This will bridge many earlier oppositions\\nbetween abstract and concrete, making it clear that both can be correlated with the same\\nreality. This has implications also for temporary and imaginary tours discussed below.\\n\\xa0\\xa0\\xa0 Thus far only isolated aspects of this integrated vision have been\\nadopted in the cultural context. For instance, city guides on the Internet are beginning\\nto list maps with major museums and galleries. CD-ROM’s of galleries such as the\\nLouvre, Pushkin, or the Uffizi typically have Quick Time VR views of the individual rooms.\\nThe technology exists to link together all these individual elements.\\n\\xa0\\xa0\\xa0 Virtual reality allows complete reconstructions of objects,\\narchaeological sites and historical monuments in three-dimensions. Some of the best\\nexamples of these possibilities are being created by Infobyte (Rome). These include\\nreconstructions of the Upper Church of San Francesco (Assisi), Saint\\nPeter’s Basilica (Vatican) and more recently the Rooms (Stanze) of\\nRaphael as part of an ongoing project which may one day recreate the whole of the Vatican\\nmuseum complex and become integrated with IBM’s Vatican Library project. The enormous\\nnumber of such reconstructions, listed in a very useful book by Maurizio Forte, attests\\nthat such examples are part of a much larger phenomenon and that some of the cultural\\nimplications are clearly appreciated. \\n\\xa0\\xa0\\xa0 Many of these reconstructions are typically viewed on a computer\\nmonitor. Sometimes glasses are used to permit stereoscopic viewing of the images.\\nSometimes this effect is achieved using a Binocular Omni-Oriented Monitor (BOOM). It is of\\ncourse possible to make this experience fully immersive by projecting images on all the\\nwalls of a room as in the case of CAVE environments. Alternatively, one could project them\\nonto the hemi-spherical surface of a planetarium using multiple projectors to create a\\nfully immersive effect as Infobyte is doing by working in conjunction with the Japanese\\nfirm GOTO. Under discussion is the possibility that Infobyte’s reconstructions could\\nbe projected onto IMAX screens. \\n\\xa0\\xa0\\xa0 One of the leading pioneers in the field of virtual reality is the\\nGerman Gesellschaft für Mathematik und Datenverarbeitung (GMD, Sankt Augustin), which has\\na section on Visualisation and Media System Design. Among its many projects is Virtual\\nXanthen. Besides its well-known mediaeval church, Xanthen has a famous Roman\\narchaeological site. The GMD project transforms a regular projection screen into an entire\\nwall. A viewer standing in front of the wall sees an entire landscape from a bird’s\\neye view. The small platform on which they are standing serves as a navigating instrument,\\npermitting one to \"fly\" higher above the landscape or get closer to the earth.\\nThis adds a whole new dimension to virtual visits. \\n\\xa0\\xa0\\xa0 Traditional blue screens permit an actor to stand in front of a\\nscreen and be projected into a scene with a completely different background, as happens,\\nfor example, with the weatherman after the evening news on television. A limitation of\\nthis technique is that the backdrop is two-dimensional whereas the actor typically moves\\nin a three-dimensional space. The GMD’s Distributed Video Project (DVP) takes these\\nprinciples considerably further. The blue screen is transformed into a blue room and the\\nactor’s movements in three-dimensional space are accompanied by three-dimensional\\nperspectival adjustments in the background. Some of the obvious applications of this new\\ntechnique are in the field of television and film production. Suppose for example, that\\none wished to do a film about the Sahara desert. Instead of needing to take a crew out to\\nextreme conditions of the North African desert, one could simply digitize views of the\\ndesert and project them onto the equivalents of four walls and then use the blue room\\ntechnique for actors to be virtually transported to the Sahara. The implications of this\\napproach for culture are considered below in section 5.\\n\\xa0\\xa0\\xa0 While such a translation of physical into virtual space constitutes\\nthe most obvious application of the new technologies, it is in a sense the least exciting.\\nFor the cultural field the most fascinating challenges lie in a new series of combinations\\nof real and virtual, some of which will now be considered.\\nTable of Contents\\n\\n\\xa0\\n5. Historical Virtual Museums\\n\\n\\xa0\\xa0\\xa0 In the case of major museums and galleries, one virtual museum will\\nnot suffice. The buildings of the Louvre, for example, have existed on the premises of the\\npresent museum since at least the eleventh century. So one will need historical virtual\\nmuseums, reconstructions, which help us to understand how what began as a mediaeval\\nfortress gradually evolved into one of the world’s great picture galleries. These\\nreconstructions will trace not only the physical growth of various rooms and galleries but\\nalso help to trace the changing arrangements of the permanent and temporary collections of\\npaintings therein. Where was Mona Lisa hanging in the eighteenth and nineteenth\\ncenturies, as opposed to today and what do these changing configurations tell us about the\\nhistory of exhibitions, taste and so on? Such digital versions of earlier spaces and\\nformer versions will allow simulations of temporal travel. \\n\\xa0\\xa0\\xa0 This principle is also being applied to urban landscapes to create\\nhistorical virtual cities. For example, CINECA, as part of the MOSAIC project, is\\nreconstructing the mediaeval city of Bologna such that one can trace the growth of the\\ncity and changes in its basic structure in the course of several centuries. This\\nreconstruction using virtual reality modelling language (VRML) allows one to walk through\\nthe streets and watch how they change as if one were in a time machine. Traditionally,\\nsome historians have claimed that Bologna developed an elaborate water and sewage drainage\\nsystem during the Middle Ages. Other historians have challenged this. The model is\\nsufficiently detailed that it can be used to check the validity and veridity of such\\nclaims. In such cases reconstructions of cultural heritage become significant for social\\nand even economic history. \\n\\xa0\\xa0\\xa0 Similarly, in the case of archaeological sites, this approach offers\\nfurther possibilities. Today, a major museum typically has a photograph and/or a small\\nmodel of the Acropolis at Athens. Students studying Greek history at school\\ntypically only have access to poor black/white images. Those with Internet access can, of\\ncourse, consult a number of colour images through the Perseus Project. Much\\nmore is technically possible. Most European countries have their own archaeological\\nschools in Athens and have developed their own theories about the Acropolis. So one\\ncould theoretically call up photographs of the site as it exists today. One could then\\ncall up various historical photographs and drawings in order to appreciate how it looked\\nbefore Elgin took his marbles, what the Greek temple looked like when it became a Muslim\\nmosque and compare it with how it looks today. One could then view various reconstructions\\nby Greek, American, British, French, German and other archaeologists. Instead of just\\nlooking at buildings as static entities, one could examine how they change in the light of\\ndifferent cultural and scholarly traditions. Such reconstructions could be available\\non-site using notepad computers, as well as on-line for study at school and home. \\n\\xa0\\xa0\\xa0 Professor Iwainsky has explored further potentials in his\\nreconstruction of the Pergamon Altar, complementing the virtual reality reconstruction of\\nthe altar now in Berlin with filmed video clips of the original landscape in Pergamon,\\nthus helping viewers to see how it would have looked in its original context. The\\nGMD’s Distributed Video Production initiative introduces new techniques to develop\\nthis approach. One can, for instance, theoretically film views of and from the Acropolis,\\nthen, using a blue room, combine this with virtual reality reconstructions of the\\nParthenon and other buildings such that one could walk through the buildings as they might\\nhave been and have realistic views of the landscape. Given sufficient bandwidth such\\nreconstructions can be on-line, permitting students and others around the world to get a\\nrealistic sense of sites long before they have a chance to actually visit the original.\\nTable of Contents\\n\\n\\xa0\\n6. Imaginary Virtual Museums\\n\\n\\xa0\\xa0\\xa0 Imaginary museums in a true sense will show paintings, sculptures\\nand other artifacts, which never physically existed together, as coherent collections.\\nRenaissance painters such as Botticelli, Leonardo and Raphael were typically commissioned\\nto paint works for a church, monastery or a private patron with the result that their\\nworks were dispersed from the outset. To overcome this, art historians developed the catalogue\\nraisonnée, but the high costs of printing typically meant that these catalogues\\noffered only black-white images of paintings and often poor ones at that. Imaginary\\nmuseums will allow one to see the collected paintings of an artist. This can happen in\\ndifferent contexts. In an actual museum such as the Uffizi, using one’s notebook\\ncomputer one could stand in front of a Madonna and Child by Raphael and ask for\\nother images by that painter on the same theme in other collections. This same principle\\ncan be extended to apply to thematic study also. Standing in front of a Baptism of\\nChrist by Piero della Francesca (London, National Gallery) one would ask Baptisms\\nwithin a given temporal or geographical framework. Alternatively one will acquire the\\nequivalent of a CD-ROM which allows one to make these comparisons on one’s computer\\nat home or at school. Increasingly these materials will be available on-line and future\\nequivalents of hard disks will function in the manner of personal libraries today. They\\nwill have subsets on topics that are of particular interest to a given scholar, amateur or\\nmember of the general public. \\n\\xa0\\xa0\\xa0 Imaginary museums can also offer viewers a whole range of\\ninterpretations concerning the structure and history of paintings. Standing in front of\\nPiero della Francesca’s Flagellation of Christ (Galleria Nazionale delle\\nMarche, Urbino), one could see the different interpretations of its perspectival space.\\nStanding in front of Leonardo’s Last Supper (Santa Maria delle Grazie, Milan),\\none could compare it with other copies, see alternative reconstructions of its\\nperspectival space and impressions of what it once looked like as well as having access to\\ndetails of restorations concerning individual figures. \\n\\xa0\\xa0\\xa0 Major collections such as the National Gallery (London) have an Art\\nin Focus series, which are effectively special exhibitions focussing each time on an\\nin depth analysis of special effects or characteristics of a given painting. Today such\\nmaterials are typically available in an exhibition catalogue, which soon goes out of\\nprint. In future, such analyses could be available using notebook computers such that one\\ncould see such special characteristics at any time. This will give extended life to the\\nconcept of special exhibitions and indeed change their significance. \\n\\xa0\\xa0\\xa0 A series of basic functions for cultural interfaces thus emerge. A\\nfirst is virtual guides in physical museums; a second is virtual museums; a third is\\nvirtual historical museums and a fourth is imaginary museums. A fifth basic function of\\ncultural interfaces entails research. Before exploring this and its implications for\\nchanging definitions of knowledge, a brief note on metadata is necessary.\\nTable of Contents\\n\\n\\xa0\\n7. Metadata\\n\\n\\xa0\\xa0\\xa0 In seeking to find data, information and knowledge on the web,\\nsystem designers and scholars have devoted increasing attention to metadata in the sense\\nof data about data. Initial efforts in this direction were focussed on identifying basic\\nkeywords and minimal descriptors (such as those being developed in the context of the\\nDublin Core) in order to permit identification of an article or book. The World Wide Web\\nConsortium introduced a potential for rating quality through their Protocol for Internet\\nContent Selection (PICS), the scope of which is being extended within their Resource\\nDescription Framework (RDF) to include Intellectual Property Rights and Privacy\\nManagement. In a recent keynote (Brisbane, April 1998), Tim Berners-Lee outlined a\\nconsiderably more ambitious goal of adding a veridity parameter to information. His vision\\nis to develop a global reasoning system within the world wide web, whereby individual\\nknowledge elements function as distributed axioms, which can be combined to reach truth\\nstatements. \\n\\xa0\\xa0\\xa0 5. Rights (Agreements) \\n\\xa0\\xa0\\xa0 4. Privacy (Copyright)\\n\\xa0\\xa0\\xa0 3. Quality (Ratings, Reviews)\\n\\xa0\\xa0\\xa0 2. Veridity (Truth Value, Axioms)\\n\\xa0\\xa0\\xa0 1. Summaries (Keywords, Descriptors)\\n\\xa0\\xa0\\xa0 0. Contents (Facts, Claims)\\nFigure 2. Schema showing basic levels of metadata. \\n\\xa0\\xa0\\xa0 Implicit in the above is a new approach to information and knowledge\\nwhereby facts and claims will no longer exist in isolation. Knowledge packages will be\\nsurrounded by five layers of metadata (figure 2). On the basis of such added parameters it\\nwill be possible to search for various subsets of materials. If one were searching for\\nadventure films, one could ask, for instance, for all five, four, three, two, and one star\\nexamples separately or all films irregardless of their rating and then explore what\\npercentage belong to each of the categories. One could compare the percentages in other\\nfields. Are there more five star films, relatively speaking, in the mystery, thriller,\\nchildrens or other category? \\n\\xa0\\xa0\\xa0 One could also begin mapping relationships of texts to commentaries\\nabout texts, statements of objective truth versus subjective claims about those truths.\\nLevels of certainty might be depicted in different diaphanous colours, such that one could\\nvisualize a given verity and all claims surrounding it becoming further removed as their\\nlevels of certainty decrease. Not all materials will be certified. So one can choose\\nwhether one wishes only certified, officially sanctioned, materials or all materials\\npertaining to a given book, painting or other cultural object. A cultural object will no\\nlonger be a single entity, it will have associated with it a series of attributes defining\\nnot only its physical characteristics but also its quality. In terms of object oriented\\nprogramming there will be objects of objects.\\n\\xa0\\xa0\\xa0 It is important to recognize that the increasing importance of\\nmetadata is part of a larger shift whereby there is a separation between knowledge and\\nviews of that knowledge. The 1960’s, for example, saw the rise of databases. These\\nallowed one to enter basic facts into fields of information, which could then be called up\\nin a number of different ways as reports without needing to alter the original facts. The\\nrise of Standardized General Markup Language (SGML) took this approach considerably\\nfurther by effectively devising one set of tags for the original content and a second set\\nof tags for ways of viewing that content. To put it slightly differently an SGML knowledge\\nobject has a \"content\" section and a \"views\" section. The evolving\\nExtensible Markup Language (XML) uses exactly the same principle with the exception that\\nit is designed for less complex situations than SGML and according is easier to use. In\\nthe case of both SGML and XML one can change or add to the \"views\" section\\nwithout altering the basic content. This is fundamentally different from the print world\\nwhere the content and layout become inextricably mixed to the extent that any decision to\\nalter layout requires all the work of a new edition. \\n\\xa0\\xa0\\xa0 Metadata, in the sense that it is being used by the World Wide Web\\nConsortium, takes the basic approach of databases and SGML another significant step\\nforward. It continues the distinction between content and views, but adds to the content\\nsection basic parameters concerning veridity and value. Facts remain constant. The ways of\\nviewing them, using them, presenting them change. This opens the way to reusable knowledge\\nin a new sense. The same repository can be used to tailor views for a beginner and an\\nexpert, without needing to rewrite the repository each time. \\n\\xa0\\n\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n\\xa0 \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n\\nFigure 3. Systematic approach to a museum or library beginning with a ground-plan,\\nview of a room, a wall, and finally a painting or book as if in file card form with basic\\ndescriptive information.\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n\\n\\n\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 Figure 4a. Visualization\\nof editions as a list, a graph, as a circle or as an undulating inverted cone.\\xa0 b)\\nVisualization of related terms as a list, as a series of surrounding terms, as a series of\\nintersecting circles or as other undulating inverted cones. \\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nFigure 5. Lists of editions of a work, reviews, and commentaries thereof, translated\\ninto spatial locations and then into inverted cones. In this approach unimportant writings\\nbecome narrow cylinders and influential important works become broad cones.\\n\\xa0\\xa0\\xa0 Marshall McLuhan characterized the history of the West as a constant\\nshift in emphases among the three elements of the trivium: grammar (substance or\\nstructure), dialectic (logic) and rhetoric (effect). Does the multimedia world of metadata\\nmark a return the structural dimensions of grammar or does it mark an entirely new chapter\\nin the evolution of knowledge? One thing is certain. As will become clear in the pages\\nthat follow, metadata is changing the nature of knowledge and the horizons of study.\\nTable of Contents\\n\\n8. Research and Knowledge\\nQuick Reference \\n\\n\\xa0\\xa0\\xa0 With respect to research, appropriate interfaces will also depend\\nvery much on the purpose at hand. Often a reader or visitor is interested in questions of\\nWho? What and Where? In such cases requiring quick access to basic facts, they will need\\non-line access to the digital reference room outlined in part one of this paper. At the\\nsimplest level, this will give them factual information about persons, places and things.\\nStanding in front of Botticelli’s Birth of Venus (Uffizi, Florence), or\\nlooking at an image of that painting on the Internet, a viewer might want to have\\nbiographical facts. This can range from wanting the most elementary listing of when he\\nborn and when he died, through a one page synopsis of his life, to reading a standard\\nbiography. Standing in front of Leonardo’s Ginevra de Benci (National Gallery,\\nWashington), one could ask about the history of previous owners in order to learn how it\\ngot from Florence via Liechtenstein to Washington. \\n\\xa0\\xa0\\xa0 Today paintings are in galleries. Cultural artifacts are in museums.\\nWhat we know about those paintings and artifacts is usually in articles and books in\\nlibraries, particularly in the reference section of libraries which contain terms\\n(classification systems), definitions (dictionaries), explanations (encyclopaedias),\\ntitles (bibliographies, catalogues), and partial contents (abstracts, reviews). Given a\\nuniversally accessible, digital reference room, viewers and readers will readily be able\\nto find definitions and explanations without having to run to dictionaries,\\nencyclopaedias, bibliographies and the like. Such searches can readily happen on a regular\\nPC or a portable notebook computer. In these cases simple lists and paragraphs in a\\ncoherent interface will usually suffice. \\n\\nObjects and Subjects\\n\\n\\xa0\\xa0\\xa0 This type of quick reference or hunting after basic facts about\\nobjects and subjects, is the most elementary level of conceptual navigation which\\ninterests us. It sounds very straightforward and yet to achieve even this will require an\\nenormous reorganization of knowledge. It would, for instance, be highly inefficient, and\\nvery time consuming, if everyone who wanted to know about an artist such as Botticelli,\\nhad to search every database around the world. Even searching through every database\\nrelating to art would take too long. \\n\\xa0\\xa0\\xa0 Using the principles of object-oriented programming, we need to\\ndevelop objects of objects, a richer kind of metadata, which will contain key information\\nconcerning them. In the case of Botticelli, for instance, this will include his variant\\nnames, his date of birth and death, where he worked, and a list of all his drawings,\\npaintings and other works. This will build on the authority files for artists’ names\\nsuch as Thieme Becker’s Allgemeine Künstler Lexikon (AKL), those of museums\\nand libraries. In addition to this key information about his primary works there will be a\\nreference to all secondary sources about Botticelli, in books, refereed journals and\\nelsewhere. To achieve this will require the development of individualized agents which\\nseek extant materials, gather them and are then vetted by the leading experts on that\\nartist, author or individual. The net result of such efforts will be a Botticelli\\n\"object,\" with all the metadata pertaining to a traditional \"complete\\nworks.\"\\n\\xa0\\xa0\\xa0 In the case of a given painting this will include preparatory\\ndrawings, versions by students, members of the workshop, school and followers; copies,\\ndifferent owners, restorers and details of their restorations; their locations and dates.\\nIn the case of a manuscript this will entail all copies, all book versions, quantities\\npublished, editions, locations, and dates. \\n\\xa0\\xa0\\xa0 Once these \"knowledge objects\" of artists, books,\\npaintings, sculptures and other cultural heritage exist, they can be combined in new ways.\\nIf, for example, one were beginning from the context of a virtual museum, one might zoom\\nin from a view of the world, to the continent of Europe, to the country of Italy, the city\\nof Florence, the ground plan of the Uffizi, to a wall in the Botticelli room, and focus on\\nhis Adoration of the Magi (figure 3a). This would bring up basic information about\\nthe painting. One could then choose to see preparatory drawings, copies, other versions,\\nother paintings by the same artist, or other paintings on the same theme by different\\nartists. \\n\\xa0\\xa0\\xa0 Three-dimensional navigation spaces are particularly valuable for\\nsuch contextualisation of knowledge. A two-dimensional title or frontispiece of a book\\ntells us nothing about its size. A three-dimensional scale rendering helps us to recognize\\nat once whether it is a pocket sized octavo or a folio sized book: a slender pamphlet or a\\nhefty tome. Hence, having chosen a title one will go to a visual image (reconstruction) of\\nthe book; see, via the map function, where it appears on the shelf of a library, do\\nvirtual browsing of the titles in its vicinity or wander elsewhere in the virtual stacks\\n(cf. figure 3b). \\n\\xa0\\xa0\\xa0 In the case of such a book, one might choose to see various editions\\nin a chronological list. One could then choose to see the same list of editions as a graph\\nshowing fluctuations over time. Alternatively one might visualize the original edition as\\na small circle linked with successive editions in the form of an inverted cone which\\nsometimes contracts and then expands further (figure 4a). Or one might begin with all the\\nkeywords related to a given edition, render these spatially either as a series of concepts\\nsurrounding the original, or as circles intersecting a central circle in the manner of a\\nVenn diagram, each of which can in turn be visualized as inverted cones (figure 4b).\\n\\nValue \\n\\n\\xa0\\xa0\\xa0 In the excursus on metadata we mentioned a trend toward creating\\nobjects of objects, which will describe their physical characteristics and their quality.\\nThere will be numerous ways of visualizing quality. An author’s primary literature\\ncould define a circle, surrounded by a larger circle of secondary literature. Influential\\nauthors would have large surrounding circles. Unimportant authors would have only their\\ninitial circle: a visualization of \"no comment.\" Alternatively, one could have a\\nsmall circle for an edition, surrounded by larger circles for reviews, commentaries and\\ncitations: effectively a section of the cone in figure 5. In some cases there will be\\nspecific ratings such that one can identify specifically the grade or rating and not just\\nthe popularity of a book, painting or cultural artifact. Not all materials on the Internet\\nwill be certified. So one can visualise an object as a circle, surrounded by a certified\\ncircle and a larger uncertified circle. Combinations of these approaches are possible,\\nsuch that one might discern which portions of reviews, commentaries and citations are\\ncertified or uncertified.\\n\\xa0\\xa0\\xa0 In itself the creation of such circles may seem a rather fatuous\\nexercise. If, however, they are produced on a specific scale and applied systematically to\\na subject, a field, a region, a country, a period, a movement or a style, or combinations\\nof these, then the approach can be very useful in helping us to see new patterns of\\ndevelopment. What correlations are there between the most influential books and the most\\nimportant books? Does the production of important books in a field change over time? Does\\nit shift from country to country? Can the reasons for the shift be determined?\\n\\xa0\\xa0\\xa0 The attentive reader will have perceived that the systematic\\napproach here outlined does not pretend that computers will use artificial intelligence\\n(AI) or other algorithms to arrive at new insights in isolation. Rather the claim is that\\ntheir systematic treatment of data and information will expand the range of questions\\nwhich can reasonably be answered. By providing comprehensive treatment of the four basic\\nquestions: Who?, What?, Where?, and When?, they will prepare the ground for new answers to\\nquestions of How and Why? In this sense computers will help in intelligence augmentation\\n(IA rather than AI in the senses of Doug Engelbart and Fred Brooks).\\n\\nTransformation of Knowledge \\n\\n\\xa0\\xa0\\xa0 This quest to achieve objects of objects which contain information\\nconcerning all the physical and qualitative characteristics of the original is analogous\\nto the quest for determining the structure of DNA and the mapping of nature in the human\\ngenome project. It is much more than just another cataloguing project. It is a quest,\\nwhich will transform the very meaning of knowledge. \\n\\xa0\\xa0\\xa0 On a seemingly quite different front, companies such as Autodesk\\nhave extended the notion of object-oriented programming to the building blocks of the\\nman-made world through what they term industry foundation classes. A door is now treated\\nas a dynamic object which contains all the information pertaining to doors in different\\ncontexts. Hence if one chooses a door for a fifty-storey skyscraper, the door object will\\nautomatically acquire certain characteristics which are very different from those of a\\ndoor for a cottage or a factory warehouse. This is leading to a revolution in\\narchitectural practice because it means that architects designing buildings will\\nautomatically have at their disposal the \"appropriate\" dimensions and\\ncharacteristics of the door, window or other architectural building block which concerns\\nthem. There is a danger that this could lead to stereotyped notions of a door or window, a\\nMcWorld effect, whereby buildings in one country are effectively copies of those in other\\ncountries, and travel loses its attractions because everywhere appears the same. \\n\\xa0\\xa0\\xa0 This same technology can be used with very different consequences if\\none extends the concept of foundation classes to include cultural and historical\\ndimensions. If this occurs, an architect in Nepal wishing to build a door, in addition to\\nthe universal principles of construction applying to such objects, will be informed about\\nthe particular characteristics of Nepalese doors, perhaps even of the distinctions between\\ndoors in Khatmandu or those near Annapurna. Similarly an Italian restorer will be informed\\nabout the particular characteristics of doors in Lucca in the fifteenth century. \\n\\xa0\\xa0\\xa0 All this may seem exaggerated and unnecessary. During the second\\nWorld War, however, some of the key historical houses with elaborate ornamental carvings\\nin Hildesheim (e.g. the Knochenhaueramtshaus or Bone Hacker’s administrative\\noffice) were bombed and it took a small group of carpenters several decades to reconstruct\\nthe original beam by beam, carving by carving. They did so on the basis of detailed\\nrecords (photographs, drawings etc.). If this knowledge is included in the cultural\\nobject-file of Hildesheim doors, windows and houses, then rebuilding such historical\\ntreasures will be much simpler in future. \\n\\xa0\\xa0\\xa0 At stake is something much more than an electronic memory of\\ncultural artefacts, which would serve as a new type of insurance against disaster. The\\nrichest cultures are not static. They change with time gradually transforming their local\\nrepertoire, often in combination with motifs from other cultures. The Romanesque churches\\nof Northern Germany adopted lions from Italy for their entrances. The church of San Marco\\nin Venice integrated Byzantine, Greek and local motifs. The architecture of Palermo\\ncreated a synthesis of Byzantine, Norman, Arabic and Jewish motifs. The architects in\\nToledo and at Las Huelgas near Burgos created their own synthesis of Jewish, Arabic and\\nChristian motifs. A comprehensive corpus of variants in local heritage thus leads to much\\nmore than a glorification of local eccentricities and provincialism. It can prove an\\ninspiration to multi-culturalism in its deepest sense.\\n\\xa0\\xa0\\xa0 The same principle, which applies to doors and windows, applies\\nequally to ornament, decoration, various objects such as tables and stools and different\\nbuilding types: temples, colosseums, monasteries, cathedrals, and churches. This\\ntransforms the meaning of knowledge. According to Plato, knowledge of a temple was to\\nrecognize in some particular manifestation the \"idea\" of some ideal temple.\\nKnowledge did not require knowing the exact dimensions of the Parthenon or any other\\ntemple. According to Aristotle knowledge lay in the precise characteristics of a temple\\nsuch as the Parthenon. Plato was interested in a universal concept, Aristotle in a\\nparticular example. Their mediaeval successors became embroiled in philosophical debates\\nwhether knowledge lay in universals or particulars. Even in schoolbooks today this problem\\nhas not been resolved. History texts typically refer to one example, the Colosseum in\\nRome, as if it were the only example, as if the particular were synonymous with the\\nuniversal class. \\n\\xa0\\xa0\\xa0 The new object oriented approach to knowledge is very different from\\nall of these precedents. For a \"temple object\" will not only contain within\\nitself the precise description of the Parthenon, but also the exact descriptions of all\\nthe other temples including those at Segesta, Selinunte, Agrigento, and Syracuse in\\nSicily, at Paestum and Rome in Italy, at Ephesus, Miletus, and Uzuncaburc in Turkey and\\nelsewhere. This new definition of knowledge resolves the age old opposition between\\nuniversal and particular, for it can describe the essential characteristics which all the\\ntemples have in common (universal) and yet render faithfully all their individual\\ndifferences (particular). \\n\\xa0\\xa0\\xa0 Knowledge now lies in a combination of the two. The Platonic idea of\\na temple reduced individual complexity to common characteristics, destroyed individual\\ndifferences and thereby the notion of uniqueness. The modern \"temple object\"\\ncentres knowledge on the fundamental significance of differences. Thus temples gain\\nuniversal value through the richness of their local variation. The universal becomes a key\\nto particular expression. Knowledge lies not in recognizing how good a copy it is but\\nrather in how well it has created a variation on the theme. \\n\\nSpatial\\n\\n\\xa0\\xa0\\xa0 The Colosseums in Rome (Italy) and El-Djem (Tunisia) were built in\\nthe same style. Nonetheless their effect is profoundly different due to their spatial\\nsettings, one in the midst of the Roman Forum, the other in a near desert setting. Hence\\nknowledge of spatial location, the co-ordinates familiar to Geographical Information\\nSystems (GIS), and Area Management/ Facilities Management (AM/FM) will also be an\\nessential part of a \"colosseum (knowledge) object\". \\n\\nTemporal\\n\\n\\xa0\\xa0\\xa0 The Colosseum in Rome was built at a given time. It was not,\\nhowever, a static building, in the sense that it remained exactly the same in the course\\nof the centuries. We know, for instance, that a large portion of it was dismantled in the\\nMiddle Ages to construct other buildings. Hence a \"colosseum (knowledge) object\"\\nwill need to include all our knowledge about changes over time: i.e. its complete history,\\nincluding all restorations and interventions for its conservation. Knowledge now includes\\ntime as well as space. \\n\\nA New Encyclopaedia \\n\\n\\xa0\\xa0\\xa0 Some will say that this new approach to knowledge is merely a\\nrevival of an age-old encyclopaedic tradition. This is potentially misleading because the\\nencyclopaedic tradition itself has undergone fundamental shifts in its goals. Aristotle\\nwas encyclopaedic but his quest was to create summaries, which were subsets of the\\noriginals such that the originals could be abandoned. That is why we have what Aristotle\\nsaid about many of the ancient authors rather than the ancient authors themselves. Their\\nworks were allowed to go lost because it was assumed that the Aristotelian summary\\nreplaced them. Vitruvius was also encyclopaedic in this sense, except here there was an\\nadded goal of making the subset readily memorizable, an aide-mémoire, rather than\\ncreating a record of all that existed. \\n\\xa0\\xa0\\xa0 Such decisions were not only guided by profound philosophical\\nreflections. They were partly pragmatic reflections of the available storage media. If\\nknowledge is writ on stone tablets, the burden of knowledge is truly great. The advent of\\nparchment, manuscripts and then printing expanded those horizons considerably. Ironically\\nthe same Renaissance which introduced the medium of printing, introduced also a tendency\\nto use media to separate knowledge: books were put into libraries, pictures into\\ngalleries, drawings into print galleries (cabinet de dessins), engravings into\\nengraving galleries (Kupferstich Kabinett), maps into map rooms and cultural\\nobjects into museums. Knowledge was decontextualized. \\n\\xa0\\xa0\\xa0 The 18th century Encyclopédistes re-introduced a vision\\nof encompassing all knowledge. But as the rate of knowledge continued to increase, even\\nthe organizers of the Encyclopaedia Britannica, decided, after 1911, to abandon the\\nquest for universality. Recent innovations in terms of macro-paedia and micro-paedia\\nsections have neither re-contextualized knowledge nor re-introduced a quest for an\\ninclusive encyclopaedic approach. \\n\\xa0\\xa0\\xa0 The new \"knowledge objects\" distinguish themselves from\\nearlier efforts in several ways. First, computers remove both the barriers of storage\\ncapacity and any need to separate knowledge on the basis of media. Second, the\\n\"knowledge objects\" require a new kind of encyclopaedic approach: one that is\\nglobally inclusive of all the variants rather than merely a local summary thereof. This\\nwill change the meaning of \"objects\" insomuch as we shall have collected in one\\nplace all quantitative and qualitative information about an object.\\nMultiple Views\\n\\xa0\\xa0\\xa0 In the past scholars typically spent a majority of their time trying\\nto locate basic facts about an object: Who painted it? Where was it made? When was it\\nfinished? For the next few generations scholars will likely be pre-occupied with assuring\\nthat the new \"knowledge objects\" are reliable and as comprehensive as they\\nclaim. Once all such information is at our fingertips, will scholars find themselves\\nredundant in the face of automation as in the case of many traditional manufacturing jobs?\\nThe answer is definitely not. It is simply that the tasks will be different. In the Middle\\nAges it took one hundred monks ten years of full-time effort to create an index to the\\nworks of Saint Thomas Aquinas. Today that same task can theoretically be performed in\\nminutes by a computer. Having an index spares us the need of reading the complete works\\neverytime we are looking for some particular thought, argument, or fact. But this does not\\nremove the challenge of choosing thoughts, arguments and facts and deciding how or why to\\napply them. The process of thinking remains. \\n\\xa0\\xa0\\xa0 Once the basic facts have been arranged, scholars will find\\nthemselves devoting more attention to presentation. Professors will become view masters in\\na new sense. Their challenge may lie less in conveying basic facts, than in teaching\\nstudents to look at facts and concepts in new ways: as a list, a chart, on a map or more\\nabstractly. To take a simple example, any book has a series of key words associated with\\nit, which provides us with some clues concerning its scope. Few keywords indicate a\\nspecialized title. Many keywords suggest a title with many applications. Such keywords can\\nbe visualized as sides of regular and semi-regular shapes and solids. In this\\nconfiguration, specialized authors produce points, lines and triangles. Generalists\\nproduce increasingly many-sided solids. This introduces new possibilities for looking at\\nthe authors in a given field, or of a certain distinction. Were Nobel laureates in 1908\\nmainly generalists or specialists? Were there significant differences between the arts and\\nscience? Did geography play an obvious role? For instance, were the Nobel laureates from\\nEurope more specialized than those from America, or conversely? Did this pattern change\\nover time? Implicit in such activities is a shift from questions about substance (Who?\\nWhat? Where? When?) to those of function (How?) and purpose (Why?). The old notion of\\nscholars as philosophers may witness a revival. \\n\\xa0\\xa0\\xa0 Presentation is much more than deciding whether to use overheads or\\nslides, whiteboards or virtual reality. It is ultimately concerned with new methods of\\nstructuring knowledge, not just individual objects, but also the larger frameworks into\\nwhich they can be arranged. This is the terra incognita of future scholarship.\\nKnowledge organization will become as important as knowledge acquisition and raise many\\nnew challenges for cultural interfaces. \\n\\xa0\\xa0\\xa0 One aspect of this structuration process will lie in integrating\\nhitherto disparate knowledge elements. For instance, to continue with our earlier example,\\nthe \"colosseum (knowledge) object\" will entail all the physical characteristics\\nof the colosseum at Rome and those of all the other colosseums at Arles, Nimes, El Djem,\\nPula and elsewhere in the empire. Using a map one will be able to see where all these\\nplaces are. Linking a time line with this map one will be able to trace the order of their\\nappearance. Were there close connections between the rise of colosseums and theatres? If\\nso were these connections geographical and chronological or only one of the above? Or were\\nthe rise of colosseums and theatres two quite distinct phenomena? Similar questions could\\nbe posed with respect to the rise of monasteries, churches and cathedrals. \\n\\xa0\\xa0\\xa0 One can imagine scholars devoting their energies to posing what they\\nthink could be fruitful or significant questions. One can also imagine a future generation\\nof agents automatically generating questions and comparisons and only reporting on cases\\nwhere some significant correlation emerged. In which case the challenge of scholarship\\nwould focus less on finding patterns and more on explaining their significance. There\\nwould be various levels of patterns, some local, others regional or national, a few\\ninternational or even global. These patterns will lead to new attempts to characterize\\nperiods, movements and styles. \\n\\xa0\\xa0\\xa0 How will these differ from the periods of traditional historical\\nstudies? Because they encompass a much larger sample of evidence they will frequently come\\nto very different conclusions. By way of illustration, it is useful to cite the case of\\nthe Renaissance. In traditional studies, ancient Greece marked a period of enlightenment.\\nDuring the \"Dark Ages\", the story goes, the lights went out. Then around 1400,\\nsomeone found the light switch and there was a Renaissance, literally a rebirth. The light\\nswitch, we are told lay in the rejection of the Dark Ages and a return to the wondrous\\ninsights of Antiquity. Thus far the received wisdom. \\nIn this stereotypical view, Renaissance art is usually reduced to the achievements of a\\nhandful of remarkable painters including Botticelli, Leonardo, Raphael and Michelangelo,\\nand museums such as the Uffizi and San Marco are of central importance. And although\\npassing reference is made to the importance of Assisi and the Arena Chapel of Giotto,\\nstandard books tend to ignore the predominant role played by fresco cycles in all the\\nmajor churches of the Renaissance, not only in Florence and Venice, but equally in\\nCastiglione d’Olona, Milan, Montefalco, Perugia, San Gimignano, Sansepolcro, Siena,\\nRome and lesser known centres such as Atri. A careful examination of these cycles reveals\\nthat they focussed on the lives of the saints, from Christ’s contemporaries such as\\nSaint John the Baptist and the Apostles (such as Peter and Paul), through the early\\nmartyrs (Steven and Lawrence) and church fathers (Augustine, Jerome), right though the\\nMiddle Ages (including more recent saints such as Thomas Aquinas and Saint Francis of\\nAssisi). Seen as a whole this corpus points to some very different conclusions about what\\nwas happening in the Renaissance. The artists of the Renaissance discovered an\\nuninterupted continuity between the time of Christ and their own period provided by the\\nlives of the saints. Far from rejecting entirely a so-called Dark Ages, one could argue\\nthat they recognized for the first time its essential role. In short the entire history of\\nwhat happened in the Renaissance needs to be rewritten.\\n\\xa0\\xa0\\xa0 In the longer term there is a larger challenge of finding ways to\\nshow how the complexity of cultural activities in the period 1300-1600 could be so reduced\\nas to make the myth about rejecting the Middle Ages a temporarily convincing\\nmisrepresentation of the truth. This is another manifestation of the relationship between\\ncontent and views. Let us posit, hypothetically, that there were 10,000 buildings of\\ncultural interest during the Renaissance. Every major art historian such as a Berenson, a\\nChastel, or a Gombrich focusses on some subset thereof. So we should have interfaces which\\nshow us how schools of scholarship in a given country both bring into focus some aspects\\nwhile at the same time obscuring many other aspects. We need to make visible the way\\nsecondary literature functions as a prism that leads us to overlook complexity while at\\nthe same time explaining other bits. \\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n\\n\\xa0\\nFigure 6. A term in a classification system shown as a two-dimensional list. This list\\nis folded ninety-degrees to a plane at right angles to the screen. Another classification\\nsystem is introduced in a plane parallel to the first. A third classification system is\\nintroduced in like manner. This is used to visualize links between the term in the three\\nsystems. \\n\\nClassification\\n\\n\\xa0\\xa0\\xa0 An important dimension of knowledge structuration lies in\\nclassification systems. The major international systems are relatively few. They include\\nBliss, Dewey, Göttingen, Library of Congress, Ranganathan, Riders International for books\\nas well as the Art and Architectural Thesaurus and Iconclass for art. To a certain extent\\nthese reflect national differences. The United States has the Library of Congress and\\nDewey. Germany has the Göttingen system and others. India has the Ranganathan system. In\\nterms of lesser systems or systems specialized on some particular field there are at least\\n950 others. Each of these presents different ways of classing the world, with different\\nbranches, facets, alternative associations, different broader and narrower terms. These\\nsystems also change over time. Ranganathan initially had little about art compared to\\nwestern systems, yet a great deal about consciousness and higher states of awareness. \\n\\xa0\\xa0\\xa0 When we find a cultural object it can be classed in many ways.\\nTraditionally museums have developed one way of classing, art galleries another and\\nlibraries another again. Yet a given painting may well represent an object which exists\\nphysically in a museum and about which there is written material in a library. This is why\\nwe need meta-data and meta-databases in order to discover the commonalities required to\\ncreate integrated knowledge objects. \\n\\xa0\\xa0\\xa0 To study a cultural object systematically we need authority lists to\\nhave their standard names and recognize which are their variants. Classification systems\\nreveal how that object has been classed as a subject, topic, theme, field, discipline and\\nso on. Such systems also reveal the hierarchies or trees within which objects have been\\nplaced. These structures change with time. So we need ways of visualizing equivalences\\neither geographically, chronologically or both. We might begin, for example, (figure 6) by\\ntreating the term on the screen as a plane, make this transparent, rotate it downwards by\\n90 degrees such that it becomes the top surface of a transparent box. The x-axis now\\nbecomes the time-axis such that we can literally trace the connections between various\\nsubjects. Such an example outlines a means of moving seamlessly from two-dimensional lists\\nto three-dimensional conceptual maps of subjects with their related topics and also offers\\na whole new way of seeing interdisciplinarity. \\n\\xa0\\xa0\\xa0 One of the challenges in moving between different cultures lies in\\nknowing where to look for equivalent terms. So a person from Canada familiar with the\\nLibrary of Congress (LC) might choose a series of Library of Congress Subjects. If they\\nwere interested in India, the system would then find the closest related terms in\\nRanganathan and use these to search other catalogues and lists. At a next stage this set\\nof terms can be used to create a cluster of closely related terms and use these for\\nsearching. \\n\\nRelated Objects and Subjects \\n\\n\\xa0\\xa0\\xa0 As noted above the quest for equivalent terms leads almost\\ninevitably to a search for related terms, objects and subjects, much in the way that\\nbrowsing in a library while looking for one book, very frequently leads us to find others,\\nwhich are as relevant or perhaps even more so than the book we originally set out to find.\\nClassification systems provide another means of contextualising our search: i.e. seeing\\nrelations between one subject and another. When we are studying a subject, we typically\\nwant to know about related subjects. \\n\\xa0\\xa0\\xa0 In the past we went to a library catalogue, found a title and saw\\nthe related topics at the bottom of the card. Electronic versions thereof exist. Recent\\nsoftware such as Apple’s Hotsauce allows us to go from a traditional\\ntwo-dimensional list of terms, choose one, and then see all the related topics arranged\\naround it. These related subjects evolve with time, so with the help of a simple time\\nscale we can watch the evolution of a field’s connections with other subjects. This\\nidea can easily be extended if we translate the main topic into a circle and the related\\nsubjects into other (usually smaller) circles intersecting the main one to create a series\\nof Venn diagrams. This visualisation allows us to choose subsets common to one or more\\nrelated fields, which is important if one is trying to understand connections between\\nfields (figure 4b).\\nRelators\\n\\xa0\\xa0\\xa0 Classification systems typically take us to broader and narrower\\nterms in our quest for related terms. But as thinkers such as Perrault and Judge have\\nnoted there are numerous other means to acquire related terms including: alternatives,\\nassociations, complementaries, duals, identicals (synonyms as in Roget’s Thesaurus),\\nopposites (antonyms), indicators, contextualizers and logical functions such as\\nalternation, conjunction, reciprocal, converse, negative, subsumptive, determinative and\\nordinal. It is feasible that these will eventaully become part of the \"knowledge\\nobjects,\" such that if one has a term, one can see its synonyms without needing to\\nrefer to a thesaurus. All these kinds of relations thus become different hooks or\\ndifferent kinds of net when one is searching for a new term and its connections. \\n\\nOntologies\\n\\n\\xa0\\xa0\\xa0 Such classification systems are the most familiar, important efforts\\nat bringing order to the world in terms of subjects. But subjects in isolation are still\\nonly somewhat ordered information. Meaning which brings knowledge and wisdom requires\\nmore, namely a systematic ordering of these subjects in terms of their logical and\\nontological relations. Efforts in this direction go back at least to the I Ching.\\nAristotle, Thomas Aquinas, Ramus, Francis Bacon and Roget were among the many contributors\\nto this tradition. In our generation, Dr. Dahlberg presented these fundamental categories\\nin a systematic matrix. More recently these have been adapted by Anthony Judge into a\\nmatrix of nine columns and nine levels (figure 6), which generates a chart of 99 subjects.\\nThese range from fundamental sciences (00), astronomy (01) and earth (02) to freedom,\\nliberation (97) and oneness, universality (99). Anthony Judge is using this as an\\n\"experimental subject configuration for the exploration of interdisciplinary\\nrelationships between organizations, problems, strategies, values and human\\ndevelopment\".\\n\\xa0\\nMatrix columns\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nMatrix levels\\n9 Condition of the whole\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nExperiential (modes of awareness)\\n8 Environmental manipulation\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nExperiential values\\n7 Resource redistribution\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nInnovative change (context strategies)\\n6 Communication reinforcement\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nInnovative change (structure)\\n5 Controlled movement\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nConcept formation (context) \\n4 Contextual renewal\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nConcept formation (structure)\\n3 Differentiated order\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nSocial action (context)\\n2 Organized relations\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nSocial action (structure)\\n1 Domain definition\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nBiosphere\\n0 Formal pre-conditions\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nCosmosphere/Geosphere\\nFigure 7. An integrative matrix of human preoccupations by Anthony Judge (Union\\nInternationale des Associations) adapted from Dr. Ingetraut Dahlberg. \\n\\xa0\\xa0\\xa0 Heiner Benking, builds upon the framework of Dahlberg and Judge (as\\nin figure 7 above), to produce his conceptual superstructure or cognitive Panorama Bridge,\\nwhich is the basis of his Rubik’s Zauberwürfel [Cube of Ecology or Magic\\nCube]. He argues that one can use planes in order to see patterns in thought. These\\nplanes, he claims, can include continua between the animate and the inanimate on one axis\\nand between micro-, macro- and meso-scales on another axis. Planes, he claims, can be used\\nto compare different viewpoints at a conceptual as well as a perceptual level; to see\\nrelations among different actions, options and strategies.\\n\\xa0\\xa0\\xa0 Seen in this context, it becomes evident that our discussion thus\\nfar has been rather narrow. It has dealt primarily with physical objects in the\\ncosmosphere/geosphere (level 0) although the comments on classification have touched\\nbriefly on concept formation (level 4). From this point of view the amount of knowledge\\nstructuration that remains to be done is staggering indeed. Scholars are not about to be\\nwithout work. \\n\\xa0\\xa0\\xa0 If we were trying to achieve a truly big picture involving the\\ninterplay of two or more of the planes in this matrix, then a three-dimensional interface\\nwith the kinds of planes outlined earlier will be essential (cf. figure 6). Parallel\\nplanes can be used to see different levels of abstraction. A challenge remains how\\nprecisely we are to navigate between such conceptual landscapes and the knowledge\\nstructures of libraries, which have been a main focus of this paper. At a programming\\nlevel this should be relatively straightforward. Each of the ninety-nine subjects is\\ntagged with its equivalents in the main classification schemes. At the user level, this\\nand similar matrices then become a natural extension of the system. When we use these\\ncategories as a filter to look at publications in the Renaissance or research trends in\\nthe late twentieth century, we have another means to comprehend which areas were the focus\\nof attention and which were abandoned, or even forgotten. Search and access systems must\\nhelp us to see absence as well as achievement, and possibly provoke us to look more\\nclosely at the spaces which are being ignored. Were they truly dead ends, have they\\nsurfaced in a new guise or do they now require new study?\\n\\xa0\\n\\nVisualising Connections in Conceptual Spaces\\n\\n\\xa0\\xa0\\xa0 The third dimension has many uses beyond producing such electronic\\ncopies of the physical world. Pioneers of virtual reality such as Tom Furness III, when\\nthey were designing virtual cockpits, realised that pilots were getting too much\\ninformation as they flew at more than twice the speed of sound. The challenge was to\\ndecrease the amount of information, to abstract from the myriad details of everyday vision\\nin order to recognise key elements of the air- and land-scape such as enemy planes and\\ntanks.\\n\\xa0\\xa0\\xa0 This principle is equally important in knowledge organisation and\\nnavigation. A library catalogue gives me the works of an author. Each catalogue entry\\ntells me under how many fields a given article or book is classed. Adding these fields\\ntogether leads to an alphabetical list of that author’s intellectual activities.\\nProducing such a list in electronic form is theoretically simple. What we need, however,\\nis a conceptual map. To what extent did an author work as a generalist in large subject\\nfields and to what extent as a specialist? This lends itself to three dimensions. Author A\\nis in one plane and the subject headings of their works are on other planes. These are\\naligned to relative positions in classification systems such that one can see at a glance\\nto what extent this person was a generalist or a specialist and linked with the matrix of\\nhuman preoccupations to discern how they relate to this (figure 8). This principle can be\\nextended in comparing the activities of two authors.\\n\\nFigure 8. Visualisation of an author’s activities whose specialist activities\\ntouch on four fields (three of which are closely related) and whose more generalist\\nactivities are limited to one major field. This can, in turn, be linked with the matrix of\\nhuman preoccupations. Further layers could be added to show how the same concepts recur in\\ndifferent places in various classification systems.\\n\\xa0\\n\\n\\xa0\\nFigure 9. Venn diagram of a subject and its related subjects, shown as intersecting\\ncircles. In addition to regular searches by subject, this visualisation allows a user to\\nchoose subsets common to one or more related fields, which is important if one is trying\\nto understand interdisciplinary relationships. Cf. fig. 4.\\n\\n\\xa0\\nFigure 10. In this diagram the large circles again represent two fields and the smaller\\ncircles represent branches of these fields. The lines joining them represent work linking\\nhitherto different branches. These lines thicken as the amount of articles and other\\nresearch activities increase and thus become a new means of tracing the growth of an\\nemerging field. \\n\\nFigure 11. Using spatial arrangements of concepts to map problems identified and to\\nvisualise which subsets thereof were financed as research projects, which were solved in\\nthe sense of producing patents, inventions and products and which led to new predictions\\nin the form of hypotheses and projections. \\n\\nSocial \\n\\n\\xa0\\xa0\\xa0 This approach can in turn be generalised for purposes of\\nunderstanding better the contributions of a group, a learned society or even a whole\\nculture. Scholars such as Maarten Ultee have been working at reconstructing the\\nintellectual networks of sixteenth and seventeenth century scholars based on their\\ncorrespondence. A contemporary version of this approach would include a series of\\nnetworks: correspondence, telephone and e-mail which would help us in visualising the\\ncomplexities of remarkable individuals be it in the world of the mind, politics or\\nbusiness.\\n\\xa0\\xa0\\xa0 The geographical aspects of these intellectual networks can be\\nvisualised using maps. Conceptually the subjects of the letters, (and the e-mials to the\\nextent that they are kept), can be classed according to the layers outlined above such\\nthat one gains a sense of the areas on which they focussed. For instance, what branches of\\nscience were favoured by members of the Royal Society? Did these change over time? It is a\\ntruism that Renaissance artists were also engineers and scientists. \\n\\nFigure 12. Diagram relating to metadatabase research at Rensselaer Polytechnic in\\nconjunction with Metaworld Enterprises entailing a Two Stage Entity Relationship (TSER) in\\nthe context of an Information Base Modelling System. \\n\\xa0\\xa0\\xa0 What particular fields did they favour? Can one perceive significant\\ndifferences between artist-engineers in Siena, Florence, Rome and Venice? We could take\\nthe members of a learned society or some other group and trace how many layers in the\\nclassification system their work entailed and then study how this changed over time. Are\\nthe trends towards specialisation in medicine closely parallel to those in science or are\\nthere different patterns of development? Alternatively by focussing on a given plane of\\nspecialization we could trace which authors contributed to this plane, study what they had\\nin common in order to understand better which individuals, networks of friends and which\\ngroups played fundamental roles in the opening of new fields. Such trends can in turn be\\nlinked with other factors such as research funding or lack thereof. In addition to\\nuniversities, major companies now have enormous research labs. Nortel, for instance, has\\nover 17,000 researchers. Hitachi has over 30,000. We need maps of Who? is doing What? and\\nWhere? In our century we could also trace where the Nobel and other prizes have gone both\\nphysically and conceptually. Navigation provides virtual equivalents of journies in the\\nphysical world. It is also a means of seeing new patterns in the conceptual world through\\nsystematic abstraction from everyday details in order to perceive new trends. \\n\\xa0\\xa0\\xa0 If we were trying to trace the evolution of a new field, we could\\nbegin by using a dynamic view of classification systems described above. We could also use\\ncombinations of these intersecting Venn diagrams. For example, the last generation has\\nseen the emergence of a new field of bio-technology. This has grown out of two traditional\\nfields, biology and technology. These could be represented as large circles surrounded by\\nsmaller ones representing, in this case, their related branches and specialties. Any\\nacademic work would be represented in the form of a line, which thickens in proportion as\\nthe connections increase. These connections are of differing kinds. Initially they tend to\\nbe in the form of sporadic articles, conferences, or isolated research projects, which\\nhave no necessary continuity. \\n\\xa0\\xa0\\xa0 Later there are books, journals, professorships, research institutes\\nand spin-off companies which bring a conscious cumulative growth to the new field. Each of\\nthese phases could be identified with different colours so arranged that one can\\ndistinguish clearly between sporadic and cumulative activities (figure 10). We can\\nintegrate these circles within the context of frames as described above. For example, the\\ntwo fields of biology and technology could be on one plane. Major universities could be on\\na second plane. We could then trace which universities are producing the most papers of\\nthese two fields and specifically on what sub-sections thereof. On another plane we could\\nlist the major research institutes in order to determine other trends. Are these areas\\nbeing studied more in the United States than Europe or Japan? If so what are the\\npercentages? Which companies dominate the field? What links are there between research\\ninstitutes and universities? Are these increasing or decreasing?\\n\\xa0\\xa0\\xa0 Experiments in the realm of metadatabase research at Rensselaer\\nPolytechnic provide a preliinary idea of how a concept at one level can be linked via\\nplanes with a series of concepts at another level. Such a notion of planes can be extended\\nto see further patterns. Plane one can list all the known problems or potential research\\nareas in a given area of science. Plane two lists which subset of these problems is\\npresently being studied. Plane three shows which problems have been solved, or rather have\\nsome solutions in the form of inventions, patents, trademarks and products. Plane four\\nlists a further subset for which solutions are predicted or which have hypotheses for\\ntheir solution (figure 12).\\n\\xa0\\xa0\\xa0 Such comparative views can help scientists and decision-makers alike\\nto understand more clearly trends in rapidly developing fields. Such matrices of problems\\ncan in turn be submitted to problem structuring methodologies whereby technical, practical\\nand emancipatory dimensions are submitted to frameworks in order to discern where they fit\\ninto what some have called a Methodology Location Matrix. \\n\\xa0\\xa0\\xa0 Returning for a moment to the framework outlined in figure 7, one\\ncan envisage the direction which a future encyclopaedia will take. For instance, level\\nseven in this framework outlines context strategies including logic, philosophy, security,\\ncommunity, peace and justice. These will be related to the context of concept formation\\n(level five) and its structure (level 4), the context of social action (level 3) and its\\nstructure (level 2).\\n\\xa0\\xa0\\xa0 Earlier we discussed the spread of ancient temples, of mediaeval\\nmonasteries, churches and cathedrals. These would be linked with the growth of religious\\nideas and the religious orders which followed from them. Which were the ideas that led to\\nmainstream religions? Which were the ideas that led to peripheral sects? Which ideas led\\nto the development of significant groups, organizations, parties, political movements?\\nEarlier we outlined the development of objects in spatio-temporal terms. The history of\\nideas will need to be explored in spatio-temporal-socio-conceptual terms, each represented\\nby levels in the third-dimension, which can be translated back to two-dimensional lists\\nand descriptions as appropriate. \\n\\nSeeing Invisible Differences \\n\\n\\xa0\\xa0\\xa0 During the Renaissance the discovery of linear perspective brought\\nnew skill in visualising the physical world, but it began by illustrating episodes from\\nthe lives of saints, which none of the artists had witnessed personally. Hence it helped\\nsimultaneously in expanding the horizons of the visible world of nature and the invisible\\nworld of the mind. This dual development continues to our day. Three-dimensional\\nvisualisations, especially using virtual reality help to illustrate both the visible and\\ninvisible, and to introduce many new possibilities. \\n\\xa0\\xa0\\xa0 If, for instance we take the Library of Congress classification, as\\nabove, and link each layer in its hierarchy with a different layer, then we arrive at a\\ntruncated pyramidal shape beginning with twenty initial topics at the top and increasing\\nto many thousands as we descend. Say we are interested in total publications in the\\nLibrary of Congress. At the top level, these publications can be linked to each of the\\ntwenty basic fields, such that each major subject is represented proportionately as a\\nsquare or circle. We can thus see at a glance to what extent the number of books on\\nscience is greater than those in the fine arts. By going down a level in the hierarchy we\\ncan see how those figures break down, e.g. to what extent there are more books on physics\\nthan chemistry or conversely. At another level we can see whether and if so to what extent\\nastro-physics has more publications than bio-physics or quantum physics. We are thus able\\nto see patterns in knowledge which we could not see simply by looking at the shelves,\\nalthough even shelves can give us some hint that one topic has more books than another. \\n\\xa0\\xa0\\xa0 A slightly more refined version would link this approach to book\\ncatalogues such that we can trace how these trends in publications change over time. From\\na global point of view we could witness the rise of the social sciences in the nineteenth\\ncentury. At a greater level of detail we could see the rise of psychology as a field. This\\nsame approach can also be applied to usage patterns as studied by scholars in reception\\ntheory. In future usage patterns by on-line readers will become important for scholars as\\nwell as those doing market studies. \\n\\xa0\\xa0\\xa0 In our quest to see significant patterns it will sometimes prove\\nuseful to have agents examine trends and draw our attention only to cases where there are\\nconsiderable changes, of say 10 or 20%. This will be another way to discover emerging\\nsubjects. This same methodology has a whole range of other applications including\\nmarketing, advertising, stock markets and even network management. Say, for example, that\\nwe want to monitor potential trouble spots on the system. Agent technologies measure usage\\nat every major node of the system in terms of a typical throughput and usage. When these\\nratios change significantly the system identifies where they occur, and introduces\\nstandard adjustment measures. If these fail, the system visualises relevant points in the\\nneighbourhood of the node such that operators can see remotely where the problem might lie\\nand take appropriate action. Hence, instead of trying to keep everything visible at all\\ntimes, the system only brings to our attention those areas where trouble could occur: an\\nelectronic equivalent of preventative medicine. Such strategies will no doubt be aided by\\nthe further development of sensory transducers whereby significant changes in heat within\\nthe system would also be rendered visible. Seeing the otherwise invisible is a key to\\nnavigating remotely through complex environments. \\n\\nComprehension and Prediction by Seeing Absence \\n\\n\\xa0\\xa0\\xa0 In the early days of the scientific revolution there was great\\nemphasis on the importance of inductive as opposed to deductive research, which entailed\\nan emphasis on experience, experiment, often on a trial and error basis. As scientists\\ngained a better understanding of the field to the extent that they were able to create\\nphysical and conceptual maps of their objects of study, it became possible to deduce what\\nthey had not yet observed. For example, from a careful observation of the motions of the\\nknown planets, astronomers were able to predict the location of Uranus and subsequently\\nother planets. In combination with induction, deduction regained its role as an important\\ningredient in science. The same proved true in chemistry. Once there was a periodic table,\\nchemists found that the known chemicals helped them to chart the positions of as yet\\nunknown compounds. Once we have a matrix we can see where there is activity and where\\nactivity is missing. By now, chemistry is expanding with enormous speed. It is estimated\\nthat every week over 14,000 new chemical combinations are discovered. As in the case of\\npilots flying at twice the speed of sound we need methods for abstraction from the day to\\nday details, new ways of seeing patterns. Access, searching and navigation are not just\\nabout seeing what we can find, but also about strategies such that we see the appropriate\\nsubsets at any given time.\\n\\xa0\\xa0\\xa0 Until a generation ago mainframe computers typically relied on punch\\ncards with holes. Each hole represented a specific configuration of a subject or field.\\nRods or wires were then used to determine which cards had the same fields. Early versions\\nof neural networks adopted a virtual version of the same principle by shining virtual\\nlights through configurations of holes. When the holes co-incided the subjects were the\\nsame. Database indexes effectively accomplish the same thing with one fundamental\\ndifference: we see the results but have no means of seeing the process. \\n\\xa0\\xa0\\xa0 To see a bigger picture we need to see how the tiny details fit into\\nthe larger categories of human endeavour so that we can discern larger patterns. Roget as\\nwe saw had six basic classes (figure 1). Dewey had ten: 0) generalities; 1) philosophy and\\nrelated disciplines; 2) religion; 3) social science; 4) language; 5) pure sciences; 6)\\ntechnology; 7) the arts; 8) literature; 9) general geography and history. The Library of\\nCongress has twenty such fundamental classes. Beneath these universal headings are many\\nlayers of subordinate categories hierarchically arranged. If we treat each of these layers\\nas a plane, and have a way of moving seamlessly from one plane to the next, then\\noperations performed at one level can be seen at various levels of abstraction. \\n\\xa0\\xa0\\xa0 Suppose, for example, that we have been searching for Renaissance\\npublications by Florentine authors. Moving up to the highest level we can see on which\\nfields they wrote: religion, science, art and so on. Moving back down a few levels we can\\nidentify which particular branches of science and art concerned them most. Going back to\\nthe top level we can also see that there were many topics which they did not discuss. The\\nRenaissance view was supposedly universal in its spirit. In practice, it often had\\ndistinct limitations. If we have access to multiple classification systems, then we can\\nsee how these patterns change as we look at them say, through the categories of Duke\\nAugust and Leibniz at Wolfenbüttel or through the categories of Ranganathan’s\\nsystem. These approaches become the more fascinating when we take a comparative approach.\\nHow did German Lutheran cities differ from Italian Catholic or Swiss Calvinist cities in\\nterms of their publications? How does religion influence the courses of study and\\nresearch? What different cultural trends are evident from a comparison of publications in\\nIndia, China, Turkey, Russia and Japan?\\nTable of Contents\\n9. Challenges\\n\\xa0\\xa0\\xa0 Most discussions of challenges today focus on input, capacity and\\ntransmission. How can the vast materials be scanned in as efficiently and quickly as\\npossible? How can we develop storage facilities capable of dealing with thousands of\\nexobytes of material? How can we develop bandwidth, which will be capable of dealing with\\nsuch vast quantities? These are hardware problems, which are being overcome and will soon\\ndwindle to everyday maintenance problems. In our view the deeper challenges lie elsewhere,\\nnamely, problems of translating verbal claims to visual viewpoints, questions of advanced\\nnavigation in terms of scale and problems of cultural filters.\\n\\nPictures and Words\\n\\n\\xa0\\xa0\\xa0 The quest to develop systematic ways of comparing objective\\ndimensions with different subjective views is important and potentially very useful for\\ncultural interfaces. However, the integration of verbal subjects and objects into a visual\\nscheme, may be more problematic than it at first appears due to fundamental differences\\nbetween words and pictures. \\n\\xa0\\xa0\\xa0 There is a long tradition of comparisons between pictures and words.\\nAlready in Antiquity, Horace made comparisons between the pictures of painting and the\\nwords of poetry. In our century, famous art historians such as my mentor, Sir Ernst\\nGombrich, began by assuming that pictures and words were effectively interchangeable. His\\nfamous Art and Illusion began as a series of Mellon lectures entitled the\\nVisibile World and the Language of Art. He gradually reached the conclusion that there\\nwere very significant differences between the two. \\n\\xa0\\xa0\\xa0 One of the fundamental differences between pictures and words is\\nthat pictures can use space systematically in a way that words cannot. Pictures are\\npotentially perspectival, words are not. Gombrich attempted to express this through his\\ndistinction between the \"What and the How\" and in his essay on the \"Visual\\nImage.\" Pictures can show what happened and precisely how it happened. Words can only\\nconvey what happened, e.g. that a given person was shot from a particular position, not\\nall the details of how it happened. \\n\\xa0\\xa0\\xa0 Notwithstanding, this fundamental difference between pictures and\\nwords, there have been surprising parallels between the growth in depicting stories in\\npictures and the quest to tell them in words. The rise of narrative in painting and\\nliterature are connected. Attempts to show pictures from a specific point of view and\\nefforts to tell stories from a given \"viewpoint\" in the sense of first person\\nnarrative also seem to be connected. \\n\\xa0\\xa0\\xa0 Metaphorically perspectives of pictures are closely linked with\\nthose of words. Luther, referring to his dogmatic position, wrote, \"Here I\\nstand\". The great philosopher Kant, wrote an essay on \"standpoint\" as a\\nfundamental philosophical act. Today, typically, we speak of literary viewpoints and even\\nliterary perspectives. We even speak of seeing a person’s point of view after\\nlistening to their story. It is essential, however, to remember, that all these are\\nmetaphorical acts rather than literal ones. Herein lies a key to understanding why is easy\\nto speak verbally of seeing another person’s viewpoint, but almost impossible to\\ndepict this verbal viewpoint pictorially. We may speak of another’s space, entering\\ninto, sharing their space, but this is hardly the same as actually seeing or depicting the\\nworld as they see it.\\n\\xa0\\xa0\\xa0 Words are about universals. The noun, dog, refers, to all dogs.\\nPictures are about particulars. One may attempt to depict all dogs, but if the picture is\\nprecise, it shows a given dog such as the neighbour’s three-month old, brown pet,\\nrather than some abstract, universal concept of dogginess. \\n\\xa0\\xa0\\xa0 For this reason, the moment we try literally to represent\\npictorially a metaphorical verbal position or viewpoint, we encounter enormous\\ndifficulties. The verbal description is almost always much less precise than a visual\\ndepiction and therefore open to a whole series of alternative possibilities. This does not\\nnecessarily mean that the quest is futile. One solution would be the direct brain\\ninterfaces, which are being explored by scientists today (cf. above p. 8*). Until these\\nbecome available an interim solution is to create alternative reconstructions, from which\\nthe author of a position can choose in deciding which is an accurate visual translation of\\ntheir verbal description. \\n\\xa0\\xa0\\xa0 As noted earlier, in terms of virtual museums, Infobyte has already\\ndeveloped a Virtual Exhibitor software, which allows museum directors and curators to\\nexplore a series of hypothetical arrangements of paintings in designing the layout for\\ntheir regular museum and for special exhibitions. In terms of verbal viewpoints, perhaps\\nwe need a variant of this software, a type of Virtual Verbal Viewpoint Exhibitor, to help\\nbridge the gap between metaphorical and literal sharing of viewpoints. It is quite\\npossible, of course, that we shall, on closer reflection, conclude that there are profound\\nreasons for keeping these viewpoints metaphorical and not translating them into\\npotentially banal literal versions. Or it may well become a matter of choice, just as a\\nnumber of persons prefer to remain silent in difficult situations rather than spelling out\\nthe situation in boring detail. The ability not to use functionalities is both a\\nprerequisite of culture and one of its highest expressions.\\n\\nScale\\n\\n\\xa0\\xa0\\xa0 In the film Powers of Ten, viewers were taken from a person\\nlying on the beach upwards by tenfold scales to the universe and then back to the\\nmicrocosmic level. More recently, Donna Cox adapted this principle for the IMAX film, Cosmic\\nJourney. This film used both real photographs and computer simulations. A project at\\nthe Sandia Labs is creating a Dynamic Solar System in scale: \\n\\n\\n\\nThe scale model of the solar system covers a spatial range of 10 km with an individual\\n    positioning resolution of ~20km. It contains 73 objects, each with appropriate motion.\\n    Tethering or locking permits a viewer to attach to an object and travel with it,\\n    duplicating all or part (e.g., center of mass) of its inertial motion while retaining the\\n    ability to move independently. Here, tethering also triggers a search of available NASA\\n    data. Photographs and associated text information are displayed on the craft wall while\\n    tethered.\\n\\n\\n\\xa0\\xa0\\xa0 Recently, thinkers such as Ullmer, have speculated how one could use\\nsimilar principles of changing scales for navigation in extremely large data spaces.\\nProper contextualization of knowledge requires being able to move seamlessly between the\\nnano-structures of the atomic particles to the macro-structures of galaxies at the cosmic\\nlevel. Being able to do without getting \"lost in space\" is truly a challenge. \\n\\nCultural Filters\\n\\n\\xa0\\xa0\\xa0 Getting at the essential facts concerning objects of culture is a\\nworthy and important goal. More subtle and elusive are the challenges of interfaces\\nreflecting a variety of cultures: problems of learning to see things in different ways,\\nthrough the eyes of different cultures. This entails a whole range of challenges including\\nterms, languages, symbols, narratives, values and questions.\\nTerms\\n\\xa0\\xa0\\xa0 As noted earlier one of the great challenges in research lies in\\nfinding equivalent and/or related terms to the topic which interests us. Classification\\nsystems offer one method. Synonyms, antonyms and indicators offer another. Such terms vary\\nculturally and often do not lend themselves to a simple translation: a public house or pub\\nin English is quite different from a maison publique in French. Burro in\\nItalian is very different from burro in Spanish. We need new methods for mapping\\nsystematically between different classification systems to continue finding equivalent\\nterms when they are classed in very different places. \\nLanguages\\n\\xa0\\xa0\\xa0 Cultural filters can potentially provide translations from one\\nlanguage to another. At the most obvious level this could entail taking a virtual tour in\\nEnglish and translating it into French, German or some other language. In other cases, it\\nmight well be looking at a painting, which has a Latin or Chinese caption. Given\\nthe rapidly evolving field of optical character recognition, one could have a simple\\nvideo-camera attached to one’s notebook computer, point this camera at the caption in\\nquestion, which would relay the caption via the computer to an on-line databank, and\\nprovide a summary translation thereof. \\n\\xa0\\xa0\\xa0 Within a major language there are many levels of expression ranging\\nfrom formal, through informal, to dialect and slang. Cultural filters will eventually need\\nto provide translations in both directions. Sometimes, for instance, there will be an\\nexpression in dialect or slang for which one wants to have the formal equivalent, as when\\nDante or Shakespeare use colloquial terms which require explanation. At other times, a\\nparticularly formal turn of phrase by a Proust would need explication in a less formal\\nstyle. In traditional publications standard editions of a famous play or novel typically\\nrelegate such explanations to footnotes. In future, these can also be offered on demand\\neither as visual captions or as verbal commentaries. \\nSymbols \\n\\xa0\\xa0\\xa0 At the level of symbolism, cultural filters are more obviously\\nimportant. In Europe, white is symbol of purity, the spirit, and life. In China, white is\\ntypically a symbol of death and mourning. On the other hand, in Europe a white cala lily\\nis a symbol of death, whereas in other parts of the world it has a more joyous meaning. As\\na result an interface with colours designed for one culture, may well have unexpected\\neffects on persons form another culture. Having identified one’s culture, the\\ninterface should \"know\" the appropriate colours and adjust itself accordingly. \\n\\xa0\\xa0\\xa0 Colours are but one small aspect of very complex traditions of\\nsymbols. In Germany, the swastika is associated with all the horrors of Nazi fascism. In\\nthe Far East the swastika is sometimes a symbol of the sun or of the Buddha’s heart.\\nIn Chinese the swastika is a pun on the word ten thousand and the bat a pun on happiness.\\nHence a bat with a swastika dangling from its mouth means \"may you have happiness ten\\nthousand-fold.\" \\n\\xa0\\xa0\\xa0 As an extension of the quick reference provided by the digital\\nreference room, one would thus be able to choose a symbol and explore its meanings in\\ndifferent cultures. This assumes, of course, that one knows the name of the item in\\none’s own culture. Once again, given rapid developments in pattern recognition with\\nsoftware such as IBM’s Query by Image Content (QBIC), new solutions are likely to\\npresent themselves in the near future. Using a video camera attached to one’s\\nnotebook computer as described above, one would point the camera at the symbol in\\nquestion, which would relay it via the computer to an on-line databank. This would\\nidentify the object and provide the viewer with the multiple meanings thereof according to\\nvarious cultures. \\nNarratives\\n\\xa0\\xa0\\xa0 Often cultural and especially religious symbols entail much more\\nthan some isolated object. They typically entail narratives, stories, based on a sacred\\ntext (e.g. the Bible or the Mahabharata) or epic literature (e.g.\\nHomer’s Iliad or Dante’s Divine Comedy). Persons within an\\nestablished culture take these narratives for granted and frequently define themselves in\\nterms of familiarity with that corpus. Outsiders find these narratives confusing or\\nmeaningless. For example, a Catholic standing in front of a painting of Moses in the\\nDesert, unconsciously calls to mind the appropriate text in the Old Testament\\nor at least the gist of the event. Similarly, in viewing Christ Walking on Water\\nthey call to mind the appropriate New Testament passage. In seeing the Saint\\nSebastian they recall Jacobus de Voragine’s Golden Legend or some more\\nsimplified Lives of the Saints. To a non-Christian unfamiliar with these sources,\\nimages of a person walking on water, or of a man remaining calm while being pierced by\\nmultiple arrows, may well seem curious, confusing or simply incomprehensible. Similarly a\\nChristian unaware of Buddhist traditions will encounter incomprehension when they confront\\nTibetan Thankas, or Chinese scrolls. \\n\\xa0\\xa0\\xa0 The digital reference room serving as a cultural Baedeker will thus\\noffer tourists much more than a geographical map of sites and artifacts. It will provide\\naccess to the narratives underlying all those otherwise enigmatic titles of paintings,\\nsculptures, and dances such as Diana and Actaeon, Majnun and Leila, or Rama\\nand Krishna. This may, in turn, have fundamental implications for battles in other\\nareas of academia. In the context of deconstructionism and its various branches, for\\ninstance, there have been enormous debates concerning the viability or non-viability of\\nspeaking about a canon of literature. Whereas earlier generations were fully confident in\\ntheir ability to define the \"greats\" and \"classics\", many would argue\\nthat these lists have become so fluid that they are almost meaningless. In Canada, for\\ninstance, only a generation ago, the Bible and Shakespeare would have been seen as\\nfundamental titles in such a canon.\\n\\xa0\\xa0\\xa0 Today, a number of persons would argue that no single canon is\\npossible, that instead we need to speak of canons for black, feminist, queer and other\\nliterature, rather than a basic heritage shared by all civilized persons. For those who\\ndefine culture and civilization in terms of a common heritage, abandoning the idea of a\\nshared corpus, implies the loss of a shared heritage by means of which we feel at ease\\nwith one another. Meanwhile, others argue that a true corpus can no longer be\\nEuro-centric. It cannot be limited to Homer, Virgil, Dante, Shakespeare and Goethe. It\\nmust include the great literature of India, China, Persia, and other cultures. Here\\nanother problem looms. The corpus will become so large that no one will have time to\\nmaster it unless they make this their sole profession. \\n\\xa0\\xa0\\xa0 From all this it will again be apparent that the question of\\n\"viewpoints\" is much more complex than is generally imagined. Viewpoints are not\\njust about comparing abstractions. They are also about different bodies of knowledge,\\nwhich are an essential ingredient of culture. An Englishman sees the world through the\\nprisms of Shakespeare and Milton, an Italian through the verses of Dante, and a German\\nthrough the poetry of Goethe and Schiller. Each of these geniuses did more than create\\npoetry: they launched a heritage of associations which are shared by every cultured person\\nin that tradition, such that there is a manageable corpus of phrases that is mutually\\nrecognised by all members. In order better to comprehend these shared traditions in the\\ncase of cultures completely foreign to us, it may prove useful to develop agents familiar\\nwith all the standard literature of those cultures such that they can help us to recognise\\nquotes which are so familiar to natives that they are expressed as ordinary phrases, e.g. To\\nbe or not to be (Shakespeare) Every beginning is difficult (Goethe), One\\nmust live to eat, not eat to live (Molière), and yet evoke a wealth of associations\\nwhich the outside visitor could not usually expect.\\n\\xa0\\xa0\\xa0 Here, at the end, we can only touch upon this most elusive aspect of\\nnavigation, which is not only about what a culture says or writes. It is about what one\\nculture asks and another does not, about which one culture discusses and the other is\\nsilent, for which one culture has a hundred terms (snow among the Inuit) and of which\\nanother culture has no experience (a nomad in parts of the Sahara).\\nValues \\n\\xa0\\xa0\\xa0 More elusive than any of these are problems of cultural values.\\nAnthony Judge of the International Union of Organizations has drawn attention to nine Systems\\nof Categories Distinguishing Cultural Biases. Maruyama (1980), for instance,\\nidentifies four epistemological mindscapes. Geert Hoftede (1984), outlines four indices of\\nwork related values power distance, uncertainty avoidance, individualism, and masculinity.\\n\\xa0\\xa0\\xa0 Mushakoji (1978) focusses on four modalilities through which the\\nhuman mind grasps reality: affirmation, negation, affirmation and negation,\\nnon–affirmation and non-negation. Will McWhinney (1991) uses four modes of reality\\nconstruction: analytic, dialectic, axiotic and mythic. Pepper (1942) expresses four world\\nhypotheses: formism, mechanism, organicism, and contextualism. Mary Douglas (1973) employs\\nfour systems of natural symbols: body conceived as an organ of communication; body as a\\nvehicle of life, practical concern with possible uses of bodily rejects, life seen as\\nspiritual and body as irrelevant matter. Gardner (1984) relies on six forms of\\nintelligence:\\n\\xa0\\xa0\\xa0 linguistic, musical, logical/mathematical, spatial,\\nbodily-kinaesthetic, and personal. Jones (1961) uses seven axes of methodological bias:\\nOrder vs. disorder, static vs dynamic, continuity vs. discreteness, inner vs outer, sharp\\nfocus vs. soft focus, this world vs. other world, spontaneity vs. process. Meanwhile, Todd\\n(1983) identifies eight family types with different socio-political systems. A complete\\nanalysis of these systems would take us beyond the scope of the present essay. What\\ninterests us here is that each of the authors has chosen between four and eight concepts\\nin order to explain fundamental orientations in thought. The challenge is, how can these\\nalternative approaches be visualized in such a way that they can be integrated into the\\nsystem.\\nQuestions\\n\\xa0\\xa0\\xa0 The subtle aspects of culture extend not only to the kind of answers\\none gives but also to the questions one asks. In some older cultures it is not polite to\\nask what a person’s father does, the assumption being that if the person is properly\\nestablished that question would be redundant, and if they were not properly established\\nthen the question could lead to an embarassing result. A person from such a culture may\\nfeel they are being polite in not asking only to find themselves accused of disinterest in\\nanother culture. As usual these variations find various humorous expressions. It is said\\nthat the English always know with whom one sleeps but would never think to ask what one\\ndid, while the French will happily supply detailed descriptions of what they did without\\never asking with whom? Such pleasantries aside, there are always topics, which can be\\ndiscussed, questions, which can be raised in one culture, which are quite taboo in others.\\nIn Irish polite society one may find persons asking detailed questions about politics\\nincluding for whom one voted, questions which be considered indiscreet or completely taboo\\neven in some other parts of the Anglo-Saxon tradition. The Internet has drawn attention to\\nfrequently asked questions (FAQs). We need new means to examine how such questions vary\\nculturally and new interfaces, to help persons discover which questions should or should\\nnot be asked where. \\n\\xa0\\xa0\\xa0 Taking into account all these factors could readily leave us with a\\nfear of being overwhelmed . In the field of training, such a threat of being overwhelmed\\nreached critical proportions in the late 1960’s. By way of a solution, manuals and\\ntraining materials were put on-line and made accessible as and when they were needed,\\nunder the slogan of \"learning on demand.\" A cultural Baedeker as outlined above,\\nwould use technology to provide \"cultural learning on demand.\" Some may object\\nto the ideas of \"just in time culture\" as being uncivilized. However, if the\\nalternative is being uncivilized pure and simple, then surely this is the preferable way,\\nespecially if it can save us from undue feelings of inadequacy when faced by many\\ndifferent cultures as we travel around the world. This is not to say that we should\\nabandon our efforts to read the great literature in our culture and as many other cultures\\nas possible. Rather, we need to discover that although the world may be shrinking in terms\\nof physical access, its horizions continue to expand in keeping with our capacity.\\nTable of Contents\\n\\n10. Two, Three and Multiple Dimensions\\n\\n\\xa0\\xa0\\xa0 The above analysis suggest that the question of appropriate cultural\\ninterfaces is considerably more complex than might at first be apparent. It depends\\nlargely on function. In the case of virtual guides in physical museums, for instance,\\ntwo-dimensional lists will typically be appropriate. Such lists will also serve well in\\nthe case of research involving quick reference. In the case of virtual, historical\\nvirtual, and imaginary virtual museums and libraries, three-dimensional recontructions\\nwill usually be appropriate whether these are perspectival representations, or virtual\\nreality versions complete with walkthroughs. In moving from an image of a painting on a\\nwall to a record outlining the basic characteristics thereof, one will wish to move from a\\nthree-dimensional space back to a two-dimensional electronic equivalent of a file card,\\nwith an ability to return to the three-dimensional space as desired.\\n\\xa0\\xa0\\xa0 With respect to research involving maps, one will typically move\\nfrom two-dimensional maps as in the case of satellite images, to three-dimensional scenes\\nas one approaches images of the physical landscape. Conceptual research will frequently\\nbegin with two-dimensional lists of persons, subjects, or objects, some item of which is\\nthen shifted to a plane, thence to be treated in the third-dimension. Such analyses\\ntypically become four-dimensional when these planes are, in turn, subjected to temporal\\nvariations. Hence a cultural interface needs to move seamlessly into and out of a number\\nof dimensions. \\n\\xa0\\xa0\\xa0 Historically the advent of three-dimensional perspective did not\\nlead artists to abandon entirely two-dimensional (re-)presentations. There were many cases\\nsuch as cityscapes where three dimensions were very useful; others where two-dimensional\\nsolutions remained a viable and even preferable alternative. Text is an excellent example,\\nwhich helps explain why text-based advertisements remain predominantly two-dimensional. If\\nwe are searching for a simple name (Who?), subject (What?), place (Where?), event (When?),\\nprocess (How?) or explanation (Why?), two-dimensional lists are likely to remain the most\\neffective means for searching and access. As suggested earlier, long lists benefit from\\nalternative presentation modes such that they can be viewed alphabetically (Who?),\\nhierarchically in tree form (What?), geographically (Where?), and chronologically (When?)\\nif appropriate. A complex spatial interface may be technologically attractive. The\\nchallenge, however, lies in integration with historically relevant interfaces, in being\\nable to encompass earlier structuration methods rather than merely replace them with\\nunfamiliar ones.\\nTable of Contents\\n\\n11.\\xa0 Abstracts\\n\\xa0\\xa0\\xa0 The newspapers and popular media constantly draw attention to an\\nincredible rise in the amount of information especially through the Internet.\\nKnight-Ridder News, for instance, recently cited an IBM study claiming that only seven\\npercent of all corporate data is ever used. The amount of knowledge in great libraries and\\nmuseums that is regularly used is less. Some major collections have as much as 94% of\\ntheir holdings in storage. Thinkers such as Pierre Levy have written about these trends in\\nterms of a second flood, as if there were no hope ever again of comprehending the masses\\nof new information.\\n\\xa0\\xa0\\xa0 Part one of this study, written as an independent paper, addressed\\nhow this great influx of information could be mastered, suggesting that a key lay in using\\nthe long tradition of ordering knowledge found in the library world. That paper outlined a\\nSystem for Universal Media Searching (SUMS), focussed on the use of traditional\\ntwo-dimensional lists and outlined briefly the potentials of three-dimensional\\npresentation methods. It also outlined how such a system linked with a global digital\\nreference room could lead to a new System for Universal Multi-Media Access (SUMMA).\\n\\xa0\\xa0\\xa0 This paper surveys different systems for visualising knowledge and\\nemerging interface technologies such as three-dimensional spaces, voice-activated\\ndisplays, haptic controls and direct connections to the brain. Since such technologies are\\noften presented as solutions in search of an application, the main body of the paper\\nfocusses on functions and needs from a user’s viewpoint. Five basic functions are\\nidentified, namely, 1) virtual guides, 2) virtual museums, libraries and spatial\\nnavigation, 3) historical virtual museums, 4) imaginary museums and 5) various kinds of\\ncultural research. The role of metadata is addressed briefly. Particular attention is\\ngiven to the realms of research, where it is suggested that the new technologies will\\ntransform our concepts of knowledge. The implications for cultural interfaces of each\\nfunction is explored. The paper ends with a series of challenges.\\nTable of Contents\\n12. Conclusions\\n\\n\\xa0\\xa0\\xa0 This paper opened with a brief outline of taxonomies of\\ninformation visualization user interfaces by data type (cf. Appendix 1) and a survey of\\nemerging interface technologies, namely, voice activation, haptic force, mobile and\\nnomadic, video activation, direct brain control, brain implants, and alternative methods.\\nIt was claimed that while such technological solutions in search of applications are of\\nsome interest, a more thorough understanding of interface problems requires an analysis of\\nuser needs. The main body of the paper addressed this challenge with respect to culture.\\nAn outline was given of five basic functions relating to cultural interfaces, namely, 1)\\nvirtual guides, 2) virtual museums, libraries and spatial navigation, 3) historical\\nvirtual museums, 4) imaginary museums and 5) various kinds of cultural research. The\\nimplications of these functions for cultural interfaces were explored. \\n\\xa0\\xa0\\xa0 This led to a brief consideration of metadata and consideration how\\nthese new developments are transforming our concepts of knowledge. Knowledge objects will\\ninclude not only basic characteristics but also information about their quality and\\nveridity. The Platonic idea destroyed individual differences and thereby the notion of\\nuniqueness. The new concept of objects centres knowledge on the fundamental significance\\nof differences. The universal becomes a key to particular expression. Knowledge lies not\\nin how good a copy it is but rather in how well it has created a variation on the theme.\\nThis will transform the scope and horizons of knowledge. \\n\\xa0\\xa0\\xa0 The paper ended with an outline of further challenges such as\\nproblems of translating verbal claims into visual viewpoints, questions of scale and\\ncultural filters. It will be a long time before all these challenges are overcome. Yet if\\nwe recognise them clearly, there is no need to be overwhelmed by them. We must continue\\nthe process of sense making and ordering the world, which began with our first libraries\\nand schools and shall continue, we hope, forever. For in this lies our humanity.\\nTechnology may offer many solutions looking for an application. Nonetheless, cultural\\ninterfaces still pose many applications looking for solutions.\\nTable of Contents\\n\\nAcknowledgements \\n\\n\\xa0\\xa0\\xa0 I am very grateful to the Ontario Library Association for their\\nsupport in providing me with an office this past two years and for the honour of giving\\nthe Cummings Lecture (November 1996), which provided an initial stimulus for putting some\\nof these ideas to paper. Particular thanks go to those who kindly read the first chapter\\nand offered comments: Larry Moore, Keith Medley and Jeff Gilbert. \\n\\xa0\\xa0\\xa0 Professor Dr. Ingetraut Dahlberg who has generously encouraged the\\nauthor for the past fifteen years. I am deeply grateful for her gentle inspiration. I am\\nvery grateful also to Deans Wiebe Bijker (Maastricht) and Hans Koolmees (IDM, Maastrict)\\nfor provocative questions during the past year. I am grateful to Dr. Anthony Judge (UIA)\\nand Heiner Benking (Ulm) for challenging me to think more thoroughly about problems in\\nmoving from two-dimensional to three-dimensional navigation. Mr. Benking kindly read the\\nmanuscript and offered suggestions. The formal stimulus for this paper was a workshop and\\npanel at the Advanced Visual Interfaces Conference in Aquila (May 1998), for which I thank\\nProfessors Tiziana Catarci and Stefano Levialdi for their generous encouragement. \\n\\xa0\\xa0\\xa0 The larger framework for this paper has grown out of discussions\\nwith friends and colleagues such as Dr. Rolf Gerling, Dipl. Ing. Udo Jauernig, Eric Dobbs,\\nJohn Orme Mills, O.P., and Professor André Corboz and many years of experience at\\nresearch institutions including the Warburg Institute (London), the Wellcome Institute for\\nthe History of Medicine (London), the Herzog August Bibliothek (Wolfenbüttel), where Frau\\nDr. Sabine Solf played an essential role, the Getty Center for the History of Art and the\\nHumanities – now the Getty Research Institute (Santa Monica), the McLuhan Program in\\nCulture and Technology at the University of Toronto and more recently the Ontario Library\\nAssociation, where Larry Moore has been most encouraging. I am grateful to the individuals\\nat all of these centres. Finally I am grateful to members of my own team who have been\\nboth generous and supportive, notably, Rakesh Jethwa, Hugh Finnegan, John Bell, Elizabeth\\nLambden and John Volpe. The diagrams were produced by Hameed Amirzada. The appendices have\\ngrown out of work for Eric Livermore (Nortel, Advanced Networks) and Stuart McLeod (CEO,\\nBell MediaLinx), to whom I am deeply indebted for their loyal support and encouragement.\\n\\n\\xa0\\n\\xa0\\n\\xa0\\nAppendix 1. Taxonomy of Information Visualization User Interfaces by Data Type\\n\\nChris North, University of Maryland at College Park\\nDataType Title Institution/Author Links to pages, publications \\nTemporal (i.e. Timelines, histories) \\nLifeLines HCIL-Maryland Homepage \\nLifeStreams Yale Homepage, Company \\nMMVIS: Temporal \\nDynamic Queries U Michigan: Hibino Thesis \\n\\n\\n\\nPerspective Wall Xerox CHI91, (Information \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nVisualizer) \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nVideoStreamer MIT Homepage \\n\\xa0\\n1 Dimensional (i.e. Linear data, text, lists) \\nDocument Lens Xerox (see Info. Visualizer) \\nFractal Views UEC-Japan: Koike TOIS\\'95 \\nSeeSoft Lucent / Bell Labs HomePage, Brochure, \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIEEE Computer 4/96 \\n\\n\\n\\n\\nTilebars Xerox CHI\\'95, (see Information \\n\\n\\n\\nVisualizer) \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWebBook Xerox (see Info. Visualizer) \\n\\xa0\\n2 Dimensional (i.e. Planar data, images, maps, layouts) \\nArcView ESRI Homepage \\nFisheye/Distortion views Resource Page \\nGroupKit Calgary HomePage \\nInformation Mural GVU-GeorgiaTech Homepage \\nPad++ New Mexico Homepage \\nPowers of Ten Homepage \\n\\xa0\\n3 Dimensional (i.e. Volumetric data, 3D images, solid models) \\nThe Neighborhood Viewer Minnesota Homepage \\nVisible Human Explorer (VHE) HCIL-Maryland Homepage \\nVolvis SUNY-SB Homepage \\nVoxelman IMDM-Hamburg Homepage \\n\\xa0\\nMulti-Dimensional (i.e. Many attributes, relational, statistical) \\nFilter-Flow HCIL-Maryland Paper \\nDynamic Queries, Query\\nPreviews (HomeFinder,\\nFilmFinder, EOSDIS) HCIL-Maryland HomePage \\nInfluence/Attribute Explorer Imperial College HomePage \\nLinkWinds JPL-NASA HomePage \\nMagic Lens Xerox Homepage \\nParallel Coordinates Thesis \\nSelective Dynamic\\nManipulation (SDM) CMU Homepage \\nSpotfire IVEE Development Homepage \\nTable Lens Xerox CHI94, (Info.Visualizer) \\nVisage CMU Homepage \\nVisDB Munich Homepage \\nWorlds Within Worlds Feiner UIST90 \\nXGobi AT&T Labs, Bellcore Homepage \\nHierarchical (i.e. Trees) \\nCone/Cam-Trees Xerox CHI91,(Info. Visualizer) \\nElastic Windows HCIL-Maryland Homepage Report \\nFractal Views UEC-Japan: Koike VL\\'93 \\nHyperbolic Trees Xerox CHI95 \\nInfo Cube Sony Homepage \\nTreeBrowser\\n(Dynamic Queries) HCIL-Maryland Abstract \\nTreeMap / WinSurfer HCIL-Maryland Viz91, Homepage, \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWinsurfer, Widget \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWebSpace U Minnesota Homepage \\nNetwork (i.e. Graphs) \\nButterfly Citation Browser Xerox CHI\\'95,(Info. Visualizer) \\nFisheye Paper \\nGalaxy of News MIT Description \\nGraphic History Browser GVU-GaTech HomePage \\nIGD\\n(Interactive Graphical Documents) Columbia: Feiner Homepage\\nIntermedia Brown Homepage \\nMulti-Trees Furnas Homepage \\nNavigational View Builder GVU-Gatech HomePage, CHI\\'95 \\nNETMAP ALTA Analytics, Inc. Homepage \\nRMM Isakowitz Homepage \\nSemNet Bellcore Paper \\nThemescape / SPIRE PNL Homepage, Abstract \\nWorkSpaces\\nCASCADE Pittsburgh Paper \\nInformation Visualizer /\\n3D Rooms / Web Forager Xerox CG&A, DLib, Paper \\nPad++ New Mexico Homepage \\nPersonal Role Managers HCIL-Maryland Homepage \\n\\n\\xa0\\n\\nAppendix 2. Key individuals in Human Computer Interface (HCI) and Visualization \\nAhlberg, C. Chalmers U. of Technology Filmfinder\\nArents, Hans C. Katholieke U. Leuven Cube of Content\\nBardon, Didier IBM, Austin\\nBelew, Richard K. U. of California, San Diego \\nBenford, Steve Nottingham U. VR-VIBE \\nBenking, Heiner FAW, Ulm Cognitive Panorama \\nBoyle, John University of Aberdeen Amaze\\nBryson, Steve NASA, Ames Virtual Windtunnel\\nBulterman, Dick Vrije U., Amsterdam \\nBuxton, Bill Alias/Wavefront, Toronto \\nCard, Stuart Xerox PARC\\nCatarci, Tiziana Sapienza, Rome\\nChalmers, Matthew Ubilab, Zurich Bead-point cloud, Bead-landscape\\nCitrin, Wayne U. of Colorado, Boulder\\nColebourne, A. Lancaster University\\nCrouch Component Scale Display\\nCruz, Isabel Tufts\\nDix, Alan Staffordshire University\\nEick, Stephen K. Lucent Visual Insights \\nFaieta, Baldo Social Insect \\nFairchild, Kim M. Singapore National U.\\nFox, Edward A. Virginia Tech Envision\\nFowler, Richard H. Panamerican University Information Navigator\\nGarg, Ashim Brown University\\nGlinert, Ephraim P. University of Washington\\nGray, Peter M.D. Aberdeen University\\nGray, Philip University of Glasgow\\nGrudin, Jonathan U. of California, Irvine \\nHearst, Marti U. of California, Berkeley Cougar\\nHendley, Bob Birmingham University\\nHemmje, Matthias GMD, Darmstadt Lyberworld\\nHollan, James D. University of New Mexico Pad++\\nIngram, Rob Nottingham University\\nIoannidis, Yannis E.University of Wisconsin\\nJacob, Rob Tufts University\\nJohn, Bonnie E. Carnegie Mellon\\nJudge, J. N. UIA Insight Capture \\nKimoto, Haruo NTT\\nKeim, Daniel A. Munich U. (VisDB)\\nKling, Ulrich GMD Darmstadt\\nKorfhage, Robert U. of Pittsburgh (BIRD) \\nKrohn, Uwe Durham U.\\nKurlander, David Microsoft\\nLeviani, Stefano La Sapienza, Rome\\nLin, Xia University of Kentucky Reading Room HyperLibrary\\nLunzer, Aran Glasgow University Reconaissance\\nMariani, John Lancaster University TripleSpace, QPIT\\nMunzner, Tamara Stanford\\nMyers, Brad Carnegie Mellon\\nNuchprayoon Pittsburgh GUIDO\\nOlsen, Dan R. Carnegie Mellon VIBE\\nPeeters, E. Philips\\nRekimoto, Jun Sony Information Cube \\nRing, Mark GMD\\nRose, Daniel Apple Piles, AIR, SCALIR\\nShieber, Stuart M. Harvard U.\\nShneiderman, Ben University of Maryland\\nSnowdon, Dave Nottingham University VR Vibe\\nSpoerri, Anselm AT&T Info Crystal \\nStasko, John Georgia Tech\\nStrong, Gary NSF, Arlington\\nWalker, Graham BT Labs\\nWickens, Chris U. of Illinois, Urbana-C.\\nWilliamson Dynamic Home Finder\\nWittenburg, Kent Bellcore\\nZhang, Jiajie Ohio State DARE, TOFIR\\n\\xa0\\n\\nAppendix 3. Major Projects in Information Visualisation\\n\\nmostly additional to those discussed by Young (1996). \\n\\nEurope\\n\\nEuropean Community Joint Research Centre, Ispra (JRC)\\nInstitute for Systems, Informatics and Safety (ISIS)\\nAdvanced Techniques for Information Analysis \\nData Visualization Group (DVG) \\nEuropean Computer Industry Research Centre, Munich (ECRC)\\nCombination of Bull, ICL and Siemens\\nAdvanced Information Management Systems \\nDistributed Computing\\nUser Interaction and Visualisation Group\\n\\nCanada\\n\\nNational Research Council, Ottawa\\nInstitute for Information Technology (IIT)\\nAl Hladny hladny@iit.nrc.ca\\nTel. 613-993-3320\\nHuman Computer Interaction \\n\\n\\n\\nIntegrated Reasoning \\n\\n\\nInteraction with Modelled Environments \\nInteractive Information \\nSeamless Personal Information\\nVisual Information Technology \\n\\nGermany\\n\\nFraunhofer Gesellschaft \\nInstitut für Graphische Datenverarbeitung, Darmstadt (IGD)\\nDocument Computing\\nMultimedia Electronic Documents (MEDoc) \\nIntelligent Online Services\\nMultimedia Extension (MME)\\nMobile Information Visualization \\nActive Multimedia Mail (Active M3)\\nLocation Information Services (LOCI)\\nVisual Computing \\n\\n\\n\\nAugmented Reality\\n\\n\\nVirtual Table \\nAbteilung Visualisierung und Virtuelle Realität, Munich\\nGudrun Klinker \\nData Visualisation\\nProfessional Television Weather Presentation (TriVis)\\nGesellschaft für Mathematik und Datenverarbeitung (GMD)\\n(IPSI)\\nCo-operative Retrieval Interface\\nbased on Natural Language Acts (CORINNA)\\n\\nJapan\\n\\nNara Institute of Science and Technology (NAIST)\\nImage Processing Lab\\nImage Recognition\\nImage Sensing\\nInformation Archaeology\\nRestoration of Relics using VR\\nSony Computer Science Laboratory, Tokyo\\nJan Rekimoto\\nKatashi Nagao and Jun Rekimoto, Agent Augmented\\nReality: A Software Agent Meets the Real World\\nTrans-Vision Collaboration Augmented Reality Testbed\\nAugmented Interaction\\nNavicam\\nComputer Augmented Bookshelf\\nVirtual Society Information Booth \\nUniversity of Electro-Communications, Chofu, Tokyo\\nSchool of Information Systems \\nInformation Visualization Lab\\nHideki Koike\\nBottom Up Project Visualization\\nEnhanced Desk\\nFractal Views\\nFractal Approaches to Visualizing Hugh Hierarchies\\nVogue\\nUniversity of Tokyo\\nDepartment of Information and Communication Engineering\\nHarashima and Kaneko Laboratory\\nProfessor Hiroshi Harashima and Masahide Kanedo\\nTakeshi Naemura Grad. Student \\nCyber Mirage Virtual Mall with Photo-realistic Product Display\\nIntegrated 3-D Picture Communication (3DTV)\\nInteractive Kansei, Face Impression Analysis\\nIntelligent Hyper Media Processing\\nMulti-Modal Advanced Human-Like Agent\\nInformation Filter\\nDepartment of Mechano-Informatics\\nHirose Lab\\nProfessor Michitaka Hirose\\nHaptic Display\\nImage Based Rendering \\n\\n\\n\\n\\n\\nImmersive Multiscreen Display\\n\\n\\n\\n\\n\\nPortugal\\n\\nUniversity of Lisbon\\nVirtual Reality Lab\\nScientific Visualization\\nSensory Ecosystems\\nSpatial Information Systems\\n\\nUnited Kingdom \\n\\nCambridge University\\nRainbow Graphics Group\\nActive Badges \\nAnimated Paper Objects\\nAutostereoscopic Display\\nMobile Computing\\nMultiresolution Terrain Modeling \\nNet White Board\\nVideo User Interfaces\\nLoughborough University\\nTelecommunications and Computer Human Interaction Research Centre (LUTCHI)\\nAdvanced Decision Environment for Process Tools (ADEPT)\\nAgent Based Systems\\nDevelopment of a European Service for Information (DESIRE)\\non Research and Education\\nDigital Audio Broadcasting and GSM (DAB) \\nFocussed Investigation of Document Delivery Option (FIDDO)\\nIntelligent User Interfaces\\nMulti-Layered Knowledge Based Interfaces for Professional Users (MULTIK)\\nMultimedia Environment for Mobiles (MEMO)\\nResource Organisation and Discovery in Subject Based Services (ROADS)\\nManchester Computing Centre\\nInfocities\\nG-MING Applications\\nJanus Visualisation Gateway Project \\nKnowledge Based Interface for National Data Sets (KINDS)\\nParallel MLn Project\\nSuper Journal with Joint Information Systems Committee (JISC)\\nManchester Visualization Centre\\nUniversity of Huddersfield\\nHCI Research Centre\\nXerox Europe, Cambridge\\nContext Based Information Systems (CBIS)\\n\\nUnited States\\n\\nGeorgia Institute of Technology (Georgia Tech)\\nGraphics Visualization and Usability Center \\n\\n\\n\\nVirtual Environments\\nInformation Mural\\n\\n\\nSchool of Civil and Environmental Engineering\\nInteractive Visualizer Project (IV)\\nScientific Visualization Lab \\nInformation Visualization\\nQuiang Alex Zhao\\nIBM \\nVisualization Space: Natural Interface\\nMarc Lucente\\nVisualization Data Explorer\\nL3 Interactive Inc., Santa Monica\\nNet Cubes\\nLucent Technologies\\nVisual Insights\\nStephen K. Eick eick@research.bell-labs.com\\n\\nLive Web Stationery\\nMassachussets Institute of Technology (MIT) \\nVisible Language Workshop\\nFounded by Muriel Cooper\\nStudent: David Small\\nNASA, Ames\\nScientific Visualization\\nMITRE \\nCollaborative Virtual Workspace\\nData Mining\\nInformation Warfare\\nNahum Gershon\\nOrbit Interaction\\nPalo Alto\\nJim Leftwich\\nInfospace: A Conceptual Method for Interacting \\nwith Information in a 3-D Virtual Environment\\nPacific Northwest National Laboratory, Richland, Washington\\nAuditory Display of Information\\nAutomated Metadata Support for Heterogeneous Information Systems\\nJames C. Brown\\nSpatial Paradigm for Information Retrieval and Explanation (SPIRE)\\ncf. Themescape \\n\\n\\n\\n(Irene) Renie McVeety \\n\\n\\nStarlight\\nText Data Visualisation Techniques\\nJohn Risch\\nRutgers University\\nCenter for Computer Aids for Industrial Productivity (CAIP) \\nGrigore Burdea, Director\\nMultimedia Information System Laboratory \\nAdaptive Voice \\nMultimodal User Interaction\\nSandia National Laboratories, Albuquerque, New Mexico; Livermore, California \\nEnterprise Engineering Viewing Environment (EVE) \\nLaser Engineered Net Shaping (LENS)\\nSynthetic Environement Lab (SEL)\\nData Analysis\\nData Fusion\\nManufacturing \\nMedical\\nModelling\\nSimulation\\nAdvanced Data Visualization and Exploration\\nEIGEN-VR\\nSilicon Graphics Incorporated, Mountain View (SGI) \\nFile System Navigator (FSN)\\nVisual and Analytical Data Mining\\nRonny Kohavi\\nUniversity of Illinois, Chicago (UIC)\\nElectronic Visualization Laboratory (EVL)\\n+ Interactive Computing Environements Lab (ICEL)=NICE\\n4D Math\\nCave Applications\\nCaterpillar: Distributed Virtual Reality\\nCAVE to CAVE communications\\nInformation Visualization, Pablo Project\\nBiomedical Visualization\\nUniversity of Illinois, Urbana Champaign (UIUC)\\nNational Center for Supercomputing Applications (NCSA)\\nDigital Information System Overview\\nElectronic Visualization Lab\\nCAVE Applications\\n\\n\\nDistributed Virtual Reality\\n\\n\\nVisualization and Virtual Environments \\nInformation Technology\\nVirtual Environments Group (VEG)\\nCave Automatic Virtual Environments (CAVE)\\nInfinity Wall (I-Wall)\\nImmersaDesk (I-Desk)\\nRenaissance Experimental Lab (REL)\\nVirtual and Interactive Computing Environments (VICE)\\nBeckman Institute Visualization Facility\\nVirtual Reality Lab\\nWorld Wide Laboratory \\nLaser Scanning Cofocal Microscopes (LSCM)\\nMagnetic Resonance Imaging (MRI)\\nChickscope\\nScanning Tunneling Microscope (STM)\\nTransmission Electron Microscope (TEM)\\nUniversity of Pittsburgh\\nDepartment of Information Science and Telecommunications, Pittsburgh\\nMichael Spring\\nMultilevel Navigation of a Document Space\\nDocuverse\\nLandmarks\\nMural\\nTilebar\\nWebview\\nUniversity of Texas-Pan American\\nDocument Explorer\\nInformation Navigator\\nSemantic Space View \\nXerox Parc\\nCone Tree \\nDocument Lens\\nInformation Visualiser\\nPerspective Wall\\nTable Lens\\n\\nFigure 13. Illustration relating to IBM’s (Almaden Laboratories) 3-D Visualization\\nof Three Item Rules.\\n\\xa0\\nNotes\\nSee Michael Kesterton, \"Social Studies,\" Globe and Mail,\\nToronto, 19 February 1998, p. A26. \\n\\xa0\\nSee the author’s \"Frontiers in Conceptual Navigation,\" Knowledge\\nOrganization, Würzburg, vol. 24, 1998, n. 4, pp. 225-245. For another approach to\\nthese issues see Don Foresta, Alain Mergier, Bernhard Serexhe, The New Space in\\nCommunication, the Interface with Culture and Artistic Creativity, Council of Europe:\\nStrasbourg, 1995 \\n\\xa0\\nBen Shneiderman, Designing the User Interface. Strategies for Effective Human\\nComputer Interaction, Reading Ma.: Addison Wesley, 1997 (first edition 1987) 3rd ed.\\n1997. For a standard introduction to some of the more philosophical questions attending\\ninterface design see the three books by Edward Tufte, Visual Display of Quantitative\\nInformation, Cheshire: Graphics Press, 1982; Envisioning Information, Cheshire:\\nGraphics Press, 1990 and Visual Explanations, Cheshire: Graphics Press, 1997. \\n\\xa0\\nSteven Johnson, Interface Culture: How New Technology Transforms the Way We\\nCreate and Communicate, San Francisco: Harper, 1998. A book by Richard Saul Wurman, Information\\nArchitects, New York: Graphis Inc., 1997 provides a stimulating survey of some popular\\ntechniques but offers little insight into developments at the research level.\\n\\xa0\\nE.g. IEEE, Technical Committee on Computer Graphics (TCCG), which publishes\\nTransactions on Visualisation and Computer Graphics (TVCG). \\nSee http://www.cs.sunysb.edu/~tvcg/.\\n\\xa0\\nAnnual or Bi-Annual Conferences \\nAssociation for Computing Machinery (ACM)\\nComputer Human Interface (CHI)\\n(INTERCHI)\\nVisualization 1998 \\nResearch Triangle Park, 18-23 October 1998 \\nIncludes IEEE Information Visualization 19-20 October 1998\\nEC ESPRIT Programme\\nFoundations of Visualisation and Multi-Modal Interfaces \\n1) Comprehensive Human Animation Resource Model (CHARM) \\n2) Foundations of Advanced Three Dimensional \\nInformation Visualisation (FADIVA) \\n3) Framework for Immersive Virtual Environments (FIVE)\\n4) Reconstruction of Reconstruction of Reality \\nfor Image Sequences (REALISE) \\nVisual Information Retrieval Interfaces (VIRI)\\nWorkshop on Advanced Visual Interfaces (AVI)\\nAquila 24-27 May 1998\\nGubbio 1996\\nBari 1994\\nRoma 1992\\nSee http://informatik.uni-trier.de/~ley/db/conf/avi/index.html\\nFoundations of Advanced Three Dimensional Information \\nVisualization Applications (FADIVA)\\nGlasgow 1996\\nGraph Drawing\\nSee: http://gd98.cs.mcgill.ca\\nMontreal 1998\\nRome 1997\\nBerkeley 1996\\nPassau 1995\\nPrinceton 1994\\nParis 1993\\n\\xa0\\nOther Significant Past Conferences in the Field:\\n1993\\nIEEE Symposium on Visual Languages \\n1994\\nInfoVis Symposium on User Interface and Technology\\nRelated to the field of information visualization is the emerging field of diagrammatic\\nreasoning:\\nSee: http://www.hcrc.ed.ac.uk/gal/Diagrams/research.html\\n\\n\\xa0\\nSee: http://www.geog.ucl.ac.uk/casa/martin/atlas/atlas.html\\n\\n\\xa0\\nDurham University, Computer Science Technical Report 12/96. \\nSee: http://www.dur.ac.uk/~dcs3py/pages/work/Documents/lit-survey/IV-Survey/index.html\\n\\xa0\\nSee http://rvprl.cs.uml.edu/shootout/viz/vizsem/3dinfoviz.htm\\n\\xa0\\nSee http://www-graphics.stanford.edu/courses/cs348c-96-fall/infovis1/slides/walk005.html\\n\\xa0\\nSee http://www-graphics.stanford.edu/courses/cs348c-96-fall/scivis/slides/\\nMark Levoy provides a summary of two taxonomies based on visual metaphors. The first is\\nby Jacques Bertin, Sémiologie graphique: les diagrammes, les réseaux, les cartes,\\navec la collaboration de Marc Barbut [et al.], Paris, Mouton, [1973, c1967]. Jacques\\nBertin, Semiology of graphics, translated by William J. Berg, Madison, Wis. :\\nUniversity of Wisconsin Press, 1983. This system is based on: \\n\\xa0\\nImposition \\n- Diagrams x -Arrangement \\n- Networks -Rectilinear\\n- Maps -Circular\\n- Symbols -Orthogonal \\n- Polar\\n\\xa0\\n\\xa0\\n\\xa0\\nRetinal Variables\\n- Size x - Association\\n- Value - Selection\\n-Texture - Order \\n- Colour - Quantity\\n- Orientation\\n- Shape\\nThe second taxonomy is from Peter R. Keller and Mary M. Keller, Visual cues :\\npractical data visualization, Los Alamitos, CA : IEEE Computer Society Press ;\\nPiscataway, NJ : IEEE Press, c1993. This system is based on:\\n\\xa0\\nActions x Data \\nIdentify Scalar\\nLocate Nominal \\nDistinguish Direction\\nCategorise Shape\\nCluster Position\\nRank Region\\nCompare Structure\\nAssociate\\nCorrelate\\nMark Levoy also distinguishes four taxonomies by data type:\\n- number of independent variables (domain)\\n- number of independent variables (range)\\n- discrete vs. continuous domain\\n- binary vs. multivalued vs. continuous range.\\nHis additonal bibliography includes: \\n1977 John Wilder Tukey, Exploratory data analysis, Reading, Mass. :\\nAddison-Wesley \\nPub. Co.\\n1992 Harry Robin, The scientific image : from cave to computer, historical\\nforeword by \\nDaniel J. Kevles, New York : H.N. Abrams.\\n1995 Computer visualization : graphics techniques for scientific and engineering \\nanalysis, edited by Richard S. Gallagher, Boca Raton : CRC Press.\\nA standard introduction to problems of visualisation is offered by the work of Edward\\nTufte, \\n\\xa0\\n\\xa0\\nSee http://isx.com/~hci\\nhttp://web.cs.bgsu.edu/hcivil\\nhttp://www.logikos.com/sef.htm\\nProtocols\\nInterface Definition Language (IDL)\\nSee http://www.cs.umbc.edu/~thurston/corbidl.htm\\nDynamic Invocation Interface (DII) \\nEnhanced Man Machine for Videotex and Multimedia (VEMMI)\\nSee http://www.mctel.fr\\n\\xa0\\nThere is of course an HCI virtual library\\nSee http://usableweb.com/hcivl/hciindex.html\\nHans de Graaf, (Technical University, Delft) has a valuable index \\nSee http://is.twi.tudelft.nl/hci \\nIsabel Cruz has made a useful collection of reports on Human Computer Interaction at \\nSee: http://www.cs.brown.edu/people/ifc/hci/finalind.html.\\nSee also: Human Computer Interface Virtual Library (HCI)\\nSee http://web.cs.bgsu.edu/hcivl/misc.html\\nhttp://www.nolan.com/~pnolan/resource/info.html\\nhttp://is.twi.tudelft.nl/hci/sources.html\\n\\nBanxia Decision Explorer\\nSee http://www.banxia.co.uk/banxia\\nDocument Visualization\\nSee http://www.psysch.uiuc.edu/docs/psych290/vincow_feb03.html\\nInformation Visualisation Resources\\nSee http://www.cs.man.ac.uk/~ngg/infovis_people.html\\nCf. http://graphics.stanford.edu/courses/cs348c-96fall/resources.html\\nInput Technologies\\nSee: http://www.dgp.toronto.edu/people/BillBuxton/InputSources.html\\nThree D imensional (3-D) User Interface Kit\\nSee http://www.cs.brown.edu/research/graphics/research/3d_toolkit/3d_toolkit.html\\nVisual Information Architecture (VIA)\\n\\xa0\\nSee http://design-paradigms.www.media.mit.edu/projects/design-\\nparadigms/improver-paradigms/via.html\\nVisualisation and Intelligent Interfaces Group\\nSee http://almond.srv.cs.cmu/edu/afs/cs/project/sage/mosaic/samples/sage/3d.html\\nPatrick J. Lynch, Annotated Bibliography of Graphical Design for the User Interface\\nSee http://www.uky.edu/~xlin/VIRIreadings.html\\nVisual Design for the User Interface\\nSee http://info.med.yale.edu/caim/publications/papers/guip1.html\\n\\xa0\\nNetwork Centric User Interfaces (NUI)\\nTom R. Halfhill, \"Good-Bye, GUI, Hello NUI,\" Byte, Lexington, vol. 22,\\nno. 7, \\nJuly 1997, pp. 60-72.\\nSee thalfhill@bix.com\\nApple\\nSee http://www.apple.com/\\nMac OS 8\\nRhapsody\\nIBM \\nSee http://www.internet.ibm.com/computers/networkstation/\\nNetwork Station\\nOS2/Warp 4\\nBluebird\\nLotus\\nSee http://kona.lotus.com\\nKona Desktop\\nMicrosoft\\nSee http://www.microsoft.com/backoffice/sbc_summary.htm#top\\nMemphis/Active Desktop\\nNetscape\\nSee http://www.netscape.com/comprod/tech_preview/idex.html\\nNetscape\\nOracle/NCI \\nSee http://www.nc.com\\nNC Desktop\\nSanta Cruz Operation See http://tarantella.sco.com/\\nTarantella Web Top\\nSun/Java Soft\\nSee http://www.javasoft.com\\nHot Java Views\\nTriTeal\\nSee http://www.softnc.triteal.com/\\nSoftNC \\nUlysses Telemedia \\nSee http://www.ulysses.net/\\nVCOS\\ncf. http://www.softlab.ece.ntua.gr/~brensham/Hci/hci.htm\\n\\n\\xa0\\nMassachussets Institute of Technology, (1995), Media Laboratory. Projects.\\nFebruary 1995, Cambridge, Mass.: MIT, 6.\\n\\xa0\\nA more interesting application is in the context of Collaborative Integrated\\nCommunications for Construction (CICC) available electronically\\nSee http://www.hhdc.bicc.com/people/dleevers/papers/cycleof.htm,\\nwhich envisages a cycle of cognition in which the landscape is but one of six elements,\\nnamely, map, landscape, room, table, theatre, home. The author of this system David\\nLeevers works with Heiner Benking through ASIS.\\nSee http://www.hhdc.bicc.com/people/dleevers/default.htm\\nFor an excellent summary of some of the major systems presently available see Peter\\nYoung (1997), Three Dimensional Information Visualisation available electronically. \\nSee http://rvprl.cs.uml.edu/shootout/viz/vizsem/3dinfoviz.htm.\\n\\xa0\\nSee: Rao, Ramana, Pedersen, Jan O., Hearst, Marti A., Mackinlay, Jock D.,\\nCard, Stuart K., Masinter, Larry, Halvorsen, Per-Kristian, Robertson, George C., (1995),\\nRich Interaction in the Digital Library, Communications of the ACM, New York,\\nApril, 38 (4), 29-39. Card, Stuart (1996), Visualizing Retrieved Information, IEEE\\nComputer Graphics and Applications. \\n\\xa0\\nKling, Ulrich (1994), \"Neue Werkzeuge zur Erstellung und\\nPräsentation von Lern und Unterrichtsmaterialien [New Tools for the Production and\\nPresentation of Learning and Instructional Materials],\" Learntec 93. Europäischer\\nKongress für Bildungstechnologie und betriebliche Bildung, ed. Beck, Uwe, Sommer,\\nWinfried, Berlin: Springer Verlag, 335-360. \\nSee http://www-cui.darmstadt.gmd.de/visit/Activities/Lyberworld.\\nThe GMD also organizes research on Foundations of Advanced Three Dimensional\\nInformation Visualization Applications (FADIVA) and Visual Information Retrieval\\nInterfaces (VIRI). \\nSee: http://www-cui.darmstadt.gmd.de/visit/Activities/Viri/visual.html. \\n\\xa0\\nIbid., 336-340. Cf. Streitz, N., Hannemann, J., Lemke, J. et al., (1992),\\nSEPIA: A Cooperative Hypermedia Authoring Environment, Proceedings of the ACM\\nConference on Hypertext, ECHT ’92, Milan, 11-22. \\n\\xa0\\nSee http://viu.eng.rpi.edu/IBMS.html\\n\\xa0\\nSee http://multimedia.pnl.gov:2080/showcase/\\n\\xa0\\nSee http://multimedia.pnl.gov/2080/showcase/pachelbel.cgi?it_content/spire.node\\n\\xa0\\nSee http://www.pnl.gov/news/1995/news95-07.htm\\n\\xa0\\nSee http://www.cs.cmu.edu/Groups/sage/sage.html\\n\\xa0\\nSee http://www.maya.com/visage\\n\\xa0\\nSee http://www.cs.cmu.edu/Groups/sage/sage.html\\n\\xa0\\nSee http://www.cs.sandia.gov/SEL/main.html\\n\\xa0\\nSee http://www.cs.sandia.gov/VIS/science.html\\n\\xa0\\nSee http://www.sandia.gov/eve/eve_toc.html\\n\\xa0\\nHe also refers to stylus devices: see digitizing tablets, lightpens, boards,\\ndesks and pads, touch screens and force feedback (\"haptic\") devices).\\n\\xa0\\nSee http://www.dgp.toronto.edu/people/BillBuxton/InputSources.html.\\nA less comprehensive list is provided by Rita Schlosser and Steve Kelly \\nSee http://ils.unc.edu/alternative/alternative.html\\nwho include glove data-input devices (VPL Data Glove, Exos Dextrous Hand Master, Mattel\\nPowerGlove, Other Types of Gloves) and eye-computer interaction and access issues. It\\nshould be noted that Alias/Wavefront is working on new three-dimensional input devices.\\n\\xa0\\nSee http://207.82.250.251/cgi-binstart\\n\\xa0\\nSee http://www.dragonsys.com/home.html\\ncf. http://www.gmsltd.com/voiceov.htm\\n\\xa0\\nSee Geoffrey Rowan, \"Computers that recognize your smile\", Globe and\\nMail, Toronto, 24 November 1997, p. B3\\n\\xa0\\nSee http://delite.darmstadt.gmd.de/delite/Projects/Corinna\\n\\xa0\\nThis includes Bolt, Beranek and Newman (BBN), Carnegie Mellon University (CMU),\\nthe Massachusetts Institute of Technology (MIT) and the former Stanford Research Institute\\n(SRI).\\n\\xa0\\nSee http://multimedia.pnl.gov:2080/showcase/\\n\\xa0\\nSee http://multimedia.pnl.gov:2080/showcase/pachelbel.cgi?it_content/auditory_display.node\\n\\xa0\\nSee http://www.al.wpafb.af.mil/cfb/biocomm.htm.\\n\\xa0\\nSee http://www.sarcos.com/Jacobsen.html\\n\\xa0\\nSee http://www.hitl.washington.edu/scivw/EVE/I.C.ForceTactile.html\\n\\xa0\\nSee Grigore Burdea, Virtual Reality and Force Feedback, New York: John\\nWiley & Sons, 1996. Cf. Grigore Burdea, Philippe Coiffet, Virtual Reality\\nTechnology, New York: John Wiley & Sons, 1994.\\n\\xa0\\nThis includes data input devices.\\n\\xa0\\nThese include IBM, Apple, Netscape, Oracle, Sun, Nokia, Hitachi, Fujitsu,\\nMitsubishi and Toshiba. Cf. ICO Global Communications at http://www.ico.com\\n\\xa0\\nSee http://www.igd.ghg.de/www/zgdv-mmvis/miv-projects_e.html#basic\\n\\xa0\\nSee http://www.ubiq.com/hypertext/weiser/IbiHome.html\\n\\xa0\\nAdaptive and User Modelling\\nAdaptive and Intelligent Systems Applications\\nSee http://www.kareltek.fi/opp/projects/index.html\\n\\nAdaptive Behavior Journal\\nAdaptive Environments\\nSee http://www.adapt.env.org\\nAdaptive Networks Laboratory (ANW)\\nSee http://www-anw.cs.umass.edu\\nAndrew G. Barto, Richard S. Sutton, Reinforcement Learning\\nUser Modeling Conference Chia Laguna\\nSee http://www.crs4.it/UM97/topics.index.html\\nKnowledge Systems Laboratory Stanford \\nAdaptive Intelligent Systems \\nSee http://www-ksl.stanford.edu/projects/BBI\\n\\xa0\\nSee http://www.lk.cs.ucla.edu\\n\\xa0\\nSee http://www.virtualvision.com\\n\\xa0\\nSee, for example, the work of Gregg Vanderheiden, Trace Center, Madison,\\nWisconsin. \\nSee http://trace.wisc.edu\\n\\xa0\\nRita Schlosser and Steve Kelly at http://ils.unc.edu/alternative/alternative.html\\nhave made a list which includes: Gaze Tracking, Human-Computer Interaction and the\\nVisually Impaired, Modelling and Mark Up Languages in Visual Aid.\\n\\xa0\\nInternationale Stiftung Neurobionik, Nordstadt Krankenhaus, Hannover. The\\ndirector of the project is Professor Dr. Madjid Samii. \\n\\xa0\\nFraunhofer Institut für Biomedizinische Technik (IBMT), D-66386, St. Ingbert\\n\\xa0\\nUniversität Tübingen, Reutlingen, Naturwisschaftlich-Medizinisches Institut\\n(NMI)\\n\\xa0\\nSee http://www3.osk.3web.ne.jp/~technosj/mctosE.htm\\nCf. Michael Kesterton, \"All in the mind?\", Globe and Mail, Toronto,\\nA14, 6 January 1998.\\n\\xa0\\nSee: Frank Beacham, \"Mental telepathy makes headway in cyberspace,\" Now,\\nToronto, 13-19 July 1997, pp. 20-21. \\n\\xa0\\nSee http://www.sciam.com/1096issue/1096lusted.html\\n\\xa0\\nSee http://www.af.mil/news/airman/0296/look.htm.\\nOther members of his team are Chris Gowan and David Pole. The section is headed by Dr. Don\\nMonk. There appears to be related work at the Crew Systems Ergonomics Information Analysis\\nCenter (CSERIAC) \\nSee http://cseriac.udri.udayton.edu.\\n\\xa0\\nSee http://www.harpercollins.co.uk/voyager/features/004/fut4.htm\\n\\xa0\\nSee http://www.premier-research.com/6chris_gallen.htm\\n\\xa0\\nI.e. Consiglio Nazionale delle Ricerche.\\n\\xa0\\nSee http://www.mic.atr.co.jp/~rieko/MetaMuseum.html\\n\\xa0\\nSee http://gn.www.media.mit.edu/groups/gn/projects/vlaptop/index.html\\n\\xa0\\nSee http://www.cs.columbia.edu/~feiner\\n\\xa0\\nRelated projects at Columbia University include Augmented Reality for\\nConstruction (ARC), Columbia Object-Oriented Testbed for Exploring Research in Interactive\\nEnvironments (COTERIE), Knowledge Based Virtual Presentation Systems (IMPROVISE),\\nKnowledge Based Augmented Reality for Maintenance Assistance (KARMA) \\nSee http://www.cs/columbia.edu/graphics/projects/karma/karma.html\\nand Windows on the World (formerly called Worlds within Worlds).\\n\\xa0\\nSee http://www.cs.columbia.edu/graphics/projects/archAnatomy/architecturalAnatomy.html\\n\\xa0\\nSee http://www.cc.columbia.edu/cu/gsapp/BT/RESEARCH/LOW/Models.html\\n\\xa0\\nSee http://www.igd.fhg.de/www/igd-a4/index.html.\\nThe Institute’s division on visualisation and virtual reality and (Abteilung\\nVisualisierung und Virtuelle Realität) works directly with the European Computer Research\\nCentre (ECRC, Munich). Cf. the Data Visualization work of Gudrun Klinker\\nSee http://www.ecrc.de/staff/gudrun.\\n\\xa0\\nSee http://www.csl.sony.co.jp/person/rekimoto/navi.html\\n\\xa0\\nSee also Katashi Nagao and Jun Rekimoto, \"Agent Augmented Reality: A\\nSoftware Agent Meets the Real World\" \\nSee http://www.csl.sony.co.jp/person/nagao/icmas96/outline.html;\\n\\nTrans-Vision Collaboration Augmented Reality Testbed \\nSee http://www.csl.sony.co.jp/person/rekimoto/transvision.html\\nVirtual Society Information Booth\\nSee http://www.csl.sony.co.jp/project/VS/index.html\\nand \\nHomepage of Jun Rekimoto\\nSee http://www.csl.sony.co.jp/projects/ar/ref.html\\n\\xa0\\nNamed after a type of old-fashioned Japanese drama.\\n\\xa0\\nSee http://dynamicdiagrams.com/siteviews.htm\\n\\xa0\\nSee http://www.almaden.ibm.com/vis/vis.lab.html\\n\\xa0\\nThe Uffizi already has available more complex versions of 30-40 MB per room.\\nIndeed, the Uffizi is scanning in their entire collection of 1300 paintings at\\napproximately 1.4 gigabytes per square meter. Assuming that the average painting is\\nslightly larger than a square meter this means that their collection will require 2.6\\nterabytes. The National Gallery in Washington is scanning images at a much lower\\nresolution of c.30 MB per painting, but with a much larger collection of 105,000 images\\nthis will still result in some 3.15 terabytes. While it is frequently assumed that only\\nexperts will want images at such high resolutions, today’s desktop PCs are not yet\\nequipped to deal with millions of paintings on-line. \\n\\xa0\\nIMAX is exploring the possibility of delivering their images on-line. This\\nwill require approximately 80 GB/second, which seems astronomical at the moment but in\\nlight of recent demonstrations at the terabyte level is rapidly becoming feasible.\\n\\xa0\\nUsing GOTO technology.\\n\\xa0\\nOnce again there are problems of terminology. While virtual museum\\ntypically means an electronic reconstruction of the physical building, virtual library\\noften means a bibliography on given subjects while digital library is frequently\\nused for electronic versions of contents of books.\\n\\xa0\\nAs Michael Ester (formerly Getty Art History Information Program, now Getty\\nInformation Institute) has shown, books involve reflected light and allow the eye to see\\nup to about 3,500 lines per inch. Computer screens, which shoot light directly into the\\neye, activate a different combination of rods and cones and only allow one to see about\\n2,000 lines per inch. This helps explain why proof-reading is so much more difficult on\\nscreen than it is on paper.\\n\\xa0\\nSir Ernst H. Gombrich, \"The Mirror and the Map, Theories of Pictorial\\nRepresentation\" in Philosophical Transactions of the Royal Society of London,\\nLondon, vol. 270, no. 903, 1975, pp. 119-149. \\n\\xa0\\nArcheologia, percorsi virtuali nelle civilta scomparse, Milan: Mondadori\\nEditore, 1996. This book has since been translated into French and English.\\n\\xa0\\nSee http://viswiz.gmd.de/VMSD/PAGES.en/index.html.\\nWorking in conjunction with Stanford University, the GMD has also been working on a\\nResponsive Workbench, which effectively transforms a traditional workbench surface into\\nthe equivalent of a monitor or computer screen showing images in virtual reality which can\\nbe viewed with the aid of stereoscopic glasses and manipulated interactively at a\\ndistance. Applications of such a workbench include TeleTeaching and Virtual Meeting This\\nbears comparison with the Virtual Workbench: \\nSee http://beast.cbmv.jhu.edu:8000/projects/workbench/workbench.shtml\\n\\nand Brainbench \\nSee http://beast.cbmv.jhu.edu:8000/projects/brainbench/brainbench.shtml\\n\\nbeing developed in the Virtual Environments Program at the Australian National\\nUniversity (Canberra) and the Virtual Table being produced by the Fraunhofer Gesellschaft\\n(Darmstadt).\\n\\xa0\\nThis is being developed in the context of the GMD’s VISIT project. Other\\nprojects of the GMD include the Digital Media Lab’s (DML) Fluid Dynamics\\nVisualisation in 3D (FluVis). \\n\\xa0\\nI am grateful to my colleague Professore Ivan Grossi for this information.\\nSee http://mosaic.cineca.it\\n\\xa0\\nSee http://hydra.perseus.tufts.edu\\n\\xa0\\nSee http://www.gfai.de/projekte/index.htm\\n\\xa0\\nSince the term virtual is used in so many ways, the French have tended to\\nadopt Malraux’s phrase and refer to all virtual museums as imaginary museums.\\n\\xa0\\nAmong those active in the realm of metadata are the following:\\nInternational Council of Scientific Unions, \\n- Committee on Data for Science and Technology \\nSee http://www.cisti.nrc.ca/programs/codata/\\n- United Nations Environment Programme (UNEP)\\nTowards the design for a Meta-Database for the Harmonization of Environmental \\nMeasurement,\" Report of the Expert Group Meeting, July 26-27, 1990,\\nNairobi: \\nUNEP, 1991, (GEMS Report Series no. 8). \\nHarmonization of Environmental Measurement Information System (HEMIS)\\nCf. Heiner Benking, Ulrich Kampffmeyer, \"Access and Assimilation: Pivotal \\nEnvironmental Information Challenges, GeoJournal, Dordrecht, 26.3/1992, \\npp.323-334.\\n- American Institute of Physics\\ncf. Heiner Benking and Ulrich Kampffmeyer, \"Harmonization of Environmental \\nMeta-Information with a Thesaurus Based Multi-Lingual and Multi-Medial \\nInformation System,\" Earth and Space Science Information Systems, ed.\\nArthur \\nZygielbaum, New York: American Institute of Physics, 1992, pp. 688-695. (AIP \\nConference Proceedings 283).\\n- Environmental Protection Agency (EPA) Scientific Metadata Standards Project\\nSee http://www.lbl.gov/~olken/epa.html#Related.WWW\\nRe: Metadata registries\\nSee http://www.lbl.gov/~olken/EPA/Workshop/recreadings.html\\nFor an attempt at a metadata taxonomy \\nSee http://www.lbl.gov/~olken/EPA/Workshop/taxonomy.html\\n\\xa0\\nA detailed survey of this important field will be the subject of a separate paper\\nfor the opening keynote of Euphorie Digital? Aspekte der Wissensvermittlung in Kunst,\\nKultur und Technologie, Heinz Nixdorf Museums Forum, Paderborn, September 1998.\\n\\xa0\\nHypertext Markup Language (HTML), as an interim solution, marked a departure from\\nthis method in that it conflated content with presentation. \\n\\xa0\\nVoice activation may be attractive at times but will frequently be impractical.\\nImagine the reading room of a library where everyone is speaking, or even a museum where\\neveryone is speaking to their computers.\\n\\xa0\\nThe Getty Research Institute’s Union List of Artists Names (ULAN) would be\\nanother example, although with only 100,000 names as opposed to the 328,000 of the AKL,\\nthe term \"union\" promises more than it delivers. \\n\\xa0\\nSee, for instance, the methods being developed by Lucent in their Live Web\\nStationery.\\nSee http://medusa.multimedia.bell-labs.com/LWS/.\\n\\n\\xa0\\nLibraries are relatively simple structures. In the case of more complex systems\\nsuch as the London Underground it is useful to move progressively from a two-dimensional\\nschematic simplification of the routes to a realistic three-dimensional rendering of the\\ncomplete system, station by station. In the context of telecommunications the so-called\\nphysical world becomes one of seven layers in the model of the International Standards\\nOrganisation (ISO). In such cases it is useful not only to treat each of the seven layers\\nseparately but also introduce visual layers to distinguish the granularity of diffferent\\nviews. In looking at the physical network, for example, we might begin with a global view\\nshowing only the main nodes for ATM switches. (Preliminary models for visualising the\\nMBone already exist (Munzner, Tamara, Hoffman, Eric, Claffy, K., Fenner, Bill, (1996),\\nVisualizing the Global Topology of the MBone, Proceeding of the 1996 IEEE Symposium on\\nInformation Visualization, San Francisco, October 28-29, 85-92 available\\nelectronically \\nSee http://www-graphics.stanford.edu/papers/bone).\\nA next layer might show lesser switches and so on such that we can move up and down a\\nhierarchy of detail, sometimes zooming in to see the configuration of an individual PC, at\\nother times looking only at the major station points. This is actually only an extension\\nof the spectrum linking Area Management/ Facilities Management (AM/FM) with Geographical\\nInformation Systems (GIS) mentioned earlier.\\n\\xa0\\nThese combinations were and remain successful because they were guided by culture\\nand taste. Combinations per se do not guarantee interesting results. If taste and\\nsensibility are lacking the results are merely hybrid versions of kitsch. So the\\ntechnology must not be seen as an answer in itself. It offers a magnificent tool, which\\nneeds to be used in combination with awareness of the uniqueness and value of local\\ntraditions.\\n\\xa0\\nAn exception is a university textbook, Atlas of Western Art History, ed.\\nJohn Steer, Anthony White, New York: Parchment Books, 1994, pp. 54-55. \\n\\xa0\\nThis problem is somewhat more complex than it at first appears. Many of the great\\ntemples are in ruins. There are conflicting interpretations about their exact dimensions\\nand appearances. Hence in this case interpretations about various ruins is more closely\\nlinked to our \"knowledge\" thereof than in the case of historical buildings which\\nare still intact. \\n\\xa0\\nFor a serious discussion of how the advent of printing changed the criteria for\\nknowledge see: Michael Giesecke, Der Buchdruck in der frühen Neuzeit. Eine historische\\nFallstudie über die Durchsetzung neuer Informations- und Kommunikationstechnologien,\\n(Frankfurt am Main: Suhrkamp, 1991).\\n\\xa0\\nJ. Perrault, \"Categories and Relators\", International Classification,\\nFrankfurt, vol. 21, no. 4, 1994, pp. 189-198, especially p. 195. The original list by\\nProfessor Nancy Williamson (Faculty of Information Studies, University of Toronto) lists\\nthese in a different order under the heading:\\nTypes of Associative Relationships\\n1. Whole-part\\n2. Field of study and object(s) studied\\n3. Process and agent or instrument of the process\\n4. Occupation and person in that occupation\\n5. Action and product of action\\n6. Action and its patient\\n7. Concepts and their properties\\n8. Concepts related to their origins\\n9. Concepts linked by causal dependence\\n10. A thing or action and its counter-agent\\n11. An action and a property associated with it\\n12. A concept and its opposite. \\n\\xa0\\nAnthony J. N. Judge, \"Envisaging the Art of Navigating conceptual\\nComplexity,\" International Classification, Frankfurt, vol. 22, n. 1, 1995, pp.\\n2-9. The same author was responsible for one of the very early publications on this theme,\\n\"Knowledge Representation in a Computer Supported Environment,\" International\\nClassification, Frankfurt, vol. 4, no. 2, 1977, pp. 76-80. The pioneering work of\\nAnthony Judge in the context of the Union Internationale des Associations is also\\navailable on-line:\\nSee http://www.uia.org\\nThis includes:\\nCoherent Organization of a Navigable Problem-Solution-Learning Space\\nSee http://www.uia.org/uiadocs/ithree2.htm\\nMetaphors as Transdisciplinary Vehicles for the Future\\nSee http://www.uia.org/uiadocs/transveh.htm\\nSacralization of Hyperlink Geometry\\nSee http://www.uia.org/uiadocs/hypgeos.htm\\nRepresentation, Comprehension and Communication of Sets: The Role of Number\\nSee http://www.uia.org/knowledg/numb0.htm\\nThe Future of Comprehension\\nSee http://www.org/uiadocs/compbasc.htm\\nDimensions of Comprehension Diversity\\nSee http://www.uia.org/uiadocs/compapl.htm\\nUsing Virtual Reality for Visualization\\nSee http://www.uia.org/uiademo/vrml/vrmldemo.htm\\nThe Territory Construed as a Map \\nSee http://www.uia.org/uiadocs/terrmap.htm\\n\\n\\xa0\\nPioneering in this field has been Eugen Wüster, Internationale\\nSprachnormierung in der Technik, Bouvier: Bonn, 1966. He distinguishes between generic\\n(logical), partitive (ontological), complementary (oppositions) and functional (syntactic)\\nrelations. For other studies see Wolfgang Dahlberg, Wissenstrukturen und Ordnungsmuster,\\nFrankfurt: Indexs Verlag, 1980 and Analogie in der Wissensrepräsentation. Case-Based\\nReasoning und räumliche Modelle, ed. Hans Czap, P. Jaenecke und P. Ohly, Frankfurt:\\nIndeks Verlage, 1996. \\n\\xa0\\nAny attempt at ontological structuring will inevitably inspire critics to\\nclaim that a slightly different arrangement would have been closer to the true hierarchy.\\nWhile such debates have their value, it is important to recognize that even if there is no\\ncomplete agreement about a final configuration, the conflicting versions can still\\ncontribute to new insights, by challenging us to look at trends from a more universal\\nlevel.\\n\\xa0\\nSee: http://www.uia.org/webints/aaintmat.htm.\\n\\xa0\\nSee: Benking, Heiner, (1997), \"Understanding and Sharing in a\\nCognitive Panorama.\" Culture of Peace and Intersymp 97. 9th\\nInternational Conference on Systems Research, Informatics and Cybernetics, August 18-23,\\nBaden-Baden, available electronically\\nSee: http://www3.informatik.uni-erlangen.de:1200/Staff/graham/benking/index.html.\\n\\nhttp://newciv.org/cob/members/benking/\\nOther articles by the same author include:\\nBenking, Heiner, (1992), \"Bridges and a Master Plan for Islands of Data in a\\nLabyrinth of Environmental and Economic Information,\" Materials and Environment.\\nDatabases and Definition Problems: Workshop M. and System Presentation. 13th\\nICSU-CODATA Conference in collaboration with the ICSU-Panel on World Data Centers,\\nBeijing, October 1992. \\nBenking, Heiner, \"Design Considerations for Spatial Metaphors- Reflections on the\\nEvolution of Viewpoint Transportation Systems,\" Workshop at the European Conference\\non Hypermedia Technology (ECHT 94), Spatial User Interface Metaphors in Hypermedia\\nSystems, September 1994, Edinburgh, 1994.\\nSee: http://www.lcc.gatech.edu/~dieberger/ECHT94.WS.Benking.html\\n\\nBenking, Heiner, (1998), \"Sharing and Changing Realities with Extra Degrees of\\nFreedom of Movement,\" Computation for Metaphors, Analogies and Agents,\\nAizu-Wakamatsu City, April 1998, University of Aizu (in press):\\nSee: http://www.ceptualinstitute.com/genre/benking/landscape.htm\\nCf. also the forthcoming Benking, Heiner and Rose, J. N., \"The House of Horizons\\nand Perspectives,\" ISSS Conference in cooperation with the International Society of\\nInterdisciplinary Studies, Atlanta, 19-24 July 1998. \\nBenking identifies six elements as part of his Panorama of Understanding, knowing and\\nnot knowing: bridges (Brücke), forest and ground (Wald und Flur), unknown\\nterritory (terra incognita), maps, filters and brokers; multimedia bridges and\\nintegration; viewable ensemble of the world of the senses (Anschauliches\\nSinnweltenensemble). \\n\\xa0\\nIf we look, for instance, at classifications of the Middle Ages there were\\nno categories for science (as we now know it) or psychology. What we call science was\\ntypically (natural) philosophy or was included under the rubric of the quadrivium\\n(arithmetic, geometry, music and astronomy). Psychology was often in literature such as\\nthe Roman de la Rose.\\nSee: http://www.hitl.washington.edu/.\\n\\xa0\\nSee http://viu.eng.rpi.edu/overview2.html\\nand http://viu.eng.rpi.edu/IBMS.html. \\n\\xa0\\nMartin, Steve, Clarke, Steve, Lehaney, Brian, (1996), \"Problem\\nSituation Resolution, and Technical, Practical and Emancipatory Aspects of Problem\\nStructuring Methods,\" PARM ’96, Practical Aspects of Knowledge Management,\\nFirst International Conference, Basel, 30-31 October 1996, 179-186. I am grateful to\\nHeiner Benking for this reference. \\n\\xa0\\nCf. Hans Robert Jauss, Ästhetische Erfahrung und literarische Hermeneutik,\\nMunich: W. Fink, 1977.\\n\\xa0\\nSee Rensselaer W. Lee, Ut pictura poesis. The Humanistic Theory of Painting,\\nNew York: W. W. Norman and Co., 1967.\\n\\xa0\\nSir Ernst Gombrich, \"The What and the How. Perspective Representation and\\nthe Phenomenal World,\" Logic and Art. Essays in Honor of Nelson Goodman, ed. R.\\nRudner and I. Scheffler, New York: Bobbs Merrill, 1972, pp. 129-149. \\n\\xa0\\nSir Ernst Gombrich, \"The Visual Image: Its Place in Communication,\" The\\nImage and The Eye, London: Phaidon, 1982, pp. 137-161. \\n\\xa0\\nSee http://www.cs.sandia.gov/SEL/Applications/saturn.html\\n\\xa0\\nBrygg Ullmer discusses Cellular Universe Multiscale Spatial Architecture (CUMSA)\\nCellular Spatial and Entity Class in Multiscale Spatial Architectures for Complex\\nInformation Spaces \\nSee http://ullmer.www.media.mit.edu/people/ullmer/papers/multiscale/node7.html.\\n\\n\\xa0\\nFor a further discussion of these problems see Veltman, Kim H., (1997), \"Why\\nCulture is Important [in a World of New Technologies],\" 28th Annual\\nConference: International Institute of Communications Conference, October 1997,\\nLondon: International Institute of Communications, 1997, 1-10. \\n\\xa0\\nAnthony J. N. Judge, \"Systems of Categories Distinguishing Cultural Biases\\nwith notes on facilitation in a multi-cultural environment\", Brussels: Union of\\nInternational Associations, n.d. [c.1992]. See also an important article by the same\\nauthor on \"Distinguishing Levels of Declarations of Principles,\" available on\\nline:\\nSee http://www.ceptualinstitute.com/genre/judge/level20.htm\\n\\n\\xa0\\nMagoreh Maruyama, \"Mindscapes, Social Patterns and Future Development of\\nScientific Types,\" Cybernetica, 1980, 23, 1, pp. 5-25.\\n\\xa0\\nGeert Hofstede, Culture’s Consequences: International Differences in Work\\nRelated Matters, London: Sage, 1984.\\n\\xa0\\nKinhide Mushakoji, Scientific Revolution and Interparadigmatic Dialogue,\\nTokyo: United Nations University, GPID Project, 1978.\\n\\xa0\\nWill McWhinney, Paths of Change: Strategic Choices for Organizations and\\nSociety, London: Sage, 1991. \\n\\xa0\\nS. Pepper, World Hypotheses: A Study in Evidence, Berkeley: University of\\nCalifornia Press, 1942.\\n\\xa0\\nMary Douglas, Natural Symbols: Explorations in Cosmology, London: Pelikan,\\n1973\\n\\xa0\\nHoward Gardner, Frames of Mind: The Theory of Multiple Intelligences,\\nLondon: Heinemann, 1984\\n\\xa0\\nW.T. Jones, The Romantic Syndrome: Toward a New Method in Cultural\\nAnthropology and the History of Ideas, The Hague: Martinus Nijhoff, 1961.\\n\\xa0\\nEmmanuel Todd, La Troisième Planète: structures familiales et systèmes\\nidéologiques, Paris, 1983.\\n\\xa0\\nSee http://www-cui.darmstadt.gmd.de/visit/People/hemmje/Activities/Virgilio/\\nCf. http://graphics.lcs.mit.edu/~becca/vie/prop.html\\n\\xa0\\nSee http://www.cs.umd.edu/users/north/infoviz.html\\nThis is being replaced by On-Line Library of Information Visualization Environments\\n(OLIVE) \\nSee http://www.otal.umd.edu/Olive/\\nwhich distinguishes between eight kinds of interfaces, namely, temporal, 1D, 2D, 3D,\\nMultiD, Tree, Network, and Workspace. \\nA similar list entitled Visual Information Interfaces was is available at the VIRI\\nsight maintained by the GMD:\\nSee http://www-cui.darmstadt.gmd.de/visit/People/hemmje/Viri/visual.html\\n\\xa0\\nSee http://www.cs.chalmers.se/~ahlberg/\\n\\xa0\\nCf. ahlberg@cs.chalmers.se\\n\\xa0\\nSee http://www.mtm.kuleuven.ac.be/~hca/index.index.eng.html\\n\\xa0\\nSee http://www.ibm.com/ibm/hci/guidelines/design/realthings/ch4cl.html\\n\\xa0\\nSee http://www-cse.ucsd.edu/~rik\\n\\xa0\\nSee http://www.crg.cs.nott.ac.uk/people/Steve.Benford\\n\\xa0\\nSee http://www.parc.xerox.com/istl/members/bier/\\n\\xa0\\nSee http://www.parc.xerox.com/istl/projects/MagicLenses/\\n\\xa0\\nSee http://www.biochem.abdn.ac.uk/~john/john.html\\n\\xa0\\ncf. http://www.biochem.abdn.uk/~john/vlq/vlq.html\\n\\xa0\\nSee http://science.nas.nasa.gov/~bryson/home.html\\n\\xa0\\nSee http://www.cwi.nl/~dcab\\n\\xa0\\nSee http://www.dgp.toronto.edu/people/BillBuxton/billbuxton.html\\n\\xa0\\nSee http://www.computer.org:80/pubs/cg%26a/report/g20063.htm\\n\\xa0\\nSee http://www.dis.uniroma1.it/AVI96/tchome.html\\n\\xa0\\nSee http://www.ubs.com/webclub/ubilab/staff/e_chalmers.htm\\n\\xa0\\nMatthew.Chalmers@ubs.com\\n\\xa0\\nTel. 41 1236 7504\\n\\xa0\\nSee http://www.ubs.com/cgi-bin/framer.pl?/webclub/ubilab/e-index.htm/Projects/hci.html\\n\\xa0\\nSee http://www.cs.unm.edu/~jon/dotplot/index.html\\n\\xa0\\nSee http://soglio.colorado.com\\n\\xa0\\nSee ftp.comp.lanc.ac.uk/pub/reports/1994/CSCW.13.94.ps.2\\n\\xa0\\nSee Crouch, D., & Korfhage, R. R. (1990). \"The Use of Visual\\nRepresentations in Information Retrieval Applications\". In T. Ichikawa, E. Jungert,\\n& R. R. Korfhage, (Eds.), Visual Languages and Applications, New York, Plenum\\nPress, 305-326.\\nDonald B. Crouch, \"The visual display of information in an information retrieval\\nenvironment,\"\\nin: SIGIR \\'86. Proceedings of 1986 ACM conference on Research and development in\\ninformation retrieval, pp. 58-67.\\n\\xa0\\nSee http://www.cs.brown.edu/people/ifc\\n\\xa0\\nSee http://www.lcc.gatech.edu/~dieberger/CSDL4_abstract.html\\n\\xa0\\ncf. andreas.dieberger@acm.org\\n\\xa0\\nSee http://www.lcc.gatech.edu/~dieberger/Proj_Vortex.html\\n\\xa0\\nSee http://www.soc.staffs.ac.uk/~cmtajd/online.html\\n\\xa0\\ncf. http://www.soc.staffs.ac.uk/~cmtajd/papers/version-PSE97\\n\\xa0\\nSee http://panda.iss.nus.sg://8000/kids/fair/\\n\\xa0\\nSee http://www.cc.gatech.edu/gvu/people/Faculty/James.D.Foley.htm\\n\\xa0\\ncf. james.foley@gvu.gatech.edu\\n\\xa0\\nSee http://fox.cs.vt.edu\\n\\xa0\\nSee http://www.cs.panam.edu/info_vis/info_nav.html\\n\\xa0\\nSee http://www.cs.brown.edu/people/ag/home.html\\n\\xa0\\nSee http://www.cs.rpi.edu/~glinert\\n\\xa0\\nSee http://www.csd.abdn.ac.uk/~pgray\\n\\xa0\\nSee http://www.dcs.gla.ac.uk/personal/personal/pdg\\n\\xa0\\nSee http://www.ics.uci.edu/~grudin\\n\\xa0\\nSee http://www.sims.berkeley.edu/~hearst\\n\\xa0\\nRe: Visualization of Complex Systems, see:\\nhttp://www.cs.bham.ac.uk/~nsd/Research/Papers/HCI/hci95.html\\n\\xa0\\nRe: Hyperspace: Web Browsing with Visualisation see:\\nhttp://www.cs.bham.ac.uk/~amw/hyperspace/www95\\n\\n\\xa0\\nSee http://www-cui.darmstadt.gmd.de/visit/People/hemmje\\n\\xa0\\nSee http://www.cs.unm.edu/~hollan/begin.html\\n\\xa0\\nSee http://www.crg.cs.nott.ac.uk/~rji\\n\\xa0\\nSee http://www.cs.wisc.edu/~pubs/faculty_info/ioannidis.html\\n\\xa0\\nSee http://www.eecs.tufts.edu/~jacob/\\n\\xa0\\nSee http://www.cs.cmu.edu/People/bej/\\n\\xa0\\nSee http://www.cs.umd.edu:80/projects/hcil/People/brianj/VisualizationResources/\\n\\xa0\\nSee kimoto@nttvdb.dq.isl.ntt.jp\\n\\xa0\\nSee http://www.dbs.informatik.uni-muenchen.de/dbs/projekt/visdb/visdb.html\\n\\xa0\\ncf. http://www.dbs.informatik.uni-muenchen.de/dbs/mitarbeiter/keim.html\\n\\xa0\\nSee http://www.darmstadt.gmd.de/~kling\\n\\xa0\\nSee http://www.pitt.edu/~korfhage/korfhage.html\\n\\xa0\\nBIRD= Browsing Interface for the Retrieval of Documents\\n\\xa0\\nSee http://www.research.microsoft.com/research//ui/djk/default.htm\\n\\xa0\\nSee http://www.uky.edu/~xlin/publication.html\\n\\xa0\\nSee http://virtual.inesc.pt/rct.30.html\\n\\n\\xa0\\nSee http://wwwwksun2.wk.or.at:8000/0x811b0205_0x00d1119;skF50A50ED\\n\\xa0\\nSee http://www.comp.lancs.ac.uk/computing/users/jam/proj300.d/qpit.html\\n\\xa0\\nSee http://www.csri.utoronto.ca/~mendel/\\n\\xa0\\nSee http://www-graphics.stanford.edu/papers.edu/papers.webviz\\n\\xa0\\nSee http://www.cs.cmu.edu/~bam\\n\\xa0\\nSee http://asnst5@lis.pitt.edu\\n\\xa0\\nSee http://www.cs.cmu.edu/~dolsen\\n\\xa0\\nSee http://www.risoe.dk/sys-mem/cmi-web.htm\\n\\xa0\\nSee Pejtersen, Annelise Mark.The Bookhouse: Modeling User\\'s Needs and Search\\nStrategies, a Basis for System Design. Roskilde, Denmark: Riso National Laboratory,\\n1989.\\n\\xa0\\nSee http://www.csl.sony.co.jp/person/rekimoto/cube.html\\n\\xa0\\nSee rose@apple.com\\n\\xa0\\nSee http://cs-tr.cs.cornell.edu/TR/Search/?publisher=CORNELLCS&number=&boolean=and&author=Salton&title=&abstract=information+retrieval\\n\\xa0\\nSee http://www.eecs.harvard.edu/~shieber\\n\\xa0\\nSee http://www.cs.umd.edu/users/ben/index.html\\n\\xa0\\nSee http://www.crg.cs.nott.ac.uk/people/Dave.Snowdon\\n\\xa0\\ncf. http://www.crg.cs.nott.ac.uk/crg/Research/pits/pits.html\\n\\n\\xa0\\nSee http://researchsmp2.cc.vt.edu/DB/db/conf/cikm/cikm93.html\\n\\xa0\\ncf. aspoerri@research.att.com\\n\\xa0\\nSee http://www.cs.gatech.edu/gvu/people/Faculty/john.stasko\\n\\xa0\\nSee http://www.cise.nsf.gov./iris/ISPPDhome.html\\n\\xa0\\nSee http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Veerasamy:Aravindan.html\\n\\xa0\\nSee http://www.labs.bt.com/innovate/informat/infovis/part1.htm\\n\\xa0\\nSee cwickens@s.psych.uiuc.edu\\n\\xa0\\nSee http://www.dq.com/\\n\\xa0\\nSee Williamson, C., & Shneiderman, B. (1992) \"The Dynamic HomeFinder:\\nEvaluating Dynamic Queries in a Real-Estate Information Exploration System,\" In: Proceedings\\nof the 15th Annual International ACM SIGIR Conference on Research and Development in\\nInformation Retrieval, Copenhagen, 338-346.\\n\\xa0\\nSee http://canyon.psy.ohio-state.edu:8080/zhang/zhang-jiajie.html\\n\\xa0\\nSee http://esba-www.jrc.it/dvgdocs/dvghome.html\\n\\xa0\\nSee http://www.ecrc.de/research/uiandv/gsp/applications.html\\n\\n\\xa0\\nSee http://www.ecrc.de/research/uiandv\\n\\xa0\\nSee http://www.nrc.ca/corpserv/m_list_e.html\\n\\xa0\\nSee http://www.igd.fhg.de/www/igd-a4/index.html\\n\\xa0\\nSee http://www.igd.ghg.de/www/zgdv-mmvis/miv-projects_e.html#basic\\n\\xa0\\nSee http://www.ecrc.de/staff/gudrun\\n\\xa0\\nSee http://delite.darmstadt.gmd.de/delite/Projects/Corinna\\n\\xa0\\nSee http://www.aist-nara.ac.jp/IS/Chihara-lab/mosaic-l.html\\n\\xa0\\nSee http://www.csl.sony.co.jp/projects/ar/ref.html\\n\\xa0\\nSee http://www.csl.sony.co.jp/person/nagao/icmas96/outline.html\\n\\xa0\\nSee http://www.csl.sony.co.jp/person/rekimoto/transvision.html\\n\\xa0\\nSee http://www.csl.sony.co.jp/person/rekimoto/navi.html\\n\\xa0\\nSee http://www.csl.sony.co.jp/project/VS/index.html\\n\\xa0\\nSee http://www.vogue.is.uec.ac.jp/research.html#1\\n\\xa0\\nSee http://www.vogue.is.uec.ac.jp/~koike/papers/v193/v193.html\\n\\xa0\\nSee http://www.vogue.is.uec.ac.jp/~koike/papers/tois95/tois95.html\\n\\xa0\\nSee http://www.hc.t.u-tokyo.ac.jp/activity-index.e.html\\n\\xa0\\nSee http://ghidorah.t.u-tokyo.ac.jp\\n\\xa0\\nSee http://virtual.dcea.fct.unl.pt/gasa/vr/\\n\\xa0\\nSee http://www.cl.cam.ac.uk/abadge/documentation/abwayin.html\\n\\xa0\\nSee http://www.cl.cam.ac.uk/Research/Rainbow\\n\\xa0\\nSee http://www.lut.ac.uk/departments/co/research-groups/lutchi.html\\n\\xa0\\nSee http://www.mcc.ac.uk/research.htm\\n\\xa0\\nSee http://www.man.ac.uk/MVC/CGU-intro.html\\n\\xa0\\nSee http://www.hud.ac.uk/schools/comp-maths/centres/hci/HCIcentre.html\\n\\xa0\\nSee http://www.xrce.xerox.com/research/cbis/cbis_1.htm\\n\\xa0\\nSee http://www.cc.gatech.edu/gvu/virtual/Venue\\n\\xa0\\nSee http://www.cc.gatech.edu/gvu/softwiz/infoviz/information_mural.html\\n\\xa0\\nSee http://cedude.ce.gatech.edu/Projects/IV/iv.html\\ncf. http://cedude.ce.gatech.edu/research/index.html\\n\\xa0\\nSee http://www.gatech.edu/scivis\\n\\xa0\\nSee http://www.cc.gatech.edu/gvu/people/qiang.a.zhao\\n\\xa0\\nSee http://www.research.ibm.com/imaging/vizspace.html\\n\\xa0\\nSee http://www.research.ibm.com/research/lucente.html\\n\\xa0\\nSee http://www.almaden.ibm.com/dx\\n\\xa0\\nSee http://www.learningcube.com/webzn.html\\n\\xa0\\nSee http://www.bell-labs.com/project/visualinsights/\\n\\xa0\\nSee http://medusa.multiemdia.bell-labs.com/LWS\\n\\xa0\\nSee http://vlw.www.media.mit.edu/groups/vlw/\\n\\xa0\\nSee http://www.ted.com/info/cooper.html\\n\\xa0\\nSee http://dsmall.media.mit.edu/people/dsmall/\\n\\xa0\\nSee http://science.nas.nasa.gov/Groups/VisTech/visWeblets.html\\n\\xa0\\nSee http://www.mitre.org\\n\\xa0\\nSee http://www.well.com/user/jleft/orbit/infospace\\n\\xa0\\nSee http://multimedia.pnl.gov:2080/showcase/\\n\\xa0\\nSee http://www.pnl.gov/news/1995/news95-07.htm\\n\\xa0\\nSee http://vizlab.rutgers.edu\\n\\xa0\\nSee http://www.sandia.gov/eve/eve_toc.html\\n\\xa0\\nSee http://www.cs.sandia.gov/SEL/main.html\\n\\xa0\\nSee http://www.sgi.com/Products/Mineset/products/vtools.html#TreeVisualizer\\n\\xa0\\nSee http://www.sgi.fr/Support/DevProj/Forum/forum96/proceeds/Visual_and_Analytical_Data_Mining/overview.html.\\n\\xa0\\nSee http://www.evl.uic.edu/EVL/index.html\\ncf. http://www.ncsa.uiuc.edu/EVL/docs.html/homepage.html\\n\\xa0\\nSee http://www.ncsa.uiuc.edu/VR/cavernus/gallery.html\\n\\xa0\\nSee http://www.ncsa.uiuc.edu/VEG/DVR\\n\\xa0\\nSee http://www-pablo.cs.uiuc.edu/Projects/VR\\n\\xa0\\nSee http://www.bvis.uic.edu\\n\\xa0\\nSee http://www.ncsa.uiuc.edu/SCMS/DigLib/text/overview.html\\n\\xa0\\nSee http://ncsa.uiuc.edu/VR/cavernus/gallery.html\\n\\xa0\\nSee http://www.ncsa.uiuc.edu/VEG/DVR\\n\\xa0\\nSee http://notme.ncsa.uiuc.edu/SCD/Vis\\n\\xa0\\nSee http://www.ncsa.uiuc.edu/ITech\\n\\xa0\\nSee http://www.ncsa.uiuc.edu/VEG/index.htm\\n\\xa0\\nSee http://notme.ncsa.uiuc/edu/Vis/VICE.html\\n\\xa0\\nSee http://delphi.beckman.uiuc.edu/WWL\\n\\xa0\\nSee http://vizlab.beckman.uiuc.edu/chickscope\\n\\xa0\\nSee http://www.lis.pitt.edu/~spring/mlnds/mlnds.html\\n\\xa0\\nSee http://cs.panam.edu/info_vis/home-info_vis.html\\n\\xa0\\nSee Ramana Rao et al., \"Rich Interaction in the Digital Library,\" as in\\nnote 217 XX, p.38.\\n\\xa0\\nhttp://www-i.almaden.ibm.com/cs/quest/demo/assoc/general.html\\n\\nfinnigan@idirect.com\\nLast Update: August 14, 1998\\n\\n\\n',\n",
       " '\\n\\nThe Symbol Grounding Problem\\n\\n\\nHarnad, S. (1990) The Symbol Grounding Problem. Physica D 42: 335-346.\\n\\n\\nTHE SYMBOL GROUNDING PROBLEM\\n\\nStevan Harnad\\nDepartment of Psychology\\nPrinceton University\\nPrinceton NJ 08544\\nharnad@cogsci.soton.ac.uk\\n\\n\\nABSTRACT: There has been much discussion\\nrecently about the scope and\\nlimits of purely symbolic models of the mind and about the proper role\\nof connectionism in cognitive modeling. This paper describes the\\n\"symbol grounding problem\": How can the semantic interpretation of a\\nformal symbol system be made\\n intrinsic to the system, rather than just parasitic on the meanings in\\nour heads? How can the meanings of the meaningless symbol tokens,\\nmanipulated solely on the basis of their (arbitrary) shapes, be\\ngrounded in anything but other meaningless symbols? The problem is\\nanalogous to trying to learn Chinese from a Chinese/Chinese dictionary\\nalone. A candidate solution is sketched: Symbolic representations must\\nbe grounded bottom-up in nonsymbolic representations of two kinds:  (1)\\n \"iconic representations\" , which are analogs of the proximal sensory\\nprojections of distal objects and events, and (2)\\n \"categorical representations\" , which are learned and innate\\nfeature-detectors that pick out the invariant features of object and\\nevent categories from their sensory projections. Elementary symbols are\\nthe names of these object and event categories, assigned on the basis\\nof their (nonsymbolic) categorical representations. Higher-order (3)\\n \"symbolic representations\" , grounded in these elementary symbols,\\nconsist of symbol strings describing category membership relations\\n(e.g., \"An X is a Y that is Z\").\\nConnectionism is one natural candidate for the mechanism that learns\\nthe invariant features underlying categorical representations, thereby\\nconnecting names to the proximal projections of the distal objects they\\nstand for. In this way connectionism can be seen as a complementary\\ncomponent in a hybrid nonsymbolic/symbolic model of the mind, rather\\nthan a rival to purely symbolic modeling. Such a hybrid model would not\\nhave an autonomous symbolic \"module,\" however; the symbolic functions\\nwould emerge as an intrinsically \"dedicated\" symbol system as a\\nconsequence of the bottom-up grounding of categories\\' names in their\\nsensory representations. Symbol manipulation would be governed not just\\nby the arbitrary shapes of the symbol tokens, but by the nonarbitrary\\nshapes of the icons and category invariants in which they are grounded.\\n\\nKEYWORDS: \\nsymbol systems, connectionism, category learning, cognitive\\nmodels, neural models\\n1. Modeling the Mind\\n1.1 From Behaviorism to Cognitivism\\nFor many years the only\\nempirical approach in psychology was behaviorism, its only explanatory\\ntools input/input and input/output associations (in the case of\\nclassical conditioning; Turkkan 1989) and the reward/punishment history\\nthat \"shaped\" behavior (in the case of operant conditioning; Catania &\\nHarnad 1988). In a reaction against the subjectivity of armchair\\nintrospectionism, behaviorism had declared that it was just as illicit\\nto theorize about what went on in the\\n head of the organism to generate its behavior as to theorize about\\nwhat went on in its\\n mind. Only observables were to be the subject matter of psychology;\\nand, apparently, these were expected to explain themselves.\\n\\nPsychology became more like an empirical science when, with the gradual\\nadvent of cognitivism (Miller 1956, Neisser 1967, Haugeland 1978), it\\nbecame acceptable to make inferences about the\\n unobservable processes underlying behavior.\\nUnfortunately, cognitivism let mentalism in again\\nby the back door too, for the hypothetical internal processes came\\nembellished with subjective interpretations. In fact, semantic interpretability\\n(meaningfulness),\\nas we shall see, was one of the defining features of the most prominent\\ncontender vying to become the theoretical vocabulary of cognitivism,\\nthe \"language of thought\" (Fodor 1975), which became the prevailing\\nview in cognitive theory for several decades in the form of the\\n\"symbolic\" model of the mind: The mind is a symbol system and cognition\\nis symbol manipulation. The possibility of generating complex behavior\\nthrough symbol manipulation was empirically demonstrated by successes\\nin the field of artificial intelligence (AI).\\n\\n1.2 Symbol Systems\\n What is a symbol system? From Newell (1980) Pylyshyn (1984), Fodor\\n(1987) and the classical work of Von Neumann, Turing, Goedel, Church,\\netc. (see Kleene 1969) on the foundations of computation, we can\\nreconstruct the following definition:\\n\\nA symbol system is:\\n\\n a set of arbitrary\\n \"physical tokens\" scratches on paper, holes on a tape, events in a\\ndigital computer, etc. that are\\n\\n manipulated on the basis of \"explicit rules\" that are\\n\\n likewise physical tokens and strings of tokens. The rule-governed\\nsymbol-token manipulation is based\\n\\n purely on the shape of the symbol tokens (not their \"meaning\"),\\ni.e., it is purely syntactic, and consists of\\n\\n \"rulefully combining\" and recombining symbol tokens. There are\\n\\n primitive atomic symbol tokens and\\n\\n composite symbol-token strings. The entire system and all its parts\\n-- the atomic tokens, the composite tokens, the syntactic manipulations\\nboth actual and possible and the rules -- are all\\n\\n \"semantically interpretable:\" The syntax can be systematically\\nassigned a meaning e.g., as standing for objects, as describing states\\nof affairs).\\n\\nAccording to proponents of the symbolic model of mind such as Fodor\\n(1980) and Pylyshyn (1980, 1984), symbol-strings of this sort capture\\nwhat mental phenomena such as thoughts and beliefs are. Symbolists\\nemphasize that the symbolic level (for them, the mental level) is a\\nnatural functional level of its own, with ruleful regularities that are\\nindependent of their specific physical realizations. For symbolists,\\nthis implementation-independence is the critical difference between\\ncognitive phenomena and ordinary physical phenomena and their\\nrespective explanations. This concept of an autonomous symbolic level\\nalso conforms to general foundational principles in the theory of\\ncomputation and applies to all the work being done in symbolic AI, the\\nbranch of science that has so far been the most successful in\\ngenerating (hence explaining) intelligent behavior.\\n\\n\\nAll eight of the properties listed above seem to be critical to this\\ndefinition of symbolic.[1]\\nMany phenomena have some of the properties, but that does not entail\\nthat they are symbolic in this explicit, technical sense. It is not\\nenough, for example, for a phenomenon to be\\n interpretable as rule-governed, for just about anything can be\\ninterpreted as rule-governed. A thermostat may be interpreted as\\nfollowing the rule: Turn on the furnace if the temperature goes below\\n70 degrees and turn it off if it goes above 70 degrees, yet nowhere in\\nthe thermostat is that rule explicitly represented.\\nWittgenstein (1953) emphasized the difference between\\n explicit and implicit rules: It is not the same thing to \"follow\" a rule\\n(explicitly) and merely to behave \"in accordance with\" a rule\\n(implicitly).[2]\\nThe critical\\ndifference is in the compositeness (7) and systematicity (8) criteria.\\nThe explicitly represented symbolic rule is part of a formal system, it is\\ndecomposable (unless primitive), its application and manipulation is\\npurely formal (syntactic, shape-dependent), and the entire system must be\\nsemantically interpretable, not just the chunk in question. An isolated\\n(\"modular\") chunk cannot be symbolic; being symbolic is a systematic property.\\n\\nSo the mere fact that a behavior is \"interpretable\" as\\nruleful does not mean that it is really governed by a symbolic\\nrule.[3]\\nSemantic interpretability must be coupled with explicit representation\\n(2), syntactic manipulability (4), and systematicity (8) in order to be\\nsymbolic. None of these criteria is arbitrary, and, as far as I can\\ntell, if you weaken them, you lose the grip on what looks like a\\nnatural category and you sever the links with the formal theory of\\ncomputation, leaving a sense of \"symbolic\" that is merely unexplicated\\nmetaphor (and probably differs from speaker to speaker). Hence it is\\nonly this formal sense of \"symbolic\" and \"symbol system\" that will be\\nconsidered in this discussion of the grounding of symbol systems.\\n\\n1.3 Connectionist systems\\nAn early rival to the symbolic model of mind appeared (Rosenblatt 1962),\\nwas overcome by symbolic AI (Minsky & Papert 1969) and has recently\\nre-appeared in a stronger form that is currently vying with AI to be\\nthe general\\ntheory of cognition and behavior (McClelland, Rumelhart et al. 1986,\\nSmolensky 1988).\\nVariously described as \"neural networks,\"\\n\"parallel distributed processing\" and \"connectionism,\" this approach\\nhas a multiple agenda, which includes providing a theory of brain\\nfunction. Now, much can be said for and against studying behavioral and\\nbrain function independently, but in this paper it will be assumed\\nthat, first and foremost, a cognitive theory must stand on its own\\nmerits, which depend on how well it explains our observable behavioral\\ncapacity. Whether or not it does so in a sufficiently brainlike way is\\nanother matter, and a downstream one, in the course of theory\\ndevelopment. Very little is known of the brain\\'s structure and its\\n\"lower\" (vegetative) functions so far; and the nature of \"higher\" brain\\nfunction is itself a theoretical matter. To \"constrain\" a cognitive\\ntheory to account for behavior in a brainlike way is hence premature in\\ntwo respects: (1) It is far from clear yet what \"brainlike\" means, and\\n(2) we are far from having accounted for a lifesize chunk of behavior\\nyet, even without added constraints.\\nMoreover, the formal principles underlying connectionism seem to be based on\\nthe associative and statistical structure of the causal interactions\\nin certain dynamical systems; a neural network is merely one\\npossible implementation of such a dynamical system.[4]\\n\\nConnectionism will accordingly only be considered here as a cognitive\\ntheory. As such, it has lately challenged the symbolic approach\\nto modeling the mind. According to connectionism, cognition is not\\nsymbol manipulation but dynamic patterns of activity in a multilayered\\nnetwork of nodes or units with weighted positive and negative\\ninterconnections. The patterns change according to internal network\\nconstraints governing how the activations and connection strengths are\\nadjusted on the basis of new inputs (e.g., the generalized \"delta\\nrule,\" or \"backpropagation,\" McClelland, Rumelhart et al. 1986). The\\nresult is a system that learns, recognizes patterns, solves problems,\\nand can even exhibit motor skills.\\n\\n1.4 Scope and Limits of Symbols and Nets\\nIt is far from clear what the actual capabilities and limitations of\\neither symbolic AI or connectionism are. The former seems better at\\nformal and language-like tasks, the latter at sensory, motor and\\nlearning tasks, but there is considerable overlap and neither has gone\\nmuch beyond the stage of \"toy\" tasks toward lifesize behavioral\\ncapacity. Moreover, there has been some disagreement as to whether or\\nnot connectionism itself is symbolic. We will adopt the position here\\nthat it is not, because connectionist networks fail to meet several of\\nthe criteria for being symbol systems, as Fodor & Pylyshyn (1988) have\\nargued recently.\\nIn particular, although, like everything else, their behavior and\\ninternal states can be given isolated semantic interpretations, nets\\nfail to meet the compositeness (7) and systematicity (8) criteria\\nlisted earlier:  The patterns of interconnections do not decompose,\\ncombine and recombine according to a formal syntax that can be given a\\nsystematic semantic interpretation.[5]\\nInstead, nets seem to do what they do\\n non symbolically.\\nAccording to\\nFodor & Pylyshyn, this is a severe limitation, because many of our\\nbehavioral capacities appear to be symbolic, and hence the most natural\\nhypothesis about the underlying cognitive processes that generate them\\nwould be that they too must be symbolic. Our linguistic capacities are\\nthe primary examples here, but many of the other skills we have --\\nlogical reasoning, mathematics, chess-playing, perhaps even our\\nhigher-level perceptual and motor skills -- also seem to be symbolic.\\nIn any case, when we interpret our sentences, mathematical formulas,\\nand chess moves (and perhaps some of our perceptual judgments and motor\\nstrategies) as having a systematic\\n meaning\\nor\\n content,\\nwe know at first hand that that\\'s literally true, and not just a figure\\nof speech. Connectionism hence seems to be at a disadvantage in\\nattempting to model these cognitive capacities.\\n\\nYet it is not clear whether connectionism should for this reason aspire to\\nbe symbolic, for the symbolic approach turns out to suffer from a\\nsevere handicap, one that may be responsible for the limited extent of\\nits success to date (especially in modeling human-scale capacities) as\\nwell as the uninteresting and ad hoc nature of the symbolic \"knowledge\"\\nit attributes to the \"mind\" of the symbol system. The handicap has been\\nnoticed in various forms since the advent of computing; I have\\ndubbed a recent manifestation of it the \"symbol grounding problem\"\\n(Harnad 1987b).\\n2. The Symbol Grounding Problem\\n2.1 The Chinese Room\\nBefore defining the symbol grounding problem I will give two examples\\nof it. The first comes from Searle\\'s (1980) celebrated \"Chinese Room\\nArgument,\" in which the symbol grounding problem is referred to as the\\nproblem of intrinsic meaning (or \"intentionality\"): Searle challenges\\nthe core assumption of symbolic AI that a symbol system able to\\ngenerate behavior indistinguishable from that of a person must have a\\nmind. More specifically, according to the symbolic theory of mind, if a\\ncomputer could pass the Turing Test (Turing 1964) in Chinese -- i.e.,\\nif it could respond to all Chinese symbol strings it receives as input\\nwith Chinese symbol strings that are indistinguishable from the replies\\na real Chinese speaker would make (even if we keep testing for a\\nlifetime) -- then the computer would understand the meaning of Chinese\\nsymbols in the same sense that I understand the meaning of English\\nsymbols.\\n\\nSearle\\'s simple demonstration that this cannot be so consists of\\nimagining himself doing everything the computer does -- receiving the\\nChinese input symbols, manipulating them purely on the basis of their shape\\n(in accordance with (1) to (8) above), and finally returning the Chinese\\noutput symbols. It is evident that Searle (who knows no Chinese) would\\nnot be understanding Chinese under those conditions -- hence neither\\ncould the computer. The symbols and the symbol manipulation, being all\\nbased on shape rather than meaning, are systematically\\n interpretable\\nas having meaning -- that, after all, is what it is to be a symbol\\nsystem, according to our definition. But the interpretation will not be\\n intrinsic\\nto the symbol system itself: It will be parasitic on the fact that the\\nsymbols have meaning for\\n us,\\nin exactly the same way that the meanings of the symbols in a book are\\nnot intrinsic, but derive from the meanings in our heads. Hence, if the\\nmeanings of symbols in a symbol system are extrinsic, rather than\\nintrinsic like the meanings in our heads, then they are not a viable\\nmodel for the meanings in our heads: Cognition cannot be just symbol\\nmanipulation.\\n\\n2.2 The Chinese/Chinese Dictionary-Go-Round\\nMy own example of the symbol grounding problem has two versions,\\none difficult, and one, I think, impossible. The difficult version is: Suppose\\nyou had to learn Chinese as a second language and the only source of\\ninformation you had was a Chinese/Chinese dictionary. The trip through\\nthe dictionary would amount to a merry-go-round, passing endlessly from\\none meaningless symbol or symbol-string (the definientes) to another (the\\ndefinienda), never coming to a halt on what anything meant.[6]\\n\\n-- Figure 1 (Chinese Dictionary Entry) about here. --\\n\\nThe only reason cryptologists of ancient languages and secret codes\\nseem to be able to successfully accomplish something very like this is\\nthat their efforts are\\n grounded\\nin a first language and in real world experience and knowledge.[7]\\nThe second variant of the Dictionary-Go-Round, however, goes far beyond\\nthe conceivable resources of cryptology: Suppose you had to learn\\nChinese as a\\n first\\nlanguage and the only source of information you had was a\\nChinese/Chinese dictionary![8]\\nThis is more like the actual task faced by\\na purely symbolic model of the mind: How can you ever get off the\\nsymbol/symbol merry-go-round? How is symbol meaning to be grounded in\\nsomething other than just more meaningless symbols?[9]\\nThis is the symbol\\ngrounding problem.[10]\\n\\n2.3 Connecting to the World\\nThe standard reply of the symbolist (e.g., Fodor 1980, 1985) is that the\\nmeaning of the symbols comes from connecting the symbol system to the\\nworld \"in the right way.\" But it seems apparent that the problem of\\nconnecting up with the world in the right way is virtually coextensive\\nwith the problem of cognition itself. If each definiens in a\\nChinese/Chinese dictionary were somehow connected to the world in the right\\nway, we\\'d hardly need the definienda! Many symbolists believe that\\ncognition, being symbol-manipulation, is an autonomous functional\\nmodule that need only be hooked up to peripheral devices in order to\\n\"see\" the world of objects to which its symbols refer (or, rather, to\\nwhich they can be systematically interpreted as referring).[11]\\nUnfortunately, this radically underestimates the difficulty of picking\\nout the objects, events and states of affairs in the world that symbols\\nrefer to, i.e., it trivializes the symbol grounding problem.\\n\\nIt is one possible candidate for a solution to this problem, confronted\\ndirectly, that will now be sketched: What will be proposed is\\na hybrid nonsymbolic/symbolic\\nsystem, a \"dedicated\" one, in which the elementary symbols are grounded\\nin two kinds of nonsymbolic representations that pick out, from their\\nproximal sensory projections, the distal object categories to which the\\nelementary symbols refer. Most of the components of which the model is\\nmade up (analog projections and transformations, discretization,\\ninvariance detection, connectionism, symbol manipulation) have also\\nbeen proposed in various configurations by others, but they will be put\\ntogether in a specific bottom-up way here that has not, to my\\nknowledge, been previously suggested, and it is on this specific\\nconfiguration that the potential success of the grounding scheme\\ncritically depends. \\n\\nTable 1 summarizes the relative strengths and weaknesses of\\nconnectionism and symbolism, the two current rival candidates for\\nexplaining\\n all\\nof cognition single-handedly. Their respective strengths will be put to\\ncooperative rather than competing use in our hybrid model, thereby also\\nremedying some of their respective weaknesses. Let us now look more\\nclosely at the behavioral capacities such a cognitive model must\\ngenerate.\\n\\n-- Table 1 about here --\\n3. Human Behavioral Capacity\\nSince the advent of cognitivism, psychologists have continued to gather\\nbehavioral data, although to a large extent the relevant\\nevidence is already in: We already know what human beings are able to\\ndo. They can (1)\\n discriminate,\\n(2)\\n manipulate,[12]\\n(3)\\n identify\\nand (4)\\n describe\\nthe objects, events and\\nstates of affairs in the world they live in, and they can also (5)\\n \"produce descriptions\"\\nand (6)\\n \"respond to descriptions\"\\nof those objects,\\nevents and states of affairs. Cognitive\\ntheory\\'s burden is now to explain\\n how\\nhuman beings (or any other devices) do all this.[13]\\n\\n3.1 Discrimination and Identification\\nLet us first look more closely at discrimination and identification.\\nTo be able to\\n discriminate\\nis to able to judge whether two inputs are\\nthe same or different, and, if different, \\n how\\ndifferent they are. Discrimination is a relative judgment, based on our\\ncapacity to tell things apart and discern their degree of similarity. To\\nbe able to\\n identify\\nis to be able to assign a unique (usually arbitrary) response -- a\\n\"name\" -- to a class of inputs, treating them all as equivalent or\\ninvariant in some respect. Identification is an absolute judgment,\\nbased on our capacity to tell whether or not a given input is a member of a\\nparticular\\n category.\\n\\nConsider the symbol \"horse.\" We are able, in viewing different horses\\n(or the same horse in different positions, or at different times) to\\ntell them apart and to judge which of them are more alike, and\\neven how alike they are. This is discrimination.\\nIn addition, in viewing a horse, we can reliably call it a horse,\\nrather than, say, a mule or a donkey (or a giraffe, or a stone). This\\nis identification. What sort of internal representation would be needed\\nin order to generate these two kinds of performance?\\n\\n3.2 Iconic and categorical representations\\nAccording to the model being proposed here, our ability\\nto discriminate inputs depends on our forming\\n \"iconic representations\"\\nof them (Harnad 1987b). These are internal analog transforms of the\\nprojections of distal objects on our sensory surfaces (Shepard &\\nCooper 1982). In the case of\\nhorses (and vision), they would be analogs of the many shapes that horses\\ncast on our retinas.[14]\\nSame/different judgments would be based on the sameness or difference\\nof these iconic representations, and similarity judgments would be\\nbased on their degree of congruity. No homunculus is involved here;\\nsimply a process of superimposing icons and registering their degree\\nof disparity. Nor are there memory problems, since the inputs are\\neither simultaneously present or available in rapid enough succession\\nto draw upon their persisting sensory icons.\\n\\nSo we need horse icons to discriminate horses. But what about\\nidentifying them? Discrimination is independent of identification. I\\ncould be discriminating things without knowing what they were. Will the\\nicon allow me to identify horses? Although there are theorists who\\nbelieve it would (Paivio 1986), I have tried to show why it could not\\n(Harnad 1982, 1987b). In a world where there were bold, easily\\ndetected natural discontinuities between all the categories we would\\never have to (or choose to) sort and identify -- a world in which the\\nmembers of one category couldn\\'t be confused with the members of any\\nanother category -- icons might be sufficient for identification. But\\nin our underdetermined world, with its infinity of confusable potential\\ncategories, icons are useless for identification because there are too\\nmany of them and because they blend continuously[15]\\ninto one another, making it an independent problem to\\n identify\\nwhich of them are icons of members of the category and which are not!\\nIcons of sensory projections are too unselective. For identification,\\nicons must be selectively reduced to those\\n \"invariant features\"\\nof the sensory projection that\\nwill reliably distinguish a member of a category from any nonmembers\\nwith which it could be confused. Let us call the output of this\\ncategory-specific feature detector the\\n \"categorical representation\" .\\nIn some cases these representations may be innate, but\\nsince evolution could hardly anticipate all of\\nthe categories we may ever need or choose to identify, most of these\\nfeatures must be learned from experience. In particular,\\nour categorical representation of a horse is probably a learned one.\\n(I will defer till section 4 the problem of how the invariant features\\nunderlying identification might be learned.)\\n\\nNote that both iconic and categorical representations are nonsymbolic.\\nThe former are analog copies of the sensory projection, preserving\\nits \"shape\" faithfully; the latter are icons that have been\\nselectively filtered to preserve only some of the features of the shape\\nof the sensory projection: those that reliably distinguish members from\\nnonmembers of a category. But both representations are still sensory\\nand nonsymbolic. There is no problem about their connection to the\\nobjects they pick out: It is a purely causal connection, based on the\\nrelation between distal objects, proximal sensory projections and the\\nacquired internal changes that result from a history of behavioral\\ninteractions with them. Nor is there any problem of semantic\\ninterpretation, or whether the semantic interpretation is justified.\\nIconic representations no more \"mean\" the objects of which they are the\\nprojections than the image in a camera does. Both icons and\\ncamera-images can of course be\\n interpreted\\nas meaning or standing for something, but the interpretation would\\nclearly be derivative rather than intrinsic.[16]\\n\\n3.3 Symbolic Representations\\nNor can categorical representations yet be interpreted as \"meaning\"\\nanything. It is true that they pick out the class of objects they\\n\"name,\" but the names do not have all the systematic properties of symbols\\nand symbol systems described earlier. They are just an inert taxonomy.\\nFor systematicity it must be possible to combine and recombine them \\nrulefully into\\npropositions that can be semantically interpreted. \"Horse\" is so far\\njust an arbitrary response that is reliably made in the presence\\nof a certain category of objects. There is\\nno justification for interpreting it holophrastically as meaning \"This\\nis a [member of the category] horse\" when produced in the presence of a\\nhorse, because the other expected systematic properties of \"this\" and\\n\"a\" and the all-important \"is\" of predication are not exhibited by mere\\npassive taxonomizing. What would be required to generate these other\\nsystematic properties? Merely that the \\ngrounded names in the category taxonomy be\\nstrung together into \\n propositions\\nabout further category\\nmembership relations. For example:\\n\\n(1) Suppose the name \"horse\" is grounded by iconic and categorical\\nrepresentations, learned from experience, that reliably discriminate\\nand identify horses on the basis of their sensory projections.\\n\\n(2) Suppose \"stripes\" is similarly grounded.\\n\\nNow consider that the following category can be constituted out of\\nthese elementary categories by a symbolic description of category\\nmembership alone:\\n\\n(3) \"Zebra\" = \"horse\" & \"stripes\"[17]\\n\\nWhat is the representation of a zebra? It is just the symbol string\\n\"horse & stripes.\" But because \"horse\" and \"stripes\" are grounded in their\\nrespective iconic and categorical representations, \"zebra\"\\ninherits the grounding, through its grounded\\n symbolic\\nrepresentation.\\nIn principle, someone who had never seen a zebra (but had seen\\nand learned to identify horses and stripes) could identify a zebra on first\\nacquaintance armed with this symbolic representation alone (plus the\\nnonsymbolic -- iconic and categorical -- representations of horses and\\nstripes that ground it).\\n\\nOnce one has the grounded set of elementary symbols provided\\nby a taxonomy of names (and the iconic and categorical\\nrepresentations that give content to the names and allow them to pick\\nout the objects they identify), the rest of the symbol strings\\nof a natural language can be generated by symbol composition alone,[18]\\nand they will all inherit the intrinsic grounding of the elementary set.[19]\\nHence, the ability to discriminate and categorize (and its\\nunderlying nonsymbolic representations) has led naturally to the ability\\nto describe and to produce and respond to descriptions through\\nsymbolic representations.\\n\\n4. A Complementary Role for Connectionism\\nThe symbol grounding scheme just described has one prominent gap: No\\nmechanism has been suggested to explain how the all-important\\ncategorical representations could be formed: How does the hybrid system\\nfind the invariant features of the sensory projection that make it\\npossible to categorize and identify objects correctly?[20]\\nConnectionism, with its general pattern learning capability, seems to\\nbe one natural candidate (though there may well be others): Icons, paired with\\nfeedback indicating their names, could be processed by a connectionist\\nnetwork that learns to identify icons correctly from the sample of\\nconfusable alternatives it has encountered by dynamically adjusting the\\nweights of the features and feature combinations that are reliably\\nassociated with the names in a way that (provisionally) resolves the\\nconfusion, thereby reducing the icons to the\\n invariant\\n(confusion-resolving) features of the category to which they are\\nassigned. In effect, the \"connection\" between the names and the objects\\nthat give rise to their sensory projections and their icons would be\\nprovided by connectionist networks.\\n\\nThis circumscribed complementary role for connectionism in a hybrid\\nsystem seems to remedy the weaknesses of the two current competitors\\nin their attempts to model the mind independently.\\nIn a pure symbolic model the crucial connection between\\nthe symbols and their referents is missing; an autonomous symbol\\nsystem, though amenable to a systematic semantic interpretation, is\\nungrounded.\\nIn a pure connectionist model, names are connected to\\nobjects through invariant patterns in their sensory projections,\\nlearned through exposure and feedback, but the crucial compositional\\nproperty is missing; a network of names, though grounded, is not\\nyet amenable to a full systematic semantic interpretation.\\nIn the hybrid system proposed here, there is no longer any autonomous\\nsymbolic level at all; instead, there is an intrinsically\\n dedicated\\nsymbol system, its elementary symbols (names) connected to nonsymbolic\\nrepresentations that can pick out the objects to which they refer, via\\nconnectionist networks that extract the invariant features of their\\nanalog sensory projections.\\n5. Conclusions\\nThe expectation has often been voiced that \"top-down\" (symbolic)\\napproaches to modeling cognition will somehow meet \"bottom-up\"\\n(sensory) approaches somewhere in between. If the grounding\\nconsiderations in this paper are valid, then this expectation is\\nhopelessly modular and there is really only one viable route from sense\\nto symbols: from the ground up. A free-floating symbolic level like the\\nsoftware level of a computer will never be reached by this route (or\\nvice versa) -- nor is it clear why we should even try to reach such a level,\\nsince it looks as if getting there would just amount to uprooting our\\nsymbols from their intrinsic meanings (thereby merely reducing\\nourselves to the functional equivalent of a programmable computer).\\n\\nIn an intrinsically dedicated symbol system there are more constraints\\non the symbol tokens than merely syntactic ones. Symbols are\\nmanipulated not only on the basis of the arbitrary shape of their\\ntokens, but also on the basis of the decidedly nonarbitrary\\n\"shape\" of the iconic and categorical representations connected to\\nthe grounded elementary symbols out of which the higher-order\\nsymbols are composed. Of these two kinds of constraints, the\\niconic/categorical ones are primary. I am not aware of any\\nformal analysis of such dedicated symbol systems,[21]\\nbut this may be\\nbecause they are unique to cognitive and robotic modeling and their\\nproperties will depend on the specific kinds of robotic (i.e., behavioral)\\ncapacities they are designed to exhibit.\\n\\nIt is appropriate that the properties of dedicated symbol systems\\nshould turn out to depend on behavioral considerations. The present\\ngrounding scheme is still in the spirit of behaviorism in that the\\nonly tests proposed for whether a semantic interpretation will bear the\\nsemantic weight placed on it consist of one formal test (does it meet\\nthe eight criteria for being a symbol system?) and one behavioral test\\n(can it discriminate, identify and describe all the objects and states\\nof affairs to which its symbols refer?). If both tests are passed, then\\nthe semantic interpretation of its symbols is \"fixed\" by the behavioral\\ncapacity of the dedicated symbol system, as exercised on the objects\\nand states of affairs in the world to which its symbols refer; the\\nsymbol meanings are accordingly not just parasitic on the meanings in the\\nhead of the interpreter, but intrinsic to the dedicated symbol system\\nitself. This is still no guarantee that our model has captured subjective\\nmeaning, of course. But if the system\\'s behavioral capacities are\\nlifesize, it\\'s as close as we can ever hope to get.\\n\\nReferences\\nCatania, A. C. & Harnad, S. (eds.) (1988)\\nThe Selection of Behavior.\\nThe Operant Behaviorism of B. F. Skinner: Comments and Consequences.\\nNew York: Cambridge University Press.\\n\\nChomsky, N. (1980) Rules and representations.\\nBehavioral and Brain Sciences\\n3: 1-61.\\n\\nDavis, M. (1958)\\nComputability and unsolvability.\\nManchester: McGraw-Hill.\\n\\nDavis, M. (1965)\\nThe undecidable.\\nNew York: Raven.\\n\\nDennett, D. C. (1983)\\nIntentional systems in cognitive ethology.\\n Behavioral and Brain Sciences 6:\\n343 - 90.\\n\\nFodor, J. A. (1975)\\nThe language of thought\\nNew York: Thomas Y. Crowell\\n\\nFodor, J. A. (1980) Methodological solipsism considered as a\\nresearch strategy in cognitive psychology.\\nBehavioral and Brain Sciences\\n3: 63 - 109.\\n\\nFodor, J. A. (1985) Pr&#233cis of \"The Modularity of Mind.\"\\nBehavioral and Brain Sciences\\n8: 1 - 42.\\n\\nFodor, J. A. (1987)\\n Psychosemantics\\nCambridge MA: MIT/Bradford.\\n\\nFodor, J. A. & Pylyshyn, Z. W. (1988) Connectionism and cognitive\\narchitecture: A critical appraisal.\\n Cognition\\n28: 3 - 71.\\n\\nGibson, J. J. (1979)\\nAn ecological approach to visual perception.\\nBoston: Houghton Mifflin\\n\\nHarnad, S. (1982) Metaphor and mental duality.\\nIn T. Simon & R. Scholes, R. (Eds.)\\nLanguage, mind and brain.\\nHillsdale, N. J.: Lawrence Erlbaum Associates\\n\\nHarnad, S. (1987a) Categorical perception: A critical overview.\\nIn S. Harnad (Ed.)\\nCategorical perception: The groundwork of Cognition.\\nNew York: Cambridge University Press\\n\\nHarnad, S. (1987b) Category induction and representation.\\nIn S. Harnad (Ed.)\\nCategorical perception: The groundwork of Cognition.\\nNew York: Cambridge University Press\\n\\nHarnad, S. (1989) Minds, Machines and Searle.\\nJournal of Theoretical and Experimental Artificial Intelligence\\n1: 5-25.\\n\\nHarnad, S. (1990) Computational Hermeneutics.\\nSocial Epistemology\\nin press.\\n\\nHaugeland, J. (1978) The nature and plausibility of cognitivism.\\nBehavioral and Brain Sciences\\n1: 215-260.\\n\\nKleene, S. C. (1969)\\nFormalized recursive functionals and formalized realizability.\\nProvidence, R.: American Mathematical Society.\\n\\nKripke, S.A. (1980) \\nNaming and Necessity.\\nCambridge MA: Harvard University Press\\n\\nLiberman, A. M. (1982) On the finding that speech is special.\\nAmerican Psychologist\\n37: 148-167.\\n\\nLucas, J. R. (1961) Minds, machines and G\\\\*\"odel.\\n Philosophy\\n36: 112-117.\\n\\nMcCarthy, J. & Hayes, P. (1969) Some philosophical problems from the\\nstandpoint of artificial intelligence. In: Meltzer B. & Michie, P.\\nMachine Intelligence\\nVolume 4. Edinburgh: Edinburgh University Press.\\n\\nMcDermott, D. (1976) Artificial intelligence meets natural stupidity.\\nSIGART Newsletter\\n57: 4 - 9.\\n\\nMcClelland, J. L., Rumelhart, D. E., and the PDP Research Group (1986)\\nParallel distributed processing: Explorations in the\\nmicrostructure of cognition,\\nVolume 1. Cambridge MA: MIT/Bradford.\\n\\nMiller, G. A. (1956) The magical number seven, plus or minus two: Some\\nlimits on our capacity for processing information.\\nPsychological Review\\n63: 81 - 97.\\n\\nMinsky, M. (1974) A framework for Representing knowledge.\\nMIT Lab Memo\\n# 306.\\n\\nMinsky, M. & Papert, S. (1969)\\nPerceptrons: An introduction to computational geometry.\\nCambridge MA: MIT Press (Reissued in an Expanded Edition, 1988).\\n\\nNewell, A. (1980) Physical Symbol Systems.\\nCognitive Science 4:\\n135 - 83.\\n\\nNeisser, U. (1967)\\nCognitive Psychology\\nNY: Appleton-Century-Crofts.\\n\\nCognitive Psychology\\n\\nPaivio, A. (1986)\\nMental representation: A dual coding approach.\\nNew York: Oxford \\n\\nPenrose, R. (1989)\\nThe emperor\\'s new mind.\\nOxford: Oxford University Press\\n\\nPylyshyn, Z. W. (1980) Computation and cognition: Issues in the\\nfoundations of cognitive science.\\nBehavioral and Brain Sciences\\n3: 111-169.\\n\\nPylyshyn, Z. W. (1984)\\nComputation and cognition.\\nCambridge MA: MIT/Bradford\\n\\nPylyshyn, Z. W. (Ed.) (1987) \\nThe robot\\'s dilemma: The frame problem in artificial intelligence.\\nNorwood NJ: Ablex\\n\\nRosch, E. & Lloyd, B. B. (1978)\\nCognition and categorization.\\nHillsdale NJ: Erlbaum Associates\\n\\nRosenblatt, F. (1962)\\n\\nPrinciples of neurodynamics.\\n\\nNY: Spartan\\nSearle, J. R. (1980) Minds, brains and programs.\\nBehavioral and Brain Sciences\\n3: 417-457.\\n\\nShepard, R. N. & Cooper, L. A. (1982)\\nMental images and their transformations.\\nCambridge: MIT Press/Bradford.\\n\\nSmolensky, P. (1988) On the proper treatment of connectionism.\\nBehavioral and Brain Sciences\\n11: 1 - 74.\\n\\nStabler, E. P. (1985) How are grammars represented?\\nBehavioral and Brain Sciences\\n6: 391-421.\\n\\nTerrace, H. (1979)\\n Nim.\\nNY: Random House.\\n\\nTurkkan, J. (1989) Classical conditioning: The new hegemony.\\nBehavioral and Brain Sciences 12:\\n121 - 79.\\n\\nTuring, A. M. (1964) Computing machinery and intelligence. In:\\nMinds and machines,\\nA. R. Anderson (ed.), Engelwood Cliffs NJ: Prentice Hall.\\n\\nUllman, S. (1980) Against direct perception.\\nBehavioral and Brain Sciences\\n3: 373 - 415.\\n\\nWittgenstein, L. (1953)\\nPhilosophical investigations.\\nNew York: Macmillan\\n\\n\\n\\nThis figure should consist of the Chinese\\ncharacters for \"zebra,\" \"horse\" and \"stripes,\" formatted\\nas a dictionary entry, thus:\\n\\n\"ZEBRA\": \"HORSE\" with \"STRIPES\"\\n\\nTable 1. Connectionism Vs. Symbol Systems\\n\\nStrengths of Connectionism:\\n\\n(1) Nonsymbolic Function:\\n\\nAs long as it does not aspire to be a symbol system, a connectionist\\nnetwork has the advantage of not being subject to the symbol grounding\\nproblem.\\n\\n(2) Generality:\\n\\nConnectionism applies the same small family of algorithms to many problems,\\nwhereas symbolism, being a methodology rather than an algorithm, relies\\non endless problem-specific symbolic rules.\\n\\n(3) \"Neurosimilitude\":\\n\\nConnectionist architecture seems more brain-like than a\\nTuring machine or a digital computer.\\n\\n(4) Pattern Learning:\\n\\nConnectionist networks are especially suited to the learning of\\npatterns from data.\\n\\nWeaknesses of Connectionism:\\n\\n(1) Nonsymbolic Function:\\n\\nConnectionist networks, because they are not symbol systems, do not\\nhave the systematic semantic properties that many cognitive phenomena\\nappear to have.\\n\\n(2) Generality:\\n\\nNot every problem amounts to pattern learning. Some cognitive tasks\\nmay call for problem-specific rules, symbol manipulation, and\\nstandard computation.\\n\\n(3) \"Neurosimilitude\" :\\n\\nConnectionism\\'s brain-likeness may be superficial and may (like toy\\nmodels) camoflauge deeper performance limitations.\\n\\nStrengths of Symbol Systems:\\n\\n(1) Symbolic Function:\\n\\nSymbols have the computing power of Turing Machines and\\nthe systematic properties of a formal syntax that is semantically\\ninterpretable.\\n\\n(2) Generality:\\n\\nAll computable functions (including all cognitive functions) are\\nequivalent to a computational state in a Turing Machine.\\n\\n(3) Practical Successes:\\n\\nSymbol systems\\' ability to generate intelligent behavior is\\ndemonstrated by the successes of Artificial Intelligence.\\n\\nWeaknesses of Symbol Systems:\\n\\n(1) Symbolic Function:\\n\\nSymbol systems are subject to the symbol grounding problem.\\n\\n(2) Generality:\\n\\nTuring power is too general. The solutions to AI\\'s many toy problems do\\nnot give rise to common principles of cognition but to a vast variety\\nof ad hoc symbolic strategies.\\nFootnotes\\n1.\\nPaul Kube (personal communication) has suggested that (2) and (3) may\\nbe too strong, excluding some kinds of Turing Machine and perhaps even\\nleading to an infinite regress on levels of explicitness and\\nsystematicity.\\n2.\\nSimilar considerations apply to Chomsky\\'s (1980) concept of\\n\"psychological reality\" (i. e., whether Chomskian rules are really\\nphysically represented in the brain or whether they merely \"fit\" our\\nperformance regularities, without being what actually governs them).\\nAnother version of the distinction concerns explicitly represented\\nrules versus hard-wired physical constraints (Stabler 1985). In each\\ncase, an explicit representation consisting of elements that can be\\nrecombined in systematic ways would be symbolic whereas an implicit\\nphysical constraint would not, although\\n both would be semantically \"intepretable\" as a \"rule\" if construed in\\nisolation rather than as part of a system.\\n3.\\nAnalogously, the mere fact that a behavior is \\n interpretable\\nas purposeful\\nor conscious or meaningful does not mean that it really is purposeful\\nor conscious. (For arguments to the contrary, see Dennett 1983).\\n4.\\nIt is not even clear yet that a \"neural network\" needs to be\\nimplemented as a net (i.e., a parallel system of interconnected units)\\nin order to do what it can do; if symbolic simulations of nets have\\nthe same functional capacity as real nets, then a connectionist model\\nis just a special kind of symbolic model, and connectionism is just a special\\nfamily of symbolic algorithms.\\n5.\\nThere is some misunderstanding of this point because it is often\\nconflated with a mere implementational issue: Connectionist\\nnetworks can be simulated using symbol systems, and symbol systems can\\nbe implemented using a connectionist architecture, but that is\\nindependent of the question of what each can do\\n qua\\nsymbol system or connectionist network, respectively. By way of\\nanalogy, silicon can be used to build a computer, and a computer can\\nsimulate the properties of silicon, but the functional\\nproperties of silicon are not those of computation, and the functional\\nproperties of computation are not those of silicon.\\n6.\\nSymbolic AI abounds with symptoms of the symbol grounding problem. One\\nwell-known (though misdiagnosed) manifestation of it is the so-called\\n\"frame\" problem\\n(McCarthy & Hayes 1969; Minsky 1974; NcDermott 1976; Pylyshyn 1987): It\\nis a frustrating but familiar experience in writing \"knowledge-based\"\\nprograms that a system apparently behaving perfectly intelligently for\\na while can be foiled by an unexpected case that demonstrates its utter\\nstupidity: A \"scene-understanding\" program will blithely describe the\\ngoings-on in a visual scene and answer questions demonstrating its\\ncomprehension (who did what, where, why?) and then suddenly reveal that\\nit doesn\\'t \"know\" that hanging up the phone and leaving the room does\\nnot make the phone disappear, or something like that. (It is important\\nto note that these are not the kinds of lapses and gaps in knowledge\\nthat people are prone to; rather, they are such howlers as to cast\\nserious doubt on whether the system has anything like \"knowledge\" at\\nall.)\\n\\nThe \"frame\" problem has been optimistically defined as the problem of\\nformally specifying (\"framing\") what varies and what stays constant in\\na particular \"knowledge domain,\" but in reality it\\'s the problem of\\nsecond-guessing all the contingencies the programmer has not\\nanticipated in symbolizing the knowledge he is attempting to\\nsymbolize. These contingencies are probably unbounded, for practical\\npurposes, because purely symbolic \"knowledge\" is ungrounded. Merely\\nadding on more symbolic contingencies is like taking a few more turns\\nin the Chinese/Chinese Dictionary-Go-Round. There is in reality no\\nground in sight: merely enough \"intelligent\" symbol-manipulation to lull\\nthe programmer into losing sight of the fact that its meaningfulness is\\njust parasitic on the meanings he is projecting onto it from the\\ngrounded meanings in his own head. (I\\'ve called this effect the\\n\"hermeneutic hall of mirrors\" [Harnad 1990]; it\\'s the reverse side of\\nthe symbol grounding problem). Yet parasitism it is, as the next \"frame\\nproblem\" lurking around the corner is ready to confirm. (A similar form\\nof over-interpretation has occurred in the ape \"language\" experiments\\n[Terrace 1979]. Perhaps both apes and computers should be trained using\\nChinese code, to immunize their experimenters and programmers against\\nspurious over-interpretations. But since the actual behavioral tasks in\\nboth domains are still so trivial, there\\'s probably no way to prevent\\ntheir being decrypted. In fact, there seems to be an irresistible\\ntendency to overinterpret toy task performance itself, preemptively\\nextrapolating and \"scaling it up\" conceptually to lifesize without any\\njustification in practice.)\\n7.\\nCryptologists also use statistical information about word frequencies,\\ninferences about what an ancient culture or an enemy government are\\nlikely to be writing about, decryption algorithms, etc.\\n8.\\nThere is of course no need to restrict the symbolic resources to a dictionary;\\nthe task would be just as impossible if one had access to the entire body of\\nChinese-language literature, including all of its computer programs\\nand anything else that can be codified in symbols.\\n9.\\nEven mathematicians, whether Platonist or formalist, point out that\\nsymbol manipulation (computation) itself cannot capture the notion of the\\nintended interpretation of the symbols (Penrose 1989). The fact that\\nformal symbol systems and their interpretations are not the same thing\\nis hence evident independently of the Church-Turing thesis (Kleene\\n1969) or the Goedel results (Davis 1958, 1965), which have been zealously\\nmisapplied to the problem of mind-modeling (e.g., by Lucas 1964) -- to\\nwhich they are largely irrelevant, in my view.\\n10.\\nNote that, strictly speaking, symbol grounding is a\\nproblem only for cognitive modeling, not for AI in general. If\\nsymbol systems alone\\nsucceed in generating all the intelligent machine\\nperformance pure AI is interested in -- e.g., an automated dictionary --\\nthen there is no reason whatsoever to demand that their symbols have\\nintrinsic meaning. On the other hand, the fact that our own symbols do\\nhave intrinsic meaning whereas the computer\\'s do not, and the fact\\nthat we can do things that the computer so far cannot, may be\\nindications that even in AI there are performance gains to be made\\n(especially in robotics and machine vision) from endeavouring to ground\\nsymbol systems.\\n11.\\nThe homuncular viewpoint inherent in this belief is quite apparent,\\nas is the effect of the \"hermeneutic hall of mirrors\" (Harnad 1990).\\n12.\\nAlthough they are no doubt as important as perceptual skills, motor\\nskills will not be explicitly considered here. It is assumed that the\\nrelevant features of the sensory story (e.g., iconicity) will\\ngeneralize to the motor story (e.g., in motor analogs; Liberman 1982).\\nIn addition, large parts of the motor story may not be cognitive,\\ndrawing instead upon innate motor patterns and sensorimotor feedback.\\nGibson\\'s (1979) concept of \"affordances\" -- the invariant stimulus\\nfeatures that are detected by the motor possibilities they \"afford\" --\\nis relevant here too, though Gibson underestimates the processing problems\\ninvolved in finding such invariants (Ullman 1980). In any case, motor\\nand sensory-motor grounding will no doubt be as important as the\\nsensory grounding that is being focused on here.\\n13.\\nIf a candidate model were to exhibit all these behavioral capacities,\\nboth\\n linguistic\\n(5-6)\\nand\\n robotic\\n(i.e., sensorimotor),\\n(1-3)\\nit would pass the\\n\"Total Turing Test\" (Harnad 1989).\\nThe standard Turing Test (Turing 1964) calls for linguistic performance\\ncapacity only: symbols in and symbols out. This makes it equivocal\\nabout the status, scope and limits of pure symbol manipulation, and\\nhence subject to the symbol grounding problem. A model that could pass\\nthe Total Turing Test, however, would be grounded in the world.\\n14.\\nThere are many problems having to do with figure/ground\\ndiscrimination, smoothing, size constancy, shape constancy,\\nstereopsis, etc., that\\nmake the problem of discrimination much more complicated than what is\\ndescribed here, but these do not change the basic fact that iconic\\nrepresentations are a natural candidate substrate for our\\ncapacity to discriminate.\\n15.\\nElsewhere (Harnad 1987a,b) I have tried to show how the phenomenon of\\n\"categorical perception\" could generate internal discontinuities where\\nthere is external continuity. There is evidence that our perceptual\\nsystem is able to segment a continuum, such as the color spectrum, into\\nrelatively discrete, bounded regions or categories. Physical\\ndifferences of equal magnitude are more discriminable across the\\nboundaries between these categories than within them. This boundary\\neffect, both innate and learned, may play an important role in the\\nrepresentation of the elementary perceptual categories out of which the\\nhigher-order ones are built.\\n16.\\nOn the other hand, the resemblance on which discrimination performance\\nis based -- the degree of isomorphism between the icon and the sensory\\nprojection, and between the sensory projection and the distal object --\\nseems to be intrinsic, rather than just a matter of interpretation. The\\nresemblance can be objectively characterized as the degree of\\ninvertibility of the physical transformation from object to icon (Harnad 1987b).\\n17.\\nFigure 1 is actually the Chinese dictionary entry for \"zebra,\" which\\nis \"striped horse.\" Note that the character for \"zebra\" actually happens\\nto be the character for \"horse\" plus the character for \"striped.\"\\nAlthough Chinese characters are iconic in structure, they function\\njust like arbitrary alphabetic lexigrams at the level of syntax and\\nsemantics.\\n18.\\nSome standard logical connectives and quantifiers are needed\\ntoo, such as not, and, all, etc.\\n19.\\nNote that it is not being claimed that\\n\"horse,\" \"stripes,\" etc. are actually elementary symbols, with direct\\nsensory grounding; the claim is only that\\n some\\nset of symbols must be directly grounded. Most sensory category\\nrepresentations are no doubt hybrid sensory/symbolic; and their\\nfeatures can change by bootstrapping: \"Horse\" can always be revised, both\\nsensorily and symbolically, even if it was previously elementary.\\nKripke (1980) gives a good example of how \"gold\" might be baptized on\\nthe shiny yellow metal in question, used for trade, decoration and\\ndiscourse, and then we might discover \"fool\\'s gold,\" which would make\\nall the sensory features we had used until then inadequate, forcing us\\nto find new ones. He points out that it is even possible in principle\\nfor \"gold\" to have been inadvertently baptized on \"fool\\'s gold\"! Of\\ninterest here are not the ontological aspects of this possibility, but\\nthe epistemic ones: We could bootstrap successfully to real gold even if every\\nprior case had been fool\\'s gold. \"Gold\" would still be the right\\nword for what we had been trying to pick out all along, and its\\noriginal provisional features would still have provided a close enough\\napproximation to ground it, even if later information were to pull the\\nground out from under it, so to speak.\\n20.\\nAlthough it is beyond the scope of this paper to discuss it at length,\\nit must be mentioned that this question has often been begged in the\\npast, mainly on the grounds of \"vanishing intersections.\" It has been\\nclaimed that one cannot find invariant features in the sensory\\nprojection because they simply do not exist: The intersection of all\\nthe projections of the members of a category such as \"horse\" is empty.\\nThe British empiricists have been criticized for thinking otherwise;\\nfor example, Wittgenstein\\'s (1953) discussion of \"games\" and \"family\\nresemblances\" has been taken to have discredited their view. And\\ncurrent research on human categorization (Rosch & Lloyd 1978) has been\\ninterpreted as confirming that intersections vanish and that hence\\ncategories are not represented in terms of invariant features. The\\nproblem of vanishing intersections (together with Chomsky\\'s [1980]\\n\"poverty of the stimulus argument\") has even been cited by thinkers\\nsuch as Fodor (1985, 1987) as a justification for extreme nativism. The\\npresent paper is frankly empiricist. In my view, the reason\\nintersections have not been found is that no one has yet looked for\\nthem properly. Introspection certainly isn\\'t the way to look. And\\ngeneral pattern learning algorithms such as connectionism are\\nrelatively new; their inductive power remains to be tested. In\\naddition, a careful distinction has not been made between pure sensory\\ncategories (which, I claim, must have invariants, otherwise we could\\nnot successfully identify them as we do) and higher-order categories\\nthat are\\n grounded\\nin sensory categories; these abstract representations may be symbolic\\nrather than sensory, and hence not based directly on sensory\\ninvariants. For further discussion of this problem, see Harnad 1987b).\\n21.\\nAlthough mathematicians investigate the formal properties of\\nuninterpreted symbol systems, all of their motivations and intuitions\\nclearly come from the intended interpretations of those systems (see\\nPenrose 1989). Perhaps these too are grounded in the iconic and\\ncategorical representations in their heads.\\n\\n\\n',\n",
       " \"\\n\\n\\n\\n\\nBasic Problem\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNext: Levenshtein Distance\\nUp: Sequence comparison\\nPrevious: Sequence comparison\\n\\nBasic Problem\\n\\nWe are interested in the notion of resemblance or similarity between\\n two words x and y of length m and n\\n respectively.\\nA dual notion is to look at the distance between these two words.\\n\\nA word\\n x =\\n x0x1···xm-1\\n of length m is a sequence of letters from an alphabet\\n .\\nThus, for 0 <= i <= m-1, \\n .\\n\\n\\nThe empty word denoted by\\n \\n has a null length:\\n  .\\n\\nA function d is a distance between words if:\\n\\n\\n and d(x,y) > 0 for all\\n  ;\\n ;\\ntriangular inequality:\\n  .\\n\\n\\nSeveral distances between words can be considered:\\n\\nthe prefix distance:\\n dpref(x,y) = |x| + |y| -\\n 2*lcp(x,y)\\n where lcp(x,y) is the length of the longest common\\n prefix of x and y;\\nthe suffix distance is defined in a similar way as the prefix\\n distance;\\nid. for the substring distance;\\nthe Hamming distance.\\n\\n\\nWe are interested in a distance which enables to transform\\n x into y using three kinds of basic operations:\\n the substitution of a letter of x by a letter of y,\\n the deletion of a letter of x or\\n the insertion of a letter of y.\\nA cost is associated to each of these operations and for each\\n letter of the alphabet:\\n\\nSub(a,b) is the cost of the substitution of the\\n letter a by the letter b;\\nDel(a) is the cost of the deletion of the letter a;\\nIns(b) is the cost of the insertion of the letter\\n b.\\n\\n\\nThe general problem consists of finding a sequence of such basic\\n operations to transform x into y minimizing the total\\n cost of the operations used.\\nThe total cost is equal to the sum of the costs of each of the basic\\n operations.\\nThis cost is a distance on the words if Sub is a distance on the\\n letters.\\n\\nWe are trying to minimize the distance between x and y\\n which is generally the same (but not always) than maximizing the\\n similarity between these two words.\\n\\nThe solution is not necessarily unique.\\n\\nA solution can be given as a sequence of basic operations of\\n substitutions, deletions and insertions.\\nIt can also be given in a similar way by an alignment.\\n\\nFor two words x of length m and y of length\\n n such as m <= n.\\nAn alignment denoted by\\n\\n\\n\\nor\\n\\n\\n\\n of length p is such that:\\n\\nn <= p <= n+m;\\n\\n or\\n \\n for 0 <= i <= p-1 and 0 <= j <= m-1;\\n\\n or\\n \\n for 0 <= i <= p-1 and 0 <= j <= n-1;\\n;\\n;\\nfor 0 <= i <= p-1,\\n \\n such that\\n .\\n\\n\\nAn aligned pair of the type\\n \\n with\\n \\n indicates the substitution of the letter a by the letter\\n b.\\n\\nAn aligned pair of the type\\n \\n with\\n \\n indicates the deletion of the letter a.\\n\\nAn aligned pair of the type\\n \\n with\\n \\n indicates the insertion of the letter b.\\n\\nIn an alignement or in an aligned pair, the symbol\\n \\n is often replaced by the symbol -.\\n\\nThis problem can be easily stated in terms of graph.\\nLet G = (V,E) be a labelled and weighted graph with\\n an application\\n \\n which associates a cost to each edge of E and an application\\n \\n which associates an aligned pair to each edge of E.\\nThe graph G is defined as follows:\\n\\n\\nV is the set of vertices defined as follows:\\n\\n\\nE is the set of edges defined as follows:\\n\\n\\n\\n for\\n \\n and\\n \\n and\\n cost((i-1,j-1),(i,j)) =\\n Sub(xi,yj) and\\n  ;\\n \\n for\\n \\n and\\n \\n and\\n cost((i-1,j),(i,j)) =\\n Del(xi) and\\n  ;\\n \\n for\\n \\n and\\n  \\n and\\n cost((i,j-1),(i,j)) =\\n Ins(yj) and\\n  .\\n\\n\\n\\xa0\\n\\n\\nFigure 1.1:\\xa0Edit graph for\\n x = ACGA and y = ATGCTA.\\n \\n\\nAll the paths from (-1,-1) to (3,5) correspond to a different\\n alignment between x and y.\\nThe thick edges correspond to the following alignment:\\n  .\\n\\nIt is only necessary to find a shortest path from the vertex\\n (-1,-1) to the vertex (m-1,n-1).\\nThe less the cost the less distant are the two words x and\\n y.\\nAs the graph G is acyclic it is possible to find a shortest\\n path considering one time and only one each vertex.\\nOne had just to consider them in a topological order.\\nSuch an order can be obtained by considering the vertices row by row\\n and from left to right within each row.\\nDynamic programming ([3]) can solve this\\n problem.\\n\\nLet T be a two-dimensional table with m+1 rows and\\n n+1 columns.\\nThe value of each square T[i,j], for\\n -1 <= i <= m-1 and -1 <= j <= n-1\\n depends only on the three squares\\n T[i-1,j], T[i,j-1] and\\n T[i-1,j-1].\\n\\nThen for\\n 0 <= i <= m-1 and 0 <= j <= n-1,\\n T[i,j] is the\\n minimum cost of a path from (-1,-1) to (i,j).\\nThe algorithm\\n DYNAMIC-PROGRAMMING\\n computes all the values of the table T.\\n\\n\\n\\nThe algorithm\\n DYNAMIC-PROGRAMMING\\n clearly has a O(mn) time complexity.\\n\\nThe algorithm\\n DYNAMIC-PROGRAMMING\\n only computes the cost of the transformation of x into y.\\nTo get a sequence of basic operations to transform x into\\n y or\\n the corresponding alignment one has to trace back in the table T\\n from square T[m-1,n-1] to square T[-1,-1]\\n taking each time the right edge (see algorithm\\n ONE-ALIGNMENT).\\n\\n\\n\\nIf all the optimal alignments are required then a call to\\n ALL-ALIGNMENT(x,\\nm-1,y,n-1,\\n \\n,T)\\n will produced them.\\n\\n\\n\\nIn this case it is necessary to store all the values of the table\\n T then this problem can be solve in O(mn) space\\n complexity.\\n\\nWhen only the cost of the transformation of x into y is\\n needed it is easy to see that a space in\\n O(min(m,n)) is sufficient\\n since the computation of a row (respectively a column) only needs\\n the values of the previous row (respectively column).\\n\\nFurthermore it is possible to compute an alignment in linear space\\n using a ``divide and conquer'' method\\n ([9],\\n  [11] and\\n  [13]).\\n\\n\\n\\n\\n\\nNext: Levenshtein Distance\\nUp: Sequence comparison\\nPrevious: Sequence comparison\\n\\n e-mails: {Christian.Charras,\\n Thierry.Lecroq}@laposte.net\\n\\nThu Feb 19 10:23:29 WET 1998\\n\\n\\n\\n\",\n",
       " '\\n\\n\\n\\n\\nMind\\n\\n\\n\\n\\n\\n\\n\\n\\nDiagnosis\\n\\nMind\\n\\n\\n\\nModelling\\n        in Diagnosis\\n\\n\\n\\n\\nFoundation\\nfor the Use of\\nMultiple Models in\\xa0Diagnostic\\xa0Systems \\nSubstantial progress has been made within the past decade in\\nthe development of techniques for modelling and reasoning about\\nphysical systems or plants. However, there lacks a fundamental\\nunderstanding of the relationship between different approaches\\nand, more importantly, knowledge of which particular techniques\\nare more appropriate for a given application problem. Thus,\\ndifferent modelling techniques are used in isolation and very\\nlittle cross-fertilization occurs. Also, this lack of\\nunderstanding is cu rrently restricting the development of\\ntechniques that utilise multiple models to enhance the\\ngenerality, effectiveness and efficiency of existing automated\\napplication systems. In particular, there is no agreement as to\\nthe primitive modelling dimensions between different approaches,\\nthereby lacking a formal basis for the development of multiple\\nmodels. Without such a foundation for characterising different\\nmodels, it will be impossible to create a coherent approach to\\nthe use of multiple models, and hen ce such important\\ndevelopments will remain ad hoc. The MIND project is to establish\\na formal foundation for the use of multiple models in diagnostic\\nsystems. \\nMore concretely, this two-year project aims at achieving the\\nfollowing four objectives: \\n\\nTo establish a basis for classifying models, and their\\n        associated inference techniques, used in diagnostic\\n        systems on the basis of their fundamental properties.\\n        This considers equational models, both quantitative and\\n        qualitative, associational models , generally\\n        characterised by rule-based representations, and\\n        procedural models in the form of graphical methods or\\n        decision trees, with a focus on equation-based\\n        approaches. \\nTo determine the relationships between these modelling\\n        techniques and hence to develop a methodology that\\n        relates the most appropriate modelling technique to the\\n        characteristics of the diagnostic application. \\nTo develop a coherent framework for the development of\\n        systems based on the use of multiple models, determining\\n        what types of models should be used and how they should\\n        be instantiated within a given diagnostic process. \\nTo verify the proposed framework with reference to a case\\n        study taken from the domain of Intelligent Fault\\n        Diagnosis of CNC Machine Tools and, further, to identify\\n        an appropriate diagnostic problem within the CNC industry\\n        for a subsequent application s-driven proposal. \\n\\nIn short, it is envisaged that such work will provide a\\nfoundation for the effective comparison of existing (mono-model)\\napproaches and formalise the basis of the next-generation of\\ndiagnostic systems based on the use of multiple models. Although\\nthe work is of a fundamental nature, the project is conducted\\nwithin an industrial relevant context. The intermediate outcome\\nof this project has set up a firm basis for the planned\\nobjectives to be successfully fulfilled in due course. \\n\\nDownload a postscript version of the MIND Final Report. \\n\\n\\n',\n",
       " '\\n\\nAxiom of Choice\\n\\n\\n\\n\\n\\n\\n\\n\\na home page for the\\nAXIOM \\xa0OF \\xa0CHOICE\\n-- an introduction and \\nlinks collection by\\nEric Schechter, \\nVanderbilt University\\n\\n\\nThe Axiom of Choice (AC) was formulated about a century ago,\\nand it was controversial for a few of decades\\nafter that; it may be considered the last\\ngreat controversy of mathematics.  It is now a basic\\nassumption used in many parts of mathematics.  In fact,\\nassuming AC is equivalent to assuming\\nany of these principles (and many others):\\n\\nGiven any two sets, one set has cardinality less than or equal\\nto that of the other set -- i.e., one set is in one-to-one\\ncorrespondence with some subset of the other.  (Historical\\nremark: It was questions like this that led to\\nZermelo\\'s\\nformulation of AC.)\\n\\nAny vector space over a field F has a basis --\\ni.e., a maximal linearly independent subset -- over that field.\\n(Remark: If we only consider the case where F is the\\nreal line, we obtain a slightly weaker statement; it is not\\nyet known whether this statement is also equivalent to AC.)\\n\\n\\nAny product of compact topological spaces is compact.  (This is\\nnow known as Tychonoff\\'s Theorem, though Tychonoff himself\\nonly had in mind a much more specialized result that is not\\nequivalent to the Axiom of Choice.)\\n\\n\\n\\nAC has many forms; here\\nis one of the simplest:\\n\\n\\nAxiom of Choice.\\nLet C be a collection of nonempty sets.  Then we can choose\\na member from each set in that collection.  In other words, there\\nexists a function f defined on C with the property that, for each\\nset S in the collection, f(S) is a member of S.\\n\\nThe function f is then called a choice function.\\n\\nTo understand this axiom better, let\\'s consider a few\\nexamples.\\n\\nIf C is the collection of all nonempty\\nsubsets of {1,2,3,...}, then we can define f quite\\neasily:  just let f(S) be the smallest member of S.\\nIf C is the collection of all intervals of real\\nnumbers with positive, finite lengths, then we can define\\nf(S) to be the midpoint of the interval S.\\nIf C is some more general collection of subsets of the\\nreal line, we may be able to define f by using a more\\ncomplicated rule.\\nHowever, if C is the collection of all nonempty\\nsubsets of the real line, it is not clear how to find\\na suitable function f.  In fact, no one has ever\\nfound a suitable function f for this collection C, and\\nthere are convincing model-theortic \\narguments that no one ever will.\\n(Of course, to prove this requires a precise\\ndefinition of \"find,\" etc.)\\n\\nThe controversy was over how to interpret the words\\n\"choose\" and \"exists\" in the axiom:\\n\\nIf we follow the constructivists, and \"exist\"\\nmeans \"find,\" then the axiom is\\nfalse, since we cannot find a choice function\\nfor the nonempty subsets of the reals.\\n\\nHowever, most mathematicians give \"exists\" a much weaker\\nmeaning, and they consider the Axiom to be true:  \\nTo define f(S), just\\narbitrarily \"pick any member\" of S.  \\n\\n\\nIn effect, when we accept the Axiom of Choice, this\\nmeans we are agreeing to the convention that we shall permit\\nourselves to use a choice function f in proofs, \\nas though it \"exists\" in some sense, even though we cannot give\\nan explicit example of it or an explicit algorithm for it.\\n(For an introduction to constructivism, you might\\ntake a look at my\\npaper on that subject.  The term has rather\\ndifferent, slightly related meanings in advanced\\nmathematics and in mathematics education; I am referring\\nto the former meaning here.)\\n\\n\\n\\n\\nThe \"existence\" of f -- or of any mathematical\\nobject, even the number \"3\" -- is purely formal.  It\\ndoes not have the same kind of solidity as your table and your\\nchair; it merely exists in the mental universe of mathematics.\\nMany different mathematical universes are possible.  When we\\naccept or reject the Axiom of Choice, we are specifying\\nwhich universe we shall work in.  Both possibilities\\nare feasible -- i.e., neither accepting nor\\nrejecting AC yields a contradiction; that follows from\\nmodels devised by \\nGödel\\nand Cohen.\\nSome mathematicians\\nhave further investigated what happens when we reject AC\\nbut accept some weakened variant -- for example,\\nCC (Countable Choice), which permits a sequence of\\narbitrary choices.  However,\\nmost \"ordinary\" mathematicians -- i.e., most mathematicians\\nwho are not logicians or set theorists --\\naccept the Axiom of Choice chiefly \\nbecause their work is simpler with the Axiom of Choice\\nthan without it.\\n\\n\\n\\nThe full strength of the Axiom of Choice does\\nnot seem to be needed for applied mathematics.\\nSome weaker principle such as CC or DC \\ngenerally would suffice.  To see this, consider that\\nany application is based on measurements, but humans\\ncan only make finitely many measurements.\\nWe can extrapolate and take limits, but usually\\nthose limits are sequential, so even in theory we\\ncannot make use of more than countably many\\nmeasurements.  The resulting spaces are separable.\\nEven if we use a nonseparable space such as\\nL¥, this\\nmay be merely to simplify our notation; the\\nrelevant action may all be happening in some\\nseparable subspace, which we could identify\\nwith just a bit more effort.  (Thus, in some\\nsense, nonseparable spaces exist only\\nin the imagination of mathematicians.)  If we restrict\\nour attention to separable spaces, then much\\nof conventional analysis still works with\\nAC replaced by CC or DC.  However, the\\nresulting exposition is then more complicated,\\nand so this route is only followed by a few\\nmathematicians who have strong philosophical\\nleanings against AC.\\n\\n\\n\\nA few pure mathematicians and many applied mathematicians\\n(including, e.g., some mathematical physicists) are\\nuncomfortable with the Axiom of Choice.  Although\\nAC simplifies some parts of mathematics, it also \\nyields some results that are unrelated to, or perhaps even\\ncontrary to, everyday \"ordinary\" experience; it implies the\\nexistence of some rather bizarre, counterintuitive\\nobjects.  Perhaps the most bizarre is the\\nBanach-Tarski Paradoxical Decomposition.  Banach\\nand Tarski\\nused the Axiom of Choice to\\nprove that it is possible to take the 3-dimensional\\nclosed unit ball,\\nB \\xa0 = \\xa0 {(x,y,z)\\nÎ R3\\n: x2 + y2 + z2\\n< 1}\\nand partition it into finitely many pieces, and move those\\npieces in rigid motions (i.e., rotations and translations,\\nwith pieces permitted to move through one another) and\\nreassemble them to form two copies of B.\\n\\nAt first glance, the Banach-Tarski Decomposition\\nseems to contradict some of our intuition about physics\\n-- e.g., the Law of Conservation of Mass, from classical\\nNewtonian physics.  Consequently, the Decomposition is\\noften called the  Banach-Tarski Paradox. But\\nactually, it only yields a complication, not a\\ncontradiction.  If we assume a uniform density, only a\\nset with a defined volume can have a defined mass.\\nThe notion of \"volume\" can be defined for many subsets\\nof  R3,  and beginners might expect the\\nnotion to apply to all subsets of R3,\\nbut it does not.  More precisely, Lebesgue measure\\nis defined on some subsets of  R3, but\\nit cannot be extended to all subsets of \\nR3 in a fashion that preserves two of\\nits most important properties:  the measure of the union\\nof two disjoint sets is the sum of their measures, and\\nmeasure is unchanged under translation and rotation.\\nThus, the Banach-Tarski Paradox does not violate the Law\\nof Conservation of Mass; it merely tells us that the\\nnotion of \"volume\" is more complicated than we might have\\nexpected. \\n\\nBy the way, the sets in the Banach-Tarski Decomposition\\ncannot be described explicitly; we are merely able to\\nprove their existence, like that of a choice\\nfunction. One or more of the sets in the decomposition\\nmust be Lebesgue unmeasurable; thus a corollary of the\\nBanach-Tarski Theorem is the fact that there exist sets\\nthat are not Lebesgue measurable.  The existence of\\nunmeasurable sets has a much shorter and easier proof,\\nwhich can be found in every introductory textbook on\\nmeasure theory.  That proof also uses the Axiom of\\nChoice, but doesn\\'t mention the Banach-Tarski\\nDecomposition.\\n\\n\\n\\nPersonally, I am not surprised to find the Axiom of Choice\\ncoming into play in a subject that is so inherently\\ncomplicated as unmeasurable sets.\\nI am much more surprised to find AC coming into play\\nin a seemingly\\nmuch simpler  and more concrete setting:  the integers.\\nLet \\nW\\nbe an infinite set\\n(for instance, the integers).\\nBy a filter on the set \\nW, we\\nmean a\\nmethod of classifying all subsets of W so that\\n\\n(i)\\xa0\\xa0 certain subsets of W are called \"large\";\\n(ii)\\xa0\\xa0 any set\\ncontaining a large set is large;\\n(iii)\\xa0\\xa0 the intersection of two\\nlarge sets is large;\\n(iv)\\xa0\\xa0 the empty set is not large.\\n\\nIt follows easily from these rules that a set and its complement\\ncannot both be large.  Thus, we do not get a filter \\non the integers by calling\\nall infinite sets \"large,\" since both the even integers and\\nthe odd integers are infinite sets.  We do get an example of a\\nfilter by calling a set \"large\" if it is cofinite -- i.e.,\\nif its complement is finite.  However, that example does not\\nsatisfy the next condition.\\nAn ultrafilter satisfies the preceding conditions\\nplus this additional condition:\\n(v)\\xa0\\xa0 for each set \\nS\\xa0Í\\xa0W,\\neither S is large or\\nthe complement of S is large.\\nFor an example of an ultrafilter, \\npick some particular point \\nw0 in\\nW, and \\nuse this classification\\nscheme:  say that a set S is \"large\" if and only if \\nw0\\xa0Î\\xa0S.\\nHowever, that example does not\\nsatisfy the next condition.  A nonprincipal\\nultrafilter satisfies the preceding conditions\\nplus this additional requirement:\\n(vi)\\xa0\\xa0 no\\nfinite set is large.\\nNow, the existence of such a classification\\nscheme can be prove using the\\nAxiom of Choice.\\n(Sketch of proof:  start from the\\nfilter of cofinite sets, and extend it to a maximal\\nfilter using Zorn\\'s Lemma, also known as the\\nKuratowski-Zorn Theorem.)\\nHowever,\\nit turns out that we cannot actually\\nfind an explicit example of a\\nnonprincipal ultrafilter.\\n(This result may be simpler than the\\nBanach-Tarski Paradox, but it\\ndoes not really get us away from measure\\ntheory.  A nonprincipal ultrafilter is essentially\\nthe same thing as a two-valued probability measure\\nthat is finitely additive but not countably additive.)\\n\\n\\nBertrand\\nRussell (more famous for his work in philosophy and political\\nactivism, but also an accomplished mathematician) once said, \\n\\nTo choose one sock from each of\\ninfinitely many pairs of socks requires the Axiom of Choice,\\nbut for shoes the Axiom is not needed.\\n\\nThe idea is that\\nthe two socks in a pair are identical in appearance, and\\nso we must make an arbitrary choice if we wish to choose\\none of them.  For shoes, we can use an explicit algorithm --\\ne.g., \"always choose the left shoe.\"  Why does Russell\\'s statement\\nmention infinitely many pairs?  Well, if we only have\\nfinitely many pairs of socks, then AC is not needed --\\nwe can choose one member of each pair using the\\ndefinition of \"nonempty,\" and we can repeat an operation\\nfinitely many times using the rules of formal logic\\n(not discussed here).\\n\\n\\n\\nJerry Bona once said,\\n\\nThe Axiom of Choice is obviously true; the Well Ordering\\nPrinciple\\nis obviously false; and who can tell about Zorn\\'s Lemma?\\n\\nThis is a joke.  In the setting of ordinary\\nset theory, all three of those principles are mathematically\\nequivalent -- i.e., if we assume any one of\\nthose principles, we can use it to prove the other two.  \\nHowever, human intuition does not\\nalways follow what is mathematically correct.  \\nThe Axiom of Choice agrees with the intuition of most\\nmathematicians; the Well Ordering Principle is contrary\\nto the intuition of most mathematicians; and Zorn\\'s Lemma\\nis so complicated that most mathematicians are not able\\nto form any intuitive opinion about it.\\n\\n\\n\\nLinks Collection for AC\\nPlease write to me\\nif you have suggestions for additions or alterations to this\\nweb page.  However, I will warn you that I am not a leading\\nauthority on the Axiom of Choice; I am not knowledgeable about much of\\nthe advanced research on the subject.  I have posted\\nthis web page chiefly because \\n(i) I like the Axiom of Choice; \\n(ii) I think I have a good\\nunderstanding of the elementary aspects of the subject;\\nand (iii) I like posting web\\npages.\\nIntroductory / elementary\\nAnother online introduction currently available for AC is the \\nMath FAQ.\\nIt also includes an introduction to\\nthe\\nBanach-Tarski paradox.\\n\\nFor more extensive information about the Banach-Tarski\\nParadox, see\\nStan Wagon\\'s book.\\n\\nHandbook of\\nAnalysis and its Foundations, by Eric Schechter. \\nThe website is an advertisement,\\nbut it does include a few interesting excerpts from the book\\n-- e.g., a \\nlist of 27 forms of the Axiom of Choice\\nand a few dozen weak forms of Choice, as well as\\na chart showing how some of the\\nweak forms are related. (The book is \\nintended for beginning graduate students;\\nonly a small portion of the book\\nis actually concerned with Choice.)\\nThe Math\\nHistory Archive contains some mentions of AC. See especially: the\\nbeginnings of set theory, \\n\\nCantor,\\n\\n\\nHausdorff,\\n\\nZermelo,\\n\\nSierpinski,\\n\\nKuratowski,\\n\\nZorn.\\n\\nGödel,\\n\\n\\nCohen\\n\\nZermelo\\'s axiom of choice : its origins, development, and\\ninfluence, by Gregory Moore.  A fascinating history of AC.\\nYou may have trouble locating a copy of this book -- I think\\nit\\'s out of print.  New York : Springer-Verlag, c1982.\\nISBN  \\n0387906703.\\n\\nConstructivism\\nis Difficult -- a brief introduction to constructivism.\\nConstructivism and AC are two different but overlapping topics.\\nAC is a nonconstructive axiom; that\\'s what made it so\\ncontroversial.\\n\\nTopological\\nEquivalents of the Axiom of Choice and of Weak Forms of Choice by Eric\\nSchechter -- a brief introduction to this subject, which I wrote for the\\n``Topology Atlas\\'\\' \\nOne of AC\\'s most important applications \\nin analysis is the Hahn-Banach Theorem.  It may be viewed as\\na weak form of Choice. Here is an on-line survey article,\\nThe \\nHahn-Banach Theorem: The Life and Times, by\\nLawrence Narici  and Edward Beckenstein.\\n\\n\\n\\n\\n\\nAxiome\\ndu Choix, by \\nDavid \\nMadore. This page\\nincludes a list of several equivalents and weaker consequences of\\nAC, and a list of some of the implications among them.\\nIf your French is weak, you might try an automatic translation\\nservice, such as\\nAV\\'s translator.\\nHowever, such translation programs don\\'t know mathematics, and so\\nsome of the results may be a bit odd.  For instance, \\nwhat we call \"well ordering\" is what the French call\\n\"bon ordre,\" but the AV translator turns that into\\n\"good command.\"\\n\\nAn \\nintroduction to the gauge integral, by me (Eric Schechter).  \\nWe might replace the Riemann integral with the gauge integral\\nin our freshman calculus courses.  It is similar in most\\nrespects, and is better in many respects.  The Axiom of Choice\\ngets mentioned near the end of the discussion (but I\\'m not\\nadvocating telling the freshmen about the Axiom of Choice).\\n\\n\\nThe ProvenMath\\nwebsite includes a web page on AC.  That page includes formal statements of\\nseveral of the equivalents of AC,\\nand formal proofs of equivalence.  To read the statements and proofs,\\nyou\\'ll need to familiarize yourself with the \\nnotation\\nthat ProvenMath uses for representing mathematical symbols on\\nweb pages.\\n\\nEspecially noteworthy books and/or researchers\\nConsequences \\nof the Axiom of Choice is a book by Paul Howard and Jean E. Rubin \\nthat was published by the American Mathematical\\nSociety in 1998.  It is a vast survey of Choice and its weaker\\nrelatives.  It is a reference book, not intended\\nfor beginners.  The authors\\nare continuing their research project, which now goes a bit beyond\\nthe book.  Their web page contains a list of the errata and addenda\\nto the book, and a form for downloading copies of the project\\'s \\nmain tables.  You may also want to look at some related\\npapers.\\n\\n\\nHome page of Thomas Jech.\\nJech is the author of the book titled The Axiom of Choice, which\\nis not recent but is still excellent.  He has worked in \\nset theory, logic, and other areas since then.  Some of his \\npapers are available online.\\n\\nSaharon Shelah\\'s papers.\\nShelah is one of the leading logicians of our century; he has made\\ngreat contributions to the theory of forcing.  My favorite among his\\nresults is the fact that Con(ZF) implies Con(ZF + DC + BP); this\\nresult was shown by J.D.M.Wright to be important to functional\\nanalysis.  (It\\'s explained further in my book.)  Among Shelah\\'s\\nsubpages is a \\nlist of his coauthors,\\nmany of whom have web pages of their own.\\n\\n\\n\\nAndreas\\nBlass\\'s home page.  It is a very old result that the Axiom\\nof Choice implies the existence of bases for vector spaces;\\nBlass can be credited with proving the converse.  Blass also\\ndid some of the early work on proving the unconstructability\\nof nonprincipal ultrafilters.  (Those are results of his\\nthat I\\'ve understood; he has probably done some other much\\nmore important things that I don\\'t understand.)\\n\\nEdward Nelson\\'s\\nhome page.  Nelson is the father of Internal Set Theory (IST), a\\nvariant of Nonstandard Analysis.  IST has acquired a large\\nfollowing; some analysts are of the opinion that IST is the most\\nintuitive approach to limits.  (Personally,\\nI suspect that most of those analysts were first trained\\nas logicians, but that may be a reflection of my own ignorance.)\\nSome of Nelson\\'s writings are available online.\\n\\n\\nHome page of Fred\\nRichman, a leading constructivist.  The page includes some\\namusing subpages, e.g.,\\nConfessions \\nof a formalist, Platonist intuitionist -- (Hey, I thought those\\nterms contradicted each other!) -- and\\nMath\\nwithout countable choice.\\n\\n\\nSlightly more advanced and specialized topics\\nRealism\\nin mathematics -- book review by Morris Hirsch (in AmsTex format) of\\nbook by Penelope Maddy. \\nThe Dehn\\nInvariant, explained by Douglas Zare.  Hilbert\\'s Third Problem, solved.\\nOne step of the solution uses AC.  \\n(David Joyce has prepared a nice website about\\nHilbert\\'s 23 problems.)\\n\\n\\nA\\ntheorem about polyomino tilings proved with AC by Michael Reid \\nHow\\nto Fill n-Dimensional Space with Hoops, by Evelyn Sander\\n\\n[intermittent server?]\\n\\nThis Week\\'s Finds\\nin Mathematical Physics (Week 68), by John Baez; includes some discussion\\nof topoi \\nNew\\nFoundations\\nhome page, by Randall Holmes.  NF is a refinement of\\nRussell\\'s theory of types, introduced by Quine in 1937. Thus, it is an\\nalternate form of set theory or higher-order logic, a little different\\nfrom conventional set theory, but still capable of doing approximately\\nthe same things. Of course, it differs from the usual set theory in a\\nfew respects; an obvious difference is that there is a universal set\\nin NF.  A surprising difference is that the Axiom of Choice is false\\nin NF; this was established in a 1953 paper by E.P.Specker.  If\\nHolmes\\'s website interests you, you might continue with T.E.Forster\\'s\\n1995 book. (For bibliographic details see Holmes\\'s website.)\\n\\n\\n\\n\\n\\nIssues\\nin commonsense set theory, an online article by Müjdat Pakkan and Varol\\nAkman. From the abstract: \"In this survey, we briefly review classical\\nset theory from an AI perspective, and then consider alternative set theories.\"\\nIncludes an overview of ZF set theory, which makes it relevant to this\\nhome page. \\n\\nFormal logic and / or automatic theorem-proving\\nMetamath\\nSolitaire, an elementary game implemented as a Java applet\\nthat lets you prove simple theorems in logic and set theory.\\nIncludes introductory explanation.\\n\\nMathematical \\nLogic around the world -- linklist provided by \\nthe Mathematical Logic Group in Bonn\\n\\nIsabelle\\nis a generic theorem prover which can support a wide variety of logics;\\nit is available for free and will run on most Unix systems.\\nYou may be interested in some of\\nLarry\\nPaulson\\'s papers on \\nmechanizing set theory using Isabelle.\\nEspecially, you may be interested in \"Mechanising \\nSet Theory: Cardinal Arithmetic and the Axiom of Choice\", coauthored by\\nKrzysztof Grabczewski.  This paper mechanizes\\nthe proof of numerous equivalents of the Axiom of Choice, covering\\nmost of Chapter 1 of Kunen\\'s Set Theory and most of\\nChapters 1 and 2 of Rubin and Rubin\\'s Equivalents of the Axiom\\nof Choice.\\n\\n\\nMerging\\nHOL with Set Theory (Talk) -- (I don\\'t really know what this is) \\nLogic Eprints -- there\\nis probably some stuff on AC in here, but I haven\\'t explored it yet. \\nA\\nSurvey of the QED Project and Related Topics\\nMiscellaneous\\nNewsgroups: \\nsci.logic, \\nsci.philosophy.tech,\\nsci.math, \\nsci.math.research\\nsometimes contain something about\\nAC. \\nKurt Godel\\nin Blue Hill, a description of Godel and some of his work,\\nby Peter Suber \\nAdib Ben Jebara has been working on a web page about\\na\\npossible connection between Fermat\\'s Last Theorem and the\\nAxiom of Choice.  It\\'s a work in progress, and\\nI don\\'t fully understand it yet, and I don\\'t know whether\\nhe\\'ll eventually be able to fill in \\nthe details that are still needed in\\nhis proof, but he asked me to put this\\nlink here.  Fermat\\'s Last Theorem, you recall,\\nsays that xn+yn=zn has\\nno solution in positive integers x,y,z,n with n>2.\\nBen Jebara is considering \\nxN+yN=zN, where\\nN is the set of all positive integers; thus\\nxN represents the Cartesian product of countably\\nmany copies of a finite set x (or, equivalently (?), the\\ncollection of all sequences that take their values in a\\nfinite set x).  Of related interest is a paper\\nby Andreas Blass, \"Sums,\\nProducts, and Choice for Finite Sets\".\\n\\n\\n\\nThe Axiom of Choice was used for \\na tongue-in-cheek \"proof\" of the existence of God, in\\n\"God exists!\", Nous 21 (1987), 345-361.\\nThe basic idea is to put a suitable\\npartial ordering on the universe, and\\nthen use Zorn\\'s Lemma to prove the existence \\nof a maximal element, which is therefore God.  A\\nweb page\\non this topic has been made available by\\nAlexander Pruss.\\n\\n\\nA musical band named Axiom of Choice. \\nTheir music is a fusion of  Persian Traditional with modern.  Okay, it\\'s\\nnot math, but I couldn\\'t resist posting this here.\\nIranian born guitarist Ramin Torkian and singer Mamek Khadem\\nwere both trained as mathematicians, and\\nthat\\'s where the group gets its name.  Their first album\\'s\\nliner notes say\\n\"There is an exciting and profound artistic value in the\\nmathematical principle, Axiom of Choice. The mathematician\\nhas the right to choose elements without explanation. In a world\\nwhere everything must be explained, these choices are voluntary\\nand do not need explanation.\"\\n\\n\\n\\n\\nAll links tested\\n29 Nov 2001.  Latest alterations \\n\\n16 Nov 2003.\\n\\nMy thanks to Andreas Blass, who assisted me with part of this page.\\nThis page has been selected by Open\\nHere.   Also, this page was chosen by KaBoL\\nas the \"cool math site of the week\" for\\nNovember 9, 1999\\n(Knot number 184).\\n\\n\\n',\n",
       " ' Darwin Awards III Survival of the Fittest The Ig Nobel Prizes: The Annals of Improbable Research --> Non Campus Mentis including Florence of Arabia --> Denmark < < < © L. Allison http://www.csse.monash.edu.au/~lloyd/ (or as otherwise indicated), School of Computer Science and Software Engineering, Monash University, Australia 3168 . Created with \"vi (Linux + Solaris)\", charset=iso-8859-1 \\n\\t\\n\\n-->\\n\\n\\n Dynamic Programming Algorithm, Edit Distance\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDynamic Programming Algorithm (DPA) for Edit-Distance\\n\\n\\n\\n\\n \\n\\nLA\\xa0home\\n\\nAlgorithms\\n\\xa0glossary\\n\\xa0Dynamic\\xa0P\\'\\n\\xa0\\xa0Edit\\xa0dist\\'\\n\\xa0\\xa0Hirschberg\\'s\\n\\n\\nBioinformatics\\n\\n\\n\\nThe words `computer\\' and `commuter\\' are very similar,\\nand a change of just one letter,\\np->m will change the first word into the second.\\nThe word `sport\\' can be changed\\ninto     `sort\\'\\nby the deletion of the `p\\',\\nor equivalently, `sort\\' can be changed into `sport\\'\\nby the insertion of `p\\'.\\n\\nThe edit distance of two strings, s1 and s2,\\nis defined as the minimum number of point mutations\\nrequired to change s1 into s2,\\nwhere a point mutation is one of:\\n\\nchange a letter,\\ninsert a letter or\\ndelete a letter\\n\\n\\nThe following recurrence relations define the edit distance, d(s1,s2),\\nof two strings s1 and s2:\\n\\nd(\\'\\', \\'\\') = 0               -- \\'\\' = empty string\\nd(s, \\'\\')  = d(\\'\\', s) = |s|  -- i.e. length of s\\nd(s1+ch1, s2+ch2)\\n  = min( d(s1, s2) + if ch1=ch2 then 0 else 1 fi,\\n         d(s1+ch1, s2) + 1,\\n         d(s1, s2+ch2) + 1 )\\n\\nThe first two rules above are obviously true,\\nso it is only necessary consider the last one.\\nHere, neither string is the empty string,\\nso each has a last character, ch1 and ch2 respectively.\\nSomehow, ch1 and ch2 have to be explained in an edit of\\ns1+ch1 into s2+ch2.\\nIf ch1 equals ch2, they can be matched for no penalty, i.e. 0,\\nand the overall edit distance is d(s1,s2).\\nIf ch1 differs from ch2, then ch1 could be changed into ch2, i.e. 1,\\ngiving an overall cost d(s1,s2)+1.\\nAnother possibility is to delete ch1 and edit s1 into s2+ch2,\\nd(s1,s2+ch2)+1.\\nThe last possibility is to edit s1+ch1 into s2 and then insert ch2,\\nd(s1+ch1,s2)+1.\\nThere are no other alternatives.\\nWe take the least expensive, i.e. min, of these alternatives.\\n\\nThe recurrence relations imply an obvious ternary-recursive routine.\\nThis is not a good idea because it is exponentially slow,\\nand impractical for strings of more than a very few characters.\\n\\nExamination of the relations reveals\\nthat d(s1,s2) depends only on d(s1\\',s2\\') where\\ns1\\' is shorter than s1, or s2\\' is shorter than s2, or both.\\nThis allows the dynamic programming technique to be used.\\n\\nA two-dimensional matrix, m[0..|s1|,0..|s2|] is used to hold\\nthe edit distance values:\\n\\nm[i,j] = d(s1[1..i], s2[1..j])\\n\\nm[0,0] = 0\\nm[i,0] = i,  i=1..|s1|\\nm[0,j] = j,  j=1..|s2|\\n\\nm[i,j] = min(m[i-1,j-1]\\n             + if s1[i]=s2[j] then 0 else 1 fi,\\n             m[i-1, j] + 1,\\n             m[i, j-1] + 1 ),  i=1..|s1|, j=1..|s2|\\n\\nm[,] can be computed row by row.\\nRow m[i,] depends only on row m[i-1,].\\nThe time complexity of this algorithm is O(|s1|*|s2|).\\nIf s1 and s2 have a `similar\\' length, about `n\\' say,\\nthis complexity is O(n2), much better than exponential!\\n\\n\\nYOU NEED A BROWSER WITH NETSC@PE\\'S\\nJAVASCRIPT ON!\\n\\n\\n©L.Allison\\nTry `go\\', change the strings and experiment:\\n\\n\\n\\n\\n\\n \\n\\n\\nComplexity\\nThe time-complexity of the algorithm is O(|s1|*|s2|),\\ni.e. O(n2) if the lengths of both strings is about `n\\'.\\nThe space-complexity is also O(n2)\\nif the whole of the matrix is kept for a trace-back\\nto find an optimal alignment.\\nIf only the value of the edit distance is needed,\\nonly two rows of the matrix need be allocated;\\nthey can be \"recycled\",\\nand the space complexity is then O(|s1|), i.e. O(n).\\n\\nVariations\\nThe costs of the point mutations can be varied\\nto be numbers other than 0 or 1.\\nLinear gap-costs are sometimes used\\nwhere a run of insertions (or deletions) of length `x\\',\\nhas a cost of `ax+b\\', for constants `a\\' and `b\\'.\\nIf b>0, this penalises numerous short runs of insertions and deletions.\\n\\nLongest Common Subsequence\\nThe longest common subsequence (LCS)\\nof two sequences, s1 and s2, is a subsequence of both s1 and of s2\\nof maximum possible length.\\nThe more alike that s1 and s2 are, the longer is their LCS.\\n\\nOther Algorithms\\nThere are faster algorithms for the edit distance problem,\\nand for similar problems.\\nSome of these algorithms are fast if certain conditions hold,\\ne.g. the strings are similar, or dissimilar, or the alphabet is large, etc..\\n\\nUkkonen (1983) gave an algorithm with worst case time complexity O(n*d),\\nand the average complexity is O(n+d2),\\nwhere n is the length of the strings, and d is their edit distance.\\nThis is fast for similar strings where d is small, i.e. when d<<n.\\n\\nApplications\\nFile Revision\\nThe Unix command\\ndiff f1 f2 finds the difference between\\nfiles f1 and f2, producing an edit script to convert f1 into f2.\\nIf two (or more) computers share copies of a large file F,\\nand someone on machine-1 edits\\nF=F.bak, making a few changes, to give F.new,\\nit might be very expensive and/or slow to transmit the whole\\nrevised file F.new to machine-2.\\nHowever, diff F.bak F.new\\nwill give a small edit script which can be transmitted quickly\\nto machine-2 where the local copy of the file can be updated to equal\\nF.new.\\n\\ndiff treats a whole line as a \"character\"\\nand uses a special edit-distance algorithm that is fast\\nwhen the \"alphabet\" is large and there are few chance matches\\nbetween elements of the two strings (files).\\nIn contrast, there are many chance character-matches in DNA where the\\nalphabet size is just 4, {A,C,G,T}.\\n\\nTry `man diff\\' to see the manual entry for diff.\\n\\nRemote Screen Update Problem\\nIf a computer program on machine-1 is\\nbeing used by someone from a screen on (distant) machine-2,\\ne.g.\\xa0via rlogin etc.,\\nthen machine-1 may need to update the screen on machine-2 as\\nthe computation proceeds.\\nOne approach is for the program (on machine-1) to keep\\na \"picture\" of what the screen currently is (on machine-2)\\nand another picture of what it should become.\\nThe differences can be found (by an algorithm related to edit-distance)\\nand the differences transmitted...\\nsaving on transmission band-width.\\n\\nSpelling Correction\\nAlgorithms related to the edit distance may be used in spelling correctors.\\nIf a text contains a word, w, that is not in the dictionary,\\na `close\\' word, i.e. one with a small edit distance to w,\\nmay be suggested as a correction.\\n\\nTransposition errors are common in written text.\\nA transposition can be treated as a deletion plus an insertion,\\nbut a simple variation on the algorithm can treat\\na transposition as a single point mutation.\\n\\nPlagiarism Detection\\nThe edit distance provides an indication of similarity\\nthat might be too close in some situations ...\\nthink about it.\\n\\nMolecular Biology\\n\\n\\nExample\\nAn example of a DNA sequence from `Genebank\\'\\ncan be found\\n[here].\\nThe simple edit distance algorithm would normally be run\\non sequences of at most a few thousand bases.\\n\\n\\nThe edit distance gives an indication of how `close\\' two strings are.\\nSimilar measures are used to compute a distance between\\nDNA sequences (strings over {A,C,G,T}, or\\nprotein sequences (over an alphabet of 20 amino acids),\\nfor various purposes, e.g.:\\n\\nto find genes or proteins that may have shared functions or properties\\nto infer family relationships and evolutionary trees over\\n  different organisms\\n\\n\\nSpeech Recognition\\nAlgorithms similar to those for the edit-distance problem\\nare used in some speech recognition systems:\\nfind a close match between a new utterance and one in\\na library of classified utterances.\\n\\nNotes\\n\\nV. I. Levenshtein. Binary codes capable of correcting deletions,\\ninsertions and reversals.\\nDoklady Akademii Nauk SSSR 163(4) p845-848, 1965,\\nalso\\nSoviet Physics Doklady 10(8) p707-710, Feb 1966.\\nDiscovered the basic DPA for edit distance.\\n\\nS. B. Needleman and C. D. Wunsch.\\nA general method applicable to the search for similarities in the\\namino acid sequence of two proteins.\\nJrnl Molec. Biol. 48 p443-453, 1970.\\nDefined a similarity score\\non molecular-biology sequences, with an O(n2) algorithm\\nthat is closely related to those discussed here.\\n\\nHirschberg (1975) presented a method of recovering an alignment (of an LCS)\\nin O(n2) time but in only linear, O(n)-space; see\\n[here].\\n\\nE. Ukkonen\\nOn approximate string matching.\\nProc. Int. Conf. on Foundations of Comp. Theory,\\nSpringer-Verlag, LNCS 158 p487-495, 1983.\\nWorst case O(nd)-time, average case O(n+d2)-time\\nalgorithm for edit-distance, where d is the edit-distance between\\nthe two strings.\\n\\nSee also exact, as opposed to approximate, (sub-)string\\n[matching].\\n\\nMore research information on \"the\" DPA and Bioinformatics\\n[here].\\n\\nIf your programming language does not support 2-dimensional arrays,\\nand requires arrays or strings to indexed from zero upwards,\\nsome home-grown address translation will be needed to program\\nthe DPA defined above.\\n\\n\\nExercises\\n\\nGive a DPA for the longest common subsequence problem (LCS).\\n\\nModify the edit distance DPA to that it treats a transposition\\nas a single point-mutation.\\n\\n\\n\\n© L. A., Department of Computer Science, UWA 1984,\\nand (HTML)\\nSchool of Computer Sci. & SWE, Monash University 1999\\n\\n \\n\\n\\n\\n\\n\\nInteresting:\\n\\n\\nLinux\\nfree op\\' sys\\'\\nOpenOffice\\nfree office suite\\nver1.1 now available\\nThe GIMP\\nfree photoshop\\n\\n\\n\\n\\n\\nDarwin Awards\\xa0IIISurvival of the Fittest\\n\\n\\n\\n\\n\\n\\nDenmark\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n©\\n    L. Allison \\xa0\\n    http://www.csse.monash.edu.au/~lloyd/ \\xa0\\n    (or as otherwise indicated),\\n    School of Computer Science and Software Engineering,\\n    Monash University,\\nAustralia 3168.\\n\\n\\n    Created with \"vi (Linux + Solaris)\", \\xa0  charset=iso-8859-1\\n    \\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '\\n\\n\\nFeature Combination\\n\\n\\nFeature Combination\\n\\nFor completeness, we should note that an alternative to selecting a subset\\nof features is to combine the features to generate a smaller but more effective\\nfeature set.* The classic procedure is to form linear\\ncombinations, so that if x is the original d-dimensional\\nfeature vector and W is an d-by-m matrix, then the new m-dimensional feature\\nvector y is given by \\ny = W\\' x .\\nThe problem is to find the matrix W.\\n\\nA frequently proposed \"solution\" to this problem is to use principal\\ncomponents analysis, or PCA. Here one forms the\\ncovariance matrix C for the example feature vectors, and finds the eigenvalues\\nand eigenvectors of C. The m eigenvectors having the largest eigenvalues\\nare then used as the columns of W. **\\n\\nPCA can be shown to be optimal in a least-squares sense for representing\\nthe example feature vectors. Unfortunately, it usually does not provide\\nthe best linear combination of features for discriminating between\\nthe different classes. A more appropriate alternative is to use multiple\\ndiscriminant analysis or MDA (see Duda\\nand Hart). A full exposition of MDA is beyond the scope of these notes.\\nHowever, the two-class case is simple. In this case, it turns out that W\\nis the d-by-1 vector w given by \\nw = C-1 (m1\\n- m2) ,\\nwhere m1 is the mean vector for Class 1 and\\nm2 is the mean vector for Class 2. The resulting\\nsingle feature y = w\\' x is often called\\nFisher\\'s linear discriminant. Unfortunately, MDA does not\\nprovide additional features if this feature turns out to be inadequate.\\n\\n__________\\n* In essence, this is what the use of a \"hidden layer\" accomplishes\\nin feedforward neural networks. Feature combination is also sometimes called\\nfeature extraction, the idea being that the combination\\ndraws together or \"extracts\" the meaningful features from the\\nprimitive features.\\n\\n** A more modern approach is to perform a singular-value decomposition\\nof the matrix of example feature vectors. However, this alternative suffers\\nfrom the same shortcomings as classical PCA.\\n\\n Back to Stepwise\\nUp to Feature Selection\\n\\n\\n',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\nGlossary\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\xa0Glossary\\nHome \\n          | Site Map |\\n          Search \\n\\n\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\nA-Z index\\n\\n\\nTo jump to a particular section of \\n          the glossary, \\n          click on a letter below.\\n\\n\\nA B\\nC D E F G H I J K L M \\n\\n\\nN O P-Q R S T U V W X-Y Z\\n\\n\\n\\n\\nFinding Terms\\n\\n\\nUse the \\n          \"Find (on this page)...\" command in your browser to locate specific terms \\n          in this glossary.\\n\\n\\n\\n\\nRelated Links\\n\\n\\n\\nThe \\n          Unicode Standard, Version 4.0\\n\\n\\nFAQ for more \\n          general information\\n\\n\\nTechnical \\n          Reports for specific topics in depth\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGlossary of Unicode Terms\\nThis glossary is originally based on \\nThe Unicode Standard, Version \\n                4.0,\\xa0and will be updated over time.\\xa0It also \\n                has been slightly modified for the Web.\\xa0In particular, \\n                some glyph examples used in definitions in the book are not \\n                included here.\\xa0All references to sections, tables, etc., \\n                refer to the relevant location in the book. For the  \\n                glossary as published in The Unicode Standard, Version 4.0, \\n            see the\\n            4.0 \\n            Glossary in the online edition of the standard.\\nA\\nAbjad. A consonant writing system. The main letters \\n            are all consonants (or long vowels) with other vowels either left \\n            out entirely or indicated with secondary marking of the consonants. \\n            The best-known example is the Arabic writing system, and the term \\n            â\\x80\\x9cabjadâ\\x80\\x9d is derived from the first four letters of the traditional \\n            order of the Arabic script. \\nAbstract Character. A unit of information used for the organization, \\n            control, or representation of textual data. (See definition D3 in \\n            Section 3.4, Characters and Encoding.)\\n\\nAbstract Character Sequence. An ordered sequence of abstract \\n            characters. (See definition D4 in\\n            Section 3.4, Characters and \\n            Encoding.)\\n\\nAbugida. A special type of writing system encompassing the many \\n            scripts of South and Southeast Asia that are historically derived \\n            from the ancient Brahmi script. The term abugida is derived from the \\n            North Semitic alphabetic order: alef, bet, gimel, \\n            dalet.\\nAccent Mark. A \\n            mark placed above, below, or to the side of a character to alter its \\n            phonetic value. (See also diacritic.)\\n\\nAlgorithm. A term used in a broad sense in the Unicode Standard, to \\n            mean the logical description of a process used to achieve a \\n            specified result. This does not require the actual procedure \\n            described in the algorithm to be followed; any implementation is \\n            conformant as long as the results are the same.\\nAlphabet. A writing system that consists of letters for the writing \\n            of both consonants and vowels. Consonants and vowels have equal \\n            status as letters in an alphabet. The Latin alphabet is the most \\n            widespread and well-known example of an alphabet. The correspondence \\n            between letters and sounds may be either more or less exact; most \\n            alphabets do not exhibit a one-to-one correspondence between \\n            distinct sounds (phonemes) and distinct letters (graphemes).\\nAlphabetic Property. Informative property of the primary units of \\n            alphabets and/or syllabaries. (See \\n            Section 4.9, Letters, Alphabetic, \\n            and Ideographic.)\\nAlphabetic Sorting. (See\\n            collation.)\\nAnnotation. The association of secondary textual content with a \\n            point or range of the primary text. (The value of a particular \\n            annotation is considered to be a part of the â\\x80\\x9ccontentâ\\x80\\x9d of the text. \\n            Typical examples include glossing, citations, exemplification, \\n            Japanese yomi, and so on.)\\nANSI. (1) The American National Standards Institute. (2) The \\n            Microsoft collective name for all Windows code pages. Sometimes used \\n            specifically for code page 1252, which is a superset of ISO/IEC \\n            8859-1.\\nArabic Digits.\\xa0 Forms of decimal digits used in most parts of the \\n            Arabic world (for instance, U+0660 â\\x80¡ , U+0661 Â· , U+0662 â\\x80\\x9a , U+0663 \\n            â\\x80\\x9e). Although \\nEuropean digits\\xa0 (1, 2, 3,â\\x80¦) derive historically from \\n            these forms, they are visually distinct and are coded separately. \\n            (Arabic digits are sometimes called Indic numerals; however, this \\n            nomenclature leads to confusion with the digits currently used with \\n            the scripts of India.) Arabic digits are referred to as Arabic-Indic \\n            digits in the Unicode Standard. Variant forms of Arabic digits used \\n            chiefly in Iran and Pakistan are referred to as Eastern Arabic-Indic \\n            digits. (See\\n            Section 8.2, Arabic.)\\nASCII.\\xa0 (1) The American Standard Code for Information Interchange, a \\n            7-bit coded character set for information interchange. It is the \\n            U.S. national variant of ISO/IEC 646, and is formally the U.S. \\n            standard ANSI X3.4. It was proposed by \\n            ANSI\\xa0 in 1963 and finalized in \\n            1968. (2) The set of 128 Unicode characters from U+0000 to U+007F, \\n            including control codes, as well as graphic characters. (3) ASCII \\n            has been incorrectly used to refer to various 8-bit character \\n            encodings that include ASCII characters in the first 128 positions.\\nAssigned Character. Synonym for assigned to an abstract character. \\n            This refers to graphic, format, control, and private-use characters \\n            which have been encoded in the Unicode Standard. (See \\n            Section 2.4, \\n            Code Points and Characters.)\\nAssigned Code Point.\\xa0 (See \\n            designated code point.)\\n\\nAtomic Character. A character that is not decomposable. (See \\n            decomposable character.)\\nB\\nBase Character. A character that does not graphically combine with \\n            preceding characters, and that is neither a control nor a format \\n            character. (See definition D13 in \\n            Section 3.6, Combination.)\\n\\nBasic Multilingual Plane. Plane 0, abbreviated as BMP.\\n\\nBicameral. A script that distinguishes between two cases. (See \\n            \\ncase.) Most often used in the context of European alphabets.\\nBIDI. Abbreviation of bidirectional, in reference to mixed \\n            left-to-right and right-to-left text.\\n\\nBidirectional Display. The process or result of mixing left-to-right \\n            oriented text and right-to-left oriented text in a single line. (See \\n            Unicode Standard \\n            Annex #9, â\\x80\\x9cThe Bidirectional Algorithm.â\\x80\\x9d)\\n\\nBig-endian. A computer architecture that stores multiple-byte \\n            numerical values with the most significant byte (MSB) values first.\\n\\nBinary Files. Files containing nontextual information.\\n\\nBlock. A grouping of related characters within the Unicode encoding \\n            space. A block may contain unassigned positions, which are reserved.\\n\\nBMP. Acronym for \\nBasic Multilingual Plane.\\n\\nBMP Character. A Unicode encoded character having a BMP code point. \\n            (See\\n            supplementary character.)\\n\\nBMP Code Point. A Unicode code point between U+0000 and U+FFFF. (See \\n            supplementary code point.)\\n\\nBNF. Abbreviation for Backus-Naur Form, a formal meta-syntax for \\n            describing context-free syntaxes. (For details, see \\n            Section 0.3, \\n            Notational Conventions.)\\n\\nBOM. Acronym for \\nbyte order mark. \\nBopomofo. An alphabetic script used primarily in the Republic of \\n            China (Taiwan) to write the sounds of Mandarin Chinese and some \\n            other dialects. Each symbol corresponds to either the \\n            syllable-initial or syllable-final sounds; it is therefore a subsyllabic script in its primary usage. The name is derived from \\n            the names of its first four elements. More properly known as zhuyin \\n            zimu or zhuyin fuhao in Mandarin Chinese.\\nBoustrophedon. A pattern of writing seen in some ancient manuscripts \\n            and inscriptions, where alternate lines of text are laid out in \\n            opposite directions, and where right-to-left lines generally use \\n            glyphs mirrored from their left-to-right forms. Literally, â\\x80\\x9cas the \\n            ox turns,â\\x80\\x9d referring to the plowing of a field.\\n\\nBraille. A writing system using a series of raised dots to be read \\n            with the fingers by people who are blind or whose eyesight is not \\n            sufficient for reading printed material. (See \\n            Section 14.9, \\n            Braille.)\\n\\nBraille Pattern. One of the 64 (for 6-dot Braille) or 256 (for 8-dot \\n            Braille) possible tangible dot combinations.\\nByte. (1) The minimal unit of addressable storage for a particular \\n            computer architecture. (2) An octet. Note that many early computer \\n            architectures used bytes larger than 8 bits in size, but the \\n            industry has now standardized almost uniformly on 8-bit bytes. The \\n            Unicode Standard follows the current industry practice in equating \\n            the term byte with octet and using the more familiar term \\n            byte in \\n            all contexts. (See octet.)\\n\\nByte Order Mark. The Unicode character U+FEFF \\n            ZERO WIDTH NO-BREAK SPACE when used to indicate the byte order of a text. (See\\n            Section \\n            2.11, Special Characters and Noncharacters, and \\n            Section 15.9, \\n            Specials.) \\n\\nByte Serialization. The order of a series of bytes determined by a \\n            computer architecture.\\n\\nByte-Swapped. Reversal of the order of a sequence of bytes.\\nC\\nCanonical. (1) Conforming to the general rules for encodingâ\\x80\\x94that is, \\n            not compressed, compacted, or in any other form specified by a \\n            higher protocol. (2) Characteristic of a normative mapping and form \\n            of equivalence specified in \\n            Chapter 3, Conformance.\\nCanonical Decomposition. (See definition D23 in \\n            Section 3.7, \\n            Decomposition.)\\n\\nCanonical Equivalent. Two character sequences are said to be \\n            canonical equivalents if their full canonical decompositions are \\n            identical. (See definition D24 in \\n            Section 3.7, Decomposition.)\\n\\nCantillation Mark. A mark that is used to indicate how a text is to \\n            be chanted or sung. \\n\\nCapital Letter. Synonym for uppercase letter. (See case.)\\nCase. (1) Feature of certain alphabets where the letters have two \\n            distinct forms. These variants, which may differ markedly in shape \\n            and size, are called the uppercase letter (also known as capital or \\n            majuscule) and the lowercase letter (also known as small or \\n            minuscule). (2) Normative property of characters, consisting of \\n            uppercase, lowercase, and titlecase (Lu, Ll, and Lt). (See \\n            Section \\n            4.2, Caseâ\\x80\\x94Normative.)\\n\\nCase Mapping. The association of the uppercase, lowercase, and titlecase forms of a letter. (See \\n            Section 5.18, Case Mappings.)\\nCCS. Acronym for \\ncoded character set.\\n\\nCedilla. A mark originally placed beneath the letter c in French, \\n            Portuguese, and Spanish to indicate that the letter is to be \\n            pronounced as an s, as in faÃ§ade. Obsolete Spanish diminutive of\\n            ceda, the letter z.\\nCEF. Acronym for \\ncharacter encoding form.\\nCES. Acronym for \\ncharacter encoding scheme.\\n\\nCharacter. (1) The smallest component of written language that has \\n            semantic value; refers to the abstract meaning and/or shape, rather \\n            than a specific shape (see also glyph), though in code tables some \\n            form of visual representation is essential for the readerâ\\x80\\x99s \\n            understanding. (2) Synonym for abstract character. (3) The basic \\n            unit of encoding for the Unicode character encoding. (4) The English \\n            name for the ideographic written elements of Chinese origin. (See \\nideograph (2).)\\n\\nCharacter Block. (See \\nblock.)\\n\\nCharacter Class. A set of characters sharing a particular set of \\n            properties.\\nCharacter Encoding Form. Mapping from a character set definition to \\n            the actual code units used to represent the data.\\n\\nCharacter Encoding Scheme. A character encoding form plus byte \\n            serialization. There are seven character encoding schemes in \\n            Unicode: UTF-8, UTF-16, UTF-16BE, UTF-16LE, UTF-32, UTF-32BE, and \\n            UTF-32LE.\\n\\nCharacter Properties. A set of property names and property values \\n            associated with individual characters. (See \\n            Chapter 4, Character \\n            Properties.) \\n\\nCharacter Repertoire. The collection of characters included in a \\n            character set.\\n\\nCharacter Sequence. Synonym for \\n            abstract character sequence.\\nCharacter Set. A collection of elements used to represent textual \\n            information.\\nCharset. (See \\ncoded character set.)\\nChu HÃ¡n. The name for Han characters used in Vietnam; derived from  hÃ\\xa0nzÃ¬. \\n\\nChu NÃ´m. A demotic script of Vietnam developed from components of \\n            Han characters. Its creators used methods similar to those used by \\n            the Chinese in creating Han characters. \\n\\nCJK. Abbreviation for Chinese, Japanese, and Korean. A variant, CJKV, \\n            means Chinese, Japanese, Korean, and Vietnamese.\\n\\nCoded Character \\n            Representation. A sequence of code points. Normally, \\n            this consists of a sequence of encoded characters, but it may also \\n            include noncharacters or reserved code points. (See definition D6 in \\n            Section 3.4, Characters and Encoding.)\\n\\nCoded Character Sequence. Synonym for \\n            coded character \\n            representation.\\n\\nCoded Character Set. A character set in which each character is \\n            assigned a numeric code point. Frequently abbreviated as \\ncharacter \\n            set, charset, or \\ncode set.\\n\\nCode Page. A coded character set, often referring to a coded \\n            character set used by a personal computerâ\\x80\\x94for example, PC code page \\n            437, the default coded character set used by the U.S. English \\n            version of the DOS operating system.\\n\\nCode Point. Any value in the Unicode codespace; that is, the range \\n            of integers from 0 to 10FFFF16. (See definition D4b in \\n            Section 3.4, \\n            Characters and Encoding.)\\nCode Position. Synonym for \\ncode point. Used in ISO character \\n            encoding standards.\\nCode Set. (See \\ncoded character set.)\\nCode Unit. The minimal bit combination that can represent a unit of \\n            encoded text for processing or interchange. The Unicode Standard \\n            uses 8-bit code units in the UTF-8 encoding form, 16-bit code units \\n            in the UTF-16 encoding form, and 32-bit code units in the UTF-32 \\n            encoding form. (See definition D28a in \\n            Section 3.9, Unicode Encoding \\n            Forms.) \\nCode Value. Synonym for \\ncode unit.\\nCodespace. A range of numerical values available for encoding \\n            characters.\\nCollation. The process of ordering units of textual information. \\n            Collation is usually specific to a particular language. Also known \\n            as alphabetizing or\\n            alphabetic sorting.\\n            Unicode Technical Report #10, \\n            \"Unicode Collation Algorithm,\" defines a complete, unambiguous, \\n            specified ordering for all characters in the Unicode Standard.\\nCombining Character. A \\n            character that graphically combines with a preceding base character. \\n            The combining character is said to apply to that base character. \\n            (See definition D14 in \\n            Section 3.6, Combination.) (See also \\n            nonspacing mark.)\\n\\nCombining Character \\n            Sequence. A character sequence consisting of \\n            either a base character followed by a sequence of one or more \\n            combining characters, or a sequence of one or more combining \\n            characters. (See definition D17 in \\n            Section 3.6, Combination.)\\n\\nCombining Class. A numeric value given to each combining Unicode \\n            character that determines with which other combining characters it \\n            typographically interacts. (See definition D46 in \\n            Section 3.11, \\n            Canonical Ordering Behavior.)\\n\\nCompatibility. (1) Consistency with existing practice or preexisting \\n            character encoding standards. (2) Characteristic of a normative \\n            mapping and form of equivalence specified in \\n            Section 3.7, \\n            Decomposition.\\n\\nCompatibility Character. A character that would not have been \\n            encoded except for compatibility and round-trip convertibility with \\n            other standards. (See \\n            Section 2.3, Compatibility Characters.)\\nCompatibility Composite Character. Synonym for \\n            compatibility \\n            decomposable character.\\nCompatibility Decomposable Character. A character whose \\n            compatibility decomposition is not identical to its canonical \\n            decomposition. (See definition D21 in \\n            Section 3.7, Decomposition.)\\n\\nCompatibility \\n            Decomposition. (See definition D20 in \\n            Section 3.7, \\n            Decomposition.)\\nCompatibility Equivalent. Two character sequences are said to be \\n            compatibility equivalents if their full compatibility decompositions \\n            are identical. (See definition D22 in \\n            Section 3.7, Decomposition.)\\nCompatibility Precomposed Character. Synonym for \\n            compatibility \\n            decomposable character.\\n\\nCompatibility Variant. A character that generally can be remapped to \\n            another character without loss of information other than formatting.\\n\\nComposite Character. (See \\n            decomposable character.) \\nComposite Character \\n            Sequence. (See \\n            combining character sequence.)\\n\\nConformance. Adherence to a specified set of criteria for use of a \\n            standard. (See \\n            Chapter 3, Conformance.)\\n\\nConjunct Form. A ligated form representing a \\n            consonant conjunct.\\n\\nConsonant Cluster. A sequence of two or more consonantal sounds. \\n            Depending on the writing system, a consonant cluster may be \\n            represented by a single character or by a sequence of characters. \\n            (Contrast digraph.)\\n\\nConsonant Conjunct. A sequence of two or more adjacent consonantal \\n            letterforms, consisting of a sequence of one or more dead consonants \\n            followed by a normal, live consonant letter. A consonant conjunct \\n            may be ligated into a single conjunct form, or it may be represented \\n            by graphically separable parts, such as subscripted forms of the \\n            consonant letters. Consonant conjuncts are associated with the \\n            Brahmi family of Indic scripts. (See \\n            Section 9.1, Devanagari.)\\n\\nContextual Variant. A text element can have a presentation form that \\n            depends on the textual context in which it is rendered. This \\n            presentation form is known as a contextual variant.\\n\\nControl Codes. The 65 characters in the ranges U+0000..U+001F and \\n            U+007F..U+009F. Also known as control characters.\\n\\nCursive. Writing where the letters of a word are connected.\\nD\\nDBCS. Acronym for \\ndouble-byte character set.\\n\\nDead Consonant. An Indic consonant character followed by a virama character. This sequence indicates that the consonant has lost its \\n            inherent vowel. (See \\n            Section 9.1, Devanagari.)\\n\\nDecimal Digits. Digits that can be used to form decimal-radix \\n            numbers.\\n\\nDecomposable Character. A character that is equivalent to a sequence \\n            of one or more other characters, according to the decomposition \\n            mappings found in the names list of \\n            Section 16.1, Character Names \\n            List, and those described in \\n            Section 3.12, Conjoining Jamo Behavior. \\n            It may also be known as a precomposed character or a composite \\n            character. (See definition D18 in \\n            Section 3.7, Decomposition.)\\n\\nDecomposition. (1) The process of separating or analyzing a text \\n            element into component units. These component units may not have any \\n            functional status, but may be simply formal unitsâ\\x80\\x94that is, abstract \\n            shapes. (2) A sequence of one or more characters that is equivalent \\n            to a decomposable character. (See definition D19 in \\n            Section 3.7, \\n            Decomposition.)\\n\\nDefective \\n            Combining Character Sequence. A \\n            combining character sequence that does not start with a base character. (See definition \\n            D17a in \\n            Section 3.6, Combination.)\\n\\nDemotic Script. (1) A script or a form of a script used to write the \\n            vernacular or common speech of some language community. (2) A \\n            simplified form of the ancient Egyptian hieratic writing.\\n\\nDependent Vowel. A symbol or sign that represents a vowel and that \\n            is attached or combined with another symbol, usually one that \\n            represents a consonant. For example, in writing systems based on \\n            Arabic, Hebrew, and Indic scripts, vowels are normally represented \\n            as dependent vowel signs.\\n\\nDeprecated. A coded character whose use is strongly discouraged. \\n            Such characters are retained in the standard, but should not be \\n            used. (See definition D7a in \\n            Section 3.4, Characters and Encoding.) \\n            (Not the same as \\n            obsolete.)\\nDesignated Code Point. Any code point that has either been assigned \\n            to an abstract character (assigned characters) or that has otherwise \\n            been given a normative function by the standard (surrogate code \\n            points and noncharacters). This definition excludes reserved code \\n            points. Also known as assigned code point. (See \\n            Section 2.4, Code \\n            Points and Characters.)\\n\\nDiacritic. (1) A mark applied or attached to a symbol to create a \\n            new symbol that represents a modified or new value. (2) A mark \\n            applied to a symbol irrespective of whether it changes the value of \\n            that symbol. In the latter case, the diacritic usually represents an \\n            independent value (for example, an accent, tone, or some other \\n            linguistic information). Also called diacritical mark or \\n            diacritical.  (See also \\ncombining character and \\n            nonspacing mark.)\\nDiaeresis. Two horizontal dots over a letter, as in \\n            naÃ¯ve. The diaeresis is not distinguished from the umlaut in the Unicode \\n            character encoding. (See  umlaut.)\\n\\nDigits. (See \\nArabic digits, \\nEuropean digits, and \\nIndic digits.)\\n\\nDigraph.  A pair of signs or \\n            symbols (two graphs), which together represent a single sound or a \\n            single linguistic unit. The English writing system employs many \\n            digraphs (for example, th, ch, sh, qu, \\n            and so on). The same two symbols may not always be interpreted as a \\n            digraph (for example, cathode versus cathouse). \\n            When three signs are so combined, they are called a trigraph. More \\n            than three are usually called an n-graph.\\n\\nDingbats. Typographical symbols and ornaments.\\n\\nDiphthong. A pair of vowels that are considered a single vowel for \\n            the purpose of phonemic distinction. One of the two vowels is more \\n            prominent than the other. In writing systems, diphthongs are \\n            sometimes written with one symbol, and sometimes with more than one \\n            symbol (for example, with a \\n            digraph). \\nDirection. (See \\nparagraph direction.)\\n\\nDirectionality Property. A property of every graphic character that \\n            determines its horizontal ordering as specified in \\n            Unicode Standard \\n            Annex #9, â\\x80\\x9cThe Bidirectional Algorithm.â\\x80\\x9d (See\\n            Section 4.4, \\n            Directionalityâ\\x80\\x94Normative.)\\nDisplay Cell. A rectangular region on a display device within which \\n            one or more glyphs are imaged.\\n\\nDisplay Order. The order of glyphs presented in text rendering.\\n\\nDouble-Byte Character Set. One of a number of character sets defined \\n            for representing Chinese, Japanese, or Korean text (for example, JIS \\n            X 0208-1990). These character sets are often encoded in such a way \\n            as to allow double-byte character encodings to be mixed with \\n            single-byte character encodings. Abbreviated DBCS. (See also\\n            multibyte character set.)\\n\\nDuctility. The ability of a cursive font to stretch or compress the \\n            connective baseline to effect text justification.\\n\\nDynamic Composition. Creation of composite forms such as accented \\n            letters or \\n            Hangul syllables from a sequence of characters.\\nE\\nEBCDIC. Acronym for Extended Binary-Coded Decimal Interchange Code. \\n            A group of coded character sets used on mainframes that consist of \\n            8-bit coded characters. EBCDIC coded character sets reserve the \\n            first 64 code positions (x00 to x3F) for control codes, and reserve \\n            the range x41 to xFE for graphic characters. The English alphabetic \\n            characters are in discontinuous segments with uppercase at xC1 to \\n            xC9, xD1 to xD9, xE2 to xE9, and lowercase at x81 to x89, x91 to \\n            x99, xA2 to xA9.\\nEmbedding. A concept relevant to bidirectional behavior. (See \\n            Unicode Standard \\n            Annex #9, â\\x80\\x9cThe Bidirectional Algorithm,â\\x80\\x9d for \\n            detailed terminology and definitions.)\\n\\nEncapsulated Text. (1) Plain text surrounded by formatting \\n            information. (2) Text recoded to pass through narrow transmission \\n            channels or to match communication protocols.\\n\\nEncoded Character. An \\nabstract character together with its \\n            associated Unicode scalar value (code point). By itself, an abstract \\n            character has no numerical value, but the process of â\\x80\\x9cencoding a \\n            characterâ\\x80\\x9d associates a particular Unicode scalar value with a \\n            particular abstract character, thereby resulting in an â\\x80\\x9cencoded \\n            character.â\\x80\\x9d\\n\\nEncoding Form. (See \\ncharacter encoding form.)\\n\\nEncoding Scheme. (See \\ncharacter encoding scheme.)\\n\\nEquivalence. In the context of text processing, the process or \\n            result of establishing whether two text elements are identical in \\n            some respect.\\n\\nEquivalent Sequence. (See canonical equivalent.)\\n\\nEscape Sequence. A sequence of bytes that is used for code \\n            extension. The first byte in the sequence is escape (hex 1B).\\n\\nEuropean Digits. Forms of decimal digits first used in Europe and \\n            now used worldwide. Historically, these digits were derived from the \\n            Arabic digits; they are sometimes called â\\x80\\x9cArabic numerals,â\\x80\\x9d but this \\n            nomenclature leads to confusion with the real Arabic \\n            digits.\\n\\n\\nF\\n\\nFancy Text. (See \\nrich text.)\\n\\nFloating (diacritic, accent, mark). (See \\nnonspacing mark.)\\n\\nFont. A collection of glyphs used for the visual depiction of \\n            character data. A font is often associated with a set of parameters \\n            (for example, size, posture, weight, and serifness), which, when set \\n            to particular values, generate a collection of imagable glyphs. \\n\\nFormatted Text. (See \\nrich text.)\\n\\nFormatting Codes. Characters that are inherently invisible but that \\n            have an effect on the surrounding characters. \\n\\nFSS-UTF. Acronym for File System Safe\\n              UCS \\n            Transformation Format, \\n            published by the X/Open Company Ltd., and intended for the UNIX \\n            environment. Now known as UTF-8.\\n\\nFullwidth. Characters of East Asian character sets whose glyph image \\n            extends across the entire character display cell. In legacy \\n            character sets, fullwidth characters are normally encoded in two or \\n            three bytes. The Japanese term for fullwidth characters is zenkaku.\\nG\\nGCGID. Acronym for Graphic Character Global Identifier. These are \\n            listed in the IBM document Character Data Representation \\n            Architecture, Level 1, Registry SC09-1391.\\n\\nGeneral Category. Partition of the characters into major classes \\n            such as letters, punctuation, and symbols, and further subclasses \\n            for each of the major classes. (See \\n            Section 4.5, General \\n            Categoryâ\\x80\\x94Normative.)\\nGenerative. Synonym for \\nproductive.\\n\\nGlyph. (1) An abstract form that represents one or more glyph \\n            images. (2) A synonym for glyph image. In displaying Unicode \\n            character data, one or more glyphs may be selected to depict a \\n            particular character. These glyphs are selected by a rendering \\n            engine during composition and layout processing. (See also \\n            character.)\\n\\nGlyph Code. A numeric code that refers to a glyph. Usually, the \\n            glyphs contained in a font are referenced by their glyph code. Glyph \\n            codes may be local to a particular font; that is, a different font \\n            containing the same glyphs may use different codes. \\n\\nGlyph Identifier. Similar to a glyph code, a glyph identifier is a \\n            label used to refer to a glyph within a font. A font may employ both \\n            local and global glyph identifiers. \\n\\nGlyph Image. The actual, concrete image of a glyph representation \\n            having been rasterized or otherwise imaged onto some display \\n            surface. \\n\\nGlyph Metrics. A collection of properties that specify the relative \\n            size and positioning along with other features of a glyph.\\n\\nGrapheme. (1) A minimally \\n            distinctive unit of writing in the context of a particular writing \\n            system. For example, â\\x80¹bâ\\x80º and â\\x80¹dâ\\x80º are distinct graphemes in English \\n            writing systems because there exist distinct words like big and dig. \\n            Conversely, a lowercase italiform letter a and a lowercase \\n            Roman letter a are not \\n            distinct graphemes because no word is distinguished on the basis of \\n            these two different forms. (2) What a user thinks of as a character.\\nGrapheme Cluster. A particular text element defined in \\n            Unicode \\n            Standard Annex #29, â\\x80\\x9cText Boundaries,â\\x80\\x9d consisting of any of the \\n            following: an atomic character, a combining character sequence \\n            consisting of a base character plus one or more nonspacing marks or \\n            enclosing marks, or a sequence of Hangul jamos equivalent to a \\n            Hangul syllable.\\n\\nGraphic Character. (1) A character typically associated with a \\n            visible display representation. (See also glyph.) (2) Any character \\n            that is not primarily associated with a control or formatting \\n            function.\\n\\nGuillemet. Punctuation marks resembling small less-than and \\n            greater-than signs, used as quotation marks in French and other \\n            languages. (See â\\x80\\x9cLanguage-Based Usage of Quotation Marksâ\\x80\\x9d in \\n            Section \\n            6.2, General Punctuation.)\\nH\\nHalant. A preferred Hindi synonym for a \\n            virama. It literally means \\n            killer, referring to its function of killing the inherent vowel of a \\n            consonant letter. (See \\nvirama.)\\n\\nHalf-Consonant Form. In the Devanagari script, and certain other \\n            scripts of the Brahmi family of Indic scripts, a dead consonant may \\n            be depicted in the so-called half-form. This form is composed of the \\n            distinctive part of a consonant letter symbol without its vertical \\n            stem. It may be used to create conjunct forms that follow a \\n            horizontal layout pattern. Also known as half-form.\\n\\nHalfwidth. Characters of East Asian character sets whose glyph image \\n            occupies half of the character display cell. In legacy character \\n            sets, halfwidth characters are normally encoded in a single byte. \\n            The Japanese term for halfwidth characters is hankaku.\\nHan Characters. Ideographic characters of Chinese origin. (See \\n            Section 11.1, Han.)\\n\\nHangul. The name of the script used to write the Korean language.\\nHangul Syllable. (1) Any of the 11,172 encoded characters of the \\n            Hangul Syllables character block, U+AC00..U+D7A3. Also called a precomposed Hangul syllable to clearly distinguish it from a Korean \\n            syllable block. (2) Loosely speaking, a \\nKorean syllable block.\\n\\nHanja. The Korean name for Han characters; derived from the Chinese \\n            word  hÃ\\xa0nzÃ¬. \\n\\nHankaku. (See \\nhalfwidth.)\\n\\nHan Unification. The process of identifying Han characters that are \\n            in common among the writing systems of Chinese, Japanese, Korean, \\n            and Vietnamese.\\n\\nHÃ\\xa0nzÃ¬. The Mandarin Chinese name for Han characters. \\n\\nHarakat. Marks that indicate vowels or other modifications of \\n            consonant letters in Arabic script.\\nHasant. The Bangla name for halant. (See \\n            virama.)\\n\\nHigher-Level Protocol. Any agreement on the interpretation of \\n            Unicode characters that extends beyond the scope of this standard. \\n            Note that such an agreement need not be formally announced in data; \\n            it may be implicit in the context. (See definition D8 in \\n            Section \\n            3.4, Characters and Encoding.)\\nHigh-Surrogate Code Point. A Unicode code point in the range U+D800 \\n            to U+DBFF. (See definition D25 in \\n            Section 3.8, Surrogates.)\\nHigh-Surrogate Code Unit. A 16-bit code unit in the range D80016 to \\n            DBFF16, used in UTF-16 as the leading code unit of a surrogate pair. \\n            Also known as a leading surrogate. (See definition D25a in \\n            Section \\n            3.8, Surrogates.)\\n\\nHiragana. One of two standard syllabaries associated with the \\n            Japanese writing system. Hiragana syllables are typically used in \\n            the representation of native Japanese words and grammatical \\n            particles.\\n\\nHTML. HyperText Markup Language. A text description language related \\n            to SGML; it mixes text format markup with plain text content to \\n            describe formatted text. HTML is ubiquitous as the source language \\n            for Web pages on the Internet. Starting with HTML 4.0, the Unicode \\n            Standard functions as the reference character set for HTML content. \\n            (See also \\nSGML.)\\nI\\nIANA. Internet Assigned Numbers Authority.\\n\\nIdeograph. (1) Any symbol that \\n            primarily denotes an idea (or meaning) in contrast to a sound (or \\n            pronunciation)â\\x80\\x94for example, a symbol showing a telephone. (2) An English term commonly used to refer to Han \\n            characters, \\n            equivalent to the borrowings hÃ\\xa0nzÃ¬, kanji, and hanja.\\n\\nIdeographic Property. Informative property of characters that are \\n            ideographs. (See \\n            Section 4.9, Letters, Alphabetic, and Ideographic.)\\nIll-Formed Code Unit Sequence. A code unit sequence that does not \\n            follow the specification of a Unicode encoding form. (See definition \\n            D30 in \\n            Section 3.9, Unicode Encoding Forms.)\\n\\nIn-band. An in-band channel conveys information about text by \\n            embedding that information within the text itself, with special \\n            syntax to distinguish it. In-band information is encoded in the same \\n            character set as the text, and is interspersed with and carried \\n            along with the text data. Examples are XML and HTML markup.\\n\\nIndependent Vowel. In Indic scripts, certain vowels are depicted \\n            using independent letter symbols that stand on their own. This is \\n            often true when a word starts with a vowel or a word consists of \\n            only a vowel. \\n\\nIndic Digits. Forms of decimal digits used in various Indic scripts \\n            (for example, Devanagari: U+0966, U+0967, U+0968, U+0969). \\n            Arabic digits (and, eventually,\\n            European digits) derive historically \\n            from these forms.\\n\\nInformative. Information in this standard that is not normative but \\n            that contributes to the correct use and implementation of the \\n            standard.\\n\\nInherent Vowel. In writing systems based on a script in the Brahmi \\n            family of Indic scripts, a consonant letter symbol normally has an \\n            inherent vowel, unless otherwise indicated. The phonetic value of \\n            this vowel differs among the various languages written with these \\n            writing systems. An inherent vowel is overridden either by \\n            indicating another vowel with an explicit vowel sign or by using \\n            virama to create a dead consonant. \\n\\nInner Caps. Mixed case format where an uppercase letter is in a \\n            position other than first in the wordâ\\x80\\x94for example, â\\x80\\x9cGâ\\x80\\x9d in the Name \\n            â\\x80\\x9cMcGowan.â\\x80\\x9d\\n\\nIPA. (1) The International Phonetic Alphabet. (2) The International \\n            Phonetic Association, which defines and maintains the International \\n            Phonetic Alphabet.\\n\\nIRG. Abbreviation for Ideographic Rapporteur Group, a subgroup of \\n            ISO/IEC JTC1/SC2/WG2. (See \\n            Appendix A, Han Unification History.) \\n\\nISCII. Acronym for Indian Script Code for Information Interchange. \\nJ\\nJamo. The Korean name for a single \\n            letter of the Hangul script. \\n            Jamos are used to form Hangul syllables.\\n\\nJoiner. An invisible character that affects the joining behavior of \\n            surrounding characters. (See \\n            Section 8.2, Arabic, and â\\x80\\x9cCursive \\n            Connectionâ\\x80\\x9d in\\n            Section 15.2, Layout Controls.)\\n\\nJTC1. The Joint Technical Committee 1 of the International \\n            Organization for Standardization and the International Electrotechnical Commission responsible for information technology \\n            standardization.\\n\\n\\nK\\n\\nKana. The name of a primarily syllabic script used by the Japanese \\n            writing system. It comes in two forms, hiragana and \\nkatakana. The \\n            former is used to write particles, grammatical affixes, and words \\n            that have no \\nkanji form; the latter is used primarily to write \\n            foreign words. \\n\\nKanji. The Japanese name for Han characters; derived from the \\n            Chinese word  hÃ\\xa0nzÃ¬. Also romanized as \\n            kanzi.\\n\\nKatakana. One of two standard syllabaries associated with the \\n            Japanese writing system. Katakana syllables are typically used in \\n            representation of borrowed vocabulary (other than that of Chinese \\n            origin), sound-symbolic interjections, or phonetic representation of \\n            â\\x80\\x9cdifficultâ\\x80\\x9d kanji characters in Japanese.\\n\\nKerning. (1) Changing the space between certain pairs of letters to \\n            improve the appearance of the text. (2) Process of mapping from \\n            pairs of glyphs to a positioning offset used to change the space \\n            between letters.\\nKorean Syllable Block. A sequence of one or more jamos of the form \\n            L* V* T*, where L represents a leading consonant (choseong), \\n            V \\n            represents a vowel (jungseong), and T represents a trailing \\n            consonant (jongseong), or any canonically equivalent sequence \\n            including a precomposed Hangul syllable. (See \\n            Section 3.12, \\n            Conjoining Jamo Behavior.)\\nL\\nLeading Surrogate. Synonym for \\n            high-surrogate code unit.\\n\\nLetter. (1) An element of an alphabet. In a broad sense, it includes \\n            elements of syllabaries and ideographs. (2) Informative property of \\n            characters that are used to write words.\\n\\nLigature. A glyph representing a combination of two or more \\n            characters. In the Latin script, there are only a few in modern use, \\n            such as the ligatures between â\\x80\\x9cfâ\\x80\\x9d and â\\x80\\x9ciâ\\x80\\x9d or â\\x80\\x9cfâ\\x80\\x9d and â\\x80\\x9clâ\\x80\\x9d. Other scripts make use of many ligatures, depending on the font \\n            and style.\\n\\nLittle-endian. A computer architecture that stores multiple-byte \\n            numerical values with the least significant byte (LSB) values first.\\n\\nLogical Order. The order in which text is typed on a keyboard. For \\n            the most part, logical order corresponds to phonetic order. (See \\n            Section 2.2, Unicode Design Principles.)\\n\\nLogical Store. Memory representation.\\nLogosyllabary. A writing system in which the units are used \\n            primarily to write words and/or morphemes of words, with some \\n            subsidiary usage to represent just syllabic sounds. The best example \\n            is the Han script.\\n\\nLowercase. (See \\ncase.)\\nLow-Surrogate Code Point. A Unicode code point in the range U+DC00 \\n            to U+DFFF. (See definition D26 in \\n            Section 3.8, Surrogates.)\\nLow-Surrogate Code Unit. A 16-bit code unit in the range DC0016 to \\n            DFFF16, used in UTF-16 as the trailing code unit of a surrogate \\n            pair. Also known as a trailing surrogate. (See definition D26a in \\n            Section 3.8, Surrogates.)\\n\\nLSB. Acronym for least significant byte.\\n\\nLZW. Acronym for Lempel-Ziv-Welch, a standard algorithm widely used \\n            for compression of data.\\n\\n\\nM\\nMajuscule. Synonym for uppercase. (See case.)\\n\\nMathematical Property. Informative property of characters that are \\n            used as operators in mathematical formulae.\\n\\nMatra. A dependent vowel in an Indic script. It is the name for \\n            vowel letters that follow consonant letters in logical order. A \\n            matra often has a completely different letterform from that for the \\n            same phonological vowel used as an independent letter.\\n\\nMBCS. Abbreviation for \\nmultibyte character set.\\n\\nMIME. Multipurpose Internet Mail Extensions. MIME is a standard that \\n            allows the embedding of arbitrary documents and other binary data of \\n            known types (images, sound, video, and so on) into e-mail handled by \\n            ordinary Internet electronic mail interchange protocols.\\n\\nMinuscule. Synonym for lowercase. (See case.)\\n\\nMirrored Property. The property of characters whose images are \\n            mirrored horizontally in text that is laid out from right to left \\n            (versus left to right). (See \\n            Section 4.7, Bidi Mirroredâ\\x80\\x94Normative.)\\n\\nMissing Glyph. (See \\nreplacement glyph.)\\n\\nModifier Letter. A character with the Lm General Category in the \\n            Unicode Character Database. Modifier letters look like letters or \\n            punctuation, and modify the pronunciation of other letters (similar \\n            to diacritics). (See \\n            Section 7.6, Modifier Letters.)\\n\\nMonotonic. Modern Greek written with the basic accent, the \\n            tonos.\\n\\nMSB. Acronym for most significant byte.\\n\\nMultibyte Character Set. A character set encoded with a variable \\n            number of bytes per character, often abbreviated as MBCS. Many large \\n            character sets have been defined as MBCS so as to keep strict \\n            compatibility with the ASCII subset and/or ISO/IEC 2022.\\nN\\nNekudot. Marks that indicate vowels or other modifications of \\n            consonantal letters in Hebrew.\\nNeutral Character. A character that can be written either right to \\n            left or left to right, depending on context. (See \\n            Unicode Standard \\n            Annex #9, â\\x80\\x9cThe Bidirectional Algorithm.â\\x80\\x9d)\\nNFC. (See Normalization Form C.)\\nNFD. (See Normalization Form D.)\\nNFKC. (See Normalization Form KC.)\\nNFKD. (See Normalization Form KD.)\\nNoncharacters. Unicode code points that are permanently reserved for \\n            internal use, and that should never be interchanged. Noncharacters \\n            consist of the values U+nFFFE and U+nFFFF, where n is from 0 to \\n            1016. \\nNon-joiner. An invisible character that affects the joining behavior \\n            of surrounding characters. (See \\n            Section 8.2, Arabic, and â\\x80\\x9cCursive \\n            Connectionâ\\x80\\x9d in \\n            Section 15.2, Layout Controls.)\\nNon-overridable. A characteristic of a Unicode character property \\n            that cannot be changed by a higher-level protocol.\\n\\nNonspacing Diacritic. A diacritic that is a nonspacing mark.\\n\\nNonspacing Mark. A combining character whose positioning in \\n            presentation is dependent on its base character. It generally does \\n            not consume space along the visual baseline in and of itself. (See \\n            definition D15 in \\n            Section 3.6, Combination.) (See also\\n            \\ncombining character.)\\n\\nNormalization. A process of removing alternate representations of \\n            equivalent sequences from textual data, to convert the data into a \\n            form that can be binary-compared for equivalence. In the Unicode \\n            Standard, normalization refers specifically to processing to ensure \\n            that canonical-equivalent (and/or compatibility-equivalent) strings \\n            have unique representations. For more information, see â\\x80\\x9cEquivalent \\n            Sequencesâ\\x80\\x9d in \\n            Section 2.2, Unicode Design Principles, and \\n            Unicode Standard Annex #15, \"Unicode \\n            Normalization Forms.\"\\n\\nNormalization Form. One of the four Unicode normalization forms \\n            defined in \\n            Unicode Standard Annex #15, \"Unicode \\n            Normalization Forms\"â\\x80\\x94namely, NFC, NFD, NFKC, and NFKD.\\nNormalization Form C (NFC). The normalization form that results from \\n            the canonical decomposition of a Unicode string, followed by the \\n            replacement of all decomposed sequences by primary composites where \\n            possible.\\nNormalization Form D (NFD). The normalization form that results from \\n            the canonical decomposition of a Unicode string.\\nNormalization Form KC (NFKC). The normalization form that results \\n            from the compatibility decomposition of a Unicode string, followed \\n            by the replacement of all decomposed sequences by primary composites \\n            where possible.\\nNormalization Form KD (NFKD). The normalization form that results \\n            from the compatibility decomposition of a Unicode string.\\n\\nNormative. Required for conformance with the Unicode Standard.\\n\\nNSM. Acronym for \\nnonspacing mark.\\n\\nNumeric Value Property. A property of characters used to represent \\n            numbers. (See \\n            Section 4.6, Numeric Valueâ\\x80\\x94Normative.)\\n\\n\\nO\\n\\nObsolete. Applies to a character that is no longer in current use, \\n            but that has been used historically. Whether a character is obsolete \\n            depends on context: For example, the Cyrillic letter big yus is \\n            obsolete for Russian, but is used in modern Bulgarian. (Not the same \\n            as \\n            deprecated.)\\nOctet. An ordered sequence of eight bits considered as a unit. The \\n            Unicode Standard follows current industry practice in referring to \\n            an octet as a byte. (See byte.)\\n\\nOut-of-Band An out-of-band channel conveys additional information \\n            about text in such a way that the textual content, as encoded, is \\n            completely untouched and unmodified. This is typically done by \\n            separate data structures that point into the text.\\nOverridable. A characteristic of a Unicode character property that \\n            may be changed by a higher-level protocol to create desired \\n            implementation effects.\\nP-Q\\nParagraph Direction. The default direction (left or \\n            right) of the \\n            text of a paragraph. This direction does not change the display \\n            order of characters within an Arabic or English word. However, it \\n            does change the display order of adjacent Arabic and English words, \\n            and the display order of neutral characters, such as punctuation and \\n            spaces. For more details, see \\n            Unicode Standard Annex #9, â\\x80\\x9cThe \\n            Bidirectional Algorithm,â\\x80\\x9d especially definitions BD2â\\x80\\x93BD5.\\n\\nPhoneme. A minimally distinct sound in the context of a particular \\n            spoken language. For example, in American English, /p/ and /b/ are \\n            distinct phonemes because pat and bat are distinct; however, the two \\n            different sounds of /t/ in tick and stick are not distinct in \\n            English, even though they are distinct in other languages such as \\n            Thai.\\n\\nPinyin. Standard system for the romanization of Chinese on the basis \\n            of Mandarin pronunciation.\\n\\nPivot Conversion. The use of a third character encoding to serve as \\n            an intermediate step in the conversion between two other character \\n            encodings. The Unicode Standard is widely used to support pivot \\n            conversion, as its character repertoire is a superset of most other \\n            coded character sets.\\n\\nPlain Text. Computer-encoded text that consists \\n            only of a sequence \\n            of code points from a given standard, with no other formatting or \\n            structural information. Plain text interchange is commonly used \\n            between computer systems that do not share higher-level protocols. \\n            (See also rich text.)\\n\\nPlane.  A range of 65,536 (1000016) \\n            contiguous Unicode code points, where the first code point is an \\n            integer multiple of 65,636 (1000016). Planes are numbered \\n            from 0 to 16, with the number being the first code point of the \\n            plane divided by 65,536. Thus Plane 0 is U+0000..U+FFFF, Plane 1 is \\n            U+10000..U+1FFFF, ..., and Plane 16 (1016) \\n            is U+100000..10FFFF. (Note that ISO/IEC 10646 uses \\n            hexadecimal notation for the plane numbersâ\\x80\\x94for example, Plane B instead of \\n            Plane 11). (See Basic \\n            Multilingual Plane and supplementary \\n            planes.)\\n\\nPoints. (1) The nonspacing vowels and other signs of written Hebrew. \\n            (2) A unit of measurement in typography.\\n\\nPolytonic. Ancient Greek written with several contrastive accents.\\nPrecomposed Character. (See decomposable character.)\\n\\nPresentation Form. A ligature or variant glyph that has been encoded \\n            as a character for compatibility. (See also \\ncompatibility character \\n            (1).)\\n\\nPrimary Composite. A character that has a canonical decomposition \\n            mapping in the Unicode Character Database (or is a canonical Hangul \\n            decomposition) but which is not in the Composition Exclusion Table. \\n            (See Unicode Standard Annex \\n            #15, \"Unicode Normalization Forms.\")\\n\\nPrivate Use. Refers to designated code points in the Unicode \\n            Standard or other character encoding standards whose interpretations \\n            are not specified in those standards and whose use may be determined \\n            by private agreement among cooperating users.\\nPrivate-Use Code Point. Code points in the ranges U+E000..U+F8FF, \\n            U+F0000..U+FFFFD, and U+100000..U+10FFFD. (See definition D12 in \\n            Section 3.5, Properties.) These code points are designated in the \\n            Unicode Standard for private use.\\nProductive. Said of a feature or rule that can be employed in novel \\n            combinations or circumstances, rather than being restricted to a \\n            fixed list. In the Unicode Standard, combining marks, particularly \\n            the accents, are productive. On the other hand, variation selectors \\n            are deliberately not productive. Also known as \\ngenerative.\\n\\nProperty. (See \\ncharacter properties.)\\nProvisional. A property or feature that is unapproved and tentative, \\n            and that may be incomplete or otherwise not in a usable state.\\nPulli. The Tamil name for virama. (See \\n            virama.)\\nR\\nRadical. A structural component of a Han character conventionally \\n            used for indexing. The traditional number of such radicals is 214.\\n\\nRendering. (1) The process of selecting and laying out glyphs for \\n            the purpose of depicting characters. (2) The process of making \\n            glyphs visible on a display device. \\n\\nRepertoire. (See \\ncharacter repertoire.)\\n\\nReplacement Character. Character used as a substitute for an uninterpretable character from another encoding. The Unicode \\n            Standard uses U+FFFD REPLACEMENT CHARACTER for this function.\\n\\nReplacement Glyph. A glyph used to render a character that cannot be \\n            rendered with the correct appearance in a particular font. It often \\n            is shown as an open  or black  rectangle. Also known as a missing \\n            glyph. (See \\n            Section 5.3, Unknown and Missing Characters.)\\n\\nReserved. Refers to undesignated code points, which are set aside \\n            for future standardization. (See \\n            Section 2.4, Code Points and \\n            Characters.)\\n\\nRich Text. Also known as styled text. The result of adding \\n            information to plain text. Examples of information that can be added \\n            include font data, color, formatting information, phonetic \\n            annotations, interlinear text, and so on. The Unicode Standard does \\n            not address the representation of rich text. It is expected that \\n            systems and applications will implement proprietary forms of rich \\n            text. Some public forms of rich text are available (for example, ODA, \\n            HTML, and SGML). When everything except primary content is removed \\n            from rich text, only plain text should remain.\\n\\nRow. A range of 256 contiguous Unicode code points, where the first \\n            code point is an integer multiple of 256. Two code points are in the \\n            same row if they share all but the last two hexadecimal digits. (See\\n            \\nplane.)\\n\\n\\nS\\nSAM. Acronym for Syriac abbreviation mark.\\n\\nSBCS. Acronym for single-byte character set. Any 1-byte character \\n            encoding. This term is generally used in  contrast with DBCS and/or\\n            MBCS.\\n\\nScalar Value. (See \\nUnicode scalar value.)\\n\\nScript. A collection of symbols used to represent textual \\n            information in one or more writing systems.\\n\\nSGML. Standard Generalized Markup Language. A standard framework, \\n            defined in ISO 8879, for defining particular text markup languages. \\n            The SGML framework allows for mixing structural tags that describe \\n            format with the plain text content of documents, so that fancy text \\n            can be fully described in a plain text stream of data. (See also HTML, XML, \\n            and rich text.)\\n\\nShaping Characters. Characters that assume different glyphic forms \\n            depending on the context.\\n\\nShift-JIS. A shifted encoding of the Japanese character encoding \\n            standard, JIS X 0208, widely deployed in PCs.\\nSinogram. Chinese character. (See ideograph.)\\nSJIS. Short for Shift-JIS.\\n\\nSmall Letter. Synonym for lowercase letter. (See case.)\\n\\nSorting. (See \\ncollation.)\\nSpacing Mark.  A\\n            combining character that is not a \\n            nonspacing mark. (See nonspacing mark.)\\nStandard Korean Syllable Block. A Korean syllable block with at \\n            least one L and one V. The L and/or \\n            V may be filler characters. (See \\n            Section 3.12, Conjoining Jamo Behavior.)\\n\\nStatic Form. (See \\ndecomposable character.)\\nStyled Text. (See rich text.)\\nSubtending Mark. A format character whose graphic form extends under \\n            a sequence of following charactersâ\\x80\\x94for example, U+0600 ARABIC NUMBER \\n            SIGN.\\n\\nSupplementary Character. A Unicode encoded character having a \\n            supplementary code point.\\n\\nSupplementary Code Point. A Unicode code point between U+10000 and \\n            U+10FFFF.\\n\\nSupplementary Planes. Planes 1 through 16, consisting of the \\n            supplementary code points.\\n\\nSurrogate Character. A misnomer. It would be an encoded character \\n            having a surrogate code point, which is impossible. Do not use this \\n            term.\\n\\nSurrogate Code Point. A Unicode code point in the range U+D800 \\n            through U+DFFF. Reserved for use by UTF-16, where a pair of \\n            surrogate code units (a high surrogate followed by a low surrogate) \\n            â\\x80\\x9cstand inâ\\x80\\x9d for a supplementary code point.\\n\\nSurrogate Pair. A representation for a single abstract character \\n            that consists of a sequence of two 16-bit code units, where the \\n            first value of the pair is a high-surrogate code unit, and the \\n            second is a low-surrogate code unit. (See definition D27 in \\n            Section \\n            3.8, Surrogates.)\\n\\nSyllabary. A type of writing system in which each symbol typically \\n            represents both a consonant and a vowel, or in some instances more \\n            than one consonant and a vowel. \\n\\nSyllable. (1) An element of a syllabary. (2) A basic unit of \\n            articulation that corresponds to a pulmonary pulse.\\n\\nSymmetric Swapping. The process of rendering a character with a \\n            mirrored glyph when its resolved directionality is right-to-left in \\n            a bidirectional context. (See mirrored property and \\n            Unicode Standard \\n            Annex #9, â\\x80\\x9cThe Bidirectional Algorithm.â\\x80\\x9d)\\nT\\nTagging. The association of attributes of text with a point or range \\n            of the primary text. The value of a particular tag is not generally \\n            considered to be a part of the â\\x80\\x9ccontentâ\\x80\\x9d of the text. A typical \\n            example of tagging is to mark the language or the font for a portion \\n            of text.\\nTailorable. A characteristic of an algorithm for which a \\n            higher-level protocol may specify different results than those \\n            specified in the algorithm. A tailorable algorithm without actual \\n            tailoring is also known as a default algorithm, and the results of \\n            an algorithm without tailoring are known as the default results.\\n\\nTeX. Computer language designed for use in typesettingâ\\x80\\x94in \\n            particular, for typesetting math and other technical material. \\n            (According to Knuth, TeX rhymes with the word blecchhh.)\\n\\nText Element. A minimum unit of text in relation to a particular \\n            text process, in the context of a given writing system. In general, \\n            the mapping between text elements and code points is many-to-many. \\n            (See Chapter 2, General Structure.)\\n\\nTitlecase. Uppercased initial letter followed by lowercase letters \\n            in words. A casing convention often used in titles, headers, and \\n            entries, as exemplified in this glossary.\\n\\nTone Mark. A\\n            diacritic or \\n            nonspacing mark that represents a phonemic \\n            tone. Tone languages are common in Southeast Asia and Africa. \\n            Because tones always accompany vowels (the syllabic nucleus), they \\n            are most frequently written using functionally independent marks \\n            attached to a vowel symbol. However, some writing systems such as \\n            Thai place tone marks on consonant symbols; Chinese does not use \\n            tone marks (except when it is written phonemically).\\nTonos. The basic accent in modern Greek, having the form of an acute \\n            accent.\\nTrailing Surrogate. Synonym for \\nlow-surrogate code unit.\\n\\nTranscoding. Conversion of character data between different \\n            character sets.\\n\\nTransformation Format. A mapping from a coded character sequence to \\n            a unique sequence of code units (typically bytes).\\n\\nTriangulation. (See \\npivot conversion.)\\n\\n\\nU\\n\\nUCA. Acronym for Unicode Collation Algorithm. (See \\n            Unicode Technical \\n            Standard #10, â\\x80\\x9cUnicode Collation Algorithm.â\\x80\\x9d)\\nUCD. Acronym for Unicode Character Database. (See \\n            Section 4.1, \\n            Unicode Character Database.)\\nUCS. Acronym for Universal Character Set, which is specified by \\n            International Standard ISO/IEC 10646.\\n\\nUCS-2. ISO/IEC 10646 encoding form: \\n            Universal Character Set coded in 2 octets. (See \\n            Appendix C, Relationship to ISO/IEC 10646.)\\n\\nUCS-4. ISO/IEC 10646 encoding form: Universal Character Set coded in \\n            4 octets. (See \\n            Appendix C, Relationship to ISO/IEC 10646.)\\n\\nUmlaut. Two horizontal dots over a letter, as in German KÃ¶pfe. The \\n            umlaut is not distinguished from the diaeresis in the Unicode \\n            character encoding. (See \\n            diaeresis.)\\n\\nUnassigned. Code points that either are reserved for future use or \\n            are never to be used.\\nUnassigned Character. Synonym for \\n            not assigned to an abstract \\n            character. This refers to surrogate code points, noncharacters, and \\n            reserved code points. (See \\n            Section 2.4, Code Points and Characters.)\\nUnassigned Code Point. (See \\nundesignated code point.)\\nUndesignated Code Point. Synonym for \\n            reserved code point. These code \\n            points are reserved for future assignment and have no other \\n            designated normative function in the standard. (See \\n            Section 2.4, \\n            Code Points and Characters.)\\nUnicameral. A script that has no \\n            case distinctions. Most often used \\n            in the context of European alphabets.\\n\\nUnicode Character \\n            Database. A collection of files providing \\n            normative and informative Unicode character properties and mappings. \\n            (See Chapter 4, Character Properties, and the\\n            Unicode Character Database.)\\nUnicode Encoding Form. A character encoding form that assigns each \\n            Unicode scalar value to a unique code unit sequence. The Unicode \\n            Standard defines three Unicode encoding forms: UTF-8, UTF-16, and \\n            UTF-32. (See definition D29 in \\n            Section 3.9, Unicode Encoding Forms.)\\nUnicode Encoding Scheme. A specified byte serialization for a \\n            Unicode encoding form, including the specification of the handling \\n            of a byte order mark (BOM), if allowed. (See definition D38 in \\n            Section 3.10, Unicode Encoding Schemes.)\\n\\nUnicode Scalar Value. Any Unicode \\ncode point except high-surrogate \\n            and low-surrogate code points. In other words, the ranges of \\n            integers 0 to D7FF16 and E00016 to 10FFFF16 inclusive. (See \\n            definition D28 in \\n            Section 3.9, Unicode Encoding Forms.)\\nUnicode Signature. An implicit marker to identify a file as \\n            containing Unicode text in a particular encoding form. An initial \\n            byte order mark (BOM) may be used as a Unicode signature.\\nUnicode String. A code unit sequence containing code units of a \\n            particular Unicode encoding form. (See definition D29a in \\n            Section \\n            3.9, Unicode Encoding Forms.)\\nUnicode Transformation Format. An ambiguous synonym for either \\n            Unicode encoding form or \\n            Unicode encoding scheme. The latter terms \\n            are now preferred.\\n\\nUnification. The process of identifying characters that are in \\n            common among writing systems.\\nUPA. Acronym for Uralic Phonetic Alphabet.\\n\\nUppercase. (See \\ncase.)\\n\\nURO. Acronym for Unified Repertoire and Ordering, the original set \\n            of CJK unified ideographs used in the Unicode Standard.\\nUTF. Acronym for Unicode (or UCS) \\n            Transformation Format.\\n\\nUTF-2. Obsolete name for \\nUTF-8.\\n\\nUTF-7. Unicode (or UCS) Transformation Format, 7-bit encoding form, \\n            specified by RFC-2152.\\n\\nUTF-8. (1) The UTF-8 encoding form. (2) The UTF-8 encoding scheme. \\n            (3) â\\x80\\x9cUCS Transformation Format 8,â\\x80\\x9d defined in Annex D of ISO/IEC \\n            10646:2003, technically equivalent to the definitions in the Unicode \\n            Standard.\\nUTF-8 Encoding Form. The Unicode encoding form which assigns each \\n            Unicode scalar value to an unsigned byte sequence of one to four \\n            bytes in length, as specified in Table 3-5, UTF-8 Bit Distribution. \\n            (See definition D36 in \\n            Section 3.9, Unicode Encoding Forms.)\\nUTF-8 Encoding Scheme. The Unicode encoding scheme that serializes a \\n            UTF-8 code unit sequence in exactly the same order as the code unit \\n            sequence itself. (See definition D39 in \\n            Section 3.10, Unicode \\n            Encoding Schemes.)\\n\\nUTF-16. (1) The UTF-16 encoding form. (2) The UTF-16 encoding \\n            scheme. (3) â\\x80\\x9cTransformation format for 16 planes of Group 00,â\\x80\\x9d \\n            defined in Annex C of ISO/IEC 10646:2003, technically equivalent to \\n            the definitions in the Unicode Standard.\\nUTF-16 Encoding Form. The Unicode encoding form which assigns each \\n            Unicode scalar value in the ranges U+0000..U+D7FF and U+E000..U+FFFF \\n            to a single unsigned 16-bit code unit with the same numeric value as \\n            the Unicode scalar value, and which assigns each Unicode scalar \\n            value in the range U+10000..U+10FFFF to a surrogate pair, according \\n            to Table 3-4, UTF-16 Bit Distribution. (See definition D35 in \\n            Section 3.9, Unicode Encoding Forms.)\\nUTF-16 Encoding Scheme. The UTF-16 encoding scheme that serializes a \\n            UTF-16 code unit sequence as a byte sequence in either big-endian or \\n            little-endian formats. (See definition D42 in \\n            Section 3.10, Unicode \\n            Encoding Schemes.)\\n\\nUTF-16BE. The Unicode encoding scheme that serializes a UTF-16 code \\n            unit sequence as a byte sequence in big-endian format. (See \\n            definition D40 in \\n            Section 3.10, Unicode Encoding Schemes.)\\n\\nUTF-16LE. The Unicode encoding scheme that serializes a UTF-16 code \\n            unit sequence as a byte sequence in little-endian format. (See \\n            definition D41 in \\n            Section 3.10, Unicode Encoding Schemes.)\\n\\nUTF-32. (1) The UTF-32 encoding form. (2) The UTF-32 encoding \\n            scheme.\\nUTF-32 Encoding Form. The Unicode encoding form which assigns each \\n            Unicode scalar value to a single unsigned 32-bit code unit with the \\n            same numeric value as the Unicode scalar value. (See definition D31 \\n            in Section 3.9, Unicode Encoding Forms.)\\nUTF-32 Encoding Scheme. The Unicode encoding scheme that serializes \\n            a UTF-32 code unit sequence as a byte sequence in either big-endian \\n            or little-endian formats. (See definition D45 in \\n            Section 3.10, \\n            Unicode Encoding Schemes.)\\n\\nUTF-32BE. The Unicode encoding scheme that serializes a UTF-32 code \\n            unit sequence as a byte sequence in big-endian format. (See \\n            definition D43 in \\n            Section 3.10, Unicode Encoding Schemes.)\\n\\nUTF-32LE. The Unicode encoding scheme that serializes a UTF-32 code \\n            unit sequence as a byte sequence in little-endian format. (See \\n            definition D44 in \\n            Section 3.10, Unicode Encoding Schemes.)\\n\\n\\nV\\n\\nVirama. From Sanskrit. The name of a sign used in many Indic \\n            and other Brahmi-derived scripts to suppress the inherent vowel of \\n            the consonant to which it is applied, thereby generating a dead \\n            consonant. (See \\n            Section 9.1, Devanagari.) The sign varies in shape \\n            from script to script, and may be known by other names in various \\n            languages. For example, in Hindi it is known as hal or \\nhalant, in \\n            Bangla it is called hasant, and in Tamil, pulli.\\nVisual Ambiguity. A situation arising from two characters (or \\n            sequences of characters) being rendered indistinguishably.\\n\\nVisual Order. Characters ordered as they are presented for reading. \\n            (Contrast with \\nlogical order.)\\n\\nVocalization. Marks placed above, below, or within consonants to \\n            indicate vowels or other aspects of pronunciation. A feature of \\n            Middle Eastern scripts.\\n\\nVowel Mark. In many scripts, a mark used to indicate a vowel or \\n            vowel quality.\\n            W3C. Acronym for World Wide Web Consortium.\\n\\n\\nW\\n\\nwchar_t. The ANSI C defined wide character type, usually implemented \\n            as either 16 or 32 bits. ANSI specifies that wchar_t be an integral \\n            type and that the C language source character set be mappable by \\n            simple extension (zero- or sign-extension).\\n\\nWriting Direction. The direction or orientation of writing \\n            characters within lines of text in a writing system. Three \\n            directions are common in modern writing systems: left to right, \\n            right to left, and top to bottom.\\n\\nWriting System. A set of rules for using one or more scripts to \\n            write a particular language. Examples include the American English \\n            writing system, the British English writing system, the French \\n            writing system, and the Japanese writing system.\\n\\n\\nX-Y\\n\\nXML. eXtensible Markup Language. A subset of SGML constituting a \\n            particular text markup language for interchange of structured data. \\n            The Unicode Standard is the reference character set for XML content. \\n            (See also \\nSGML and rich text.) XML is a trademark of the World Wide \\n            Web Consortium.\\n\\n\\nZ\\n\\nZenkaku. (See \\nfullwidth.)\\n\\nZero Width. Characteristic of some spaces or format control \\n            characters that do not advance text along the horizontal baseline. \\n            (See \\n            nonspacing mark.)\\n            \\xa0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " \"\\n\\n\\n\\n\\n5.2 Genetic Algorithms\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n Next: 5.2.1 Solution Representation\\nUp: 5 Methods of Last \\n Previous: 5.1.5 SA Algorithm Performance\\n  \\n5.2 Genetic Algorithms\\n\\xa0\\n\\nKirkpatrick [40] has described SA as ``an\\nexample of an evolutionary  process  modeled  accurately  by  purely\\nstochastic  means,''  but this is more literally true of another class\\nof  new  optimization  routines  known   collectively   as   Genetic\\nAlgorithms  (GAs).  These  attempt  to  simulate  the  phenomenon of\\nnatural  evolution  first  observed  by  Darwin  [12]  and  recently\\nelaborated  by  Dawkins  [14].\\n\\nIn  natural  evolution each species\\nsearches for beneficial adaptations in an ever-changing environment.\\nAs   species   evolve  these  new  attributes  are  encoded  in  the\\nchromosomes of individual members. This information does  change  by\\nrandom  mutation,  but  the  real  driving force behind evolutionary\\ndevelopment is the combination and exchange of chromosomal  material\\nduring  breeding.\\n\\nAlthough  sporadic attempts to incorporate these\\nprinciples in optimization routines have been made since  the  early\\n1960s  (see  a review in Chapter 4 of Goldberg [31]), GAs were first\\nestablished on a sound theoretical basis by Holland  [37].  The  two\\nkey  axioms  underlying  this  innovative work were that complicated\\nnonbiological structures could be described by  simple  bit  strings\\nand  that  these  structures could be improved by the application of\\nsimple  transformations  to  these   strings.\\n\\nGAs   differ   from\\ntraditional  optimization  algorithms in four important respects:\\n They work using an encoding of the control  variables,  rather  than\\nthe  variables  themselves.\\n They  search from one population of\\nsolutions to another, rather than from individual to individual. \\n They  use  only  objective function information, not derivatives.  \\n They use probabilistic, not  deterministic,  transition  rules.   \\n\\nOf course,  GAs  share  the  last  two  attributes  with  SA  and,  not\\nsurprisingly, have found applications in many  of  the  same  areas.\\n\\nThe  basic structure of a GA is shown in Figure 20. One minor change\\nfrom the standard optimization routine flow diagram is  the  use  of\\nthe   word   `population'  rather  than  `solution.'  A  more  major\\ndifference is that the usual operation of generating a new  solution\\nhas   been  replaced  by  three  separate  activities - population\\nselection, recombination  and  mutation.\\n\\n\\nFigure 20  The Basic Structure of a Genetic Algorithm.\\n    View Figure \\n\\n\\n \\n\\n 5.2.1 Solution Representation\\n 5.2.2 Population Selection\\n 5.2.3 Population Recombination\\n 5.2.4 Population Mutation\\n 5.2.5 Advanced Operators\\n 5.2.6 Population Assessment\\n 5.2.7 Control Parameters\\n 5.2.8 GA Computational Considerations\\n 5.2.9 GA Algorithm Performance\\n\\n    \\n\\n\\n Next: 5.2.1 Solution Representation\\nUp: 5 Methods of Last \\n Previous: 5.1.5 SA Algorithm Performance\\n  \\n \\n\\nverena@csep1.phy.ornl.gov\\n\\n\\n\",\n",
       " ' The September 11th Fund --> CTK Exchange Front Page Movie shortcuts Personal info Awards Reciprocal links Terms of use Privacy Policy Interactive Activities Cut The Knot! MSET99 Talk Games & Puzzles Arithmetic/Algebra Algebra --> Geometry Probability Eye Opener Analog Gadgets Inventor\\'s Paradox Did you know?... Proofs Math as Language Things Impossible My Logo Math Poll Other Math sites Guest book News sites Recommend this site Distance Between Strings It\\'s a nice feature of the latest word processor programs that they are capable of suggesting a replacement for a mistyped word. Spelling checkers know how to evaluated distance between a misspelled word and the words in its files. Words whose evaluated distance is the smallest are offered as candidates for replacement. The applet below helps you acquaint yourself with two possible distances between strings. These metric functions attempt to ascribe a numeric (actually, integer) value to the degree of dissimilarity between two strings. That the functions do indeed satisfy the metric axioms can be shown by induction . (Which is a good exercise too.) Hamming Distance The Hamming distance H is defined only for strings of the same length. For two strings s and t, H(s, t) is the number of places in which the two string differ, i.e., have different characters. Levenshtein Distance The Levenshtein (or edit ) distance is more sophisticated. It\\'s defined for strings of arbitrary length. It counts the differences between two strings, where we would count a difference not only when strings have different characters but also when one has a character whereas the other does not. The formal definition follows. For a string s, let s(i) stand for its i th character. For two characters a and b, define r(a, b) = 0 if a = b. Let r(a, b) = 1, otherwise. Assume we are given two strings s and t of length n and m, respectively. We are going to fill an (n+1)×(m+1) array d with integers such that the low right corner element d(n+1, m+1) will furnish the required values of the Levenshtein distance L(s, t). The definition of entries of d is recursive. First set d(i, 0) = i, i = 0, 1,..., n, and d(0, j) = j, j = 0, 1, ..., m. For other pairs i, j use (1) d(i, j) = min(d(i-1, j)+1, d(i, j-1)+1, d(i-1, j-1) + r(s(i), t(j))) (Type your strings in the two edit controls at the bottom of the applet. Click \"Do It!\".) You can experiment with the applet to verify the triangle inequality before trying to prove it. You may try to discover other features of functions H and L. If strings s and t have the same length then both H(s, t) and L(s, t) are defined. Must they be equal? Copyright © 1996-2004 Alexander Bogomolny Assume two strings s and t have the same length. Is it true that H(s, t) = L(s, t)? No, not necessarily. For example, as you can easily check, H(\"abc\", \"bca\") = 3, whereas L(\"abc\", \"bca\") = 2. Copyright © 1996-2004 Alexander Bogomolny Both Hamming and Levenshtein functions are metrics . The only portion of the definitions that may appear non-trivial is the triangle inequality. We show it by induction. What do functions H and L stand for? H(s, t), by definition, is the number of places at which strings s and t differ. In other words, it takes H(s, t) changes to make string t out of string s. The triangle inequality H(s, t) H(s, r) + H(r, t) just asserts that changing s to t \"via\" r could not be worse than changing s into t directly. L(s, t) has a similar interpretation, although the allowed \"changes\" are more sophisticated, e.g. one is allowed to add or remove a character. These are known as edit operations . L(s, t) is the minimum number of edit operations that transform s into t. (As a matter of fact, this assertion requires a proof of its own, see below.) However, to edit s into t one may first edit s into r and then r into t. That such a change from s to t is not necessarily optimal is exactly the subject of the triangle inequality: L(s, t) L(s, r) + L(r, t). Now, why is L(s, t) is the minimum number of edit operations that transform s into t. Let\\'s look more carefully at the definition: (1) d(i, j) = min(d(i-1, j)+1, d(i, j-1)+1, d(i-1, j-1) + r(s(i), t(j))) To estimate the effort needed to transform s(0, i) - the first i characters of s - into t(0, j) - the first j characters of t - consider the three terms in (1), which correspond to the following situations: Delete s(i) from s(0, i) and use d(i-1, j) operations to edit s(0, i-1) into t(0, j) . Use d(i, j-1) operations to edit s(0, i) into t(0, j-1) and then append t(j). Use d(i-1, j-1) operations to edit s(0, i-1) into t(0, j-1) and then replace s(i) with t(j), if necessary. If the three terms on the right in (1) are optimal, then the term on the left is also optimal, for, on the last step of editing, one can\\'t do better than making a single change or not making any change at all. Copyright © 1996-2004 Alexander Bogomolny 8504604 . --> --> Advertise --> --> Buy It Now! --> --> G o o g l e Web Search Latest on CTK Exchange Just For Fun! Magic T Posted by DoubleE 1 messages 05:07 PM, Jan-24-04 math problem no answer in sight Posted by Dmom 17 messages 01:00 PM, Jan-15-04 solving exponential equations Posted by Ray 5 messages 07:21 PM, Jan-25-04 ShadowFire\\'s Ideas Posted by ShadowFire 4 messages 09:39 PM, Jan-24-04 Math OWNZ! SO does cut the knot! ^^ Posted by ShadowFire 0 messages 10:58 PM, Jan-22-04 Ceva in Circumscribed Quadrilateral Posted by darij 2 messages 09:05 AM, Jan-25-04 \\n\\t\\n\\n-->\\n\\n\\nDistance between strings\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCTK Exchange\\n\\nFront Page\\n\\nMovie shortcuts\\n\\nPersonal info\\n\\nAwards\\n\\nReciprocal links\\n\\nTerms of use\\n\\nPrivacy Policy\\n\\n\\nInteractive Activities\\n\\n\\nCut The Knot!\\nMSET99 Talk\\n\\nGames & Puzzles\\n\\nArithmetic/Algebra\\n\\nGeometry\\n\\nProbability\\nEye Opener\\nAnalog Gadgets\\nInventor\\'s Paradox\\nDid you know?...\\n\\nProofs\\n\\nMath as Language\\n\\nThings Impossible\\n\\nMy Logo\\n\\nMath Poll\\n\\nOther Math sites\\n\\nGuest book\\n\\nNews sites\\n\\nRecommend this site\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDistance Between Strings\\nIt\\'s a nice feature of the latest word processor programs that they are capable of suggesting\\na replacement for a mistyped word. Spelling checkers know how to evaluated distance between a misspelled word and the words in its files. Words whose evaluated distance is the smallest are offered as candidates for replacement. The applet below helps you acquaint yourself with two possible distances between strings. These metric functions attempt to ascribe a numeric (actually, integer) value to the degree of dissimilarity between two strings. That the functions do indeed satisfy the metric axioms can be shown by induction. (Which is a good exercise too.)\\nHamming Distance\\nThe Hamming distance H is defined only for strings of the same length. For two strings s and t, H(s, t) is the number of places in which the two string differ, i.e., have different characters.\\nLevenshtein Distance\\nThe Levenshtein (or edit) distance is more sophisticated. It\\'s defined for strings of arbitrary length. It counts the differences between two strings, where we would count a difference not only when strings have different characters but also when one has a character whereas the other does not. The formal definition follows.\\nFor a string s, let s(i) stand for its ith character. For two characters a and b, define\\n\\xa0\\nr(a, b) = 0 if a = b. Let r(a, b) = 1, otherwise.\\n\\nAssume we are given two strings s and t of length n and m, respectively. We are going to fill an (n+1)×(m+1) array d with integers such that the low right corner element d(n+1,\\xa0m+1) will furnish the required values of the Levenshtein distance L(s, t).\\nThe definition of entries of d is recursive. First set d(i, 0) = i, i = 0, 1,..., n, and d(0, j) = j, j = 0, 1, ..., m. For other pairs i, j use\\n(1)\\nd(i, j) = min(d(i-1, j)+1, d(i, j-1)+1, d(i-1, j-1) + r(s(i), t(j)))\\n\\n\\xa0\\n\\n\\n\\n\\n\\n(Type your strings in the two edit controls at the bottom of the applet. Click \"Do It!\".)\\nYou can experiment with the applet to verify the triangle inequality before trying to prove it. You may try to discover other features of functions H and L. If strings s and t have the same length then both H(s, t) and L(s, t) are defined. Must they be equal?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCopyright © 1996-2004 Alexander Bogomolny\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\nAssume two strings s and t have the same length. Is it true that H(s, t) = L(s, t)? No, not necessarily. For example, as you can easily check, H(\"abc\", \"bca\") = 3, whereas L(\"abc\", \"bca\") = 2.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCopyright © 1996-2004 Alexander Bogomolny\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\nBoth Hamming and Levenshtein functions are metrics. The only portion of the definitions that may appear non-trivial is the triangle inequality. We show it by induction.\\nWhat do functions H and L stand for? H(s, t), by definition, is the number of places at which strings s and t differ. In other words, it takes H(s, t) changes to make string t out of string s. The triangle inequality\\n\\xa0 \\nH(s, t)  H(s, r) + H(r, t)\\n\\njust asserts that changing s to t \"via\" r could not be worse than changing s into t directly.\\nL(s, t) has a similar interpretation, although the allowed \"changes\" are more sophisticated, e.g. one is allowed to add or remove a character. These are known as edit operations. L(s, t) is the minimum number of edit operations that transform s into t. (As a matter of fact, this assertion requires a proof of its own, see below.) However, to edit s into t one may first edit s into r and then r into t. That such a change from s to t is not necessarily optimal is exactly the subject of the triangle inequality:\\n\\xa0 \\nL(s, t)  L(s, r) + L(r, t).\\n\\nNow, why is L(s, t) is the minimum number of edit operations that transform s into t. Let\\'s look more carefully at the definition:\\n(1)\\nd(i, j) = min(d(i-1, j)+1, d(i, j-1)+1, d(i-1, j-1) + r(s(i), t(j)))\\n\\nTo estimate the effort needed to transform s(0, i) - the first i characters of s - into t(0, j) - the first j characters of t - consider the three terms in (1), which correspond to the following situations:\\n\\nDelete s(i) from s(0, i) and use d(i-1, j) operations to edit s(0, i-1) into t(0, j).\\nUse d(i, j-1) operations to edit s(0, i) into t(0, j-1) and then append t(j).\\nUse d(i-1, j-1) operations to edit s(0, i-1) into t(0, j-1) and then replace s(i) with t(j), if necessary.\\n\\nIf the three terms on the right in (1) are optimal, then the term on the left is also optimal, for, on the last step of editing, one can\\'t do better than making a single change or not making any change at all.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCopyright © 1996-2004 Alexander Bogomolny\\n\\n8504604\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGoogleWeb Search\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLatest on CTK Exchange\\n\\n\\n\\nJust For Fun! Magic T\\n\\nPosted by DoubleE\\n        1 messages\\n\\t05:07\\xa0PM, Jan-24-04\\n\\n\\n\\nmath problem no answer in sight\\n\\nPosted by Dmom\\n        17 messages\\n\\t01:00\\xa0PM, Jan-15-04\\n\\n\\n\\nsolving exponential equations\\n\\nPosted by Ray\\n        5 messages\\n\\t07:21\\xa0PM, Jan-25-04\\n\\n\\n\\nShadowFire\\'s Ideas\\n\\nPosted by ShadowFire\\n        4 messages\\n\\t09:39\\xa0PM, Jan-24-04\\n\\n\\n\\nMath OWNZ! SO does cut the knot! ^^\\n\\nPosted by ShadowFire\\n        0 messages\\n\\t10:58\\xa0PM, Jan-22-04\\n\\n\\n\\nCeva in Circumscribed Quadrilateral\\n\\nPosted by darij\\n        2 messages\\n\\t09:05\\xa0AM, Jan-25-04\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\n\\n',\n",
       " '\\n\\nHow to sell soap\\n\\nSearch MAA OnlineMAA Home\\n\\n\\nHow to sell soap\\n\\n\\n\\n   Devlin\\'s\\nAngle \\nJuly/August 2000 \\nHow to sell soap\\n\\nThe studio boss looked at the fresh and eager faces \\nseated around the long conference table at Plato \\nTelevision Studios. Jabbing his long, fat cigar toward \\nthem to punch home each point, he began:\\n\\n\"You guys are the best series scriptwriters in town. \\nI\\'ve invited you here today to offer you the chance of \\na lifetime: to work on the best television series the \\nworld has ever seen. Here\\'s the outline of what I \\nhave in mind.\" \\n\\nHe glanced at the young woman sitting at the far end \\nof the table by the slide projector. \"Alice, can you give \\nus the first slide, please?\" \\n\\nAlice dimmed the lights and switched on the \\nprojector. The studio boss continued in the half light: \\n\\n\\n\"There are these two families, you see: the Points \\nand the Lines. Basically, the series is about what \\nthese two families do, and the relationship between \\nthem. One of the great things about this idea is that \\nI\\'ve set it up so the series is going to be really cheap \\nto make. As you can see on the slide, we don\\'t need \\nto hire any actors for the Points, because the Points \\nhave no parts.\" \\n\\nThe studio boss used his cigar to gesture toward the \\nwords on the screen at the first sentence, which \\nread:\\n\\n\\nPoints have no parts.\\n\\n\\n\"The idea,\" he continued, \"is to use digital special \\neffects to represent the Points. Those computer \\ngraphics guys are cheap these days - every time the \\naerospace industry downsizes or a computer \\ncompany is bought out by Microsoft, another \\nthousand of them are thrown out of work and we can \\npick them up for a song.\" \\n\\nThere was an audible sigh of relief around the room \\nas each of the eleven scriptwriters realized how \\neasily it could have been them to be forced to \\nchange careers in mid-life, but for the fortunate \\naccident that they had flunked Algebra 2 at high \\nschool and studied English Lit. instead. \\n\\n\"We\\'ll also save a ton of money on the Lines,\" the \\nstudio boss beamed, obviously pleased at his \\ningenuity. \"My idea is that a Line has no breadth. \\nThat means we don\\'t need trained actors. We can \\nuse more of those out-of-work schmucks from \\naerospace.\" All eyes turned to the second item \\nwritten on the slide, as the studio boss jabbed at it \\nwith his cigar. It read:\\n\\n\\nA Line has no breadth.\\n\\n\\n\"Now, you guys are the best, so I\\'m going to give you \\na lot of freedom on this project,\" the boss growled. All \\nyou have to do is stick to five guiding principles for \\nthe way I want the series to go. Next slide Alice.\" \\n\\nEveryone looked as the next slide appeared on the \\nscreen. \\n\\n\"I\\'ll give you each a copy of this,\" said the studio \\nboss, \"but let me summarize the five points you can \\nsee.\" \\n\\n\"Item one. You can have a Line that connects any \\ntwo Points. That should be clear enough.\" \\n\\n\"Number two. You can continue any Line as long as \\nyou want. No problem there, either. Hell, they do that \\nin lots of long running series.\" \\n\\n\"The next one might need a bit of explaining. As you \\ncan see, what is says is \\'Any Point can be the center \\nof a Circle of any size.\\' My idea is that each episode \\ncan center around a particular person in the Point \\nfamily. That episode will concentrate on the circle of \\nfriends of that person. Some weeks, it might be a \\nsmall circle, other weeks a really big one - since the \\ncircle will be made up of Points, it doesn\\'t make any \\ndifference to the budget.\" \\n\\nThere was a chorus of appreciation around the table \\nas the scriptwriters began to see the potential of the \\nstudio boss\\'s overall idea to keep the budget down. \\n\\n\\n\"Next one: \\'All right angles are the same.\\' Every \\nseries has to have an angle. You all learned that in \\nTelevision 101. You also know that some angles are \\nright for the intended audience, some are wrong. In \\nmy series, there\\'s just one right angle. All other \\nangles are wrong, and anyone who tries to introduce \\none will be off the show faster than I can say \\'soap.\\' \\nUnderstood?\" \\n\\nThe studio boss looked around, daring anyone to \\nrespond to his challenge. No one did. The thought of \\nall those unemployed former aerospace workers was \\nstill fresh in their minds, and they did not want to join \\nthem in the line for unemployment benefit. \\n\\n\"I\\'m not entirely sure I need the fifth guideline,\" said \\nthe studio boss, who was clearly enjoying showing \\noff the genius of his new idea. \"It might be \\nsuperfluous, given the other four. But I put it down \\njust to be sure. It\\'s a bit hard to follow - I\\'ll get Alice to \\nwork on the text. But what it boils down to in simple \\nterms is this: If one of you sets it up so that two of the \\nLines are not supposed to meet, then no matter who \\nelse takes over the storyline in a later episode, those \\ntwo Lines are still not going to meet. Ever! Capisce?\" \\n\\n\\nEveryone nodded. The boss leant back in his chair \\nand chewed on his cigar. \\n\\n\"That\\'s it. Any questions?\" \\n\\nThere was silence for a moment, then a young \\nwoman half way along the table raised her hand. \"I \\nthink it\\'s a great concept,\" she began. \"but I\\'ve got \\none question.\" \\n\\n\"Fire away,\" replied the studio boss. \\n\\n\"How long do you expect this to run? Thirteen \\nweeks? Fifty-two? Or are you planning on something \\nthat goes on for years, like As the World Turns or \\nGeneral Hospital?\" \\n\\nThe studio boss chuckled. \"Little lady, I don\\'t think \\nyou\\'ve quite got the message as far as my overall \\nconcept is concerned.\" He gestured toward the five \\nguiding principles on the screen. \"This idea is so \\ninsanely great, it has so much potential, it\\'s going to \\nchange the world. Believe me, once it catches on \\nand we get enough sponsors, this baby is going to \\nrun for thousands of years. Or my name\\'s not Euclid.\" \\n\\n\\nThe above is taken intact from Devlin\\'s new book\\n\\nThe Math Gene: How Mathematical Ability Evolved \\nand Why Numbers Are Like Gossip, to be published \\nby Basic Books in August. The UK edition,\\n\\nThe Maths Gene: Why Everybody Has It But Most \\nPeople Don\\'t Use It, was published in the UK in April\\nby Weidenfeld and Nicolson.\\n\\n\\nDevlin\\'s Angle is updated at the beginning of each month.\\n\\n\\nKeith Devlin\\n(\\ndevlin@stmarys-ca.edu) is Dean of Science at\\nSaint Mary\\'s College of California, in Moraga,\\nCalifornia, and a Senior Researcher at Stanford\\nUniversity.\\n\\n\\n\\n\\nCopyright ©2003 The Mathematical Association of AmericaPlease send comments, suggestions, or corrections about this page to webmaster@maa.org.\\n\\n']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path_article = \"/workspaces/workshop-on-open-web-search-tu-dresden-01/genre-classifier-snorkel/data/dataset/genre-corpus-04/articles\"\n",
    "body_list = extract_posts(dataset_path_article)\n",
    "body_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTxt(txtPath):\n",
    "    htmlDict = {0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: []}\n",
    "    with open(txtPath, \"r\") as file:\n",
    "        for index, line in enumerate(file):\n",
    "            if (index >= 9):\n",
    "                parts = line.split(\";\")\n",
    "                type = parts[0].to_int()\n",
    "                htmlFile = parts[1].to_string()\n",
    "                htmlDict[type].append(htmlFile)\n",
    "\n",
    "    return htmlDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
